[
  {
    "filename": "kappa_effect_paths.gif",
    "description": "Animation showing the path of gradient descent on a quadratic function as the condition number (kappa) increases. As the level sets become more elongated ellipses, the gradient descent path exhibits increasingly severe zig-zagging behavior, demonstrating the 'condition number tax' on convergence speed."
  },
  {
    "filename": "steepest_descent_norms.gif",
    "description": "Visual comparison of steepest descent directions under Euclidean (L2) and L1 norms. The gradient vector rotates. The L2 steepest direction always points opposite to the gradient. The L1 steepest direction 'snaps' to the coordinate axes, changing direction only when the dominant component of the gradient changes."
  },
  {
    "filename": "backtracking_armijo.gif",
    "description": "Demonstration of the backtracking line search (Armijo condition) on a 1D slice of a function. The animation shows the step size t being reduced by a factor beta until the function value f(x + t*d) falls below the linear extrapolation (tangent line adjusted by alpha), guaranteeing sufficient decrease."
  },
  {
    "filename": "newton_quadratic_model.gif",
    "description": "Visual interpretation of Newton's method in 1D. At each iteration x_k, a quadratic approximation (Taylor expansion) is constructed using the local gradient and Hessian. The next iterate x_{k+1} is the exact minimizer of this local quadratic model."
  },
  {
    "filename": "preconditioning_comparison.gif",
    "description": "Comparison of optimization paths on an ill-conditioned quadratic problem. Standard Gradient Descent (L2) zig-zags slowly. Preconditioned Gradient Descent (using diagonal of Hessian) takes a more direct path. Newton's method jumps directly to the optimum in one step (for a quadratic), demonstrating the power of affine invariance."
  },
  {
    "filename": "l1_coordinate_descent_vs_gd.gif",
    "description": "Comparison of L1-steepest descent versus standard L2 gradient descent on a quadratic. The L1-steepest descent moves exclusively along coordinate axes (like Coordinate Descent), while L2 gradient descent moves orthogonal to level sets."
  },
  {
    "filename": "damped_newton_backtracking_2d.gif",
    "description": "Animation of Damped Newton's method with backtracking line search on a non-quadratic function (sum of exponentials). It visualizes the 'Damped Phase' where step sizes t < 1 are chosen to ensure global convergence, followed by the 'Quadratic Phase' where t=1 is accepted and convergence becomes extremely fast. The Newton decrement lambda is shown decreasing."
  },
  {
    "filename": "gd_zigzag.gif",
    "description": "A close-up animation of gradient descent zig-zagging on a single ill-conditioned quadratic function."
  }
]
