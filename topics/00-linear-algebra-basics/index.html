<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. Linear Algebra Basics â€” Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-linear-algebra-advanced/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>00. Linear Algebra Basics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, review, linear-algebra, foundational</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture establishes the rigorous linear algebra foundations required for convex optimization. We begin by defining the core objectsâ€”scalars, vectors, and matricesâ€”and their fundamental operations. We then explore the geometry of high-dimensional spaces through subspaces, inner products, norms, and orthogonality. We delve into matrix calculus, a crucial tool for optimization, before examining Positive Semidefinite (PSD) matrices, which underpin convexity. Finally, we apply these tools to orthogonal projections and the method of least squares. This material provides the necessary language to describe convex sets, functions, and optimization problems with precision.</p>
        <p><strong>Prerequisites:</strong> Basic multivariable calculus and familiarity with standard matrix notation.</p>
        <p><strong>Forward Connections:</strong> The projection techniques introduced here are essential for geometric interpretation of constrained optimization. PSD matrices are the cornerstone of convex quadratic programs (QP) and Semidefinite Programming (SDP) covered in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The four fundamental subspaces provide geometric intuition for duality theory (<a href="../09-duality/index.html">Lecture 09</a>).</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Manipulate Core Objects:</b> Perform operations on vectors and matrices and understand the geometric meaning of linear, affine, and convex combinations.</li>
        <li><b>Analyze Subspaces:</b> Identify the four fundamental subspaces of a matrix and apply the Rank-Nullity Theorem.</li>
        <li><b>Use Invariants and Norms:</b> Compute trace, determinant, and eigenvalues; apply vector norms ($\ell_1, \ell_2, \ell_\infty$) and inequalities like Cauchy-Schwarz.</li>
        <li><b>Apply Matrix Calculus:</b> Compute gradients and Hessians for linear and quadratic functions.</li>
        <li><b>Characterize PSD Matrices:</b> Determine positive semidefiniteness using eigenvalues, variational forms ($\mathbf{x}^\top A\mathbf{x} \ge 0$), and the Schur Complement.</li>
        <li><b>Solve Least Squares:</b> Derive the normal equations and interpret the solution geometrically as an orthogonal projection.</li>
      </ul>
    </section>

    <!-- SECTION 1: VECTORS, MATRICES, COMBINATIONS -->
    <section class="section-card" id="section-basics">
      <h2>1. Vectors, Matrices, and Linear Combinations</h2>

      <h3>1.1 Scalars, Vectors, and Matrices</h3>
      <p>We work in the real vector space $\mathbb{R}^n$.
      <ul>
        <li><b>Scalars:</b> Real numbers $\alpha, \beta, c \in \mathbb{R}$. We denote them with italic lowercase letters.</li>
        <li><b>Vectors:</b> An ordered $n$-tuple of real numbers, denoted by bold lowercase letters (e.g., $\mathbf{x} \in \mathbb{R}^n$). By default, vectors are column vectors:
        $$ \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} $$
        </li>
        <li><b>Matrices:</b> A rectangular array of numbers, denoted by capital letters (e.g., $A \in \mathbb{R}^{m \times n}$).
        </li>
      </ul></p>

      <h3>1.2 Fundamental Operations</h3>
      <ul>
        <li><b>Transpose:</b> The transpose of $\mathbf{x}$ is a row vector $\mathbf{x}^\top = [x_1, \dots, x_n]$. For matrices, $(AB)^\top = B^\top A^\top$.</li>
        <li><b>Inner Product:</b> The standard inner product (dot product) of $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ is:
        $$ \langle \mathbf{x}, \mathbf{y} \rangle = \mathbf{x}^\top \mathbf{y} = \sum_{i=1}^n x_i y_i $$
        </li>
        <li><b>Matrix-Vector Multiplication:</b> The product $\mathbf{y} = A\mathbf{x}$ can be viewed in two powerful ways:
          <ul>
            <li><b>Row View (Analysis):</b> $y_i = \mathbf{a}_i^\top \mathbf{x}$, where $\mathbf{a}_i^\top$ is the $i$-th row of $A$. Each entry is a dot product.</li>
            <li><b>Column View (Synthesis):</b> $\mathbf{y} = \sum_{j=1}^n x_j \mathbf{a}_j$, where $\mathbf{a}_j$ is the $j$-th column. The output is a linear combination of the columns.</li>
          </ul>
        </li>
      </ul>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/matrix-multiplication.svg"
             alt="Matrix multiplication diagram"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 1:</i> Matrix-vector multiplication combines columns linearly.</figcaption>
      </figure>

      <h3>1.3 Lines and Line Segments</h3>
      <p>Geometric objects are defined by mixing vectors.</p>
      <ul>
        <li><b>Line:</b> The line passing through distinct points $\mathbf{x}_1, \mathbf{x}_2$ is the set of affine combinations:
        $$ \mathbf{y} = \theta \mathbf{x}_1 + (1-\theta)\mathbf{x}_2, \quad \theta \in \mathbb{R} $$
        </li>
        <li><b>Line Segment:</b> Restricting $\theta \in [0, 1]$ yields the segment $[\mathbf{x}_1, \mathbf{x}_2]$. This is a <b>convex combination</b>.
        $$ \mathbf{y} = \theta \mathbf{x}_1 + (1-\theta)\mathbf{x}_2, \quad 0 \le \theta \le 1 $$
        </li>
      </ul>

      <figure style="text-align: center;">
        <img src="assets/line-vs-segment.png"
             alt="Line vs Line Segment"
             style="max-width: 60%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 2:</i> The line segment (blue) is a subset of the infinite line (grey).</figcaption>
      </figure>

      <h3>1.4 Types of Combinations</h3>
      <p>Different constraints on coefficients $\theta_i$ generate different geometric structures.</p>
      <table class="data-table">
        <thead>
          <tr>
            <th>Type</th>
            <th>Constraint on $\theta_i$</th>
            <th>Geometric Object</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>Linear</b></td>
            <td>None</td>
            <td><b>Subspace</b> (Span)</td>
          </tr>
          <tr>
            <td><b>Affine</b></td>
            <td>$\sum \theta_i = 1$</td>
            <td><b>Affine Set</b> (Line, Plane)</td>
          </tr>
          <tr>
            <td><b>Convex</b></td>
            <td>$\sum \theta_i = 1, \theta_i \ge 0$</td>
            <td><b>Convex Set</b> (Polygon, Hull)</td>
          </tr>
          <tr>
            <td><b>Conic</b></td>
            <td>$\theta_i \ge 0$</td>
            <td><b>Cone</b> (Ray, Sector)</td>
          </tr>
        </tbody>
      </table>
      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Combinations</h3>
        <div id="widget-convex-combination" style="width: 100%; height: 300px; position: relative;"></div>
        <script type="module">
            import { initConvexCombination } from '../02-introduction/widgets/js/convex-combination.js';
            initConvexCombination('widget-convex-combination');
        </script>
        <p><i>Note: Full interactive convex hull explorer available in Lecture 02.</i></p>
      </div>
    </section>

    <!-- SECTION 2: SUBSPACES -->
    <section class="section-card" id="section-subspaces">
      <h2>2. Subspaces and Dimension</h2>

      <h3>2.1 Linear Subspaces</h3>
      <p>A <a href="#" class="definition-link">subspace</a> $\mathcal{S} \subseteq \mathbb{R}^n$ is a set closed under addition and scalar multiplication.
      <br>Every subspace must contain the zero vector $\mathbf{0}$. The span of any set of vectors is a subspace.</p>

      <h3>2.2 The Four Fundamental Subspaces</h3>
      <p>Given $A \in \mathbb{R}^{m \times n}$, four subspaces characterize the linear map $\mathbf{x} \mapsto A\mathbf{x}$:</p>
      <ol>
        <li><b>Column Space (Range):</b> $\mathcal{R}(A) = \{A\mathbf{x} \mid \mathbf{x} \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$. This is the set of all possible outputs of the linear map, i.e., the span of the columns of $A$.</li>
        <li><b>Nullspace (Kernel):</b> $\mathcal{N}(A) = \{\mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0}\} \subseteq \mathbb{R}^n$. This is the set of all input vectors that are mapped to the zero vector.</li>
        <li><b>Row Space:</b> $\mathcal{R}(A^\top) \subseteq \mathbb{R}^n$. This is the span of the rows of $A$.</li>
        <li><b>Left Nullspace:</b> $\mathcal{N}(A^\top) \subseteq \mathbb{R}^m$. This is the nullspace of the transpose of $A$.
        <br><i>Why do we care?</i> The Left Nullspace defines the <b>constraints on solvability</b>. The system $A\mathbf{x}=\mathbf{b}$ has a solution if and only if $\mathbf{b}$ is orthogonal to every vector in the Left Nullspace ($\mathbf{b} \perp \mathcal{N}(A^\top)$). This is the <b>Fredholm Alternative</b>.</li>
      </ol>

      <div class="proof-box">
        <h4>Proof: $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$</h4>

        <div class="proof-step">
          <strong>Setup:</strong> Let $\mathbf{v} \in \mathcal{R}(A)$ and $\mathbf{y} \in \mathcal{N}(A^\top)$. By definition, $\mathbf{v} = A\mathbf{x}$ for some $\mathbf{x} \in \mathbb{R}^n$, and $A^\top \mathbf{y} = \mathbf{0}$.
        </div>

        <div class="proof-step">
          <strong>Compute inner product:</strong> We compute the dot product $\langle \mathbf{v}, \mathbf{y} \rangle$:
          $$ \mathbf{v}^\top \mathbf{y} = (A\mathbf{x})^\top \mathbf{y} = \mathbf{x}^\top A^\top \mathbf{y} = \mathbf{x}^\top (A^\top \mathbf{y}) $$
        </div>

        <div class="proof-step">
          <strong>Substitute Nullspace Condition:</strong> Since $\mathbf{y} \in \mathcal{N}(A^\top)$, we have $A^\top \mathbf{y} = \mathbf{0}$.
          $$ \mathbf{x}^\top (\mathbf{0}) = 0 $$
        </div>

        <div class="proof-step">
          <strong>Conclusion:</strong> The inner product is zero for any $\mathbf{v} \in \mathcal{R}(A)$ and $\mathbf{y} \in \mathcal{N}(A^\top)$. Thus, every vector in the column space is orthogonal to every vector in the left nullspace.
        </div>
      </div>

      <h3>2.3 Rank and the Rank-Nullity Theorem</h3>
      <p>The <b>rank</b> of $A$ is the dimension of its column space, $\dim \mathcal{R}(A)$. A fundamental result relates these dimensions:</p>
      <div class="theorem-box">
        <h4>Rank-Nullity Theorem</h4>
        <p>For $A \in \mathbb{R}^{m \times n}$:</p>
        $$ \dim \mathcal{R}(A) + \dim \mathcal{N}(A) = n $$
        <p><i>Interpretation:</i> The information in the input domain $\mathbb{R}^n$ is split into two parts: the part that "survives" the map (Rank) and the part that gets crushed to zero (Nullity).</p>
      </div>

      <div class="proof-box">
        <h4>Proof of the Rank-Nullity Theorem</h4>

        <div class="proof-step">
          <strong>Step 1: Construct a basis for the Nullspace.</strong> Let $k = \dim(\mathcal{N}(A))$ be the nullity. Let $\{\mathbf{v}_1, \dots, \mathbf{v}_k\}$ be a basis for the nullspace $\mathcal{N}(A)$. These are $k$ linearly independent vectors in $\mathbb{R}^n$ such that $A\mathbf{v}_i = \mathbf{0}$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Extend to a basis for $\mathbb{R}^n$.</strong> By the Basis Extension Theorem, we can always extend a linearly independent set (like the nullspace basis) to a full basis for the vector space. We add $r = n-k$ vectors $\{\mathbf{w}_1, \dots, \mathbf{w}_r\}$ such that the full set $B = \{\mathbf{v}_1, \dots, \mathbf{v}_k, \mathbf{w}_1, \dots, \mathbf{w}_r\}$ is a basis for $\mathbb{R}^n$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Analyze the image of the basis vectors.</strong> Consider the image of an arbitrary vector $\mathbf{x} \in \mathbb{R}^n$. Since $B$ is a basis, we can write $\mathbf{x}$ uniquely as:
          $$ \mathbf{x} = \sum_{i=1}^k c_i \mathbf{v}_i + \sum_{j=1}^{r} d_j \mathbf{w}_j $$
          Applying the linear map $A$:
          $$ A\mathbf{x} = \sum_{i=1}^k c_i \underbrace{A \mathbf{v}_i}_{\mathbf{0}} + \sum_{j=1}^{r} d_j A \mathbf{w}_j = \sum_{j=1}^{r} d_j A \mathbf{w}_j $$
          This equation shows that any vector in the column space ($A\mathbf{x}$) can be written as a linear combination of the vectors $\{A\mathbf{w}_1, \dots, A\mathbf{w}_r\}$. Thus, these $r$ vectors span the column space $\mathcal{R}(A)$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Prove Linear Independence in the Range.</strong> To verify that $\{A\mathbf{w}_1, \dots, A\mathbf{w}_r\}$ form a basis for $\mathcal{R}(A)$, we must show they are linearly independent. Suppose we have a linear combination summing to zero:
          $$ \sum_{j=1}^{r} \alpha_j A \mathbf{w}_j = \mathbf{0} $$
          By linearity, this implies $A \left( \sum_{j=1}^{r} \alpha_j \mathbf{w}_j \right) = \mathbf{0}$.
          Let $\mathbf{u} = \sum_{j=1}^{r} \alpha_j \mathbf{w}_j$. Since $A\mathbf{u} = \mathbf{0}$, the vector $\mathbf{u}$ must lie in the nullspace $\mathcal{N}(A)$.
        </div>

        <div class="proof-step">
          <strong>Step 5: Prove Independence via Basis Uniqueness.</strong> Since $\mathbf{u} \in \mathcal{N}(A)$, it can be written as a linear combination of the nullspace basis $\{\mathbf{v}_i\}$:
          $$ \mathbf{u} = \sum_{i=1}^k \beta_i \mathbf{v}_i $$
          Equating the two expressions for $\mathbf{u}$:
          $$ \sum_{j=1}^{r} \alpha_j \mathbf{w}_j = \sum_{i=1}^k \beta_i \mathbf{v}_i \quad \implies \quad \sum_{j=1}^{r} \alpha_j \mathbf{w}_j - \sum_{i=1}^k \beta_i \mathbf{v}_i = \mathbf{0} $$
          This equation is a linear combination of the full basis set $\{\mathbf{v}_1, \dots, \mathbf{v}_k, \mathbf{w}_1, \dots, \mathbf{w}_r\}$ summing to zero. Since these vectors form a basis for $\mathbb{R}^n$, they are linearly independent, implying all coefficients must be zero. Specifically, $\alpha_j = 0$ for all $j=1, \dots, r$.
          <br>Since the only solution to $\sum \alpha_j A \mathbf{w}_j = \mathbf{0}$ is the trivial solution $\alpha = 0$, the set $\{A\mathbf{w}_1, \dots, A\mathbf{w}_r\}$ is linearly independent.
        </div>

        <div class="proof-step">
          <strong>Step 6: Count Dimensions.</strong> Since $\{A\mathbf{w}_1, \dots, A\mathbf{w}_r\}$ is a linearly independent spanning set for $\mathcal{R}(A)$, it is a basis.
          Thus, the dimension of the column space (rank) is exactly $r$.
          From Step 2, we defined $r = n - k$. Rearranging gives the theorem:
          $$ \dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A)) = r + k = n $$
        </div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p>Visualize how column space and nullspace relate for a matrix transformation.</p>
        <div id="widget-rank-nullspace" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 3: INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>3. Algebraic Invariants</h2>
      <p>Invariants are properties of a matrix that depend only on the underlying linear operator, not the basis.</p>

      <h3>3.1 Trace and Determinant</h3>
      <p>For $A \in \mathbb{R}^{n \times n}$ with eigenvalues $\lambda_1, \dots, \lambda_n$:</p>
      <ul>
        <li><b>Trace:</b> $\mathrm{tr}(A) = \sum A_{ii} = \sum \lambda_i$.
          <br><i>Property:</i> $\mathrm{tr}(AB) = \mathrm{tr}(BA)$ (Cyclic property).</li>
        <li><b>Determinant:</b> $\det(A) = \prod \lambda_i$.
          <br><i>Property:</i> $\det(AB) = \det(A)\det(B)$. Represents volume scaling.</li>
      </ul>

      <div class="proof-box">
        <h4>Proof: $\mathrm{tr}(AB) = \mathrm{tr}(BA)$</h4>
        <div class="proof-step">
          <strong>Coordinate Definition:</strong> Let $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times m}$. The diagonal elements of the product $AB$ are:
          $$ (AB)_{ii} = \sum_{k=1}^n a_{ik} b_{ki} $$
        </div>
        <div class="proof-step">
          <strong>Summing the diagonal:</strong>
          $$ \mathrm{tr}(AB) = \sum_{i=1}^m (AB)_{ii} = \sum_{i=1}^m \sum_{k=1}^n a_{ik} b_{ki} $$
        </div>
        <div class="proof-step">
          <strong>Switching order:</strong> Since the sums are finite, we can swap the order of summation. We also commute the scalar terms $a_{ik} b_{ki} = b_{ki} a_{ik}$:
          $$ \sum_{i=1}^m \sum_{k=1}^n a_{ik} b_{ki} = \sum_{k=1}^n \sum_{i=1}^m b_{ki} a_{ik} $$
        </div>
        <div class="proof-step">
          <strong>Recognize the diagonal of BA:</strong> The inner sum $\sum_{i=1}^m b_{ki} a_{ik}$ is exactly the element $(BA)_{kk}$ (the $k$-th diagonal entry of the matrix product $BA$).
          $$ \sum_{k=1}^n \left( \sum_{i=1}^m b_{ki} a_{ik} \right) = \sum_{k=1}^n (BA)_{kk} $$
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> The sum of the diagonal entries is the trace.
          $$ \sum_{k=1}^n (BA)_{kk} = \mathrm{tr}(BA) $$
        </div>
      </div>

      <h3>3.2 Eigenvalues and Eigenvectors</h3>
      <p>An eigenvector $\mathbf{v} \neq \mathbf{0}$ satisfies $A\mathbf{v} = \lambda \mathbf{v}$.
      <br>The set of eigenvalues $\{\lambda_i\}$ is the <b>spectrum</b> of $A$.</p>

      <div class="theorem-box">
        <h4>Lemma: Scalar Matrices</h4>
        <p>If every non-zero vector $\mathbf{x} \in \mathbb{R}^n$ is an eigenvector of $A$ (i.e., $A\mathbf{x} = \lambda(\mathbf{x})\mathbf{x}$), then $A$ must be a scalar multiple of the identity: $A = \lambda I$.</p>
      </div>
      <div class="proof-box">
        <h4>Proof of the Isotropic Scaling Lemma</h4>
        <div class="proof-step">
            <strong>Step 1: Linearly Independent Vectors Share Eigenvalues.</strong>
            Let $\mathbf{u}, \mathbf{v}$ be linearly independent vectors. By assumption, they are eigenvectors: $A\mathbf{u} = \lambda_u \mathbf{u}$ and $A\mathbf{v} = \lambda_v \mathbf{v}$.
            Consider their sum $\mathbf{w} = \mathbf{u}+\mathbf{v}$. Since every vector is an eigenvector, $\mathbf{w}$ must also be one: $A(\mathbf{u}+\mathbf{v}) = \lambda_w (\mathbf{u}+\mathbf{v})$.
            By linearity:
            $$ \lambda_w \mathbf{u} + \lambda_w \mathbf{v} = \lambda_w(\mathbf{u}+\mathbf{v}) = A(\mathbf{u}+\mathbf{v}) = A\mathbf{u} + A\mathbf{v} = \lambda_u \mathbf{u} + \lambda_v \mathbf{v} $$
            Rearranging terms to group by vector:
            $$ (\lambda_w - \lambda_u)\mathbf{u} + (\lambda_w - \lambda_v)\mathbf{v} = \mathbf{0} $$
            Since $\mathbf{u}$ and $\mathbf{v}$ are linearly independent, the only solution to this linear combination is the trivial one. Thus, their coefficients must vanish:
            $$ \lambda_w - \lambda_u = 0 \quad \text{and} \quad \lambda_w - \lambda_v = 0 $$
            This implies $\lambda_u = \lambda_w = \lambda_v$. Conclusion: Any two linearly independent vectors must correspond to the same eigenvalue.
        </div>
        <div class="proof-step">
            <strong>Step 2: Linearly Dependent Vectors Share Eigenvalues.</strong>
            Suppose $\mathbf{v}$ and $\mathbf{u}$ are linearly dependent, meaning $\mathbf{v} = c\mathbf{u}$ for some scalar $c \neq 0$. We cannot use the independence argument directly on the pair $(\mathbf{u}, \mathbf{v})$.
            <br>However, assuming the dimension $n \ge 2$, the space is not just a line. We can choose a third vector $\mathbf{z}$ that is linearly independent of $\mathbf{u}$. Since $\mathbf{v}$ is a multiple of $\mathbf{u}$, $\mathbf{z}$ is also linearly independent of $\mathbf{v}$.
            <br>Applying the result from Step 1 to the pair $(\mathbf{u}, \mathbf{z})$, we get $\lambda_u = \lambda_z$.
            <br>Applying it to the pair $(\mathbf{v}, \mathbf{z})$, we get $\lambda_v = \lambda_z$.
            <br>By transitivity, $\lambda_u = \lambda_v$.
        </div>
        <div class="proof-step">
            <strong>Step 3: Conclusion.</strong>
            Since all non-zero vectors share the same eigenvalue $\lambda$, we have $A\mathbf{x} = \lambda \mathbf{x}$ for all $\mathbf{x}$. This is exactly the definition of the scalar matrix $A = \lambda I$.
        </div>
      </div>

      <div class="theorem-box">
        <h4>Spectral Mapping Theorem</h4>
        <p>If $A\mathbf{v} = \lambda \mathbf{v}$, then for any polynomial $P(t)$, $P(A)\mathbf{v} = P(\lambda)\mathbf{v}$.
        <br><i>Example:</i> If $A\mathbf{v} = \lambda \mathbf{v}$, then $(A + tI)\mathbf{v} = (\lambda + t)\mathbf{v}$ (Spectral Shift).</p>
      </div>
    </section>

    <!-- SECTION 4: INNER PRODUCTS -->
    <section class="section-card" id="section-norms">
      <h2>4. Inner Products, Norms, and Geometry</h2>

      <h3>4.1 Inner Products</h3>
      <p>An inner product $\langle \mathbf{x}, \mathbf{y} \rangle$ defines geometry (angles, projection). The standard dot product is $\mathbf{x}^\top \mathbf{y}$.
      <br><b>Weighted Inner Product:</b> For $P \succ 0$ (Positive Definite), $\langle \mathbf{x}, \mathbf{y} \rangle_P = \mathbf{x}^\top P \mathbf{y}$ is a valid inner product.</p>

      <h3>4.2 Vector Norms</h3>
      <p>A norm $\|\mathbf{x}\|$ measures length. Common norms:</p>
      <ul>
        <li><b>$\ell_2$ (Euclidean):</b> $\|\mathbf{x}\|_2 = \sqrt{\sum x_i^2}$. The natural "physical" length.</li>
        <li><b>$\ell_1$ (Manhattan):</b> $\|\mathbf{x}\|_1 = \sum |x_i|$. Promotes sparsity in optimization.</li>
        <li><b>$\ell_\infty$ (Chebyshev):</b> $\|\mathbf{x}\|_\infty = \max |x_i|$.</li>
      </ul>

      <div class="proof-box">
        <h4>Proof: Triangle Inequalities for $\ell_1$ and $\ell_\infty$</h4>
        <div class="proof-step">
          <strong>For $\ell_1$:</strong> Using scalar triangle inequality $|a+b|\le |a|+|b|$:
          $$ \|\mathbf{x}+\mathbf{y}\|_1 = \sum_i |x_i+y_i| \le \sum_i (|x_i|+|y_i|) = \sum_i |x_i| + \sum_i |y_i| = \|\mathbf{x}\|_1 + \|\mathbf{y}\|_1. $$
        </div>
        <div class="proof-step">
          <strong>For $\ell_\infty$:</strong> For any index $k$:
          $$ |x_k+y_k| \le |x_k|+|y_k| \le \|\mathbf{x}\|_\infty + \|\mathbf{y}\|_\infty $$
          Taking the maximum over $k$ on the LHS gives $\|\mathbf{x}+\mathbf{y}\|_\infty \le \|\mathbf{x}\|_\infty + \|\mathbf{y}\|_\infty$.
        </div>
      </div>

      <div class="proof-box">
        <h4>Proof: Cauchy-Schwarz Inequality ($|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$)</h4>
        <div class="proof-step">
          <strong>Step 1: Construct a quadratic.</strong> For any $t \in \mathbb{R}$, $\|\mathbf{x} - t\mathbf{y}\|_2^2 \ge 0$.
          $$ 0 \le \langle \mathbf{x} - t\mathbf{y}, \mathbf{x} - t\mathbf{y} \rangle = \|\mathbf{x}\|_2^2 - 2t\langle \mathbf{x}, \mathbf{y} \rangle + t^2 \|\mathbf{y}\|_2^2. $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Discriminant condition.</strong> This is a quadratic $At^2 + Bt + C \ge 0$ with $A=\|\mathbf{y}\|_2^2$.
          Assuming $\mathbf{y} \neq \mathbf{0}$ (otherwise trivial), we must have $B^2 - 4AC \le 0$:
          $$ (-2\langle \mathbf{x}, \mathbf{y} \rangle)^2 - 4(\|\mathbf{y}\|_2^2)(\|\mathbf{x}\|_2^2) \le 0 $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          $$ 4\langle \mathbf{x}, \mathbf{y} \rangle^2 \le 4\|\mathbf{x}\|_2^2 \|\mathbf{y}\|_2^2 \implies |\langle \mathbf{x}, \mathbf{y} \rangle| \le \|\mathbf{x}\|_2 \|\mathbf{y}\|_2. $$
          <i>Equality holds</i> iff the quadratic touches zero, i.e., $\mathbf{x} = t\mathbf{y}$ (collinear).
        </div>
      </div>

      <div class="theorem-box">
        <h4>Lemma: Conjugate of a Norm</h4>
        <p>This lemma is crucial for dual derivations. Let $\|\cdot\|$ be any norm and $\|\cdot\|_*$ its dual norm. The conjugate of the scaled norm $f(\mathbf{x}) = \rho \|\mathbf{x}\|$ is the indicator function of the dual ball:
        $$ (\rho\|\mathbf{x}\|)^*(\mathbf{z}) = I_{\{\|\mathbf{z}\|_* \le \rho\}}(\mathbf{z}) = \begin{cases} 0 & \|\mathbf{z}\|_* \le \rho \\ \infty & \text{otherwise} \end{cases} $$
        <i>Proof sketch:</i> If $\|\mathbf{z}\|_* \le \rho$, then $\mathbf{z}^\top \mathbf{x} - \rho\|\mathbf{x}\| \le \|\mathbf{z}\|_*\|\mathbf{x}\| - \rho\|\mathbf{x}\| \le 0$ (max is 0 at $\mathbf{x}=\mathbf{0}$). If $\|\mathbf{z}\|_* > \rho$, we can align $\mathbf{x}$ with $\mathbf{z}$ to make the expression go to $+\infty$.
        </p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: The Geometry of Norms</h3>
        <p>Compare the unit balls of L1, L2, and L-infinity norms.</p>
        <div id="widget-norm-geometry" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>4.3 Angles and Dual Norms</h3>
      <p><b>Angle:</b> The angle $\theta$ between vectors is defined by $\cos \theta = \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{x}\|_2 \|\mathbf{y}\|_2}$.
      <br><b>Dual Norm:</b> $\|\mathbf{y}\|_* = \sup_{\|\mathbf{x}\| \le 1} \mathbf{x}^\top \mathbf{y}$.
      <br>Pairs: $(\ell_2, \ell_2)$, $(\ell_1, \ell_\infty)$.</p>
    </section>

    <!-- SECTION 5: ORTHOGONALITY -->
    <section class="section-card" id="section-orthogonality">
      <h2>5. Orthogonality and Orthonormal Bases</h2>

      <p>Vectors $\mathbf{x}, \mathbf{y}$ are <b>orthogonal</b> if $\mathbf{x}^\top \mathbf{y} = 0$.
      <br>Subspaces $\mathcal{S}, \mathcal{T}$ are orthogonal if every vector in $\mathcal{S}$ is orthogonal to every vector in $\mathcal{T}$.</p>

      <h3>5.1 Fundamental Theorem of Linear Algebra</h3>
      <p>The row space and nullspace are orthogonal complements in $\mathbb{R}^n$:
      $$ \mathcal{N}(A) = \mathcal{R}(A^\top)^\perp $$
      This means any vector $\mathbf{x}$ can be uniquely decomposed as $\mathbf{x} = \mathbf{x}_{\text{row}} + \mathbf{x}_{\text{null}}$.</p>

      <h3>5.2 Orthonormal Bases and Matrices</h3>
      <p>A set of vectors $\{q_1, \dots, q_k\}$ is <b>orthonormal</b> if $q_i^\top q_j = \delta_{ij}$.
      <br>A square matrix $Q$ is <b>orthogonal</b> if its columns form an orthonormal basis: $Q^\top Q = I \implies Q^{-1} = Q^\top$.
      <br>Orthogonal matrices represent isometries (rotations/reflections): $\|Q\mathbf{x}\|_2 = \|\mathbf{x}\|_2$.</p>

      <div class="proof-box">
        <h4>Proof: $\|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)}$</h4>
        <div class="proof-step">
          <strong>Step 1: Squared Norm.</strong>
          $$ \|X\|_2^2 = \sup_{\mathbf{v} \neq \mathbf{0}} \frac{\|X\mathbf{v}\|_2^2}{\|\mathbf{v}\|_2^2} = \sup_{\mathbf{v} \neq \mathbf{0}} \frac{\mathbf{v}^\top X^\top X \mathbf{v}}{\mathbf{v}^\top \mathbf{v}} $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Rayleigh Quotient.</strong> The expression $\frac{\mathbf{v}^\top M \mathbf{v}}{\mathbf{v}^\top \mathbf{v}}$ for a symmetric matrix $M$ is the Rayleigh Quotient. By the Spectral Theorem, we can diagonalize $M$ in an orthonormal basis of eigenvectors. The maximum value of the quotient is achieved when $\mathbf{v}$ is the eigenvector corresponding to the largest eigenvalue. Thus, the maximum is exactly $\lambda_{\max}(M)$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          $$ \|X\|_2^2 = \lambda_{\max}(X^\top X) \implies \|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)} $$
        </div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Orthogonality & Projections</h3>
        <p>Visualize dot products and orthogonal projections.</p>
        <div id="widget-orthogonality" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 6: MATRIX CALCULUS -->
    <section class="section-card" id="section-matrix-calculus">
      <h2>6. Matrix Calculus Basics</h2>
      <p>Optimization requires computing derivatives with respect to vectors and matrices. The gradient $\nabla f(\mathbf{x})$ is the vector of partial derivatives.</p>

      <div class="proof-box">
        <h4>Detailed Derivation: Gradient of Quadratic Form</h4>
        <p>Let $f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$. We apply the perturbation method.</p>
        <div class="proof-step">
          <strong>Step 1: Perturb.</strong> Let $\mathbf{x} \to \mathbf{x} + \mathbf{h}$.
          $$ f(\mathbf{x}+\mathbf{h}) = (\mathbf{x}+\mathbf{h})^\top A (\mathbf{x}+\mathbf{h}) = \mathbf{x}^\top A \mathbf{x} + \mathbf{x}^\top A \mathbf{h} + \mathbf{h}^\top A \mathbf{x} + \mathbf{h}^\top A \mathbf{h} $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Linearize.</strong>
          Identify terms linear in $\mathbf{h}$: $L(\mathbf{h}) = \mathbf{x}^\top A \mathbf{h} + \mathbf{h}^\top A \mathbf{x}$.
          The term $\mathbf{h}^\top A \mathbf{h}$ is quadratic (second-order) and vanishes as $\mathbf{h} \to \mathbf{0}$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Rearrange to Inner Product form.</strong>
          Recall that a scalar equals its transpose: $\mathbf{h}^\top A \mathbf{x} = (\mathbf{h}^\top A \mathbf{x})^\top = \mathbf{x}^\top A^\top \mathbf{h}$.
          Thus, $L(\mathbf{h}) = \mathbf{x}^\top A \mathbf{h} + \mathbf{x}^\top A^\top \mathbf{h} = \mathbf{x}^\top (A + A^\top) \mathbf{h}$.
          Rewriting as an inner product:
          $$ L(\mathbf{h}) = \langle (A + A^\top)\mathbf{x}, \mathbf{h} \rangle $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Identify Gradient.</strong>
          By definition, the vector multiplying $\mathbf{h}$ in the inner product is the gradient.
          $$ \nabla f(\mathbf{x}) = (A + A^\top)\mathbf{x} $$
          If $A$ is symmetric, $A = A^\top$, so $\nabla f(\mathbf{x}) = 2A\mathbf{x}$.
        </div>
      </div>

      <div class="proof-box">
          <h4>Derivation: Gradient of Log Determinant</h4>
          <p>We derive the gradient of $f(X) = \log \det X$ (defined for $\det X > 0$) by identifying the linear term in the Taylor expansion of $f(X+\Delta)$ around $X$. This step-by-step derivation uses the perturbation method, which is robust and general.</p>
          <div class="proof-step">
            <strong>Step 1: Factor out X.</strong>
            We want to isolate the perturbation $\Delta$. Since $X$ is invertible, we can write $X+\Delta = X(I + X^{-1}\Delta)$.
            $$ \log \det (X + \Delta) = \log \det (X(I + X^{-1}\Delta)) $$
            Using the multiplicative property of the determinant ($\det(AB) = \det(A)\det(B)$) and the additive property of the logarithm ($\log(ab) = \log a + \log b$):
            $$ = \log (\det X \cdot \det(I + X^{-1}\Delta)) = \log \det X + \log \det(I + X^{-1}\Delta) $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Relate Determinant to Trace via Eigenvalues.</strong>
            Let $E = X^{-1}\Delta$. We need to approximate $\log \det(I + E)$ for small $E$.
            Let $\lambda_i$ be the eigenvalues of $E$. The eigenvalues of $I+E$ are then $1+\lambda_i$.
            $$ \det(I + E) = \prod_{i=1}^n (1 + \lambda_i) $$
            Taking the natural logarithm turns the product into a sum:
            $$ \log \det(I + E) = \sum_{i=1}^n \log(1 + \lambda_i) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Linearize via Taylor Series.</strong>
            We use the scalar Taylor series expansion $\log(1+z) = z - \frac{z^2}{2} + O(z^3)$ for small $z$.
            Applying this to each term $\log(1+\lambda_i)$:
            $$ \sum_{i=1}^n \log(1 + \lambda_i) = \sum_{i=1}^n \left( \lambda_i - \frac{\lambda_i^2}{2} + \dots \right) = \sum_{i=1}^n \lambda_i - \frac{1}{2}\sum_{i=1}^n \lambda_i^2 + \dots $$
            The linear term is the sum of eigenvalues, which is the <b>trace</b> of the matrix $E$:
            $$ \sum_{i=1}^n \lambda_i = \mathrm{tr}(E) = \mathrm{tr}(X^{-1}\Delta) $$
            Thus, to first order, $\log \det(I + E) \approx \mathrm{tr}(E)$.
          </div>
          <div class="proof-step">
            <strong>Step 4: Extract the Gradient.</strong>
            Substituting back, the first-order approximation of the function is:
            $$ f(X+\Delta) \approx f(X) + \mathrm{tr}(X^{-1}\Delta) $$
            The gradient $\nabla f(X)$ is defined as the matrix $G$ such that the linear term is $\langle G, \Delta \rangle = \mathrm{tr}(G^\top \Delta)$.
            We rewrite our linear term to match this form:
            $$ \mathrm{tr}(X^{-1}\Delta) = \mathrm{tr}((X^{-\top})^\top \Delta) = \langle X^{-\top}, \Delta \rangle $$
            Comparing terms, we identify $G = X^{-\top}$.
            <br><b>Result:</b> $\nabla_X \log \det X = X^{-\top}$.
            <br>If $X$ is symmetric ($X=X^\top$), then $\nabla f(X) = X^{-1}$.
          </div>
        </div>

      <h3>6.1 Gradients of Standard Forms</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Function $f(\mathbf{x})$</th>
            <th>Gradient $\nabla f(\mathbf{x})$</th>
            <th>Hessian $\nabla^2 f(\mathbf{x})$</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Linear: $\mathbf{a}^\top \mathbf{x}$</td>
            <td>$\mathbf{a}$</td>
            <td>$0$</td>
          </tr>
          <tr>
            <td>Quadratic: $\mathbf{x}^\top A \mathbf{x}$</td>
            <td>$(A + A^\top)\mathbf{x}$</td>
            <td>$A + A^\top$</td>
          </tr>
          <tr>
            <td>Least Squares: $\|\mathbf{x}\|_2^2$</td>
            <td>$2\mathbf{x}$</td>
            <td>$2I$</td>
          </tr>
        </tbody>
      </table>

      <h3>6.2 Gradients with respect to Matrices</h3>
      <p>For $f: \mathbb{R}^{m \times n} \to \mathbb{R}$, the gradient $\nabla_X f(X)$ is the matrix of partials.
      <br><b>Trace Rule:</b> $\nabla_X \mathrm{tr}(A^\top X) = A$.
      <br><b>Log-Det Rule:</b> $\nabla_X \log \det X = X^{-\top}$ (for invertible $X$).</p>
    </section>

    <!-- SECTION 7: PSD MATRICES -->
    <section class="section-card" id="section-psd">
      <h2>7. Positive Semidefinite Matrices</h2>

      <h3>7.1 Definition and Intuition</h3>
      <p>A symmetric matrix $A \in \mathbb{S}^n$ is <b>Positive Semidefinite (PSD)</b>, denoted $A \succeq 0$, if:
      $$ \mathbf{x}^\top A \mathbf{x} \ge 0 \quad \text{for all } \mathbf{x} \in \mathbb{R}^n $$
      <b>Positive Definite (PD)</b>, $A \succ 0$, means strictly positive for $\mathbf{x} \neq \mathbf{0}$.
      <br><i>Intuition:</i> PSD matrices generalize non-negative numbers. They define convex quadratic bowls.</p>

      <div class="proof-box">
        <h4>Proof: Equivalence of Definitions</h4>
        <p>We prove that $(\mathbf{x}^\top A \mathbf{x} \ge 0) \iff (\lambda_i \ge 0)$.</p>

        <div class="proof-step">
          <strong>Step 1: $(\Rightarrow)$</strong> Assume $\mathbf{x}^\top A \mathbf{x} \ge 0$ for all vectors $\mathbf{x}$. Let $\mathbf{v}$ be an eigenvector of $A$ with eigenvalue $\lambda$.
          $$ A \mathbf{v} = \lambda \mathbf{v} $$
          Substitute $\mathbf{x} = \mathbf{v}$ into the variational definition:
          $$ \mathbf{v}^\top A \mathbf{v} = \mathbf{v}^\top (\lambda \mathbf{v}) = \lambda (\mathbf{v}^\top \mathbf{v}) = \lambda \|\mathbf{v}\|_2^2 $$
          Since $\mathbf{x}^\top A \mathbf{x} \ge 0$, we have $\lambda \|\mathbf{v}\|_2^2 \ge 0$. Since eigenvectors are non-zero, $\|\mathbf{v}\|_2^2 > 0$. Therefore, <b>$\lambda \ge 0$</b>.
        </div>

        <div class="proof-step">
          <strong>Step 2: $(\Leftarrow)$</strong> Assume all $\lambda_i \ge 0$. Use the Spectral Decomposition $A = Q \Lambda Q^\top$. For any arbitrary vector $\mathbf{x}$:
          $$ \mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top (Q \Lambda Q^\top) \mathbf{x} = (Q^\top \mathbf{x})^\top \Lambda (Q^\top \mathbf{x}) $$
          Let $\mathbf{y} = Q^\top \mathbf{x}$. Then:
          $$ \mathbf{x}^\top A \mathbf{x} = \mathbf{y}^\top \Lambda \mathbf{y} = \sum_{i=1}^n \lambda_i y_i^2 $$
          Since $\lambda_i \ge 0$ and $y_i^2 \ge 0$, the sum is non-negative. Thus <b>$\mathbf{x}^\top A \mathbf{x} \ge 0$</b>.
        </div>
      </div>

      <h3>7.2 Eigenvalue Characterization</h3>
      <p>By the Spectral Theorem, $A = Q \Lambda Q^\top$.
      $$ \mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top Q \Lambda Q^\top \mathbf{x} = \mathbf{y}^\top \Lambda \mathbf{y} = \sum \lambda_i y_i^2 $$
      Thus, $A \succeq 0 \iff \lambda_i \ge 0$ for all $i$.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Hessian Landscape</h3>
        <p>See how eigenvalues determine the shape of $f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$.</p>
        <div id="widget-hessian-landscape" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>7.3 The Schur Complement</h3>
      <p>For block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ with $A \succ 0$,
      $$ M \succeq 0 \iff C - B^\top A^{-1} B \succeq 0 $$
      This tool converts nonlinear constraints into linear matrix inequalities (LMIs).</p>

      <div class="proof-box">
        <h4>1. Block Matrix Setup and Dimensions</h4>
        <p>Consider a block matrix $M$ partitioned as follows:</p>
        $$ M = \begin{bmatrix} A & B \\ C & D \end{bmatrix} $$
        <p>where $A \in \mathbb{R}^{p \times p}$, $B \in \mathbb{R}^{p \times q}$, $C \in \mathbb{R}^{q \times p}$, and $D \in \mathbb{R}^{q \times q}$. Assume the bottom-right block $D$ is invertible.</p>
        <p>The <b>Schur complement</b> of $D$ in $M$ is the matrix defined as:</p>
        $$ S := A - B D^{-1} C $$

        <h4>2. Constructing the Elimination Matrix</h4>
        <p>To understand the origin of $S$, we apply <b>Block Gaussian Elimination</b>. The goal is to zero out the $C$ block (bottom-left) to obtain a block upper-triangular matrix, from which properties like determinant and invertibility are easily read.</p>
        <p>We achieve this by post-multiplying $M$ by an elimination matrix $E$:</p>
        $$ E = \begin{bmatrix} I_p & 0 \\ -D^{-1}C & I_q \end{bmatrix} $$
        <p>This matrix $E$ is block lower-triangular with identity matrices on the diagonal, so $\det(E) = 1$. It acts as a "column operation": it subtracts $D^{-1}C$ times the second column-block from the first column-block.</p>

        <p>Multiplying $M$ by $E$ yields the factorization:</p>
        $$ ME = \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} I_p & 0 \\ -D^{-1}C & I_q \end{bmatrix} = \begin{bmatrix} A - BD^{-1}C & B \\ 0 & D \end{bmatrix} = \begin{bmatrix} S & B \\ 0 & D \end{bmatrix} $$

        <div class="proof-box">
          <h4>Line-by-Line Verification of Block Multiplication</h4>
          <p>We verify this multiplication explicitly using the standard block multiplication rule $\begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} X & Y \\ Z & W \end{bmatrix} = \begin{bmatrix} AX+BZ & AY+BW \\ CX+DZ & CY+DW \end{bmatrix}$.</p>
          <p>Here we set $X=I_p, Y=0, Z=-D^{-1}C, W=I_q$.</p>
          <ul>
            <li><b>Top-left (The Pivot):</b> $A(I_p) + B(-D^{-1}C) = A - BD^{-1}C = S$. This term is exactly the "pivot" remaining after elimination.</li>
            <li><b>Top-right:</b> $A(0) + B(I_q) = B$. (Unchanged).</li>
            <li><b>Bottom-left (The Zero):</b> $C(I_p) + D(-D^{-1}C) = C - C = 0$. (This was the specific design goal of the elimination step).</li>
            <li><b>Bottom-right:</b> $C(0) + D(I_q) = D$. (Unchanged).</li>
          </ul>
        </div>

        <div class="insight">
          <h4>ðŸ’¡ Choice of Elimination Matrix</h4>
          <p>The goal is to eliminate the bottom-left block $C$, analogous to Gaussian elimination. We require the $(2,1)$ block of the product to be zero:
          $$ C \cdot I_p + D \cdot Z = 0 \implies D Z = -C \implies Z = -D^{-1}C $$
          This determines the entry $-D^{-1}C$ in the elimination matrix $E$.</p>
        </div>

        <p>Since $ME$ is block upper-triangular, its determinant is the product of the determinants of its diagonal blocks: $\det(ME) = \det(S)\det(D)$. Because $\det(E) = 1$, we derive the <b>Determinant Identity</b>:</p>
        $$ \det(M) = \det(D) \det(A - B D^{-1} C) $$

        <div class="example">
          <h4>Numerical Example (Sanity Check)</h4>
          <p>Let $A=[2], B=[1], C=[3], D=[4]$. Then $M = \begin{bmatrix} 2 & 1 \\ 3 & 4 \end{bmatrix}$.</p>
          <ul>
            <li><b>Direct Determinant:</b> $\det(M) = 2(4) - 1(3) = 8 - 3 = 5$.</li>
            <li><b>Schur Complement:</b> $S = A - B D^{-1} C = 2 - 1(\frac{1}{4})3 = 2 - 0.75 = 1.25$.</li>
            <li><b>Formula Check:</b> $\det(D)\det(S) = 4(1.25) = 5$.</li>
          </ul>
          <p>It matches perfectly.</p>
        </div>

        <h4>3. Symmetric Variant: Schur Complement of $A$</h4>
        <p>Similarly, if $A$ is invertible, we can eliminate $C$ using the Schur complement of $A$, defined as $D - C A^{-1} B$. The determinant identity becomes:</p>
        $$ \det(M) = \det(A) \det(D - C A^{-1} B) $$

        <h4>4. Schur Complement and Positive Semidefiniteness</h4>
        <p>This is the crucial property for convex optimization. Let $M$ be a symmetric matrix ($C = B^\top$):</p>
        $$ M = \begin{bmatrix} A & B \\ B^\top & D \end{bmatrix} $$
        <p><b>Theorem:</b> If $D \succ 0$, then $M \succeq 0$ if and only if the Schur complement $S = A - B D^{-1} B^\top \succeq 0$.</p>

        <div class="proof-step">
          <strong>Proof ($\Rightarrow$):</strong> Assume $M \succeq 0$.
          We need to show that the Schur complement is PSD, i.e., $\mathbf{x}^\top S \mathbf{x} \ge 0$ for any $\mathbf{x}$.
          Since $M \succeq 0$, we know that $\mathbf{z}^\top M \mathbf{z} \ge 0$ for <i>any</i> vector $\mathbf{z}$. The trick is to choose a specific test vector $\mathbf{z}$ that isolates $S$.
          <br>Let $\mathbf{z} = \begin{bmatrix} \mathbf{x} \\ \mathbf{y} \end{bmatrix}$. The quadratic form is:
          $$ \mathbf{z}^\top M \mathbf{z} = \mathbf{x}^\top A \mathbf{x} + 2 \mathbf{x}^\top B \mathbf{y} + \mathbf{y}^\top D \mathbf{y} $$
          We want to choose $\mathbf{y}$ to minimize this expression for a fixed $\mathbf{x}$, effectively "concentrating" the condition onto $\mathbf{x}$. Setting the gradient with respect to $\mathbf{y}$ to zero:
          $$ \nabla_y (\mathbf{z}^\top M \mathbf{z}) = 2B^\top \mathbf{x} + 2D\mathbf{y} = 0 \implies \mathbf{y} = -D^{-1}B^\top \mathbf{x} $$
          Now, we substitute this optimal $\mathbf{y}$ back into the quadratic form:
          $$ \mathbf{z}^\top M \mathbf{z} = \mathbf{x}^\top A \mathbf{x} + 2 \mathbf{x}^\top B (-D^{-1}B^\top \mathbf{x}) + (-D^{-1}B^\top \mathbf{x})^\top D (-D^{-1}B^\top \mathbf{x}) $$
          $$ = \mathbf{x}^\top A \mathbf{x} - 2 \mathbf{x}^\top B D^{-1} B^\top \mathbf{x} + \mathbf{x}^\top B D^{-1} D D^{-1} B^\top \mathbf{x} $$
          $$ = \mathbf{x}^\top A \mathbf{x} - 2 \mathbf{x}^\top B D^{-1} B^\top \mathbf{x} + \mathbf{x}^\top B D^{-1} B^\top \mathbf{x} $$
          $$ = \mathbf{x}^\top (A - B D^{-1} B^\top) \mathbf{x} = \mathbf{x}^\top S \mathbf{x} $$
          Since $M \succeq 0$, we must have $\mathbf{z}^\top M \mathbf{z} \ge 0$. Therefore, $\mathbf{x}^\top S \mathbf{x} \ge 0$ for all $\mathbf{x}$, which means $S \succeq 0$.
        </div>

        <div class="proof-step">
          <strong>Proof ($\Leftarrow$):</strong> Assume $S \succeq 0$ and $D \succ 0$.
          Take any $\mathbf{z} = [\mathbf{x}^\top, \mathbf{y}^\top]^\top$. We can factor $M$ using the elimination matrix from the proof of the lemma:
          $$ \mathbf{z}^\top M \mathbf{z} = \mathbf{x}^\top S \mathbf{x} + (\mathbf{y} + D^{-1}B^\top \mathbf{x})^\top D (\mathbf{y} + D^{-1}B^\top \mathbf{x}) $$
          Since $S \succeq 0$, the first term is non-negative. Since $D \succ 0$, the second term is non-negative. Thus $\mathbf{z}^\top M \mathbf{z} \ge 0$ for all $\mathbf{z}$.
        </div>
      <div class="example">
        <h4>Example: Determinant of Symmetric Block Matrix</h4>
        <p>Consider the matrix $M = \begin{bmatrix} Y & \mathbf{x} \\ \mathbf{x}^\top & t \end{bmatrix}$ where $Y \succ 0$.
        Using the general determinant formula with $A=Y, B=\mathbf{x}, C=\mathbf{x}^\top, D=t$:
        $$ \det M = \det Y \cdot \det(t - \mathbf{x}^\top Y^{-1} \mathbf{x}) = (\det Y)(t - \mathbf{x}^\top Y^{-1} \mathbf{x}) $$
        This scalar formula is frequently used to check if $M \succ 0$: we need $\det Y > 0$ (given) and the Schur complement $t - \mathbf{x}^\top Y^{-1} \mathbf{x} > 0$.</p>
      </div>

      </div>

        <div class="insight">
          <h4>The Triangle of Equivalence</h4>
          <p>For a positive definite matrix $Y \succ 0$, the following three conditions are equivalent. They represent three different "faces" of the same object:</p>
          $$
          \boxed{
          \begin{matrix}
          \textbf{Scalar Inequality} & & \textbf{Block Matrix LMI} \\
          t \ge \mathbf{x}^\top Y^{-1} \mathbf{x} & \iff & \begin{bmatrix} Y & \mathbf{x} \\ \mathbf{x}^\top & t \end{bmatrix} \succeq 0 \\
          & \Updownarrow & \\
          & \textbf{Rank-1 Update} & \\
          & tY - \mathbf{x}\mathbf{x}^\top \succeq 0 &
          \end{matrix}
          }
          $$
          <ul>
            <li><b>Scalar:</b> Useful for epigraphs of quadratic-over-linear functions.</li>
            <li><b>Block Matrix:</b> Useful for Semidefinite Programming (SDP) constraints.</li>
            <li><b>Rank-1 Update:</b> Useful for algebraic manipulation ($tY \succeq \mathbf{x}\mathbf{x}^\top$).</li>
          </ul>
          <div class="proof-box">
            <h4>Detailed Derivation: From Rank-1 to Scalar</h4>
            <p>We want to show $tY - \mathbf{x}\mathbf{x}^\top \succeq 0 \iff t \ge \mathbf{x}^\top Y^{-1} \mathbf{x}$.</p>
            <div class="proof-step">
                <strong>Step 1: Variational Definition.</strong>
                $tY - \mathbf{x}\mathbf{x}^\top \succeq 0$ means for all $\mathbf{v} \in \mathbb{R}^n$:
                $$ \mathbf{v}^\top (tY - \mathbf{x}\mathbf{x}^\top) \mathbf{v} \ge 0 \implies t (\mathbf{v}^\top Y \mathbf{v}) - (\mathbf{v}^\top \mathbf{x})(\mathbf{x}^\top \mathbf{v}) \ge 0 \implies (\mathbf{x}^\top \mathbf{v})^2 \le t (\mathbf{v}^\top Y \mathbf{v}) $$
            </div>
            <div class="proof-step">
                <strong>Step 2: Change of Variables.</strong>
                Since $Y \succ 0$, it has a square root $Y^{1/2}$. Let $\mathbf{u} = Y^{1/2} \mathbf{v}$. Then $\mathbf{v} = Y^{-1/2} \mathbf{u}$.
                The term $\mathbf{v}^\top Y \mathbf{v} = \mathbf{u}^\top Y^{-1/2} Y Y^{-1/2} \mathbf{u} = \mathbf{u}^\top \mathbf{u} = \|\mathbf{u}\|_2^2$.
                The term $\mathbf{x}^\top \mathbf{v} = \mathbf{x}^\top Y^{-1/2} \mathbf{u} = (Y^{-1/2} \mathbf{x})^\top \mathbf{u}$. Let $\mathbf{w} = Y^{-1/2} \mathbf{x}$. Then $\mathbf{x}^\top \mathbf{v} = \mathbf{w}^\top \mathbf{u}$.
            </div>
            <div class="proof-step">
                <strong>Step 3: Apply Cauchy-Schwarz.</strong>
                The inequality becomes $(\mathbf{w}^\top \mathbf{u})^2 \le t \|\mathbf{u}\|_2^2$ for all $\mathbf{u}$.
                We want to find the minimum $t$ that satisfies this. This is equivalent to:
                $$ t \ge \sup_{\mathbf{u} \neq \mathbf{0}} \frac{(\mathbf{w}^\top \mathbf{u})^2}{\|\mathbf{u}\|_2^2} $$
                By the Cauchy-Schwarz inequality, $(\mathbf{w}^\top \mathbf{u})^2 \le \|\mathbf{w}\|_2^2 \|\mathbf{u}\|_2^2$, with equality when $\mathbf{u}$ is parallel to $\mathbf{w}$.
                Thus, the supremum is $\|\mathbf{w}\|_2^2$.
            </div>
            <div class="proof-step">
                <strong>Step 4: Conclusion.</strong>
                $$ t \ge \|\mathbf{w}\|_2^2 = \mathbf{w}^\top \mathbf{w} = (Y^{-1/2} \mathbf{x})^\top (Y^{-1/2} \mathbf{x}) = \mathbf{x}^\top Y^{-1} \mathbf{x} $$
            </div>
          </div>
        </div>

    </section>

    <!-- SECTION 8: PROJECTIONS & LEAST SQUARES -->
    <section class="section-card" id="section-projections-ls">
      <h2>8. Projections and Least Squares</h2>

      <h3>8.1 Orthogonal Projection</h3>
      <p>The projection of $\mathbf{b}$ onto a subspace $\mathcal{S}$ is the unique point $\mathbf{p} \in \mathcal{S}$ minimizing $\|\mathbf{b} - \mathbf{p}\|_2$.
      <br><b>Orthogonality Principle:</b> The error $\mathbf{e} = \mathbf{b} - \mathbf{p}$ is orthogonal to $\mathcal{S}$.
      $$ \mathbf{v}^\top (\mathbf{b} - \mathbf{p}) = 0 \quad \forall \mathbf{v} \in \mathcal{S} $$</p>

      <div class="proof-box">
        <h4>Geometric Interpretation and Derivation</h4>
        <p>This formula says: "Project $(\mathbf{y} - \mathbf{x}_0)$ onto the nullspace $\mathcal{N}(F)$, then shift back by $\mathbf{x}_0$."</p>

        <div class="proof-step">
          <strong>Step 1: Translate to subspace problem.</strong>
          We want to find $\mathbf{x} \in \mathcal{A}$ minimizing $\|\mathbf{x} - \mathbf{y}\|_2$. Let $\mathbf{x} = \mathbf{x}_0 + \mathbf{w}$, where $\mathbf{w} \in \mathcal{N}(F)$.
          The problem becomes minimizing $\|(\mathbf{x}_0 + \mathbf{w}) - \mathbf{y}\|_2 = \|\mathbf{w} - (\mathbf{y} - \mathbf{x}_0)\|_2$ subject to $\mathbf{w} \in \mathcal{N}(F)$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Project onto nullspace.</strong>
          This is now a projection of the vector $(\mathbf{y} - \mathbf{x}_0)$ onto the subspace $\mathcal{N}(F)$.
          Since the columns of $Z$ form a basis for $\mathcal{N}(F)$, the projection matrix onto this subspace is $P_{\mathcal{N}(F)} = Z(Z^\top Z)^{-1} Z^\top$.
          Thus, the optimal $\mathbf{w}^*$ is:
          $$ \mathbf{w}^* = \Pi_{\mathcal{N}(F)}(\mathbf{y} - \mathbf{x}_0) = Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - \mathbf{x}_0) $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Shift back.</strong>
          The projection onto the affine set is $\mathbf{x}^* = \mathbf{x}_0 + \mathbf{w}^*$.
          $$ \Pi_{\mathcal{A}}(\mathbf{y}) = \mathbf{x}_0 + Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - \mathbf{x}_0) $$
        </div>
      </div>

      <div class="proof-box">
        <h4>Derivation via Lagrange Multipliers</h4>
        <div class="proof-step">
          <strong>Step 1: Formulation.</strong> Minimize $\frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2$ subject to $F\mathbf{x} = \mathbf{g}$.
          The Lagrangian is $L(\mathbf{x}, \mathbf{\nu}) = \frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2 + \mathbf{\nu}^\top (F\mathbf{x} - \mathbf{g})$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Optimality Conditions.</strong>
          Gradient w.r.t $\mathbf{x}$: $\mathbf{x} - \mathbf{y} + F^\top \mathbf{\nu} = 0 \implies \mathbf{x} = \mathbf{y} - F^\top \mathbf{\nu}$.
          Primal feasibility: $F\mathbf{x} = \mathbf{g}$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Solve for Dual Variable $\mathbf{\nu}$.</strong>
          Substitute $\mathbf{x}$ into the constraint:
          $$ F(\mathbf{y} - F^\top \mathbf{\nu}) = \mathbf{g} \implies F\mathbf{y} - F F^\top \mathbf{\nu} = \mathbf{g} \implies F F^\top \mathbf{\nu} = F\mathbf{y} - \mathbf{g} $$
          Assuming $F$ has full row rank, $F F^\top$ is invertible:
          $$ \mathbf{\nu} = (F F^\top)^{-1} (F\mathbf{y} - \mathbf{g}) $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Substitute back.</strong>
          $$ \mathbf{x} = \mathbf{y} - F^\top (F F^\top)^{-1} (F\mathbf{y} - \mathbf{g}) $$
        </div>
      </div>

      <h3>8.2 Least Squares</h3>
      <p>Problem: Solve $A\mathbf{x} = \mathbf{b}$ where $m > n$. Minimize squared error:
      $$ \min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2^2 $$
      The optimal $\mathbf{p} = A\mathbf{x}$ is the projection of $\mathbf{b}$ onto $\mathcal{R}(A)$.
      <br>Using orthogonality: $A^\top (\mathbf{b} - A\mathbf{x}) = 0$.
      <br><b>Normal Equations:</b>
      $$ A^\top A \mathbf{x} = A^\top \mathbf{b} $$
      If $A$ has full column rank, $\mathbf{x}^* = (A^\top A)^{-1} A^\top \mathbf{b}$.</p>

      <div class="proof-box">
        <h4>Derivation: Hessian of Least Squares</h4>
        <p>Let $f(\mathbf{x}) = \|A\mathbf{x} - \mathbf{b}\|_2^2$. We show $\nabla^2 f(\mathbf{x}) = 2A^\top A$ and prove its convexity.</p>

        <div class="proof-step">
          <strong>Step 1: Expand.</strong>
          $$ f(\mathbf{x}) = (A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = \mathbf{x}^\top A^\top A \mathbf{x} - 2\mathbf{b}^\top A \mathbf{x} + \mathbf{b}^\top \mathbf{b} $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Gradient.</strong>
          Using the rules derived above:
          <ul>
            <li>$\nabla (\mathbf{x}^\top (A^\top A) \mathbf{x}) = 2A^\top A \mathbf{x}$ (since $A^\top A$ is symmetric).</li>
            <li>$\nabla (-2(A^\top \mathbf{b})^\top \mathbf{x}) = -2A^\top \mathbf{b}$.</li>
            <li>$\nabla (\mathbf{b}^\top \mathbf{b}) = \mathbf{0}$.</li>
          </ul>
          Summing these: $\nabla f(\mathbf{x}) = 2A^\top A \mathbf{x} - 2A^\top \mathbf{b} = 2A^\top (A\mathbf{x} - \mathbf{b})$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Hessian.</strong>
          Differentiating $\nabla f(\mathbf{x})$ with respect to $\mathbf{x}$:
          $$ \nabla^2 f(\mathbf{x}) = \nabla_\mathbf{x} (2A^\top A \mathbf{x}) = 2A^\top A $$
          The Hessian is constant, characteristic of quadratic functions.
        </div>

        <div class="proof-step">
          <strong>Step 4: Convexity (PSD Check).</strong>
          For any vector $\mathbf{v} \in \mathbb{R}^n$:
          $$ \mathbf{v}^\top \nabla^2 f(\mathbf{x}) \mathbf{v} = \mathbf{v}^\top (2A^\top A) \mathbf{v} = 2(A\mathbf{v})^\top (A\mathbf{v}) = 2\|A\mathbf{v}\|_2^2 \ge 0 $$
          Since the quadratic form is always non-negative, the Hessian is Positive Semidefinite (PSD). Thus, the least squares objective is always <b>convex</b>.
        </div>
      </div>

      <div class="theorem-box">
        <h4>Lemma: Quadratic Minimization (Completion of the Square)</h4>
        <p>This result is the "workhorse" of convex optimization derivation. Let $H \in \mathbb{S}_{++}^n$ (symmetric positive definite) and $\mathbf{g} \in \mathbb{R}^n$. Then the unique minimizer of the quadratic form:
        $$ \inf_{\mathbf{x}\in\mathbb R^n}\left\{\frac12 \mathbf{x}^\top H\mathbf{x} + \mathbf{g}^\top \mathbf{x}\right\} $$
        is given by $\boxed{\mathbf{x}^\star = -H^{-1}\mathbf{g}}$, and the optimal value is $\boxed{-\frac12 \mathbf{g}^\top H^{-1}\mathbf{g}}$.</p>
        <p><i>Proof:</i> Complete the square. Let $\mathbf{x} = \mathbf{z} - H^{-1}\mathbf{g}$.
        $$ \frac12 (\mathbf{z} - H^{-1}\mathbf{g})^\top H (\mathbf{z} - H^{-1}\mathbf{g}) + \mathbf{g}^\top (\mathbf{z} - H^{-1}\mathbf{g}) $$
        Expanding this yields $\frac12 \mathbf{z}^\top H \mathbf{z} - \frac12 \mathbf{g}^\top H^{-1}\mathbf{g}$. Since $H \succ 0$, the minimum is at $\mathbf{z}=\mathbf{0}$.
        </p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Least Squares</h3>
        <p>Geometry of least squares as a projection.</p>
        <div id="widget-least-squares" style="width: 100%; height: 500px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 9: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>9. Review & Cheat Sheet</h2>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>Formula/Def</th>
            <th>Property</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Inner Product</td>
            <td>$\mathbf{x}^\top \mathbf{y}$</td>
            <td>Linear, Symmetric, PD</td>
          </tr>
          <tr>
            <td>Rank-Nullity</td>
            <td>$\dim \mathcal{R}(A) + \dim \mathcal{N}(A) = n$</td>
            <td>Fund. Theorem</td>
          </tr>
          <tr>
            <td>PSD ($A \succeq 0$)</td>
            <td>$\mathbf{x}^\top A \mathbf{x} \ge 0$</td>
            <td>Eigenvalues $\ge 0$</td>
          </tr>
          <tr>
            <td>Normal Eqn</td>
            <td>$A^\top A \mathbf{x} = A^\top \mathbf{b}$</td>
            <td>$\mathbf{b} - A\mathbf{x} \perp \mathcal{R}(A)$</td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- SECTION 10: EXERCISES -->
    <section class="section-card" id="section-exercises">
      <h2><i data-feather="edit-3"></i> 10. Exercises</h2>

      <div class="recap-box">
        <h4 style="margin-top: 0;">Exercise Guide</h4>
        <ul>
          <li><b>Basics:</b> P0.1, P0.2</li>
          <li><b>Invariants & Norms:</b> P0.3, P0.4</li>
          <li><b>Matrix Calculus:</b> P0.5, P0.6, P0.7</li>
          <li><b>PSD & Schur:</b> P0.8, P0.9, P0.10</li>
          <li><b>Projections & LS:</b> P0.11, P0.12, P0.13</li>
        </ul>
      </div>

      <div class="problem">
        <h3>P0.1 â€” Linear Independence</h3>
        <p>Determine if $\mathbf{v}_1 = (1, 2, 3)^\top, \mathbf{v}_2 = (4, 5, 6)^\top, \mathbf{v}_3 = (7, 8, 9)^\top$ are linearly independent.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Dependent. $\mathbf{v}_2 - \mathbf{v}_1 = (3, 3, 3)$ and $\mathbf{v}_3 - \mathbf{v}_2 = (3, 3, 3)$.
          Thus $\mathbf{v}_1 - 2\mathbf{v}_2 + \mathbf{v}_3 = \mathbf{0}$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.2 â€” Rank-Nullity</h3>
        <p>For a $10 \times 15$ matrix of rank 8, what is the nullity?</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Rank-Nullity Theorem: $r + k = n$.
          Here $n=15$ (columns). Rank $r=8$.
          Nullity $k = 15 - 8 = 7$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.3 â€” Trace and Determinant</h3>
        <p>Show $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$ but $\mathrm{tr}(ABC) \neq \mathrm{tr}(BAC)$ in general.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Cyclic property: $\mathrm{tr}(XY) = \mathrm{tr}(YX)$. Let $X=AB, Y=C$. Then $\mathrm{tr}(ABC) = \mathrm{tr}(CAB) = \mathrm{tr}(BCA)$.
          Counter-example for swap: Use elementary matrices or $A, B$ such that $AB \neq BA$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.4 â€” Norm Equivalence</h3>
        <p>Prove $\|\mathbf{x}\|_\infty \le \|\mathbf{x}\|_2 \le \sqrt{n}\|\mathbf{x}\|_\infty$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>$\|\mathbf{x}\|_\infty^2 = \max x_i^2 \le \sum x_i^2 = \|\mathbf{x}\|_2^2$.
          $\|\mathbf{x}\|_2^2 = \sum x_i^2 \le \sum \max x_j^2 = n \|\mathbf{x}\|_\infty^2$.
          Take square roots.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.5 â€” Matrix Calculus: Trace Gradient</h3>
        <p>Compute $\nabla_X \mathrm{tr}(A X B)$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>$\mathrm{tr}(AXB) = \mathrm{tr}(BAX)$. Since $\nabla_X \mathrm{tr}(MX) = M^\top$, let $M=BA$.
          $\nabla_X f(X) = (BA)^\top = A^\top B^\top$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.6 â€” Gradient of Least Squares</h3>
        <p>Derive $\nabla f(\mathbf{x})$ for $f(\mathbf{x}) = \|A\mathbf{x} - \mathbf{b}\|_2^2$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Expand: $(\mathbf{x}^\top A^\top - \mathbf{b}^\top)(A\mathbf{x} - \mathbf{b}) = \mathbf{x}^\top A^\top A \mathbf{x} - 2\mathbf{b}^\top A \mathbf{x} + \mathbf{b}^\top \mathbf{b}$.
          Gradient of quadratic $\mathbf{x}^\top Q \mathbf{x}$ is $2Q\mathbf{x}$. Gradient of linear $-\mathbf{c}^\top \mathbf{x}$ is $-\mathbf{c}$.
          $\nabla f(\mathbf{x}) = 2A^\top A \mathbf{x} - 2A^\top \mathbf{b} = 2A^\top(A\mathbf{x} - \mathbf{b})$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.7 â€” Hessian of Cubic</h3>
        <p>For $f(\mathbf{x}) = x_1^3 + x_2^3 + 2x_1 x_2$, find the region where it is locally convex.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Hessian $H = \begin{bmatrix} 6x_1 & 2 \\ 2 & 6x_2 \end{bmatrix}$.
          Convex $\iff H \succeq 0$.
          Trace $6(x_1+x_2) \ge 0$. Det $36x_1 x_2 - 4 \ge 0 \implies x_1 x_2 \ge 1/9$.
          Since product is positive, signs must match. Trace implies positive.
          Region: $x_1 > 0, x_2 > 0, x_1 x_2 \ge 1/9$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.8 â€” Testing PSD</h3>
        <p>Is $A = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$ PSD?</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Yes. Det = $4-1=3 > 0$. Trace = 4 > 0.
          Or eigenvalues: $(2-\lambda)^2 - 1 = 0 \implies \lambda = 1, 3$. Both positive.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.9 â€” Schur Complement</h3>
        <p>For what $x$ is $M = \begin{bmatrix} x & 1 \\ 1 & x \end{bmatrix} \succeq 0$?</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Need $A > 0 \implies x > 0$.
          Schur: $C - B^\top A^{-1} B \ge 0 \implies x - 1/x \ge 0 \implies x^2 \ge 1$.
          Since $x>0$, we need $x \ge 1$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.10 â€” Loewner Order</h3>
        <p>Prove transitivity: $X \succeq Y, Y \succeq Z \implies X \succeq Z$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>$\mathbf{v}^\top (X-Y) \mathbf{v} \ge 0$ and $\mathbf{v}^\top (Y-Z) \mathbf{v} \ge 0$.
          Summing: $\mathbf{v}^\top (X-Z) \mathbf{v} \ge 0$. True for all $\mathbf{v}$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.11 â€” Projection onto a Line</h3>
        <p>Project $\mathbf{b}=(1,0,0)$ onto $\mathbf{a}=(1,1,1)$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>$\mathbf{p} = \frac{\mathbf{a}^\top \mathbf{b}}{\mathbf{a}^\top \mathbf{a}} \mathbf{a} = \frac{1}{3} (1,1,1)$.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.12 â€” Projector Matrices</h3>
        <p>Show $P$ is an orthogonal projector iff $P^2=P$ and $P^\top = P$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>See text Section 7. Idempotence means projection; Symmetry means orthogonality (residual orthogonal to range).</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.13 â€” Least Squares with Nullspace</h3>
        <p>If $A$ has a nullspace, is the least squares solution unique?</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>No. If $A\mathbf{x} = \mathbf{p}$, then $A(\mathbf{x} + \mathbf{n}) = \mathbf{p}$ for any $\mathbf{n} \in \mathcal{N}(A)$.
          The projected point $\mathbf{p}$ is unique, but $\mathbf{x}$ is not.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.14 â€” Orthogonal Complements</h3>
        <p>Let $S$ be a subspace of $\mathbb{R}^n$. The orthogonal complement is $S^\perp = \{y \mid y^\top x = 0 \ \forall x \in S\}$.</p>
        <ol type="a">
          <li>Prove that $S^\perp$ is a subspace.</li>
          <li>Prove that $S \cap S^\perp = \{0\}$.</li>
          <li>If $S = \text{span}((1, 0, 0)^\top, (0, 1, 0)^\top)$, calculate $S^\perp$.</li>
        </ol>
        <div class="solution-box">
          <h4>Solution</h4>
          <ol type="a">
            <li><b>Subspace:</b> Let $\mathbf{y}_1, \mathbf{y}_2 \in S^\perp$. Then $\mathbf{y}_1^\top \mathbf{x} = 0$ and $\mathbf{y}_2^\top \mathbf{x} = 0$.
            For any linear combination $\alpha \mathbf{y}_1 + \beta \mathbf{y}_2$, we have $(\alpha \mathbf{y}_1 + \beta \mathbf{y}_2)^\top \mathbf{x} = \alpha(\mathbf{y}_1^\top \mathbf{x}) + \beta(\mathbf{y}_2^\top \mathbf{x}) = 0$.
            Thus closed under linear combinations. Contains $\mathbf{0}$ since $\mathbf{0}^\top \mathbf{x} = 0$.
            </li>
            <li><b>Intersection:</b> Let $\mathbf{x} \in S \cap S^\perp$. Since $\mathbf{x} \in S$ and $\mathbf{x} \in S^\perp$, it must be orthogonal to itself: $\mathbf{x}^\top \mathbf{x} = 0$.
            $\|\mathbf{x}\|^2 = 0 \implies \mathbf{x} = 0$. Thus the intersection contains only the zero vector.
            </li>
            <li><b>Calculation:</b> $S$ is the $xy$-plane ($z=0$). $S^\perp$ is the set of vectors orthogonal to $(1,0,0)$ and $(0,1,0)$.
            $\mathbf{y} \cdot \mathbf{e}_1 = y_1 = 0$. $\mathbf{y} \cdot \mathbf{e}_2 = y_2 = 0$.
            Thus $\mathbf{y} = (0, 0, y_3)^\top$. $S^\perp$ is the $z$-axis (span of $\mathbf{e}_3$).
            </li>
          </ol>
        </div>
      </div>

      <div class="problem">
        <h3>P0.15 â€” Frobenius Norm Submultiplicativity</h3>
        <p>We proved $\|AB\|_F \le \|A\|_F \|B\|_F$ in the lecture using Cauchy-Schwarz.
        <br>Re-derive this result by writing $\|X\|_F^2 = \mathrm{tr}(X^\top X)$ and using the property $\mathrm{tr}(M) = \sum \lambda_i(M)$ along with the fact that $\lambda_{\max}(A^\top A) \le \mathrm{tr}(A^\top A)$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            We want to bound $\|AB\|_F^2 = \mathrm{tr}(B^\top A^\top A B)$.
            Using the cyclic property: $\mathrm{tr}(B^\top (A^\top A) B) = \mathrm{tr}((A^\top A) (B B^\top))$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Trace Inequality.</strong>
            Let $P = A^\top A$ and $Q = B B^\top$. Both are symmetric positive semidefinite (PSD).
            We claim $\mathrm{tr}(PQ) \le \lambda_{\max}(P) \mathrm{tr}(Q)$.
            <br><i>Proof:</i> Since $Q$ is symmetric, it has an eigendecomposition $Q = U \Lambda U^\top$ with $\lambda_i \ge 0$.
            $$ \mathrm{tr}(PQ) = \mathrm{tr}(P U \Lambda U^\top) = \mathrm{tr}(U^\top P U \Lambda) $$
            Let $M = U^\top P U$. The diagonal entry $M_{ii} = u_i^\top P u_i$.
            Since $u_i$ is a unit vector (column of orthogonal matrix $U$), the Rayleigh quotient implies:
            $$ \lambda_{\min}(P) \le u_i^\top P u_i \le \lambda_{\max}(P) $$
            Thus $M_{ii} \le \lambda_{\max}(P)$.
            $$ \mathrm{tr}(M \Lambda) = \sum_i M_{ii} \lambda_i \le \sum_i \lambda_{\max}(P) \lambda_i = \lambda_{\max}(P) \sum_i \lambda_i = \lambda_{\max}(P) \mathrm{tr}(Q) $$
          </div>
          <div class="proof-step">
            Applying this: $\|AB\|_F^2 \le \lambda_{\max}(A^\top A) \mathrm{tr}(B B^\top) = \|A\|_2^2 \|B\|_F^2$.
            Since the spectral norm $\|A\|_2$ satisfies $\|A\|_2 \le \|A\|_F$, we have $\|A\|_2^2 \le \|A\|_F^2$.
            <br>Thus $\|AB\|_F^2 \le \|A\|_F^2 \|B\|_F^2$. Taking square roots gives the result.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.16 â€” Spectral Norm Properties</h3>
        <p>Let $\|A\|_2$ denote the spectral norm (max singular value).</p>
        <ol type="a">
          <li>Show that $\|A\|_2 = 0$ if and only if $A=0$.</li>
          <li>Show that $\|Q A\|_2 = \|A\|_2$ for any orthogonal matrix $Q$. (Orthogonal invariance).</li>
        </ol>
        <div class="solution-box">
          <h4>Solution</h4>
          <ol type="a">
            <li>$\|A\|_2 = \sigma_{\max}(A)$. Singular values are non-negative. The max is 0 iff all singular values are 0. If $\Sigma=0$ in SVD $A=U\Sigma V^\top$, then $A=0$. Conversely if $A=0$, the maximum stretch is 0.</li>
            <li>$\|QA\|_2 = \sup_{\|\mathbf{x}\|=1} \|QA\mathbf{x}\|_2$. Since $Q$ is orthogonal, it preserves Euclidean norms: $\|QA\mathbf{x}\|_2 = \|A\mathbf{x}\|_2$.
            Thus $\sup_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|_2 = \|A\|_2$. The spectral norm is invariant under left (and right) orthogonal multiplication.</li>
          </ol>
        </div>
      </div>

      <div class="problem">
        <h3>P0.17 â€” Isometries</h3>
        <p>An isometry is a linear map $f(\mathbf{x}) = Q\mathbf{x}$ that preserves distances: $\|Q\mathbf{x} - Q\mathbf{y}\|_2 = \|\mathbf{x} - \mathbf{y}\|_2$ for all $\mathbf{x}, \mathbf{y}$.
        <br>Show that this condition implies $Q^\top Q = I$. Thus, isometries are represented by orthogonal matrices.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Step 1: Norm Preservation.</strong>
            Setting $\mathbf{y}=0$, linearity implies $Q(\mathbf{0})=\mathbf{0}$, so $\|Q\mathbf{x}\|_2 = \|\mathbf{x}\|_2$ for all $\mathbf{x}$.
            Squaring both sides:
            $$ \mathbf{x}^\top Q^\top Q \mathbf{x} = \mathbf{x}^\top I \mathbf{x} \implies \mathbf{x}^\top (Q^\top Q - I) \mathbf{x} = 0 $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Polarization Identity.</strong>
            Let $M = Q^\top Q - I$. We have $\mathbf{x}^\top M \mathbf{x} = 0$ for all $\mathbf{x}$. Since $M$ is symmetric, we can recover the values of the bilinear form $\mathbf{x}^\top M \mathbf{y}$ using the polarization identity:
            $$ \mathbf{x}^\top M \mathbf{y} = \frac{1}{4} \left( (\mathbf{x}+\mathbf{y})^\top M (\mathbf{x}+\mathbf{y}) - (\mathbf{x}-\mathbf{y})^\top M (\mathbf{x}-\mathbf{y}) \right) $$
            Since the quadratic form is zero for any vector (including $\mathbf{x}+\mathbf{y}$ and $\mathbf{x}-\mathbf{y}$), the right-hand side is zero.
            Thus, $\mathbf{x}^\top M \mathbf{y} = 0$ for all $\mathbf{x}, \mathbf{y}$.
          </div>
          <div class="proof-step">
            <strong>Step 3: Conclusion.</strong>
            Choosing $\mathbf{x} = \mathbf{e}_i$ and $\mathbf{y} = \mathbf{e}_j$ yields $\mathbf{e}_i^\top M \mathbf{e}_j = M_{ij} = 0$.
            Since all entries are zero, $M = 0$, so $Q^\top Q = I$.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P0.18 â€” Weighted Inner Product</h3>
        <p>Let $A \in \mathbb{S}^n_{++}$ be a symmetric positive definite matrix.
        <br>(a) Prove that $\langle \mathbf{x}, \mathbf{y} \rangle_A = \mathbf{x}^\top A \mathbf{y}$ satisfies all axioms of an inner product.
        <br>(b) Write down the Cauchy-Schwarz inequality for this inner product.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <ol type="a">
            <li><b>Symmetry:</b> $\langle \mathbf{x}, \mathbf{y} \rangle_A = \mathbf{x}^\top A \mathbf{y} = (\mathbf{x}^\top A \mathbf{y})^\top = \mathbf{y}^\top A^\top \mathbf{x} = \mathbf{y}^\top A \mathbf{x} = \langle \mathbf{y}, \mathbf{x} \rangle_A$ (since $A=A^\top$).
            <br><b>Linearity:</b> Linear in first arg (matrix multiplication distributes).
            <br><b>Positive Definiteness:</b> $\langle \mathbf{x}, \mathbf{x} \rangle_A = \mathbf{x}^\top A \mathbf{x}$. Since $A \succ 0$, this is $>0$ for all $\mathbf{x} \neq \mathbf{0}$.
            </li>
            <li><b>Cauchy-Schwarz:</b> $|\langle \mathbf{x}, \mathbf{y} \rangle_A| \le \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle_A} \sqrt{\langle \mathbf{y}, \mathbf{y} \rangle_A}$.
            Explicitly: $|\mathbf{x}^\top A \mathbf{y}| \le \sqrt{\mathbf{x}^\top A \mathbf{x}} \sqrt{\mathbf{y}^\top A \mathbf{y}}$.
            </li>
          </ol>
        </div>
      </div>

      <div class="problem">
        <h3>P0.19 â€” PSD Cone in 2D</h3>
        <p>Consider the space of $2 \times 2$ symmetric matrices, which has dimension 3.
        <br>Write down the explicit inequalities defining the PSD cone $S \succeq 0$ in terms of the matrix entries $x, y, z$ (where $S = \begin{bmatrix} x & y \\ y & z \end{bmatrix}$). Relate this to the trace and determinant conditions.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>A $2 \times 2$ matrix is PSD if and only if:
          1. Trace $\ge 0$: $x + z \ge 0$.
          2. Determinant $\ge 0$: $xz - y^2 \ge 0$.
          <br>Note that $xz \ge y^2 \ge 0$ implies $x$ and $z$ have the same sign.
          Since their sum is non-negative, both must be non-negative: $x \ge 0, z \ge 0$.
          <br>Thus the conditions are:
          $$ x \ge 0, \quad z \ge 0, \quad xz \ge y^2 $$
          Geometrically, this is a cone in $\mathbb{R}^3$. The boundary $xz=y^2$ is a rotated quadratic cone surface.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P0.20 â€” General Quadratic Minimization</h3>
        <p>Solve the unconstrained minimization problem for a general quadratic function:
        $$ \min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}) := \frac{1}{2} \mathbf{x}^\top H \mathbf{x} + \mathbf{g}^\top \mathbf{x} + c $$
        where $H \in \mathbb{S}_{++}^n$ (Positive Definite).</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>By Lemma A2 (Quadratic Minimization), the minimizer is obtained by setting the gradient to zero.
          $$ \nabla f(\mathbf{x}) = H\mathbf{x} + \mathbf{g} = \mathbf{0} \implies \mathbf{x}^\star = -H^{-1}\mathbf{g} $$
          Substituting this back into the objective to find the optimal value:
          $$ f(\mathbf{x}^\star) = \frac{1}{2} (-H^{-1}\mathbf{g})^\top H (-H^{-1}\mathbf{g}) + \mathbf{g}^\top (-H^{-1}\mathbf{g}) + c $$
          $$ = \frac{1}{2} \mathbf{g}^\top H^{-1} H H^{-1} \mathbf{g} - \mathbf{g}^\top H^{-1} \mathbf{g} + c $$
          $$ = \frac{1}{2} \mathbf{g}^\top H^{-1} \mathbf{g} - \mathbf{g}^\top H^{-1} \mathbf{g} + c = c - \frac{1}{2} \mathbf{g}^\top H^{-1} \mathbf{g} $$
          This formula is the foundation for almost all dual derivations (where we minimize the Lagrangian w.r.t $\mathbf{x}$).</p>
        </div>
      </div>

    </section>

    <!-- SECTION 11: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>11. Readings & Resources</h2>
      <ul>
        <li><b>Boyd & Vandenberghe:</b> Appendix A.</li>
        <li><b>Strang:</b> Introduction to Linear Algebra, Ch 2-3.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>Â© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/glossary-loader.js"></script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initLeastSquaresVisualizer } from './widgets/js/least-squares-visualizer.js';
    initLeastSquaresVisualizer('widget-least-squares');
  </script>
</body>
</html>
