# 5.6

**User:** Anonymous (meqrvsl88948@hotmail.com)  
**Created:** 12/28/2025 2:11:36  
**Updated:** 12/28/2025 16:22:47  
**Exported:** 12/28/2025 16:23:15  
**Link:** [https://chatgpt.com/g/g-p-694aced048448191ad02c68b2c4055b0/c/6950609c-6024-8328-86aa-c87c8559dc98](https://chatgpt.com/g/g-p-694aced048448191ad02c68b2c4055b0/c/6950609c-6024-8328-86aa-c87c8559dc98)  

## Response:
Thought for 2m 37s

Letâ€™s translate the page into a clean mental model, then solve (a) and (b) carefully from first principles.

We have a matrix  $A\in\mathbb R^{m\times n}$  with  $\operatorname{rank}(A)=n$  (full column rank), and a vector  $b\in\mathbb R^m$ .

Define the residual (error) for any  $x\in\mathbb R^n$ :

$$
e(x)\;:=\;Ax-b\in\mathbb R^m.
$$

There are two optimization problems:

1.  **Chebyshev /  $\ell_\infty$  approximation**
    
$$
\minimize_x \ \|Ax-b\|_\infty \;=\;\minimize_x \ \|e(x)\|_\infty.
$$

Let  $x_{\text{ch}}$  be an optimal solution (maybe not unique). Define the optimal value

$$
p^\star \;:=\;\|Ax_{\text{ch}}-b\|_\infty.
$$
2.  **Least-squares /  $\ell_2$  approximation**
    
$$
\minimize_x \ \|Ax-b\|_2 \;=\;\minimize_x \ \|e(x)\|_2.
$$

Because  $A$  has full column rank, the LS solution is unique and equals

$$
x_{\text{ls}} \;=\;\arg\min_x \|Ax-b\|_2 \;=\;(A^TA)^{-1}A^Tb.
$$

The big question: â€œIf I already computed  $x_{\text{ls}}$ , how bad is it for the  $\ell_\infty$  problem?â€

* * *

Part (a): prove  $\|Ax_{\text{ls}}-b\|_\infty \le \sqrt{m}\,\|Ax_{\text{ch}}-b\|_\infty$ 
-----------------------------------------------------------------------------------------

The only â€œtoolâ€ youâ€™re allowed is the standard inequality relating  $\ell_2$  and  $\ell_\infty$  norms. For any  $z\in\mathbb R^m$ ,

$$
\frac{1}{\sqrt{m}}\|z\|_2 \le \|z\|_\infty \le \|z\|_2.
$$

(Equivalently:  $\|z\|_2 \le \sqrt{m}\|z\|_\infty$  and  $\|z\|_\infty \le \|z\|_2$ .)

Letâ€™s prove the target inequality in tiny steps.

Step 1: start from the left side.

$$
\|Ax_{\text{ls}}-b\|_\infty \;=\;\|e(x_{\text{ls}})\|_\infty.
$$

Step 2: use  $\|z\|_\infty \le \|z\|_2$  with  $z=e(x_{\text{ls}})$ .

$$
\|e(x_{\text{ls}})\|_\infty \;\le\; \|e(x_{\text{ls}})\|_2.
$$

Step 3: use the defining optimality of  $x_{\text{ls}}$  for the  $\ell_2$  problem.  
Since  $x_{\text{ls}}$  minimizes  $\|e(x)\|_2$  over all  $x$ , in particular it does no worse than the specific choice  $x=x_{\text{ch}}$ :

$$
\|e(x_{\text{ls}})\|_2 \;\le\; \|e(x_{\text{ch}})\|_2.
$$

Step 4: relate  $\|e(x_{\text{ch}})\|_2$  to  $\|e(x_{\text{ch}})\|_\infty$  using  $\|z\|_2 \le \sqrt{m}\|z\|_\infty$ .

$$
\|e(x_{\text{ch}})\|_2 \;\le\; \sqrt{m}\,\|e(x_{\text{ch}})\|_\infty.
$$

Step 5: chain the three inequalities.

$$
\|e(x_{\text{ls}})\|_\infty \;\le\; \|e(x_{\text{ls}})\|_2 \;\le\; \|e(x_{\text{ch}})\|_2 \;\le\; \sqrt{m}\,\|e(x_{\text{ch}})\|_\infty.
$$

That is exactly

Â $$
\boxed{\ \|Ax_{\text{ls}}-b\|_\infty \le \sqrt{m}\,\|Ax_{\text{ch}}-b\|_\infty\ }.
$$

Interpretation: least-squares gives a ** $\sqrt{m}$ \-approximation** to the Chebyshev optimum in the worst case.

Also note the â€œlower boundâ€ viewpoint: rearrange it as

Â $$
\boxed{\ \|Ax_{\text{ch}}-b\|_\infty \ge \frac{1}{\sqrt{m}}\|Ax_{\text{ls}}-b\|_\infty\ }.
$$

So from  $x_{\text{ls}}$  alone, you can certify that the true Chebyshev optimum  $p^\star$  cannot be smaller than  $\frac{1}{\sqrt m}\|Ax_{\text{ls}}-b\|_\infty$ .

* * *

Part (b): use the dual to get _data-dependent_ lower bounds from the LS residual
--------------------------------------------------------------------------------

They state the dual of the Chebyshev problem as:

$$
\begin{aligned} \maximize_\nu\quad & b^T\nu\\ \text{s.t.}\quad & \|\nu\|_1 \le 1,\\ & A^T\nu = 0. \end{aligned} \tag{D}
$$

Key fact (weak duality): for any feasible  $\nu$ ,  $b^T\nu$  is a **lower bound** on the primal optimal value  $p^\star=\|Ax_{\text{ch}}-b\|_\infty$ :

$$
b^T\nu \;\le\; p^\star.
$$

(You donâ€™t even need strong duality for â€œlower bound from feasible dual.â€)

They define the least-squares residual (note the sign!)

$$
r_{\text{ls}} \;=\; b - Ax_{\text{ls}}.
$$

Assume  $r_{\text{ls}}\neq 0$ .

They ask you to show that

$$
\hat\nu = -\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}, \qquad \bar\nu = \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}
$$

are both feasible in (D), hence both give lower bounds  $b^T\hat\nu$  and  $b^T\bar\nu$ .

### 1) Why does  $A^T r_{\text{ls}} = 0$ ? (the normal equations)

Least squares minimizes  $\|Ax-b\|_2$ . Itâ€™s standard (and important) to square it because itâ€™s smooth:

$$
\min_x \ \|Ax-b\|_2^2.
$$

Expand the objective:

$$
\|Ax-b\|_2^2 = (Ax-b)^T(Ax-b).
$$

Differentiate w.r.t.  $x$ . A clean way: the gradient is

$$
\nabla_x \|Ax-b\|_2^2 = 2A^T(Ax-b).
$$

At the minimizer  $x_{\text{ls}}$ , first-order optimality gives

$$
2A^T(Ax_{\text{ls}}-b)=0 \quad\Longrightarrow\quad A^T(Ax_{\text{ls}}-b)=0 \quad\Longrightarrow\quad A^T(b-Ax_{\text{ls}})=0.
$$

But  $b-Ax_{\text{ls}}=r_{\text{ls}}$ . Therefore

$$
\boxed{A^T r_{\text{ls}} = 0.}
$$

Geometric meaning: the residual is orthogonal to the column space of  $A$ .

### 2) Feasibility of  $\hat\nu$  and  $\bar\nu$ 

Check the dual constraints one by one.

Constraint (i):  $\|\nu\|_1\le 1$ .

Because  $r_{\text{ls}}\neq 0$ ,  $\|r_{\text{ls}}\|_1>0$ . Then

$$
\left\|\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_1}=1.
$$

So  $\|\bar\nu\|_1=1$  and  $\|\hat\nu\|_1=1$  (sign doesnâ€™t matter in  $\ell_1$ ). Hence both satisfy  $\|\nu\|_1\le 1$ .

Constraint (ii):  $A^T\nu=0$ .

Use  $A^T r_{\text{ls}}=0$ . Then scaling preserves it:

$$
A^T\bar\nu = A^T\left(\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right) = \frac{1}{\|r_{\text{ls}}\|_1}A^T r_{\text{ls}} = 0.
$$

Similarly  $A^T\hat\nu=0$ .

So both are feasible.

### 3) The actual lower bounds  $b^T\hat\nu$  and  $b^T\bar\nu$ 

Compute  $b^T r_{\text{ls}}$  first; itâ€™s the key simplification.

Since  $r_{\text{ls}}=b-Ax_{\text{ls}}$ , rewrite  $b$  as

$$
b = Ax_{\text{ls}} + r_{\text{ls}}.
$$

Then

$$
b^T r_{\text{ls}} = (Ax_{\text{ls}}+r_{\text{ls}})^T r_{\text{ls}} = x_{\text{ls}}^T A^T r_{\text{ls}} + r_{\text{ls}}^T r_{\text{ls}}.
$$

But  $A^T r_{\text{ls}}=0$ . So the cross term vanishes:

$$
b^T r_{\text{ls}} = r_{\text{ls}}^T r_{\text{ls}} = \|r_{\text{ls}}\|_2^2.
$$

Now plug into each candidate:

For  $\bar\nu = r_{\text{ls}}/\|r_{\text{ls}}\|_1$ ,

$$
b^T\bar\nu = \frac{b^T r_{\text{ls}}}{\|r_{\text{ls}}\|_1} = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.
$$

For  $\hat\nu = -r_{\text{ls}}/\|r_{\text{ls}}\|_1$ ,

$$
b^T\hat\nu = -\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.
$$

So the two â€œlower boundsâ€ are one positive and one negative (since  $\|r_{\text{ls}}\|_2^2>0$ ). The better one is obviously the positive one.

Conclusion for (b):

$$
\boxed{ \text{Better bound is } b^T\bar\nu = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}. }
$$

And by weak duality,

$$
\boxed{ \|Ax_{\text{ch}}-b\|_\infty \;\ge\; \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}. }
$$

### 4) Compare this dual lower bound to the bound from part (a)

From (a) we got the certified lower bound

$$
\|Ax_{\text{ch}}-b\|_\infty \ge \frac{1}{\sqrt{m}}\|Ax_{\text{ls}}-b\|_\infty.
$$

But  $\|Ax_{\text{ls}}-b\|_\infty = \|r_{\text{ls}}\|_\infty$  (same vector up to sign), so:

$$
\boxed{ \|Ax_{\text{ch}}-b\|_\infty \ge \frac{1}{\sqrt{m}}\|r_{\text{ls}}\|_\infty. }
$$

Dual bound gives

$$
\boxed{ \|Ax_{\text{ch}}-b\|_\infty \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}. }
$$

Which is larger (better)? The dual one is always at least as good as the  $(1/\sqrt m)\|r\|_\infty$  bound:

Use  $\|r\|_1 \le \sqrt{m}\|r\|_2$  (Cauchyâ€“Schwarz) to get

$$
\frac{\|r\|_2^2}{\|r\|_1} \;\ge\; \frac{\|r\|_2^2}{\sqrt{m}\|r\|_2} = \frac{1}{\sqrt{m}}\|r\|_2.
$$

And since  $\|r\|_2 \ge \|r\|_\infty$ ,

$$
\frac{1}{\sqrt{m}}\|r\|_2 \ge \frac{1}{\sqrt{m}}\|r\|_\infty.
$$

Chain them:

$$
\boxed{ \frac{\|r\|_2^2}{\|r\|_1} \ge \frac{1}{\sqrt{m}}\|r\|_\infty. }
$$

So the dual-feasible-vector bound from  $\bar\nu$  is **never worse** than the crude norm-inequality bound extracted from (a), and can be much better depending on the shape of the residual.

A useful intuition:  $\|r\|_2^2/\|r\|_1$  gets close to  $\|r\|_\infty$  when the residual is â€œspikyâ€ (dominated by a few entries). The  $(1/\sqrt m)\|r\|_\infty$  bound ignores that structure and always pays the worst-case  $\sqrt m$  penalty.

* * *

If you want to push one layer deeper later: we can derive the dual (D) directly from the identity  $\|y\|_\infty=\max_{\|\nu\|_1\le 1}\nu^T y$  and a minâ€“max swap, and also show how to use the dual bound to get a _data-dependent_ approximation factor for  $x_{\text{ls}}$  that is often far smaller than  $\sqrt m$ .

## Response:
Hereâ€™s a roadmap for mastering exactly this â€œLS vs Chebyshev / duality gives boundsâ€ theme, in small, bite-sized steps. The idea is to build a little toolkit: (i) norm geometry, (ii) primal/dual patterns, (iii) how to manufacture bounds from anything you already computed (like LS residuals), (iv) how to compare bounds and get approximation factors.

Roadmap (micro-steps, in the order weâ€™ll walk them)

Step 0 â€” Notation and the three objects you juggle  
You get fluent with: residual  $r(x)=b-Ax$ , optimal values  $p_2^\star=\min\|r(x)\|_2$  and  $p_\infty^\star=\min\|r(x)\|_\infty$ , and solutions  $x_{\text{ls}}, x_{\text{ch}}$ . Goal: be able to rewrite everything in residual language quickly.

Step 1 â€” Norm relations as â€œautomatic inequalitiesâ€  
Prove from scratch:

$$
\|z\|_\infty \le \|z\|_2 \le \sqrt{m}\|z\|_\infty,\qquad \|z\|_2 \le \|z\|_1 \le \sqrt{m}\|z\|_2,
$$

and learn when each is tight. Goal: you can see where  $\sqrt m$  comes from and when itâ€™s pessimistic.

Step 2 â€” Least squares optimality = orthogonality (normal equations)  
Derive  $A^T(Ax_{\text{ls}}-b)=0$ , equivalently  $A^Tr_{\text{ls}}=0$ . Interpret geometrically (â€œprojection onto column spaceâ€). Goal: this becomes your main trick in later dual-feasibility constructions.

Step 3 â€” Chebyshev ( $\ell_\infty$ ) primal as an LP/SOCP template  
Rewrite

$$
\min_x \|Ax-b\|_\infty
$$

as

$$
\min_{x,t}\ t\quad \text{s.t.}\quad -t\mathbf 1 \le Ax-b \le t\mathbf 1.
$$

Goal: recognize it as convex and ready for duality.

Step 4 â€” The key identity:  $\ell_\infty$  is the dual norm of  $\ell_1$   
Prove

$$
\|y\|_\infty=\max_{\|\nu\|_1\le 1}\ \nu^Ty.
$$

Goal: this one line essentially generates the dual problem and every lower bound.

Step 5 â€” Derive the dual of Chebyshev from first principles  
Use Step 4 + minâ€“max logic (or LP duality) to derive:

$$
\max_\nu\ b^T\nu\quad \text{s.t.}\quad \|\nu\|_1\le 1,\ A^T\nu=0.
$$

Goal: you understand where each constraint comes from and what it means.

Step 6 â€” Weak duality as a â€œlower-bound machineâ€  
Prove: for any feasible  $\nu$ ,

$$
b^T\nu \le p_\infty^\star.
$$

Goal: you can manufacture certificates (â€œthis optimal value cannot be below Xâ€).

Step 7 â€” Construct feasible dual points from the LS residual  
Show  $r_{\text{ls}}\neq 0\Rightarrow \nu=r_{\text{ls}}/\|r_{\text{ls}}\|_1$  is feasible (because  $A^Tr_{\text{ls}}=0$ ). Compute

$$
b^T\nu=\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.
$$

Goal: you can turn â€œI already ran LSâ€ into a meaningful lower bound on the Chebyshev optimum.

Step 8 â€” Compare lower bounds: crude norm-bound vs dual residual-bound  
Compare:

$$
\frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty \quad \text{vs}\quad \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.
$$

Prove the dual one is always at least as good (never smaller). Goal: you know which certificate to use and why.

Step 9 â€” Approximation factors (upper / lower sandwich)  
You always have an â€œupper boundâ€ on  $p_\infty^\star$  by evaluating at any  $x$ :

$$
p_\infty^\star \le \|r(x)\|_\infty.
$$

In particular with LS:

$$
p_\infty^\star \le \|r_{\text{ls}}\|_\infty.
$$

Combine with Step 7 lower bound to get a data-dependent approximation ratio:

$$
\frac{\|r_{\text{ls}}\|_\infty}{p_\infty^\star}\le \frac{\|r_{\text{ls}}\|_\infty}{\|r_{\text{ls}}\|_2^2/\|r_{\text{ls}}\|_1}.
$$

Goal: quantify â€œhow suboptimal is LS for  $\ell_\infty$ â€ with numbers from your computed residual.

Step 10 â€” When are bounds tight? (diagnostics)  
Study residual shapes:

*   flat residual (many equal magnitudes),
*   spiky residual (few big entries),  
    and see how each bound behaves. Goal: intuition + ability to predict when  $\sqrt m$  is hopelessly pessimistic.

How weâ€™ll proceed  
We will do exactly one step per message, with full proofs and tiny sanity checks.

Letâ€™s start now with Step 0 in the next message: weâ€™ll set up residual notation, define optimal values cleanly, and write the two problems in a unified â€œresidual minimizationâ€ form so everything later becomes plug-and-play.

## Response:
Great â€” letâ€™s carefully unpack **Exercise 5.6** from Boyd & Vandenberghe (Convex Optimization), and build a structured roadmap to tackle it in small, rigorous, zero-to-hero steps.

* * *

ğŸŒ„ ROADMAP for Exercise 5.6 (Lower bounds on Chebyshev approximation from least-squares)
----------------------------------------------------------------------------------------

We are comparing two optimization problems:

### âœ… **Problem A**: Least-squares (minimizing 2-norm error)

$$
x_{\text{ls}} = \arg\min_x \|Ax - b\|_2
$$

### âœ… **Problem B**: Chebyshev approximation (minimizing âˆ-norm error)

$$
x_{\text{cheb}} = \arg\min_x \|Ax - b\|_\infty
$$

We are given the least-squares solution  $x_{\text{ls}}$ , but not the optimal Chebyshev solution  $x_{\text{cheb}}$ . We want to:

> **Estimate how suboptimal  $x_{\text{ls}}$  is for the Chebyshev problem.**  
> How much worse is  $\|Ax_{\text{ls}} - b\|_\infty$  than the optimal  $\|Ax_{\text{cheb}} - b\|_\infty$ ?

* * *

### ğŸ” Our roadmap will proceed as follows:

* * *

âœ… Step 1. \[Understand the two problems and their relation\]
------------------------------------------------------------

We begin by fully understanding both optimization problems, their objective functions, and the difference in norms.

* * *

âœ… Step 2. \[Prove the bound in part (a)\]
-----------------------------------------

We are asked to prove a simple inequality:

$$
\|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty
$$

using the inequality:

$$
\frac{1}{\sqrt{m}} \|z\|_2 \le \|z\|_\infty \le \|z\|_2 \quad \text{for any } z \in \mathbb{R}^m
$$

* * *

âœ… Step 3. \[Understand and derive the dual problem from part (b)\]
------------------------------------------------------------------

We move to duality for the Chebyshev problem (min âˆ-norm). Using results from Section 5.6 and duality of norms, weâ€™ll reconstruct the dual LP:

$$
\max_{\nu} \nu^T b \quad \text{s.t. } \|\nu\|_1 \le 1, \quad A^T \nu = 0
$$

* * *

âœ… Step 4. \[Construct dual feasible points using residuals\]
------------------------------------------------------------

Define the residual  $r_{\text{ls}} = b - Ax_{\text{ls}}$ .  
From this residual, construct two specific dual feasible vectors  $\hat{\nu}, \tilde{\nu}$ , and verify feasibility of each. Then compute bounds from duality.

* * *

âœ… Step 5. \[Compare all bounds\]
--------------------------------

Compare the original upper bound from Step 2 with the dual lower bounds from Step 4, to see which gives tighter (better) information.

* * *

Now letâ€™s begin with:

* * *

âœ… Step 1. Understand the two problems and their relation
========================================================

* * *

We are given a matrix  $A \in \mathbb{R}^{m \times n}$ , with **rank  $A = n$ **.  
So:

*   The number of rows  $m \ge n$  (more equations than unknowns),
*   The columns of  $A$  are linearly independent (full column rank).

We also have a vector  $b \in \mathbb{R}^m$ .  
We are considering **two norm-based regression problems**:

* * *

### ğŸ”¶ Problem 1: Least-Squares (2-norm approximation)

We minimize the **2-norm** of the residual:

$$
\min_x \|Ax - b\|_2
$$

The closed-form solution is:

$$
x_{\text{ls}} = (A^T A)^{-1} A^T b
$$

This is the classical pseudoinverse solution, assuming full column rank.

We define the **residual**:

$$
r_{\text{ls}} := b - Ax_{\text{ls}}
$$

So the least-squares error is:

$$
\|Ax_{\text{ls}} - b\|_2 = \|r_{\text{ls}}\|_2
$$

* * *

### ğŸ”· Problem 2: Chebyshev Approximation (âˆ-norm approximation)

We minimize the **âˆ-norm** of the residual:

$$
\min_x \|Ax - b\|_\infty = \min_x \max_i |(Ax - b)_i|
$$

This is called **Chebyshev approximation**.

It tries to **minimize the worst-case error** over all entries of the residual.  
Thereâ€™s no closed-form formula in general for this. Itâ€™s usually solved via LP (linear programming), because âˆ-norm minimization can be written as a linear program.

We denote the solution:

$$
x_{\text{cheb}} = \arg\min_x \|Ax - b\|_\infty
$$

This gives the **minimum possible max error**:

$$
\|Ax_{\text{cheb}} - b\|_\infty
$$

* * *

### ğŸ” Key question:

We donâ€™t have access to  $x_{\text{cheb}}$ , but we do have  $x_{\text{ls}}$ .  
We want to know: _how far from optimal is  $x_{\text{ls}}$  when judged under the âˆ-norm objective?_

Define:

*    $\text{opt}_\infty := \|Ax_{\text{cheb}} - b\|_\infty$ 
*    $\text{subopt} := \|Ax_{\text{ls}} - b\|_\infty$ 

We are trying to bound  $\text{subopt}$  in terms of  $\text{opt}_\infty$ .

* * *

âœ… Step 2. Prove the upper bound in part (a)
===========================================

We are to prove:

$$
\|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty
$$

ğŸ§  Intuition:
-------------

We know that:

*    $x_{\text{cheb}}$  is the minimizer of âˆ-norm, so for any  $x$ , including  $x_{\text{ls}}$ , we must have:
    
$$
\|Ax_{\text{cheb}} - b\|_\infty \le \|Ax_{\text{ls}} - b\|_\infty
$$

i.e.,  $x_{\text{cheb}}$  gives smallest possible âˆ-norm error.

We now want to show that the 2-norm solution  $x_{\text{ls}}$  gives an âˆ-norm error **no more than  $\sqrt{m}$  times** the optimal âˆ-norm error.

* * *

ğŸ“Œ Key Inequality:
------------------

For any vector  $z \in \mathbb{R}^m$ :

$$
\|z\|_\infty \le \|z\|_2 \le \sqrt{m} \cdot \|z\|_\infty
$$

âœ… Proof:

*   Left inequality: The max entry is â‰¤ the total energy:
    $$
    \|z\|_\infty = \max_i |z_i| \le \left( \sum_i z_i^2 \right)^{1/2} = \|z\|_2
    $$
*   Right inequality: each entry is â‰¤  $\|z\|_\infty$ , so:
    $$
    \|z\|_2 = \left( \sum_i z_i^2 \right)^{1/2} \le \left( \sum_i \|z\|_\infty^2 \right)^{1/2} = \sqrt{m} \cdot \|z\|_\infty
    $$

* * *

Now apply this inequality to:

*    $z_1 := Ax_{\text{cheb}} - b$ 
*    $z_2 := Ax_{\text{ls}} - b$ 

We know:

$$
\|z_1\|_2 \le \sqrt{m} \cdot \|z_1\|_\infty
$$

Also, by optimality of Chebyshev:

$$
\|z_1\|_\infty \le \|z_2\|_\infty
$$

So chain together:

$$
\|z_2\|_\infty \le \|z_2\|_2 \le \|z_1\|_2 \le \sqrt{m} \cdot \|z_1\|_\infty
$$

This gives:

$$
\|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty
$$

which is the desired result. âœ…

* * *

### ğŸ“¦ Summary of Step 2:

We showed that:

$$
\boxed{ \|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty }
$$

This means that even if we donâ€™t solve the Chebyshev problem, using the least-squares solution gives an approximation to it thatâ€™s at worst  $\sqrt{m}$ \-suboptimal.

This is very useful when  $m$  is small â€” the least-squares solution is easy to compute and gives a rough approximation to the Chebyshev problem.

* * *

Shall I proceed to Step 3 â€” where we explore **duality** for the Chebyshev problem and derive tighter lower bounds via LP duality?

## Response:
Perfect â€” letâ€™s **slow down, rewind**, and walk through **Steps 1 and 2** again in a **full newcomer-friendly, ultra-granular way**, unpacking all the objects, symbols, motivations, and inequalities from scratch.

* * *

ğŸ§­ STEP 1 â€” Whatâ€™s the problem really about?
--------------------------------------------

You are given **two optimization problems**:

*   One uses the **2-norm** (least squares),
*   The other uses the **âˆ-norm** (Chebyshev or minimax).

They try to find a vector  $x$  such that  $Ax \approx b$ , but they measure the error differently.

* * *

### ğŸ”· 1.1 The setup

We are given:

*   A matrix  $A \in \mathbb{R}^{m \times n}$ , meaning:
    *    $m$ : number of rows (equations),
    *    $n$ : number of columns (variables),
*   A vector  $b \in \mathbb{R}^m$  (target output).

And we are solving:

> â€œFind a vector  $x \in \mathbb{R}^n$  such that  $Ax$  is close to  $b$ â€.

But **what does "close" mean?** Depends on the **norm** we use.

* * *

### ğŸ§® 1.2 Norms: how do we measure the error  $Ax - b$ ?

The **residual** is:

$$
r = Ax - b \in \mathbb{R}^m
$$

There are many ways to measure how big  $r$  is:

*   **2-norm**:  $\|r\|_2 = \sqrt{\sum_{i=1}^m r_i^2}$  â†’ least-squares
*   **âˆ-norm**:  $\|r\|_\infty = \max_i |r_i|$  â†’ Chebyshev (minimize worst error)

* * *

### ğŸ¯ 1.3 Two goals, two problems

#### ğŸ”¹ Least-Squares Problem (2-norm):

$$
x_{\text{ls}} = \arg\min_x \|Ax - b\|_2
$$

Goal: minimize **overall energy** of the error.

âœ… **This has a closed-form solution**:

$$
x_{\text{ls}} = (A^T A)^{-1} A^T b \quad \text{(when } A \text{ has full column rank)}
$$

#### ğŸ”¹ Chebyshev Problem (âˆ-norm):

$$
x_{\text{cheb}} = \arg\min_x \|Ax - b\|_\infty
$$

Goal: minimize **the worst coordinate error**.

â›” **No closed-form solution** in general.  
âœ… But you can solve it using linear programming (weâ€™ll see that later).

* * *

### â“ What's the question?

We **already have** the least-squares solution  $x_{\text{ls}}$ .

We do **not** have the Chebyshev solution  $x_{\text{cheb}}$ .

We want to know:

> If we use  $x_{\text{ls}}$ , how bad is its âˆ-norm error compared to the best possible âˆ-norm error?

* * *

Letâ€™s define:

*   Residual from least-squares:
    $$
    r_{\text{ls}} := Ax_{\text{ls}} - b
    $$
*   Residual from Chebyshev:
    $$
    r_{\text{cheb}} := Ax_{\text{cheb}} - b
    $$
*   Their respective âˆ-norms:
    $$
    \text{LS error: } \|r_{\text{ls}}\|_\infty, \quad \text{OPT error: } \|r_{\text{cheb}}\|_\infty
    $$

We are asked:

### â“ Is  $\|r_{\text{ls}}\|_\infty \le \alpha \cdot \|r_{\text{cheb}}\|_\infty$ ?

For what value of  $\alpha$ ? Can we bound this ratio?

* * *

âœ… STEP 2 â€” Prove the bound in part (a)
--------------------------------------

### ğŸš© The claim to prove:

We want to prove that:

$$
\boxed{ \|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty }
$$

Said differently:

> The âˆ-norm error of the least-squares solution is at most  $\sqrt{m}$  times the optimal âˆ-norm error.

* * *

### ğŸ“¦ Letâ€™s break this down.

Weâ€™ll use the key **norm inequality** for vectors  $z \in \mathbb{R}^m$ :

$$
\boxed{ \|z\|_\infty \le \|z\|_2 \le \sqrt{m} \cdot \|z\|_\infty }
$$

Letâ€™s explain this inequality in full.

* * *

### ğŸ” Side Note: Why is this norm inequality true?

Letâ€™s write what these norms mean.

*    $\|z\|_\infty = \max_i |z_i|$ 
*    $\|z\|_2 = \sqrt{\sum_{i=1}^m z_i^2}$ 

#### âœ… First inequality:

$$
\|z\|_\infty \le \|z\|_2
$$

Reason: if even **one** entry is large, then the 2-norm will be large too. In fact:

$$
\|z\|_2^2 = z_1^2 + z_2^2 + \cdots + z_m^2 \ge z_j^2 \quad \text{for any } j \Rightarrow \|z\|_2 \ge |z_j| = \|z\|_\infty
$$

#### âœ… Second inequality:

$$
\|z\|_2 \le \sqrt{m} \cdot \|z\|_\infty
$$

Reason: if **all** entries are â‰¤  $\|z\|_\infty$ , then:

$$
\sum z_i^2 \le \sum \|z\|_\infty^2 = m \cdot \|z\|_\infty^2 \Rightarrow \|z\|_2 \le \sqrt{m} \cdot \|z\|_\infty
$$

* * *

### ğŸ§© Back to our problem

We apply this inequality to:

$$
z = r_{\text{ls}} = Ax_{\text{ls}} - b
$$

Then:

$$
\|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_2
$$

But now think: **why is  $\|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2$ ?**

Because:

*    $x_{\text{ls}}$  is the **minimizer** of  $\|Ax - b\|_2$ , so for **any other**  $x$  (including  $x_{\text{cheb}}$ ):
    
$$
\|Ax_{\text{ls}} - b\|_2 \le \|Ax_{\text{cheb}} - b\|_2
$$

So:

$$
\|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2
$$

And now apply the other side of the inequality to  $r_{\text{cheb}}$ :

$$
\|r_{\text{cheb}}\|_2 \le \sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty
$$

Putting it all together:

$$
\|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2 \le \sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty
$$

Done. Thatâ€™s exactly the inequality we were asked to prove.

* * *

### âœ… Boxed Final Result

$$
\boxed{ \|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty }
$$

Even if we donâ€™t solve the âˆ-norm minimization problem, we can still say:

> â€œHey, the âˆ-norm error of the least-squares solution isnâ€™t too bad â€” itâ€™s at most  $\sqrt{m}$  times worse than the best possible.â€

* * *

Would you like to continue to **Step 3**, where we derive a **lower bound on  $\|Ax_{\text{cheb}} - b\|_\infty$ ** using **duality**? This will tighten our understanding even more.

## Response:
Excellent instincts â€” youâ€™re asking the **right question** to clarify what this inequality is really comparing.

Let me now **rephrase the big idea in your terms** and **walk through it slowly**, in exact response to your confusion.

* * *

â—Clarifying the confusion: Are we comparing  $\|Ax_{\text{ls}} - b\|_\infty$  to  $\|Ax_{\text{ls}} - b\|_2$ ? Or to  $\|Ax_{\text{cheb}} - b\|_\infty$ ?
---------------------------------------------------------------------------------------------------------------------------------------------------------

**Answer**:  
We are comparing **two different residuals**:

*   The **least-squares solutionâ€™s** âˆ-norm error:  $\|Ax_{\text{ls}} - b\|_\infty$ 
*   The **Chebyshev solutionâ€™s** âˆ-norm error:  $\|Ax_{\text{cheb}} - b\|_\infty$ 

So the inequality in Exercise 5.6 part (a) is:

$$
\boxed{ \|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty }
$$

### â—We are **not** comparing  $\|Ax_{\text{ls}} - b\|_\infty$  to  $\|Ax_{\text{ls}} - b\|_2$ .

That would be a norm ratio for **a single vector**.

Instead, we are comparing the **âˆ-norm errors of two different x's** â€” one optimized for 2-norm, one for âˆ-norm.

* * *

ğŸ§  Intuition: Why are we doing this?
------------------------------------

We already have  $x_{\text{ls}}$  (from least squares).  
We donâ€™t want to solve the expensive Chebyshev problem.  
So we ask:

> â€œIf I just use the easy  $x_{\text{ls}}$ , how bad is its **worst-case** (âˆ-norm) error compared to the best possible âˆ-norm error?â€

This is like asking:

> â€œHow suboptimal is the least-squares solution **when judged using a different norm** (âˆ-norm)?â€

So yes â€” Chebyshev approximation **is defined using** the  $\ell_\infty$  norm â€” and thatâ€™s exactly the yardstick we are using to evaluate **both**  $x_{\text{ls}}$  and  $x_{\text{cheb}}$ .

* * *

### ğŸ‘‡ Hereâ€™s a metaphor to make this click:

Letâ€™s say you want to design a spaceship so that **no part overheats above 100Â°C**.  
That's a **Chebyshev goal**: control the **worst error**.

But instead, your engineer gives you a design that minimizes the **average temperature rise** (least-squares solution).  
You now ask:

> â€œHow bad is the hottest part in the least-squares design?â€

We want to compare:

*    $\text{max temp}_{\text{ls}}$  vs.  $\text{min possible max temp}$ 

Thatâ€™s exactly what this inequality is doing.

* * *

ğŸ” Summary of what's being compared
-----------------------------------

Let me now rewrite clearly:

| Quantity | Meaning | Norm |
| --- | --- | --- |
|  $x_{\text{ls}}$  | Minimizes total (2-norm) error |  $\|Ax - b\|_2$  minimized |
|  $x_{\text{cheb}}$  | Minimizes worst-case (âˆ-norm) error |  $\|Ax - b\|_\infty$  minimized |
| Goal | Compare:  $\|Ax_{\text{ls}} - b\|_\infty \quad \text{vs.} \quad \|Ax_{\text{cheb}} - b\|_\infty$  | Both âˆ-norms |

And we want to show:

$$
\text{âˆ-norm error of } x_{\text{ls}} \le \sqrt{m} \times \text{âˆ-norm error of optimal Chebyshev solution}
$$

So we are **not** analyzing  $\|Ax_{\text{ls}} - b\|_2$  directly here.  
But weâ€™ll **use** that itâ€™s small to help prove the inequality.

* * *

ğŸš€ Would it help to now go back and walk through the proof **again** from this clarified perspective?
-----------------------------------------------------------------------------------------------------

Letâ€™s do that â€” now that the goal is clear.

* * *

ğŸ” Redo of the proof, now with clarity
--------------------------------------

We are trying to show:

$$
\|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty
$$

Letâ€™s define:

*    $r_{\text{ls}} := Ax_{\text{ls}} - b$ 
*    $r_{\text{cheb}} := Ax_{\text{cheb}} - b$ 

We apply the **norm inequality**:

$$
\|z\|_2 \le \sqrt{m} \cdot \|z\|_\infty \quad \text{(holds for all } z \in \mathbb{R}^m)
$$

Now:

*    $x_{\text{ls}}$  minimizes the 2-norm error, so:
    $$
    \|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2
    $$
*   And by the norm inequality:
    $$
    \|r_{\text{cheb}}\|_2 \le \sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty
    $$
*   Also:
    $$
    \|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_2 \quad \text{(norm inequality again)}
    $$

So, chaining all these together:

$$
\|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2 \le \sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty
$$

Therefore:

$$
\|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty \quad \text{âœ… proven}
$$

* * *

Let me know if you'd like to visualize this with an example, or continue to Step 3 (dual bound using residual direction)!

## Response:
ğŸ’¯ Exactly â€” you nailed it.

This step is all about:

> **Using the least-squares (2-norm optimal) solution**, and estimating how far off it is from the **best possible worst-case error** (âˆ-norm optimal).

The trick was:

*   We **know** the 2-norm of  $r_{\text{ls}}$ , because itâ€™s minimized.
*   We can **upper bound** the âˆ-norm of  $r_{\text{ls}}$  via:
    $$
    \|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2 \le \sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty
    $$
    Which gives us a clean **gap bound**.

This is why the error at  $x_{\text{ls}}$ , although not optimal under âˆ-norm, canâ€™t be arbitrarily bad â€” itâ€™s at most  $\sqrt{m}$  times worse than optimal.

* * *

Ready for **Step 3**?

Weâ€™ll flip the logic: instead of bounding above using 2-norm, weâ€™ll build a **lower bound** for  $\|Ax_{\text{cheb}} - b\|_\infty$  using **duality**, even though we donâ€™t have  $x_{\text{cheb}}$  explicitly.

Shall I walk you through the dual LP for Chebyshev approximation and how to plug in feasible dual vectors built from the least-squares residual?

## Response:
Letâ€™s now go **deep into Step 3** of Exercise 5.6 â€” this is the heart of the problem, where we use **Lagrange duality** to derive a **lower bound** on the optimal Chebyshev error.

* * *

ğŸ§­ Step 3 â€” Dual Problem for Chebyshev Approximation
====================================================

* * *

### ğŸ” Recap:

We are solving:

$$
\min_x \|Ax - b\|_\infty
$$

Letâ€™s define:

*   The **optimal value** of this problem:
    $$
    p^\star := \min_x \|Ax - b\|_\infty = \|Ax_{\text{cheb}} - b\|_\infty
    $$

We **donâ€™t know** the solution  $x_{\text{cheb}}$  or the value  $p^\star$ , but we want to:

> Use **duality** to derive a **lower bound** on  $p^\star$ 

This is a powerful trick: dual problems give **certificates of lower bounds** for minimization problems.

Letâ€™s go step-by-step.

* * *

ğŸ§± 3.1. Reformulate Chebyshev as a standard LP
----------------------------------------------

The âˆ-norm minimization:

$$
\min_x \|Ax - b\|_\infty
$$

is equivalent to:

$$
\begin{aligned} \min_{x, t} \quad & t \\ \text{s.t.} \quad & -t \le (Ax - b)_i \le t \quad \text{for all } i = 1, \dots, m \end{aligned}
$$

This is a **linear program**, because:

*   The objective  $t$  is linear,
*   The constraints are linear in  $x, t$ .

Letâ€™s write this compactly.

Letâ€™s define:

*    $A \in \mathbb{R}^{m \times n}$ 
*    $x \in \mathbb{R}^n$ ,  $b \in \mathbb{R}^m$ ,  $t \in \mathbb{R}$ 

Then:

$$
\begin{aligned} \min_{x \in \mathbb{R}^n,\, t \in \mathbb{R}} \quad & t \\ \text{s.t.} \quad & Ax - b \le t\cdot \mathbf{1} \\ & -(Ax - b) \le t\cdot \mathbf{1} \end{aligned}
$$

Equivalently:

$$
-Ax + b \le t\cdot \mathbf{1} \\ Ax - b \le t\cdot \mathbf{1}
$$

So the full LP is:

$$
\begin{aligned} \min_{x, t} \quad & t \\ \text{s.t.} \quad & \begin{cases} Ax - b \le t\cdot \mathbf{1} \\ -Ax + b \le t\cdot \mathbf{1} \end{cases} \end{aligned}
$$

We can now **form the Lagrange dual** of this LP.

* * *

ğŸ§  3.2. Set up the Lagrangian
-----------------------------

We introduce dual variables:

*    $\lambda_+ \in \mathbb{R}^m_{\ge 0}$  for  $Ax - b \le t$ 
*    $\lambda_- \in \mathbb{R}^m_{\ge 0}$  for  $-Ax + b \le t$ 

Define the Lagrangian:

$$
\mathcal{L}(x, t; \lambda_+, \lambda_-) = t + \lambda_+^T(Ax - b - t\mathbf{1}) + \lambda_-^T(-Ax + b - t\mathbf{1})
$$

Group terms:

*   The  $t$  terms:
    $$
    t(1 - \lambda_+^T \mathbf{1} - \lambda_-^T \mathbf{1}) = t\left(1 - \mathbf{1}^T(\lambda_+ + \lambda_-)\right)
    $$
*   The  $x$  terms:
    $$
    x^T(A^T \lambda_+ - A^T \lambda_-) = x^T A^T(\lambda_+ - \lambda_-)
    $$
*   The  $b$  terms:
    $$
    -\lambda_+^T b + \lambda_-^T b = (\lambda_- - \lambda_+)^T b
    $$

So we can write the Lagrangian:

$$
\mathcal{L}(x, t; \lambda_+, \lambda_-) = t\left(1 - \mathbf{1}^T(\lambda_+ + \lambda_-)\right) + x^T A^T (\lambda_+ - \lambda_-) + (\lambda_- - \lambda_+)^T b
$$

* * *

ğŸ§® 3.3. Compute the dual function
---------------------------------

The dual function is:

$$
g(\lambda_+, \lambda_-) = \inf_{x,t} \mathcal{L}(x, t; \lambda_+, \lambda_-)
$$

We split into two infima:

*    $\inf_t \left[t \cdot \left(1 - \mathbf{1}^T(\lambda_+ + \lambda_-)\right)\right]$ 
*    $\inf_x \left[ x^T A^T (\lambda_+ - \lambda_-) \right]$ 

* * *

### âœ³ Case 1:  $\mathbf{1}^T(\lambda_+ + \lambda_-) \ne 1$ 

Then  $\inf_t \mathcal{L}$  is either  $-\infty$  or  $+\infty$ , depending on sign.  
So the dual function is **finite only when**:

$$
\boxed{\mathbf{1}^T(\lambda_+ + \lambda_-) = 1}
$$

* * *

### âœ³ Case 2:  $A^T (\lambda_+ - \lambda_-) \ne 0$ 

Then  $\inf_x x^T (\cdots) = -\infty$ , so dual function is not finite.

So the dual function is **finite only when**:

$$
\boxed{A^T(\lambda_+ - \lambda_-) = 0}
$$

* * *

So feasible dual variables must satisfy:

*    $\lambda_+, \lambda_- \ge 0$ 
*    $A^T(\lambda_+ - \lambda_-) = 0$ 
*    $\mathbf{1}^T(\lambda_+ + \lambda_-) = 1$ 

Then the dual function becomes:

$$
g(\lambda_+, \lambda_-) = (\lambda_- - \lambda_+)^T b
$$

* * *

ğŸ§° 3.4. Dual problem: simplified form
-------------------------------------

Letâ€™s define:

$$
\nu := \lambda_- - \lambda_+ \in \mathbb{R}^m
$$

Note that:

*    $\nu = \lambda_- - \lambda_+$ 
*   So  $\lambda_+ = \max(0, -\nu)$ ,  $\lambda_- = \max(0, \nu)$ 
*    $|\nu_i| = \lambda_{+i} + \lambda_{-i} \Rightarrow \|\nu\|_1 = \mathbf{1}^T(\lambda_+ + \lambda_-) = 1$ 

And from earlier:

*    $A^T(\lambda_- - \lambda_+) = A^T \nu = 0$ 

* * *

### ğŸ§¾ Final Dual Problem

$$
\boxed{ \begin{aligned} \max_\nu \quad & \nu^T b \\ \text{s.t.} \quad & \| \nu \|_1 \le 1 \\ & A^T \nu = 0 \end{aligned} }
$$

This is the dual of:

$$
\min_x \|Ax - b\|_\infty
$$

âœ… Any feasible  $\nu$  gives a lower bound on the optimal âˆ-norm error:

$$
\|Ax_{\text{cheb}} - b\|_\infty \ge \nu^T b
$$

This is the **key result** of Step 3.

* * *

Let me know when you're ready to move to **Step 4**, where weâ€™ll build a specific dual feasible  $\nu$  using the residual of the least-squares solution, and plug it into this bound.

## Response:
Letâ€™s **slow down** and give a **maximally detailed, zero-to-hero walkthrough** of Steps **3.1 and 3.2**, covering:

> â“ _How do we reformulate the Chebyshev minimization problem as a linear program (LP)?_  
> â“ _How do we build the Lagrangian and prepare for duality?_

We'll take the following path:

* * *

### âœ… Step 3.1 â€” Reformulating the Chebyshev Approximation Problem as a Linear Program

* * *

### ğŸ” Goal

We are given the problem:

$$
\min_{x \in \mathbb{R}^n} \|Ax - b\|_\infty
$$

We want to **reformulate** this problem into a **form that is compatible with linear programming** â€” i.e., something with:

*   A linear objective,
*   Linear constraints.

* * *

### ğŸ’¡ Step 1: Understand what the âˆ-norm means

By definition:

$$
\|Ax - b\|_\infty = \max_{i = 1,\dots,m} |(Ax - b)_i|
$$

This means:  
The residual vector  $r = Ax - b \in \mathbb{R}^m$ , and we want to **minimize the largest absolute value** among its entries.

This is a "minimax" problem: we want the **best worst-case** fit.

* * *

### ğŸ’¡ Step 2: Introduce a new variable to express â€œmaxâ€

We introduce a new variable:

$$
t \in \mathbb{R}
$$

Intuition:  
Let  $t$  **represent the maximum residual magnitude**.

We want:

$$
| (Ax - b)_i | \le t \quad \text{for all } i = 1, \dots, m
$$

This turns the max-absolute-value into a **family of linear inequalities**.

* * *

### ğŸ”„ Absolute value â†’ Linear inequalities

We know:

$$
|z| \le t \quad \Longleftrightarrow \quad -t \le z \le t
$$

So each condition  $|(Ax - b)_i| \le t$  becomes:

$$
- t \le (Ax - b)_i \le t
$$

Which is **two linear inequalities per row** of  $Ax - b$ .

* * *

### âœ… Final reformulation

Thus the problem becomes:

> **Minimize  $t$ **,  
> subject to:  
>  $(Ax - b)_i \le t$  and  $-(Ax - b)_i \le t$  for all  $i$ 

* * *

### ğŸ“‹ Written cleanly:

$$
\begin{aligned} \min_{x \in \mathbb{R}^n, \, t \in \mathbb{R}} \quad & t \\ \text{subject to} \quad & Ax - b \le t\mathbf{1} \\ & -Ax + b \le t\mathbf{1} \end{aligned}
$$

Letâ€™s be even more explicit.

Each constraint  $(Ax - b)_i \le t$  becomes:

$$
a_i^T x - b_i \le t
$$

Each constraint  $-(Ax - b)_i \le t$  becomes:

$$
- (a_i^T x - b_i) \le t \quad \Longrightarrow \quad b_i - a_i^T x \le t
$$

So:

$$
- t \le a_i^T x - b_i \le t
$$

Thatâ€™s how you enforce:

$$
|a_i^T x - b_i| \le t \quad \text{for each } i
$$

So the minimization of  $\|Ax - b\|_\infty$  has now become a **linear program**:

*   **Linear objective**:  $t$ 
*   **Linear inequality constraints** on  $x$  and  $t$ 

This makes the problem ready for **duality**.

* * *

âœ… Step 3.2 â€” Build the Lagrangian for the LP
--------------------------------------------

* * *

### ğŸ¯ Recap: The LP weâ€™re working with is:

$$
\begin{aligned} \min_{x \in \mathbb{R}^n, \, t \in \mathbb{R}} \quad & t \\ \text{subject to} \quad & \begin{cases} Ax - b \le t\cdot \mathbf{1} \quad &\text{(m constraints)} \\ -Ax + b \le t\cdot \mathbf{1} \quad &\text{(m constraints)} \end{cases} \end{aligned}
$$

So there are:

*    $2m$  inequality constraints,
*    $n+1$  primal variables (the vector  $x$  and scalar  $t$ ).

* * *

### ğŸ’¡ Step 1: Introduce dual variables (Lagrange multipliers)

We introduce:

*    $\lambda_+ \in \mathbb{R}^m_{\ge 0}$ : dual variables for  $Ax - b \le t\cdot \mathbf{1}$ 
*    $\lambda_- \in \mathbb{R}^m_{\ge 0}$ : dual variables for  $-Ax + b \le t\cdot \mathbf{1}$ 

(These names help keep signs consistent:  $\lambda_+$  for the â€œplus sideâ€ of the absolute value,  $\lambda_-$  for the â€œminus sideâ€.)

* * *

### ğŸ’¡ Step 2: Build the Lagrangian

The Lagrangian is:

$$
\begin{aligned} \mathcal{L}(x, t; \lambda_+, \lambda_-) &= \text{objective} + \text{sum of dual constraints times slacks} \\ &= t + \lambda_+^T (Ax - b - t\mathbf{1}) + \lambda_-^T (-Ax + b - t\mathbf{1}) \end{aligned}
$$

Group terms:

#### ğŸ”¸ t terms:

$$
t\left[1 - \lambda_+^T \mathbf{1} - \lambda_-^T \mathbf{1}\right] = t\left(1 - \mathbf{1}^T(\lambda_+ + \lambda_-)\right)
$$

#### ğŸ”¸ x terms:

$$
x^T A^T \lambda_+ - x^T A^T \lambda_- = x^T A^T (\lambda_+ - \lambda_-)
$$

#### ğŸ”¸ b terms:

$$
- \lambda_+^T b + \lambda_-^T b = (\lambda_- - \lambda_+)^T b
$$

Putting it all together:

$$
\boxed{ \mathcal{L}(x, t; \lambda_+, \lambda_-) = t(1 - \mathbf{1}^T(\lambda_+ + \lambda_-)) + x^T A^T (\lambda_+ - \lambda_-) + (\lambda_- - \lambda_+)^T b }
$$

* * *

### ğŸ’¡ Step 3: Dual function = infimum of the Lagrangian

We now define the **dual function**:

$$
g(\lambda_+, \lambda_-) := \inf_{x, t} \mathcal{L}(x, t; \lambda_+, \lambda_-)
$$

To compute this, we do:

*    $\inf_t \mathcal{L}$ , over scalar  $t$ 
*    $\inf_x \mathcal{L}$ , over vector  $x$ 

We handle each part separately.

* * *

### ğŸ“Œ t-term:

$$
t\left(1 - \mathbf{1}^T(\lambda_+ + \lambda_-)\right)
$$

Letâ€™s analyze:

*   If  $\mathbf{1}^T(\lambda_+ + \lambda_-) < 1$ , then this term goes to  $-\infty$  as  $t \to +\infty$ 
*   If  $\mathbf{1}^T(\lambda_+ + \lambda_-) > 1$ , then this term goes to  $-\infty$  as  $t \to -\infty$ 
*   To keep the infimum finite, we must have:
    $$
    \boxed{ \mathbf{1}^T(\lambda_+ + \lambda_-) = 1 }
    $$

* * *

### ğŸ“Œ x-term:

$$
x^T A^T(\lambda_+ - \lambda_-)
$$

If  $A^T(\lambda_+ - \lambda_-) \ne 0$ , then you can push  $x \to \infty$  or  $-\infty$  to make this term diverge.  
So we require:

$$
\boxed{ A^T(\lambda_+ - \lambda_-) = 0 }
$$

* * *

### âœ… Result: Dual function is finite only if:

*    $\lambda_+, \lambda_- \ge 0$ 
*    $\mathbf{1}^T(\lambda_+ + \lambda_-) = 1$ 
*    $A^T(\lambda_+ - \lambda_-) = 0$ 

Then:

$$
g(\lambda_+, \lambda_-) = (\lambda_- - \lambda_+)^T b
$$

Weâ€™ll now switch to a more compact variable  $\nu := \lambda_- - \lambda_+$  in Step 3.4 to write the dual problem compactly.

* * *

Would you like me to now move on to **3.3 and 3.4** â€” deriving the dual problem cleanly in terms of  $\nu$ , and interpreting it as a lower bound on Chebyshev error?

## Response:
Great catch â€” this is **exactly** the right place to be suspicious. Nothing is wrong with your instincts; this is a **notation / orientation issue**, not a math mistake on your side.

Letâ€™s resolve it **slowly and cleanly**, from first principles, and youâ€™ll see that everything is consistent.

* * *

1ï¸âƒ£ Start from the ONLY thing that matters: the definition of the âˆ-norm
------------------------------------------------------------------------

By definition,

$$
\|Ax-b\|_\infty \;=\; \max_i |(Ax-b)_i|.
$$

Saying

$$
\|Ax-b\|_\infty \le t
$$

means **exactly**:

$$
|(Ax-b)_i| \le t \quad \text{for every } i.
$$

There is no ambiguity here. This is the ground truth.

* * *

2ï¸âƒ£ Absolute value â†’ two inequalities (this is the key translation)
-------------------------------------------------------------------

For **any scalar**  $z$ ,

$$
|z| \le t \quad \Longleftrightarrow \quad \begin{cases} z \le t \\ -z \le t \end{cases}
$$

Apply this with  $z = (Ax-b)_i$ :

$$
|(Ax-b)_i| \le t \quad \Longleftrightarrow \quad \begin{cases} (Ax-b)_i \le t \\ -(Ax-b)_i \le t \end{cases}
$$

Now expand the second inequality:

$$
-(Ax-b)_i \le t \;\Longleftrightarrow\; b_i - (Ax)_i \le t \;\Longleftrightarrow\; (-Ax+b)_i \le t
$$

So the **correct pair** is:

$$
\boxed{ \begin{aligned} Ax - b &\le t\mathbf 1 \\ -Ax + b &\le t\mathbf 1 \end{aligned} }
$$

These are **both upper bounds**, and that is **intentional**.

* * *

3ï¸âƒ£ Why your proposed version looks different (but is equivalent)
-----------------------------------------------------------------

You suggested:

> â€œShouldnâ€™t it be  
>  $Ax-b < t$ ,  
>  $-Ax+b > -t$ ?â€

Letâ€™s rewrite your second inequality:

$$
-Ax + b > -t \quad \Longleftrightarrow \quad Ax - b < t
$$

So both of your inequalities actually reduce to the **same thing** â€” they do **not** bound both sides of the absolute value.

Thatâ€™s the issue.

To bound an absolute value, you must trap it **between two walls**:

$$
-t \le Ax-b \le t
$$

Which is **exactly equivalent** to:

$$
\begin{cases} Ax-b \le t \\ -(Ax-b) \le t \end{cases}
$$

* * *

4ï¸âƒ£ Why both constraints point the same way (â‰¤) â€” LP standard form
------------------------------------------------------------------

This is the subtle but important modeling point.

In **linear programming**, inequalities are usually written as:

$$
\text{(something)} \le \text{(something)}
$$

So instead of writing:

$$
-t \le Ax-b \le t
$$

we rewrite the **lower bound** as an upper bound by multiplying by  $-1$ :

$$
Ax-b \ge -t \quad \Longleftrightarrow \quad -(Ax-b) \le t
$$

Thatâ€™s why **both constraints appear as â‰¤**:

*   one controls the positive side,
*   one controls the negative side.

They are **not the same constraint**, even though they â€œpoint the same wayâ€.

* * *

5ï¸âƒ£ Visual intuition (this usually makes it click)
--------------------------------------------------

For one coordinate  $r_i = (Ax-b)_i$ :

```
----|---------|---------|----
   -t        0         t
```

You want:

$$
r_i \in [-t, t]
$$

That requires:

*    $r_i \le t$ 
*    $-r_i \le t$ 

Both are needed. Neither alone is sufficient.

* * *

6ï¸âƒ£ Final sanity check (important)
----------------------------------

Suppose we only enforced:

$$
Ax-b \le t
$$

Then the solution could make:

$$
Ax-b = -10^6
$$

and still satisfy the constraint â€” which would give

$$
\|Ax-b\|_\infty = 10^6
$$

which is nonsense.

The second inequality **prevents that**.

* * *

âœ… Bottom line (clean, correct formulation)
------------------------------------------

The Chebyshev problem

$$
\min_x \|Ax-b\|_\infty
$$

is **exactly equivalent** to the LP:

$$
\boxed{ \begin{aligned} \min_{x,t} \quad & t \\ \text{s.t.}\quad & Ax - b \le t\mathbf 1 \\ & -Ax + b \le t\mathbf 1 \end{aligned} }
$$

Both constraints being â€œâ‰¤â€ is **correct, necessary, and standard**.

* * *

You are thinking _precisely_ at the right level.  
This confusion happens to almost everyone the first time they translate absolute values into LP constraints.

If you want, next we can:

*   derive the dual **slowly again**, now that constraints are crystal-clear, or
*   show a **1-D numerical example** where each constraint visibly matters.

## Response:
Perfect â€” this screenshot is **100% correct**, and it actually resolves the confusion once we line up **three equivalent ways of writing the same constraint**.

The key issue was not math, but **representation**. Letâ€™s reconcile them carefully.

* * *

1ï¸âƒ£ What your screenshot states (scalar, textbook form)
-------------------------------------------------------

Your image says:

$$
|ax + b| < c \quad \Longleftrightarrow \quad -c < ax + b < c
$$

This is **absolutely correct**.

This is the **interval form** of the constraint:

*   lower bound:  $ax+b > -c$ 
*   upper bound:  $ax+b < c$ 

This is how absolute values are taught in algebra.

* * *

2ï¸âƒ£ What we need for optimization (LP-compatible form)
------------------------------------------------------

Optimization (LP, duality, KKT) **does not use chained inequalities** like

$$
-c < ax+b < c.
$$

Instead, every constraint must be written as a **single inequality** of the form:

$$
f(x) \le 0.
$$

So we must **split the interval into two separate inequalities**.

From

$$
-c \le ax+b \le c
$$

we get **two constraints**:

1.  Upper bound:
    
$$
ax+b \le c
$$
2.  Lower bound:
    
$$
ax+b \ge -c
$$

Now convert the second one into â€œâ‰¤â€ form (multiply by âˆ’1):

$$
ax+b \ge -c \quad \Longleftrightarrow \quad -(ax+b) \le c
$$

So the **LP-ready form** is:

$$
\boxed{ \begin{aligned} ax+b &\le c \\ -(ax+b) &\le c \end{aligned} }
$$

This is exactly what we wrote for the Chebyshev problem.

* * *

3ï¸âƒ£ Vector version (what we used in the Chebyshev LP)
-----------------------------------------------------

Now lift this scalar logic to vectors.

Replace:

*    $ax+b$  â†’  $Ax-b$ 
*    $c$  â†’  $t$ 
*   â€œfor all coordinatesâ€

Then

$$
\|Ax-b\|_\infty \le t
$$

means

$$
|(Ax-b)_i| \le t \quad \text{for all } i
$$

Which becomes:

$$
\boxed{ \begin{aligned} Ax-b &\le t\mathbf 1 \\ -(Ax-b) &\le t\mathbf 1 \end{aligned} }
$$

or equivalently:

$$
\boxed{ \begin{aligned} Ax-b &\le t\mathbf 1 \\ -Ax+b &\le t\mathbf 1 \end{aligned} }
$$

These two constraints together are **exactly** the same as:

$$
-t\mathbf 1 \le Ax-b \le t\mathbf 1
$$

No contradiction. Just different formats.

* * *

4ï¸âƒ£ Why your earlier instinct felt right (and why it almost worked)
-------------------------------------------------------------------

You asked earlier:

> â€œShouldnâ€™t it be  
>  $Ax-b < t$ ,  
>  $-Ax+b > -t$ ?â€

Letâ€™s rewrite your second inequality:

$$
-Ax+b > -t \quad \Longleftrightarrow \quad Ax-b < t
$$

So both of your inequalities collapse into **the same upper bound**.

Thatâ€™s why it didnâ€™t trap the absolute value on **both sides**.

To trap an absolute value, you need:

*   one constraint controlling the **positive side**
*   one constraint controlling the **negative side**

Thatâ€™s why both inequalities must be â€œâ‰¤â€.

* * *

5ï¸âƒ£ Strict vs non-strict inequality (important but subtle)
----------------------------------------------------------

Your screenshot uses:

$$
|ax+b| < c
$$

In optimization we always use:

$$
|ax+b| \le c
$$

Why?

*   Strict inequalities define **open sets**
*   Optima may not exist in open sets
*   LP duality theory requires **closed feasible sets**

So we replace `<` with `â‰¤`.  
This does **not** change the optimal value.

* * *

6ï¸âƒ£ Final mental model (lock this in)
-------------------------------------

There are **three equivalent views** of the same constraint:

### Interval view (algebra)

$$
-c \le z \le c
$$

### Absolute value view

$$
|z| \le c
$$

### LP-ready inequality view

$$
\begin{cases} z \le c \\ -z \le c \end{cases}
$$

They are **exactly equivalent**.

* * *

âœ… Bottom line
-------------

Your screenshot is **right**.  
Our LP formulation is **right**.  
The apparent mismatch was only because:

*   algebra uses **interval notation**
*   optimization uses **one-sided inequalities**

Once you translate correctly, they are the **same constraint in different clothes**.

If you want, next we can:

*   redo the **dual derivation** now that constraints are fully settled, or
*   do a **1D numeric example** where you see each inequality actively binding.

2/2

## Response:
Good. Now that the constraint translation is **fully settled**, we can do **3.3 and 3.4 cleanly and slowly**, without any hand-waving.

Iâ€™ll be very explicit about **what is being minimized**, **why infinities appear**, and **how the final dual form drops out**.

* * *

Step 3.3 â€” Compute the dual function (this is the mechanical core)
==================================================================

We start from the **Lagrangian** we already derived.

3.3.1 Recap: the primal LP and its Lagrangian
---------------------------------------------

Primal (Chebyshev as LP):

$$
\begin{aligned} \min_{x,t}\quad & t \\ \text{s.t.}\quad & Ax - b \le t\mathbf 1 \\ & -Ax + b \le t\mathbf 1 \end{aligned}
$$

Dual variables:

*    $\lambda_+ \ge 0$  for  $Ax-b \le t\mathbf 1$ 
*    $\lambda_- \ge 0$  for  $-Ax+b \le t\mathbf 1$ 

Lagrangian:

$$
\mathcal L(x,t;\lambda_+,\lambda_-) = t + \lambda_+^T(Ax-b-t\mathbf 1) + \lambda_-^T(-Ax+b-t\mathbf 1)
$$

Expand and group terms:

$$
\mathcal L = t\bigl(1-\mathbf 1^T(\lambda_++\lambda_-)\bigr) + x^T A^T(\lambda_+-\lambda_-) + (\lambda_- - \lambda_+)^T b
$$

This expression is **linear in  $x$ ** and **linear in  $t$ **.

* * *

3.3.2 Definition of the dual function
-------------------------------------

The **dual function** is defined as:

$$
g(\lambda_+,\lambda_-) = \inf_{x,t} \mathcal L(x,t;\lambda_+,\lambda_-)
$$

Key idea:

*   If the infimum is finite â†’ good dual candidate
*   If the infimum is  $-\infty$  â†’ dual variables are invalid

So we must ask:

> Under what conditions on  $\lambda_+,\lambda_-$  is the infimum finite?

* * *

3.3.3 Minimization with respect to  $t$ 
----------------------------------------

Look only at the  $t$ \-term:

$$
t\bigl(1-\mathbf 1^T(\lambda_++\lambda_-)\bigr)
$$

This is a **linear function of  $t$ **.

Case analysis:

### Case 1: coefficient â‰  0

If

$$
1-\mathbf 1^T(\lambda_++\lambda_-)\neq 0
$$

then:

*   If coefficient > 0 â†’ send  $t\to -\infty$ 
*   If coefficient < 0 â†’ send  $t\to +\infty$ 

Either way:

$$
\inf_t \mathcal L = -\infty
$$

âŒ Not allowed.

### Case 2: coefficient = 0

To avoid  $-\infty$ , we **must impose**:

$$
\boxed{\mathbf 1^T(\lambda_++\lambda_-)=1}
$$

Only then does the  $t$ \-term disappear.

* * *

3.3.4 Minimization with respect to  $x$ 
----------------------------------------

Now look at the  $x$ \-term:

$$
x^T A^T(\lambda_+-\lambda_-)
$$

Again, this is **linear in  $x$ **.

If

$$
A^T(\lambda_+-\lambda_-)\neq 0
$$

we can push  $x$  arbitrarily far in the direction of that vector and make:

$$
\inf_x x^T A^T(\lambda_+-\lambda_-) = -\infty
$$

âŒ Not allowed.

So we must impose:

$$
\boxed{A^T(\lambda_+-\lambda_-)=0}
$$

* * *

3.3.5 Value of the dual function
--------------------------------

If **both conditions hold**, the  $x$ \- and  $t$ \-terms vanish and we are left with:

$$
g(\lambda_+,\lambda_-) = (\lambda_- - \lambda_+)^T b
$$

Summary of **dual feasibility conditions**:

$$
\begin{cases} \lambda_+ \ge 0,\quad \lambda_- \ge 0 \\ \mathbf 1^T(\lambda_++\lambda_-)=1 \\ A^T(\lambda_+-\lambda_-)=0 \end{cases}
$$

* * *

Step 3.4 â€” Simplify the dual (introduce the real dual variable)
===============================================================

Now comes the conceptual cleanup.

* * *

3.4.1 Define the signed dual variable
-------------------------------------

Define:

$$
\nu := \lambda_- - \lambda_+ \in \mathbb R^m
$$

This captures **direction + magnitude** in one vector.

* * *

3.4.2 Translate the constraints
-------------------------------

### Constraint 1:  $A^T(\lambda_+-\lambda_-)=0$ 

Becomes:

$$
\boxed{A^T\nu = 0}
$$

Interpretation:

> The dual vector must be orthogonal to all columns of  $A$ .

* * *

### Constraint 2:  $\mathbf 1^T(\lambda_++\lambda_-)=1$ 

Observe:

$$
|\nu_i| = \lambda_{+i} + \lambda_{-i}
$$

So:

$$
\mathbf 1^T(\lambda_++\lambda_-) = \sum_i |\nu_i| = \|\nu\|_1
$$

Thus:

$$
\boxed{\|\nu\|_1 = 1}
$$

(We usually relax to  $\le 1$ ; the optimum always saturates it.)

* * *

### Constraint 3: nonnegativity disappears

Because  $\lambda_+,\lambda_- \ge 0$  is already encoded in  $\|\nu\|_1$ .

* * *

3.4.3 Final dual problem
------------------------

Putting everything together:

$$
\boxed{ \begin{aligned} \max_\nu\quad & b^T\nu \\ \text{s.t.}\quad & A^T\nu = 0 \\ & \|\nu\|_1 \le 1 \end{aligned} }
$$

This is the **dual of the Chebyshev (âˆ-norm) approximation problem**.

* * *

3.4.4 Why this dual gives a lower bound (critical intuition)
------------------------------------------------------------

For any feasible  $\nu$ :

$$
b^T\nu \;\le\; \|Ax_{\text{cheb}}-b\|_\infty
$$

Why?

*   This is **weak duality**: dual â‰¤ primal for minimization problems.
*   No optimality required â€” **any feasible**  $\nu$  gives a valid bound.

This is the entire point:

> If you can **construct any vector**  $\nu$  satisfying  
>  $A^T\nu=0$ ,  $ \|\nu\|_1\le 1$ ,  
> then  $b^T\nu$  is a certified lower bound on the best possible Chebyshev error.

* * *

Mental checkpoint (important)
-----------------------------

You should now see the structure clearly:

*   **Primal**: choose  $x$  to make residuals small in âˆ-norm
*   **Dual**: choose a signed weighting of residuals
    *   orthogonal to column space of  $A$ 
    *   total weight â‰¤ 1
*   **Value**: weighted sum of residuals â†’ lower bound

* * *

### What comes next (Step 4)

Next, we do the **killer move** of Exercise 5.6:

> Take the **least-squares residual**  
>  $r_{\text{ls}} = b-Ax_{\text{ls}}$   
> and show it **automatically satisfies**  $A^T r_{\text{ls}}=0$ .

Then normalize it to build a **valid dual vector**, and get a **numerical lower bound** on the Chebyshev error **without solving the Chebyshev problem**.

Say when youâ€™re ready for Step 4 â€” this is where everything clicks into place.

## Response:
Perfect â€” youâ€™ve just finished **building the dual problem** for the Chebyshev approximation via Lagrange duality (Steps 3.1 and 3.2). Letâ€™s now wrap up the derivation cleanly and interpret it â€” this is Step 3.3 and Step 3.4:

* * *

âœ… Step 3.3 â€” Derive Final Dual Problem (from Lagrangian)
========================================================

We ended Step 3.2 with the **dual function**:

$$
g(\lambda_+, \lambda_-) = (\lambda_- - \lambda_+)^T b \quad \text{if} \begin{cases} \lambda_+, \lambda_- \ge 0 \\ A^T (\lambda_+ - \lambda_-) = 0 \\ \mathbf{1}^T (\lambda_+ + \lambda_-) = 1 \end{cases}
$$

We now want to **reparametrize** this in terms of a single variable.

* * *

ğŸ¯ Define  $\nu := \lambda_- - \lambda_+ \in \mathbb{R}^m$ 
-----------------------------------------------------------

This is the **signed combination** of the two Lagrange multipliers.

Letâ€™s see how everything changes:

* * *

### ğŸ’¡ Constraint 1:  $A^T(\lambda_+ - \lambda_-) = 0$ 

That becomes:

$$
A^T(-\nu) = 0 \quad \Rightarrow \quad A^T \nu = 0
$$

* * *

### ğŸ’¡ Constraint 2:  $\mathbf{1}^T (\lambda_+ + \lambda_-) = 1$ 

Letâ€™s relate this to  $\nu$ .

Letâ€™s observe:

$$
|\nu_i| = \begin{cases} \lambda_{+i} + \lambda_{-i}, & \text{since exactly one of them is nonzero} \end{cases} \Rightarrow \|\nu\|_1 = \mathbf{1}^T (\lambda_+ + \lambda_-)
$$

So:

$$
\boxed{ \|\nu\|_1 = 1 }
$$

* * *

### ğŸ’¡ Objective:  $(\lambda_- - \lambda_+)^T b = \nu^T b$ 

* * *

### âœ… Final Dual Problem

$$
\boxed{ \begin{aligned} \max_{\nu \in \mathbb{R}^m} \quad & \nu^T b \\ \text{subject to} \quad & A^T \nu = 0 \\ & \|\nu\|_1 \le 1 \end{aligned} } \tag{Dual}
$$

* * *

âœ… Step 3.4 â€” What does the dual problem mean?
=============================================

* * *

ğŸ¯ Interpretation:
------------------

This dual problem **gives a lower bound** on the Chebyshev (âˆ-norm) approximation value:

$$
p^\star = \min_x \|Ax - b\|_\infty
$$

The dual says:

> For **any** feasible  $\nu$  (i.e.  $\|\nu\|_1 \le 1$ ,  $A^T \nu = 0$ ),  
> the value  $\nu^T b$  is a **lower bound** on  $p^\star$ .

So we can **construct candidate vectors  $\nu$ ** satisfying the constraints, plug them into  $\nu^T b$ , and obtain guaranteed lower bounds on the optimal error â€” **without ever solving the primal**.

* * *

ğŸ”§ Why is this useful?
----------------------

Suppose we compute:

*   The least-squares solution  $x_{\text{ls}}$ 
*   Its residual:  $r_{\text{ls}} := b - Ax_{\text{ls}}$ 

Weâ€™ll use  $r_{\text{ls}}$  to **construct specific feasible dual variables**  $\nu$ , and compute:

$$
\text{Lower Bound on } \|Ax_{\text{cheb}} - b\|_\infty \ge \nu^T b
$$

This gives us a **cheap-to-compute certificate** on how small the Chebyshev error can be.

* * *

### âœ… Summary of what we've done so far:

We started from:

$$
\min_x \|Ax - b\|_\infty
$$

and derived the **dual**:

$$
\boxed{ \max_{\nu} \nu^T b \quad \text{s.t.} \quad \|\nu\|_1 \le 1, \quad A^T \nu = 0 }
$$

This tells us:

*   Any  $\nu$  satisfying those constraints gives a lower bound on the optimal âˆ-norm error.
*   Weâ€™ll construct those  $\nu$ 's next using the **least-squares residual**.

* * *

Shall I now move on to **Step 4**, where we use the residual  $r_{\text{ls}}$  to construct **two dual-feasible vectors**  $\hat{\nu}, \tilde{\nu}$ , and see which gives the tighter bound?

## Response:
Perfect. Letâ€™s now finish **Exercise 5.6** with a complete, rigorous Step 4 â€” constructing specific **dual-feasible vectors** from the **least-squares residual**, computing bounds, and comparing them to the earlier  $\sqrt{m}$  bound.

Weâ€™ll do three things:

* * *

### âœ… Step 4.1 â€” Use the least-squares residual to construct feasible dual vectors

### âœ… Step 4.2 â€” Prove they are feasible for the dual

### âœ… Step 4.3 â€” Evaluate their objective values (i.e., lower bounds) and compare to upper bound

* * *

âœ… Step 4.1: Letâ€™s construct candidate dual vectors from  $r_{\text{ls}} = b - Ax_{\text{ls}}$ 
----------------------------------------------------------------------------------------------

We are given the **least-squares residual**:

$$
r_{\text{ls}} := b - Ax_{\text{ls}} \in \mathbb{R}^m
$$

We are told to define two vectors:

* * *

### ğŸ”· Option 1: Normalize by  $\ell_1$ \-norm

$$
\hat{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1} \quad \text{(1-norm normalization)}
$$

* * *

### ğŸ”· Option 2: Normalize by  $\ell_\infty$ \-norm

$$
\tilde{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty} \quad \text{(âˆ-norm normalization)}
$$

Note:

*   Both are just **directional scalings** of the same residual vector.
*   They **preserve** the direction of  $r_{\text{ls}}$ , but scale its magnitude to make it norm-bounded.

* * *

âœ… Step 4.2: Are  $\hat{\nu}, \tilde{\nu}$  feasible for the dual?
-----------------------------------------------------------------

Recall: the dual constraints are:

$$
\boxed{ \begin{aligned} A^T \nu &= 0 \quad \text{(orthogonal to columns of A)} \\ \|\nu\|_1 &\le 1 \\ \end{aligned} }
$$

Letâ€™s check each vector.

* * *

### âœ… 1. Orthogonality constraint:  $A^T \nu = 0$ 

We are told that the residual  $r_{\text{ls}}$  satisfies:

$$
A^T (Ax_{\text{ls}} - b) = -A^T r_{\text{ls}} = 0 \quad \Rightarrow \quad A^T r_{\text{ls}} = 0
$$

So  $r_{\text{ls}} \in \text{null}(A^T)$ , meaning:

$$
A^T r_{\text{ls}} = 0 \Rightarrow A^T \hat{\nu} = 0, \quad A^T \tilde{\nu} = 0
$$

Why? Because scaling by a scalar doesnâ€™t change orthogonality.

âœ… So both vectors satisfy  $A^T \nu = 0$ .

* * *

### âœ… 2.  $\ell_1$ \-norm constraint:  $\|\nu\|_1 \le 1$ 

Letâ€™s compute:

*    $\|\hat{\nu}\|_1 = \left\| \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1} \right\|_1 = 1$ 
*    $\|\tilde{\nu}\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_\infty} \ge 1$ 

So:

*    $\hat{\nu}$  is feasible
*    $\tilde{\nu}$  may **not** be feasible unless  $\|r_{\text{ls}}\|_1 \le \|r_{\text{ls}}\|_\infty$  (which never happens unless vector is sparse with one nonzero)

Therefore, **only  $\hat{\nu}$ ** is always dual feasible.

* * *

âœ… Step 4.3: Use dual objective to compute lower bounds
------------------------------------------------------

Dual objective is:

$$
\text{maximize } \nu^T b \quad \text{(for feasible } \nu)
$$

We compute:

* * *

### ğŸ”· 1. Bound from  $\hat{\nu}$ :

$$
\hat{\nu} = \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1} \quad \Rightarrow \quad \hat{\nu}^T b = \frac{r_{\text{ls}}^T b}{\|r_{\text{ls}}\|_1}
$$

But remember:

$$
r_{\text{ls}} = b - Ax_{\text{ls}} \Rightarrow r_{\text{ls}}^T b = b^T r_{\text{ls}} = \|r_{\text{ls}}\|_2^2 + x_{\text{ls}}^T A^T r_{\text{ls}} = \|r_{\text{ls}}\|_2^2 + 0
$$

Because  $A^T r_{\text{ls}} = 0$ 

So:

$$
\boxed{ \hat{\nu}^T b = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} } \quad \text{(valid lower bound)}
$$

* * *

### ğŸ”· 2. Bound from  $\tilde{\nu}$ :

$$
\tilde{\nu} = \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty} \quad \Rightarrow \quad \tilde{\nu}^T b = \frac{r_{\text{ls}}^T b}{\|r_{\text{ls}}\|_\infty} = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty}
$$

But in this case:

*   It is a **better (larger)** numerical value
*   However,  $\tilde{\nu}$  is **not necessarily feasible** (if  $\|\tilde{\nu}\|_1 > 1$ ), so we canâ€™t always use it

If  $\tilde{\nu}$  is feasible (i.e.  $\|r_{\text{ls}}\|_1 \le \|r_{\text{ls}}\|_\infty$ ), then:

$$
\boxed{ \tilde{\nu}^T b = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty} } \quad \text{(valid lower bound only if feasible)}
$$

Otherwise: discard.

* * *

âœ… Final Summary of All Bounds
-----------------------------

Letâ€™s gather everything:

| Bound Type | Expression | When Valid | Value |
| --- | --- | --- | --- |
| ğŸ”¼ **Upper bound** (from Step 2) |  $\|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty$  | Always | Loose but safe |
| ğŸ”½ **Lower bound** via  $\hat{\nu}$  |  $\|Ax_{\text{cheb}} - b\|_\infty \ge \dfrac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}$  | Always | Safe |
| ğŸ”½ **Lower bound** via  $\tilde{\nu}$  |  $\|Ax_{\text{cheb}} - b\|_\infty \ge \dfrac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty}$  | Only if feasible | Tighter |

So:

*    $\hat{\nu}$  is always dual feasible â†’ always safe to use
*    $\tilde{\nu}$  may violate  $\|\nu\|_1 \le 1$  â†’ check before using

* * *

âœ… Final Takeaways from Exercise 5.6
-----------------------------------

*   The least-squares solution is **not optimal** for âˆ-norm error, but its âˆ-norm error is upper bounded by  $\sqrt{m}$  times the optimal one.
*   The **duality-based lower bounds** give us certificates for how small the true Chebyshev error can be.
*   These dual bounds come **for free** if youâ€™ve already computed the least-squares solution (no extra optimization required).

* * *

Would you like me to summarize all this in a compact "one-sheet" version for your notes or course materials?

## Response:
Sure â€” here is the **final full solution for part (a)** of Exercise 5.6 (Boyd & Vandenberghe), clearly, cleanly, and self-contained:

* * *

âœ… Exercise 5.6(a) â€” Upper bound on âˆ-norm error of least-squares solution
-------------------------------------------------------------------------

We are given:

*   A matrix  $A \in \mathbb{R}^{m \times n}$  with **full column rank**  $\text{rank}(A) = n$ 
*   A vector  $b \in \mathbb{R}^m$ 
*   Two problems:
    *   **Chebyshev (âˆ-norm) approximation**:
        $$
        x_{\text{cheb}} := \arg\min_x \|Ax - b\|_\infty
        $$
    *   **Least-squares (2-norm) approximation**:
        $$
        x_{\text{ls}} := \arg\min_x \|Ax - b\|_2 = (A^T A)^{-1} A^T b
        $$

We want to **compare**:

$$
\|Ax_{\text{ls}} - b\|_\infty \quad \text{vs.} \quad \|Ax_{\text{cheb}} - b\|_\infty
$$

* * *

### ğŸ¯ Goal:

**Prove** the inequality:

$$
\boxed{ \|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty }
$$

This tells us that the âˆ-norm error from the **least-squares solution** is at most  $\sqrt{m}$  times larger than the optimal âˆ-norm error.

* * *

### ğŸ§  Key facts:

For any vector  $z \in \mathbb{R}^m$ , we have:

$$
\boxed{ \|z\|_\infty \le \|z\|_2 \le \sqrt{m} \cdot \|z\|_\infty } \quad \text{(norm equivalence)}
$$

* * *

### âœ… Step-by-step proof:

Let:

*    $r_{\text{ls}} := Ax_{\text{ls}} - b$ 
*    $r_{\text{cheb}} := Ax_{\text{cheb}} - b$ 
    1.  By norm inequality:
    $$
    \|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_2
    $$
2.  By definition of  $x_{\text{ls}}$  as 2-norm minimizer:
    $$
    \|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2
    $$
3.  Again by norm inequality:
    $$
    \|r_{\text{cheb}}\|_2 \le \sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty
    $$

Chaining these:

$$
\|Ax_{\text{ls}} - b\|_\infty = \|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_2 \le \|r_{\text{cheb}}\|_2 \le \sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty = \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty
$$

* * *

### âœ… Final Result:

$$
\boxed{ \|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty } \quad \text{(proved)}
$$

This shows that the 2-norm solution is at worst  $\sqrt{m}$ \-suboptimal when judged under âˆ-norm.

* * *

Let me know if you want the one-sheet summary for all of Exercise 5.6 (including part b)!

## Response:
Here is a **clean, complete, rigorous final solution for Exercise 5.6(b)** from Boyd & Vandenberghe â€” fully self-contained.

* * *

âœ… Exercise 5.6(b) â€” Dual-based lower bounds for Chebyshev approximation
-----------------------------------------------------------------------

* * *

### ğŸ¯ Problem:

Given the Chebyshev (âˆ-norm) approximation problem:

$$
\min_x \|Ax - b\|_\infty
$$

Its **dual** is derived in Section 5.6 (page 254), and can be written as:

$$
\boxed{ \begin{aligned} \max_{\nu \in \mathbb{R}^m} \quad & \nu^T b \\ \text{subject to} \quad & \|\nu\|_1 \le 1 \\ & A^T \nu = 0 \end{aligned} } \tag{1}
$$

By **weak duality**, any feasible  $\nu$  gives a **lower bound**:

$$
\boxed{ \|Ax_{\text{cheb}} - b\|_\infty \ge \nu^T b }
$$

We now construct two **candidate dual vectors** from the **least-squares residual**:

$$
r_{\text{ls}} := b - Ax_{\text{ls}} \neq 0
$$

* * *

### ğŸ”§ Construct dual candidates:

Let:

1.  **1-norm normalized vector**:
    $$
    \hat{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}
    $$
2.  **âˆ-norm normalized vector**:
    $$
    \tilde{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty}
    $$

* * *

### âœ… Check feasibility of each vector for the dual:

#### 1\. Orthogonality:

We know from least-squares theory that:

$$
A^T r_{\text{ls}} = 0 \quad \Rightarrow \quad A^T \hat{\nu} = A^T \tilde{\nu} = 0
$$

âœ… Both satisfy  $A^T \nu = 0$ 

#### 2\. 1-norm bound:

*    $\|\hat{\nu}\|_1 = 1$  âœ…
*    $\|\tilde{\nu}\|_1 = \dfrac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_\infty} \ge 1$ 

âœ…  $\hat{\nu}$  is always feasible  
âš ï¸  $\tilde{\nu}$  is **not always feasible**

* * *

### ğŸ§® Compute lower bounds:

**Dual objective** is  $\nu^T b$ 

* * *

#### For  $\hat{\nu}$ :

$$
\begin{aligned} \hat{\nu}^T b &= \frac{r_{\text{ls}}^T b}{\|r_{\text{ls}}\|_1} = \frac{r_{\text{ls}}^T (Ax_{\text{ls}} + r_{\text{ls}})}{\|r_{\text{ls}}\|_1} = \frac{r_{\text{ls}}^T Ax_{\text{ls}} + \|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} \end{aligned}
$$

But from least-squares orthogonality:

$$
r_{\text{ls}}^T A = 0 \quad \Rightarrow \quad r_{\text{ls}}^T Ax_{\text{ls}} = 0
$$

So:

$$
\boxed{ \hat{\nu}^T b = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} } \quad \text{(valid lower bound)}
$$

* * *

#### For  $\tilde{\nu}$ :

Assuming  $\tilde{\nu}$  is feasible (i.e.,  $\|\tilde{\nu}\|_1 \le 1$ ), we get:

$$
\boxed{ \tilde{\nu}^T b = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty} } \quad \text{(only valid if } \|\tilde{\nu}\|_1 \le 1 \text{)}
$$

* * *

ğŸ“Š Compare bounds
-----------------

| Method | Bound on  $\|Ax_{\text{cheb}} - b\|_\infty$  | Always valid? |
| --- | --- | --- |
| ğŸ”¼ Upper bound from part (a) |  $\sqrt{m} \cdot \|r_{\text{cheb}}\|_\infty$  | âœ… |
| ğŸ”½ Lower bound via  $\hat{\nu}$  |  $\dfrac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}$  | âœ… |
| ğŸ”½ Lower bound via  $\tilde{\nu}$  |  $\dfrac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty}$  | âŒ Only if feasible |

**Which is better?**  
If feasible,  $\tilde{\nu}^T b > \hat{\nu}^T b$  (since  $\|r_{\text{ls}}\|_\infty \le \|r_{\text{ls}}\|_1$ ), so it gives a **tighter bound**.

Otherwise,  $\hat{\nu}$  is **safe and always usable**.

* * *

âœ… Final boxed answers
---------------------

**(1) Dual formulation:**

$$
\boxed{ \max_\nu \ \nu^T b \quad \text{s.t.} \quad \|\nu\|_1 \le 1, \quad A^T \nu = 0 }
$$

**(2) Dual feasible vectors:**

$$
\boxed{ \hat{\nu} = \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}, \quad \tilde{\nu} = \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty} }
$$

**(3) Bounds:**

$$
\boxed{ \|Ax_{\text{cheb}} - b\|_\infty \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} \quad \text{(always valid)} }
$$
 
$$
\boxed{ \|Ax_{\text{cheb}} - b\|_\infty \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty} \quad \text{(if feasible)} }
$$

* * *

This completes Exercise 5.6 â€” both primal inequality and dual certificate bounds, fully unpacked.

Let me know if you want these rendered as a printable PDF / typeset LaTeX sheet.

## Response:
Absolutely â€” let's now **unpack part (b) of Exercise 5.6** in the same ultra-granular, step-by-step, from-scratch style. Weâ€™ll make everything crystal clear for a newcomer.

* * *

ğŸ§­ What are we doing in Part (b)?
---------------------------------

We're continuing from the Chebyshev approximation problem:

$$
\min_x \|Ax - b\|_\infty
$$

This problem minimizes the **maximum residual error** â€” i.e., the largest entry in absolute value of  $r = Ax - b \in \mathbb{R}^m$ . Itâ€™s a **minimax** or **worst-case** approximation.

In part (a), we upper-bounded the âˆ-norm error from the least-squares solution (2-norm minimizer) using a  $\sqrt{m}$  factor.

Now, in part (b), we want to:

> **Construct lower bounds** on the Chebyshev approximation error  
> without solving the optimization problem directly,  
> using **duality** and the known least-squares residual.

* * *

âœ… Step 1: Recall the dual problem (from Section 5.6)
----------------------------------------------------

We already derived in previous steps that the **dual** of the Chebyshev approximation problem is:

$$
\boxed{ \max_{\nu \in \mathbb{R}^m} \ \nu^T b \quad \text{subject to:} \quad \| \nu \|_1 \le 1, \quad A^T \nu = 0 } \tag{D}
$$
*   This is a **linear program**.
*   The dual objective  $\nu^T b$  gives a **lower bound** on the primal optimal value:
    $$
    \nu^T b \le \min_x \|Ax - b\|_\infty = \|Ax_{\text{cheb}} - b\|_\infty
    $$

âœ… So **any feasible  $\nu$ ** (i.e., satisfying the dual constraints) gives a valid lower bound.

* * *

âœ… Step 2: Define the least-squares residual
-------------------------------------------

We are told to use the **least-squares residual**:

$$
r_{\text{ls}} := b - Ax_{\text{ls}} \in \mathbb{R}^m
$$

This is the vector of errors when using the 2-norm optimal solution.

From least-squares theory:

$$
x_{\text{ls}} := (A^T A)^{-1} A^T b \quad \Rightarrow \quad A^T r_{\text{ls}} = 0
$$

âœ… This tells us that  $r_{\text{ls}} \in \text{null}(A^T)$ , which is exactly what we want for dual feasibility.

So we want to use this vector  $r_{\text{ls}}$  to **construct feasible dual variables**  $\nu$ .

* * *

âœ… Step 3: Construct two candidate dual vectors
----------------------------------------------

We want to take  $r_{\text{ls}}$ , and **normalize it** to satisfy  $\| \nu \|_1 \le 1$ .

We try two normalizations:

* * *

### ğŸ”¹ Option 1 â€” 1-norm normalization:

$$
\boxed{ \hat{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1} }
$$
*   The denominator  $\|r_{\text{ls}}\|_1$  is a scalar.
*   This guarantees:
    *    $A^T \hat{\nu} = 0$  âœ… (since  $r_{\text{ls}} \in \text{null}(A^T)$ )
    *    $\|\hat{\nu}\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_1} = 1$  âœ…

Therefore,  $\hat{\nu}$  is **dual feasible**.

* * *

### ğŸ”¹ Option 2 â€” âˆ-norm normalization:

$$
\boxed{ \tilde{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty} }
$$
*   This guarantees:
    *    $A^T \tilde{\nu} = 0$  âœ… (same reason)
    *   But  $\|\tilde{\nu}\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_\infty} \ge 1$ 

So  $\tilde{\nu}$  **is not necessarily feasible** â€” only if  $\|r_{\text{ls}}\|_1 \le \|r_{\text{ls}}\|_\infty$ , which is rare.

Hence:

*    $\hat{\nu}$  is always feasible âœ…
*    $\tilde{\nu}$  might not be âŒ

* * *

âœ… Step 4: Plug into the dual objective
--------------------------------------

The dual objective is:

$$
\nu^T b \le \|Ax_{\text{cheb}} - b\|_\infty
$$

Letâ€™s compute  $\nu^T b$  for both cases.

* * *

### ğŸ”¸ For  $\hat{\nu} = \dfrac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}$ 

$$
\hat{\nu}^T b = \frac{r_{\text{ls}}^T b}{\|r_{\text{ls}}\|_1} = \frac{(b - Ax_{\text{ls}})^T b}{\|r_{\text{ls}}\|_1}
$$

Now expand:

$$
r_{\text{ls}}^T b = b^T b - x_{\text{ls}}^T A^T b
$$

But from least-squares optimality:

$$
x_{\text{ls}} = (A^T A)^{-1} A^T b \quad \Rightarrow \quad A^T (Ax_{\text{ls}} - b) = 0 \quad \Rightarrow \quad A^T r_{\text{ls}} = 0
$$

So:

$$
r_{\text{ls}}^T Ax_{\text{ls}} = 0 \quad \Rightarrow \quad r_{\text{ls}}^T b = \|r_{\text{ls}}\|_2^2
$$

Therefore:

$$
\boxed{ \hat{\nu}^T b = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} } \quad \text{(lower bound on Chebyshev error)}
$$

âœ… Always valid.

* * *

### ğŸ”¸ For  $\tilde{\nu} = \dfrac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty}$ 

If  $\tilde{\nu}$  is feasible (i.e.,  $\|\tilde{\nu}\|_1 \le 1$ ), then:

$$
\boxed{ \tilde{\nu}^T b = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty} } \quad \text{(tighter lower bound if feasible)}
$$

* * *

âœ… Step 5: Compare bounds and close the argument
-----------------------------------------------

### ğŸ”¼ From part (a):

We had the upper bound:

$$
\boxed{ \|Ax_{\text{ls}} - b\|_\infty \le \sqrt{m} \cdot \|Ax_{\text{cheb}} - b\|_\infty }
$$

### ğŸ”½ From part (b):

We now have dual-based **lower bounds**:

| Bound source | Value | Feasibility |
| --- | --- | --- |
|  $\hat{\nu}$  |  $\dfrac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}$  | Always feasible âœ… |
|  $\tilde{\nu}$  |  $\dfrac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty}$  | Only if  $\|r_{\text{ls}}\|_1 \le \|r_{\text{ls}}\|_\infty$  âŒ usually invalid |

So:

*   If you're unsure: **use  $\hat{\nu}$ ** â€” always valid
*   If you check feasibility and it passes, ** $\tilde{\nu}$ ** gives a tighter (larger) bound

* * *

ğŸ“¦ Final Summary
----------------

Letâ€™s summarize part (b) completely:

### âœ… The dual of the Chebyshev problem is:

$$
\boxed{ \max_\nu \nu^T b \quad \text{s.t.} \quad A^T \nu = 0, \ \|\nu\|_1 \le 1 }
$$

### âœ… We construct feasible dual points from the least-squares residual:

$$
\boxed{ \hat{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1} \quad\text{and}\quad \tilde{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty} }
$$

### âœ… These give lower bounds:

$$
\boxed{ \|Ax_{\text{cheb}} - b\|_\infty \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} \quad \text{(always valid)} }
$$
 
$$
\boxed{ \|Ax_{\text{cheb}} - b\|_\infty \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_\infty} \quad \text{(if feasible)} }
$$

âœ… These can be compared to the upper bound from part (a) to bracket the true optimal error  $\|Ax_{\text{cheb}} - b\|_\infty$ .

* * *

Would you like a LaTeX or typeset note version of both parts now?

## Response:
Excellent. Letâ€™s now **slow down and fully unpack the first three steps of Part (b)** of Exercise 5.6, **line by line, from scratch**, as a complete newcomer walkthrough.

Weâ€™ll go step-by-step:

* * *

âœ… Step 1 â€” Understanding what we want: a **lower bound** on Chebyshev error
---------------------------------------------------------------------------

* * *

### ğŸ¯ The primal problem:

We are working with the **Chebyshev approximation** problem, which is:

$$
\min_x \|Ax - b\|_\infty
$$

This means:

> Among all possible vectors  $x \in \mathbb{R}^n$ , find one that makes the residual  $Ax - b \in \mathbb{R}^m$  as **uniformly small as possible**, by minimizing the **largest absolute value** among its entries.

We donâ€™t have a closed-form for the Chebyshev solution  $x_{\text{cheb}}$ , but we want to **estimate** the true minimum value:

$$
\|Ax_{\text{cheb}} - b\|_\infty
$$

* * *

### â—Key goal of part (b):

We already derived the **upper bound** on this quantity in part (a) using the least-squares solution. Now we want to:

> Use **duality theory** to compute **lower bounds** on this quantity, by constructing feasible points in the **dual** problem.

Why? Because in any convex minimization problem:

$$
\boxed{ \text{Dual value } \le \text{Primal optimal value} }
$$

Thatâ€™s called **weak duality** â€” and it lets us safely **underestimate** the true solution by solving or evaluating the dual.

* * *

âœ… Step 2 â€” Recall or derive the dual problem
--------------------------------------------

Weâ€™re told (and weâ€™ve derived) that the dual of the Chebyshev problem is:

$$
\boxed{ \begin{aligned} \max_{\nu \in \mathbb{R}^m} \quad & \nu^T b \\ \text{subject to} \quad & \|\nu\|_1 \le 1 \\ & A^T \nu = 0 \end{aligned} } \tag{â˜…}
$$

* * *

### ğŸ§  Why this makes sense:

*    $\nu^T b$  is the **dual objective** â€” it gives us a lower bound on  $\|Ax - b\|_\infty$ 
*   The constraint  $A^T \nu = 0$  ensures compatibility with the primal's residual structure
*   The constraint  $\|\nu\|_1 \le 1$  comes from the dual norm of the âˆ-norm:
    $$
    \text{dual of } \ell_\infty = \ell_1
    $$

* * *

### âœ… So the plan is:

Weâ€™ll **construct a vector  $\nu$ ** satisfying:

*    $A^T \nu = 0$ 
*    $\|\nu\|_1 \le 1$ 

Then plug it into  $\nu^T b$  to get:

$$
\boxed{ \|Ax_{\text{cheb}} - b\|_\infty \ge \nu^T b } \quad \text{(dual lower bound)}
$$

* * *

âœ… Step 3 â€” Construct dual-feasible  $\nu$  from least-squares residual
----------------------------------------------------------------------

Hereâ€™s the clever trick.

We already **know** the least-squares solution  $x_{\text{ls}}$ , which minimizes  $\|Ax - b\|_2$ , and we can compute its residual:

$$
r_{\text{ls}} := b - Ax_{\text{ls}}
$$

### â—Key fact:

From least-squares optimality, the residual  $r_{\text{ls}}$  is **orthogonal** to the column space of  $A$ :

$$
\boxed{ A^T r_{\text{ls}} = 0 }
$$

This tells us:

*    $r_{\text{ls}} \in \text{null}(A^T)$ 
*   So **any scaling of  $r_{\text{ls}}$ ** also lies in the nullspace:
    $$
    A^T \left( \alpha \cdot r_{\text{ls}} \right) = \alpha \cdot A^T r_{\text{ls}} = 0 \quad \forall \alpha \in \mathbb{R}
    $$

Therefore, we can **use  $r_{\text{ls}}$ ** as a building block for a dual vector  $\nu$  satisfying  $A^T \nu = 0$ .

* * *

### Now we need to enforce:  $\|\nu\|_1 \le 1$ 

We do this by **normalizing**  $r_{\text{ls}}$ :

* * *

### ğŸ”¹ Candidate 1: Normalize by  $\ell_1$ \-norm

Letâ€™s define:

$$
\boxed{ \hat{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1} }
$$

Check feasibility:

*    $A^T \hat{\nu} = A^T \left( \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1} \right) = 0$ 
*    $\|\hat{\nu}\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_1} = 1$ 

âœ… This is a valid dual point. Always feasible.

* * *

### ğŸ”¹ Candidate 2: Normalize by  $\ell_\infty$ \-norm

Define:

$$
\boxed{ \tilde{\nu} := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_\infty} }
$$

Check feasibility:

*    $A^T \tilde{\nu} = 0$  âœ…
*    $\|\tilde{\nu}\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_\infty}$ 

Now:

*   If  $\|r_{\text{ls}}\|_1 \le \|r_{\text{ls}}\|_\infty$ , then  $\|\tilde{\nu}\|_1 \le 1$  âœ… feasible
*   But usually  $\|r_{\text{ls}}\|_1 \gg \|r_{\text{ls}}\|_\infty$  (unless the residual is sparse), so:
    
$$
\boxed{ \tilde{\nu} \text{ is usually not dual-feasible} }
$$

* * *

So:

*    $\hat{\nu}$ : always valid, safe bound
*    $\tilde{\nu}$ : better bound if feasible, but must be checked

* * *

Would you like me to now walk through **Steps 4â€“5**, where we compute and interpret the lower bounds  $\nu^T b$ ?

## Response:
Thought for 30s

Below is a single, complete, rigorous, step-by-step solution to **Exercise 5.6 (a)â€“(b)**.

* * *

Problem setup and notation
--------------------------

Let  $A\in\mathbb R^{m\times n}$  with  $\operatorname{rank}(A)=n$  (full column rank) and  $b\in\mathbb R^m$ .

Define the **Chebyshev (âˆ-norm) approximation problem**

$$
\min_{x\in\mathbb R^n}\ \|Ax-b\|_\infty. \tag{P\(_\infty\)}
$$

Let  $x_{\text{ch}}$  be an optimal solution and denote the optimal value by

$$
p^\star := \|Ax_{\text{ch}}-b\|_\infty.
$$

Define the **least-squares (2-norm) approximation problem**

$$
\min_{x\in\mathbb R^n}\ \|Ax-b\|_2. \tag{P\(_2\)}
$$

Because  $A$  has full column rank, the least-squares minimizer is unique and equals

$$
x_{\text{ls}} = (A^TA)^{-1}A^Tb.
$$

Define the least-squares residual

$$
r_{\text{ls}} := b-Ax_{\text{ls}}.
$$

Note  $\|Ax_{\text{ls}}-b\|_\infty = \|-r_{\text{ls}}\|_\infty = \|r_{\text{ls}}\|_\infty$ .

We will use the standard norm relations (given):

$$
\frac{1}{\sqrt m}\|z\|_2 \le \|z\|_\infty \le \|z\|_2,\qquad \forall z\in\mathbb R^m.
$$

Equivalently,

$$
\|z\|_2 \le \sqrt m\,\|z\|_\infty \quad \text{and}\quad \|z\|_\infty\le \|z\|_2.
$$

* * *

(a) Prove Â  $\ \|Ax_{\text{ls}}-b\|_\infty \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty$ 
-------------------------------------------------------------------------------------

### Step 1: Start with the LS âˆ-error and bound it by its 2-error

Apply  $\|z\|_\infty \le \|z\|_2$  to  $z = Ax_{\text{ls}}-b$ :

$$
\|Ax_{\text{ls}}-b\|_\infty \le \|Ax_{\text{ls}}-b\|_2. \tag{1}
$$

### Step 2: Use the defining optimality of  $x_{\text{ls}}$  for the 2-norm problem

Since  $x_{\text{ls}}$  minimizes  $\|Ax-b\|_2$  over all  $x$ , it does no worse than the particular choice  $x=x_{\text{ch}}$ :

$$
\|Ax_{\text{ls}}-b\|_2 \le \|Ax_{\text{ch}}-b\|_2. \tag{2}
$$

### Step 3: Relate the Chebyshev residualâ€™s 2-norm to its âˆ-norm

Apply  $\|z\|_2 \le \sqrt m\,\|z\|_\infty$  to  $z = Ax_{\text{ch}}-b$ :

$$
\|Ax_{\text{ch}}-b\|_2 \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty. \tag{3}
$$

### Step 4: Chain (1)â€“(3)

$$
\|Ax_{\text{ls}}-b\|_\infty \le \|Ax_{\text{ls}}-b\|_2 \le \|Ax_{\text{ch}}-b\|_2 \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty.
$$

So

Â $$
\boxed{\ \|Ax_{\text{ls}}-b\|_\infty \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty.\ }
$$

A useful rearrangement (a **lower bound** on the unknown optimum  $p^\star$ ) is:

Â $$
\boxed{\ p^\star=\|Ax_{\text{ch}}-b\|_\infty \ge \frac{1}{\sqrt m}\|Ax_{\text{ls}}-b\|_\infty = \frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty.\ }
$$

* * *

(b) Construct dual-feasible  $\nu$  from  $r_{\text{ls}}$ , compute the bounds, compare
---------------------------------------------------------------------------------------

### Step 0: The dual problem and the â€œlower bound machineâ€

You are given the dual of  $(P_\infty)$ :

$$
\boxed{ \begin{aligned} \max_{\nu\in\mathbb R^m}\quad & b^T\nu\\ \text{s.t.}\quad & \|\nu\|_1 \le 1,\\ & A^T\nu = 0. \end{aligned}} \tag{D}
$$

**Weak duality** says: for any feasible  $\nu$ ,

$$
b^T\nu \ \le\ p^\star=\min_x\|Ax-b\|_\infty.
$$

So every feasible  $\nu$  gives a certified **lower bound**  $b^T\nu$  on the optimal Chebyshev error.

* * *

### Step 1: Show  $A^T r_{\text{ls}} = 0$  (normal equations)

Least squares minimizes  $\|Ax-b\|_2^2$ . The gradient is

$$
\nabla_x \|Ax-b\|_2^2 = 2A^T(Ax-b).
$$

At the minimizer  $x_{\text{ls}}$ , first-order optimality gives

$$
A^T(Ax_{\text{ls}}-b)=0 \quad\Longleftrightarrow\quad A^T(b-Ax_{\text{ls}})=0 \quad\Longleftrightarrow\quad \boxed{A^T r_{\text{ls}}=0.}
$$

* * *

### Step 2: Define the two proposed dual candidates and prove feasibility

Assume  $r_{\text{ls}}\neq 0$ . Define

$$
\hat\nu := -\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}, \qquad \bar\nu := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}.
$$

#### (i) Check the  $\ell_1$  constraint

$$
\|\bar\nu\|_1 = \left\|\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_1} = 1 \le 1.
$$

Similarly  $\|\hat\nu\|_1 = 1\le 1$  (sign doesnâ€™t affect  $\ell_1$ ).

#### (ii) Check the orthogonality constraint

Using  $A^Tr_{\text{ls}}=0$ ,

$$
A^T\bar\nu = A^T\left(\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right) =\frac{1}{\|r_{\text{ls}}\|_1}A^Tr_{\text{ls}}=0.
$$

Similarly  $A^T\hat\nu=0$ .

So both  $\hat\nu$  and  $\bar\nu$  are feasible for (D).

* * *

### Step 3: Evaluate the dual objectives  $b^T\hat\nu$  and  $b^T\bar\nu$ 

We first compute  $b^T r_{\text{ls}}$ .

Since  $r_{\text{ls}} = b-Ax_{\text{ls}}$ , we can write  $b = Ax_{\text{ls}} + r_{\text{ls}}$ . Then

$$
b^T r_{\text{ls}} = (Ax_{\text{ls}} + r_{\text{ls}})^T r_{\text{ls}} = x_{\text{ls}}^T A^T r_{\text{ls}} + r_{\text{ls}}^T r_{\text{ls}}.
$$

But  $A^T r_{\text{ls}}=0$ , so the first term vanishes:

$$
b^T r_{\text{ls}} = \|r_{\text{ls}}\|_2^2.
$$

Now:

$$
b^T\bar\nu = b^T\left(\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right) = \frac{b^T r_{\text{ls}}}{\|r_{\text{ls}}\|_1} = \boxed{\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}}.
$$
 
$$
b^T\hat\nu = b^T\left(-\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right) = -\boxed{\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}}.
$$

By weak duality, both are lower bounds on  $p^\star$ :

$$
\boxed{p^\star \ge b^T\bar\nu = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}},\qquad \boxed{p^\star \ge b^T\hat\nu = -\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}}.
$$

* * *

### Step 4: Which lower bound is better?

Since  $\|r_{\text{ls}}\|_2^2/\|r_{\text{ls}}\|_1>0$ , we have:

*    $b^T\bar\nu$  is **positive**
*    $b^T\hat\nu$  is **negative**

Because  $p^\star=\min_x\|Ax-b\|_\infty \ge 0$  always, the negative lower bound is essentially useless (it is true but very weak).

So the better bound is:

$$
\boxed{\text{Better lower bound: } \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.}
$$

* * *

### Step 5: Compare the dual lower bound to the bound from part (a)

From part (a) we obtained the (computable) lower bound:

$$
p^\star \ge \frac{1}{\sqrt m}\|Ax_{\text{ls}}-b\|_\infty = \frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty. \tag{LB\(_a\)}
$$

From part (b) we obtained the dual-feasible lower bound:

$$
p^\star \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}. \tag{LB\(_b\)}
$$

Now compare them. Using Cauchyâ€“Schwarz,  $\|r\|_1 \le \sqrt m\,\|r\|_2$ . Therefore

$$
\frac{\|r\|_2^2}{\|r\|_1} \ \ge\ \frac{\|r\|_2^2}{\sqrt m\,\|r\|_2} = \frac{1}{\sqrt m}\|r\|_2.
$$

And since  $\|r\|_2 \ge \|r\|_\infty$ ,

$$
\frac{1}{\sqrt m}\|r\|_2 \ge \frac{1}{\sqrt m}\|r\|_\infty.
$$

Combine:

Â $$
\boxed{\ \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} \ \ge\ \frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty. \ }
$$

So the dual lower bound from  $\bar\nu$  is **always at least as good as** the part (a) lower bound (and often strictly better).

* * *

Final â€œclosedâ€ summary (what you can compute from  $x_{\text{ls}}$ )
--------------------------------------------------------------------

Let  $r_{\text{ls}}=b-Ax_{\text{ls}}\neq 0$ . Then:

*   **Upper bound on the unknown optimum** (just evaluate the primal at  $x_{\text{ls}}$ ):
    $$
    p^\star \le \|Ax_{\text{ls}}-b\|_\infty = \|r_{\text{ls}}\|_\infty.
    $$
*   **Lower bound from part (a)**:
    $$
    p^\star \ge \frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty.
    $$
*   **Dual-based lower bound from part (b)** (better):
    $$
    p^\star \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.
    $$
*   Among the two proposed dual vectors,  $\bar\nu = r_{\text{ls}}/\|r_{\text{ls}}\|_1$  yields the useful positive bound;  $\hat\nu$  yields the same magnitude but negative, hence weaker.

That completes the full solution.

## Response:
Thought for 26s

Got it. Iâ€™ll give you a **more detailed, more rigorous, more â€œnothing implicitâ€** solution for **both (a) and (b)**, including (i) the norm inequalities, (ii) why least-squares gives the â€œorthogonalityâ€ condition, (iii) exactly why dual-feasible  $\nu$  produces a **lower bound** on the Chebyshev optimum, and (iv) the clean comparison to the bound from (a). No leaps.

* * *

0) What the problem is comparing
--------------------------------

You have two different ways to measure the residual error  $Ax-b\in\mathbb R^m$ .

*   Chebyshev (âˆ-norm, â€œworst coordinateâ€):
    
$$
(P_\infty)\quad \min_{x\in\mathbb R^n}\ \|Ax-b\|_\infty, \qquad \|y\|_\infty := \max_{i=1,\dots,m}|y_i|.
$$

Let an optimizer be  $x_{\text{ch}}$  and define the optimal value

$$
p^\star := \|Ax_{\text{ch}}-b\|_\infty.
$$
*   Least squares (2-norm, â€œenergyâ€):
    
$$
(P_2)\quad \min_{x\in\mathbb R^n}\ \|Ax-b\|_2, \qquad \|y\|_2 := \sqrt{\sum_{i=1}^m y_i^2}.
$$

Let the (unique) minimizer be  $x_{\text{ls}}$  (unique because  $\operatorname{rank}(A)=n$ ), and define the least-squares residual

$$
r_{\text{ls}} := b-Ax_{\text{ls}}.
$$

Note:  $Ax_{\text{ls}}-b = -r_{\text{ls}}$ , so  $\|Ax_{\text{ls}}-b\|_\infty=\|r_{\text{ls}}\|_\infty$ , etc.

The point of the exercise: **If you already computed  $x_{\text{ls}}$ , what can you say about the unknown Chebyshev optimum  $p^\star$ ?**

* * *

1) The norm inequalities (proved, not assumed)
----------------------------------------------

You are allowed to use

$$
\frac{1}{\sqrt m}\|z\|_2 \le \|z\|_\infty \le \|z\|_2 \quad \forall z\in\mathbb R^m.
$$

Let me prove both sides so nothing feels like magic.

### 1.1 Prove  $\|z\|_\infty \le \|z\|_2$ 

Let  $j$  be an index that attains the max:  $|z_j|=\|z\|_\infty$ .  
Then

$$
\|z\|_2^2=\sum_{i=1}^m z_i^2 \ge z_j^2 = \|z\|_\infty^2.
$$

Taking square roots:

$$
\|z\|_2 \ge \|z\|_\infty.
$$

### 1.2 Prove  $\|z\|_2 \le \sqrt m\,\|z\|_\infty$ 

For every coordinate  $i$ ,  $|z_i| \le \|z\|_\infty$ , so  $z_i^2 \le \|z\|_\infty^2$ . Sum over  $i$ :

$$
\|z\|_2^2=\sum_{i=1}^m z_i^2 \le \sum_{i=1}^m \|z\|_\infty^2 = m\|z\|_\infty^2.
$$

Take square roots:

$$
\|z\|_2 \le \sqrt m\,\|z\|_\infty.
$$

Equivalently  $\frac{1}{\sqrt m}\|z\|_2 \le \|z\|_\infty$ .

* * *

(a) Prove  $\|Ax_{\text{ls}}-b\|_\infty \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty$ 
----------------------------------------------------------------------------------

We want to compare the **âˆ-error** of the LS solution to the **optimal âˆ-error**.

Define two residual vectors:

$$
r_{\text{ls}}^{(A)} := Ax_{\text{ls}}-b,\qquad r_{\text{ch}}^{(A)} := Ax_{\text{ch}}-b.
$$

(Just to remind ourselves these are residuals in the  $Ax-b$  sign convention.)

### Step A1: Convert LS âˆ-error to LS 2-error

Using  $\|z\|_\infty \le \|z\|_2$  with  $z=r_{\text{ls}}^{(A)}$ ,

$$
\|Ax_{\text{ls}}-b\|_\infty = \|r_{\text{ls}}^{(A)}\|_\infty \le \|r_{\text{ls}}^{(A)}\|_2 = \|Ax_{\text{ls}}-b\|_2.
$$

### Step A2: Use that  $x_{\text{ls}}$  minimizes the 2-norm residual

By definition of  $x_{\text{ls}}$ ,

$$
\|Ax_{\text{ls}}-b\|_2 \le \|Ax-b\|_2 \quad \forall x.
$$

In particular, choose  $x=x_{\text{ch}}$ :

$$
\|Ax_{\text{ls}}-b\|_2 \le \|Ax_{\text{ch}}-b\|_2.
$$

### Step A3: Convert Chebyshev 2-error to Chebyshev âˆ-error

Using  $\|z\|_2 \le \sqrt m\,\|z\|_\infty$  with  $z=r_{\text{ch}}^{(A)}$ ,

$$
\|Ax_{\text{ch}}-b\|_2 \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty.
$$

### Step A4: Chain the three inequalities

Putting A1â€“A3 together:

$$
\|Ax_{\text{ls}}-b\|_\infty \le \|Ax_{\text{ls}}-b\|_2 \le \|Ax_{\text{ch}}-b\|_2 \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty.
$$

So

Â $$
\boxed{\ \|Ax_{\text{ls}}-b\|_\infty \le \sqrt m\,\|Ax_{\text{ch}}-b\|_\infty.\ }
$$

A very useful rearrangement (a computable lower bound on the unknown optimum  $p^\star$ ) is:

Â $$
\boxed{\ p^\star = \|Ax_{\text{ch}}-b\|_\infty \ge \frac{1}{\sqrt m}\|Ax_{\text{ls}}-b\|_\infty.\ }
$$

* * *

(b) Use the dual to get _explicit lower bounds_ from  $r_{\text{ls}}$ 
----------------------------------------------------------------------

### 2) The dual problem (given)

The dual of  $(P_\infty)$  is:

$$
(D)\quad \max_{\nu\in\mathbb R^m}\ b^T\nu \quad\text{s.t.}\quad \|\nu\|_1 \le 1,\ \ A^T\nu=0.
$$

What part (b) asks you to do:

1.  Using  $r_{\text{ls}}:=b-Ax_{\text{ls}}\neq 0$ , define
    
$$
\hat\nu := -\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}, \qquad \bar\nu := \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}.
$$
2.  Show both are feasible for (D).
3.  Therefore  $b^T\hat\nu$  and  $b^T\bar\nu$  are lower bounds on  $p^\star$ .
4.  Compare these bounds to the bound from (a).

Weâ€™ll do each line carefully.

* * *

2.1 Why dual-feasible  $\nu$  gives a lower bound (weak duality, unpacked)
--------------------------------------------------------------------------

We want to justify:

$$
\text{If } \|\nu\|_1\le 1,\ A^T\nu=0,\ \text{ then } b^T\nu \le p^\star.
$$

A clean way to see it (no black boxes) is to use a basic inequality about norms.

### Fact: For any  $y\in\mathbb R^m$  and any  $\nu$  with  $\|\nu\|_1\le 1$ ,

$$
\nu^T y \le \|y\|_\infty.
$$

Why? Because

$$
\nu^T y = \sum_{i=1}^m \nu_i y_i \le \sum_{i=1}^m |\nu_i||y_i| \le \left(\max_i |y_i|\right)\sum_{i=1}^m |\nu_i| = \|y\|_\infty \|\nu\|_1 \le \|y\|_\infty.
$$

Thatâ€™s fully explicit: triangle inequality + bounding each  $|y_i|$  by the max.

Now choose  $y = b-Ax$ . Note  $\|b-Ax\|_\infty=\|Ax-b\|_\infty$  (same entries, just sign).  
Then for any  $x$  and any  $\nu$  with  $\|\nu\|_1\le 1$ ,

$$
\nu^T(b-Ax) \le \|Ax-b\|_\infty.
$$

If in addition  $A^T\nu=0$ , then  $\nu^T Ax = x^T(A^T\nu)=0$ , so

$$
\nu^T(b-Ax) = \nu^T b - \nu^T Ax = \nu^T b.
$$

Therefore, for all  $x$ ,

$$
b^T\nu \le \|Ax-b\|_\infty.
$$

Now take the minimum over  $x$  on the right:

$$
b^T\nu \le \min_x \|Ax-b\|_\infty = p^\star.
$$

So indeed any dual-feasible  $\nu$  yields a certified lower bound.

Thatâ€™s the logic of duality here: feasibility in the dual gives a universal inequality that holds for every primal  $x$ .

* * *

2.2 Show  $A^T r_{\text{ls}}=0$  (least-squares normal equations, fully derived)
--------------------------------------------------------------------------------

This is the key â€œbridgeâ€ from least squares to the dual constraint  $A^T\nu=0$ .

Least squares minimizes  $\|Ax-b\|_2$ . Itâ€™s easier (and equivalent) to minimize the square:

$$
\min_x \|Ax-b\|_2^2.
$$

Expand:

$$
\|Ax-b\|_2^2 = (Ax-b)^T(Ax-b) = x^TA^TAx - 2b^TAx + b^Tb.
$$

Differentiate w.r.t.  $x$ . Standard result (you can verify term-by-term):

*   derivative of  $x^TA^TAx$  is  $2A^TAx$ ,
*   derivative of  $-2b^TAx$  is  $-2A^Tb$ ,
*   derivative of constant  $b^Tb$  is 0.

So

$$
\nabla_x \|Ax-b\|_2^2 = 2A^T(Ax-b).
$$

At the minimizer  $x_{\text{ls}}$ , the gradient must be zero:

$$
2A^T(Ax_{\text{ls}}-b)=0 \quad\Longrightarrow\quad A^T(Ax_{\text{ls}}-b)=0 \quad\Longrightarrow\quad A^T(b-Ax_{\text{ls}})=0.
$$

But  $b-Ax_{\text{ls}} = r_{\text{ls}}$ , so

$$
\boxed{A^T r_{\text{ls}}=0.}
$$

Geometric meaning:  $Ax_{\text{ls}}$  is the orthogonal projection of  $b$  onto the column space of  $A$ , and the residual  $r_{\text{ls}}$  is orthogonal to that space.

* * *

2.3 Construct  $\hat\nu,\bar\nu$  and prove feasibility
-------------------------------------------------------

Assume  $r_{\text{ls}}\neq 0$  so  $\|r_{\text{ls}}\|_1>0$  and the definitions make sense:

$$
\hat\nu = -\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}, \qquad \bar\nu = \frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}.
$$

Dual feasibility requires two things:

### (i)  $\|\nu\|_1 \le 1$ 

Compute:

$$
\|\bar\nu\|_1 = \left\|\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right\|_1 = \frac{\|r_{\text{ls}}\|_1}{\|r_{\text{ls}}\|_1} = 1.
$$

Similarly  $\|\hat\nu\|_1=1$  (sign doesnâ€™t change the  $\ell_1$ \-norm).

So both satisfy  $\|\nu\|_1\le 1$ .

### (ii)  $A^T\nu=0$ 

Using  $A^Tr_{\text{ls}}=0$ :

$$
A^T\bar\nu = A^T\left(\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right) = \frac{1}{\|r_{\text{ls}}\|_1}A^Tr_{\text{ls}}=0.
$$

And

$$
A^T\hat\nu = -\frac{1}{\|r_{\text{ls}}\|_1}A^Tr_{\text{ls}}=0.
$$

Therefore,

$$
\boxed{\hat\nu\ \text{and}\ \bar\nu\ \text{are both feasible for the dual}.}
$$

* * *

2.4 Evaluate the dual objectives  $b^T\hat\nu$  and  $b^T\bar\nu$ 
------------------------------------------------------------------

Since these are feasible, both values are lower bounds on  $p^\star$ . Letâ€™s compute them explicitly.

First compute  $b^T r_{\text{ls}}$ .

Because  $r_{\text{ls}} = b-Ax_{\text{ls}}$ , we can write

$$
b = Ax_{\text{ls}} + r_{\text{ls}}.
$$

Then

$$
b^T r_{\text{ls}} = (Ax_{\text{ls}} + r_{\text{ls}})^T r_{\text{ls}} = (Ax_{\text{ls}})^T r_{\text{ls}} + r_{\text{ls}}^T r_{\text{ls}}.
$$

Now show  $(Ax_{\text{ls}})^T r_{\text{ls}}=0$ . Indeed,

$$
(Ax_{\text{ls}})^T r_{\text{ls}} = x_{\text{ls}}^T A^T r_{\text{ls}} = x_{\text{ls}}^T \cdot 0 = 0.
$$

So

$$
b^T r_{\text{ls}} = r_{\text{ls}}^T r_{\text{ls}} = \|r_{\text{ls}}\|_2^2.
$$

Now compute each objective:

*   For  $\bar\nu$ :
    
$$
b^T\bar\nu = b^T\left(\frac{r_{\text{ls}}}{\|r_{\text{ls}}\|_1}\right) = \frac{b^T r_{\text{ls}}}{\|r_{\text{ls}}\|_1} = \boxed{\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}}.
$$
*   For  $\hat\nu$ :
    
$$
b^T\hat\nu = -\frac{b^T r_{\text{ls}}}{\|r_{\text{ls}}\|_1} = -\boxed{\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}}.
$$

By weak duality (proved in 2.1),

$$
\boxed{p^\star \ge b^T\bar\nu = \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}}, \qquad \boxed{p^\star \ge b^T\hat\nu = -\frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}}.
$$

Which is better? Since  $p^\star\ge 0$  always (norms are nonnegative), the negative bound is automatically weaker. The useful bound is:

$$
\boxed{\text{Best of the two is } \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.}
$$

* * *

2.5 Compare this lower bound to the lower bound from part (a)
-------------------------------------------------------------

From (a), we can rearrange:

$$
\|Ax_{\text{ls}}-b\|_\infty \le \sqrt m\,p^\star \quad\Longrightarrow\quad p^\star \ge \frac{1}{\sqrt m}\|Ax_{\text{ls}}-b\|_\infty.
$$

Since  $\|Ax_{\text{ls}}-b\|_\infty=\|r_{\text{ls}}\|_\infty$ ,

$$
\boxed{p^\star \ge \frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty.} \tag{LB\(_a\)}
$$

From (b), we have

$$
\boxed{p^\star \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.} \tag{LB\(_b\)}
$$

Now show  $LB_b \ge LB_a$  (the dual bound is never worse).

Use two standard inequalities (both provable by Cauchyâ€“Schwarz and norm ordering):

1.   $\|r\|_1 \le \sqrt m\,\|r\|_2$ .  
    Proof:  $\|r\|_1=\sum |r_i| \le \sqrt{m}\sqrt{\sum r_i^2}=\sqrt m\,\|r\|_2$ .
2.   $\|r\|_2 \ge \|r\|_\infty$ . (proved earlier)

Then:

$$
\frac{\|r\|_2^2}{\|r\|_1} \ge \frac{\|r\|_2^2}{\sqrt m\,\|r\|_2} = \frac{1}{\sqrt m}\|r\|_2 \ge \frac{1}{\sqrt m}\|r\|_\infty.
$$

Apply this to  $r=r_{\text{ls}}$ :

$$
\boxed{ \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} \ge \frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty. }
$$

So the dual-based lower bound from (b) is always at least as strong as the crude lower bound you can extract from (a).

* * *

3) Edge case: what if  $r_{\text{ls}}=0$ ?
------------------------------------------

The problem assumes  $r_{\text{ls}}\neq 0$  so division by  $\|r_{\text{ls}}\|_1$  is defined. But itâ€™s worth knowing what happens:

If  $r_{\text{ls}}=0$ , then  $b=Ax_{\text{ls}}$ , so you can fit exactly. Then

$$
p^\star = \min_x \|Ax-b\|_\infty = 0.
$$

Everything becomes trivial.

* * *

4) Final â€œclosed-formâ€ conclusion you can compute from least squares
--------------------------------------------------------------------

Once you compute  $x_{\text{ls}}$  and  $r_{\text{ls}}=b-Ax_{\text{ls}}$ , you have:

*   A trivial **upper bound** on the Chebyshev optimum (evaluate primal at  $x_{\text{ls}}$ ):
    
$$
p^\star \le \|Ax_{\text{ls}}-b\|_\infty = \|r_{\text{ls}}\|_\infty.
$$
*   A dual-based **lower bound** (best bound from part (b)):
    
$$
p^\star \ge \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1}.
$$
*   And a weaker lower bound implied by part (a):
    
$$
p^\star \ge \frac{1}{\sqrt m}\|r_{\text{ls}}\|_\infty,
$$

which is always dominated by the dual bound above.

So you can **sandwich** the unknown optimum  $p^\star$  as:

$$
\boxed{ \frac{\|r_{\text{ls}}\|_2^2}{\|r_{\text{ls}}\|_1} \ \le\ p^\star \ \le\ \|r_{\text{ls}}\|_\infty. }
$$

That is the â€œcomplete closureâ€: without solving the Chebyshev LP, you still get a certified interval where the optimal worst-case error must lie.

* * *

If you want, I can also rewrite the whole thing as a compact â€œexam solutionâ€ version (still correct, just shorter), or I can add a small numeric toy example to make each inequality feel physically real.



---
Powered by [ChatGPT Exporter](https://www.chatgptexporter.com)