<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>02. Introduction: What Makes a Problem Convex? — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "../../static/lib/three/three.module.js"
      }
    }
  </script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-linear-algebra-advanced/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../03-convex-sets-geometry/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>02. Introduction: What Makes a Problem Convex?</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-28</span>
        <span>Duration: 90 min</span>
        <span>Tags: intro, motivation, overview, modeling, fundamentals</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture introduces the central theme of the course: convex optimization. We define what makes a problem "convex" and prove the fundamental theorem that guarantees any local minimum is also a global minimum. We examine the hierarchy of canonical problem families—Linear Programs (LP), Quadratic Programs (QP), Second-Order Cone Programs (SOCP), and Semidefinite Programs (SDP)—and explore the "loss + regularizer + constraints" modeling paradigm. Finally, we develop practical techniques for reformulating problems into standard convex forms, providing the tools to recognize and solve tractable optimization problems.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> and <a href="../01-linear-algebra-advanced/index.html">Lecture 01: Linear Algebra Advanced</a> are recommended, particularly projections, PSD matrices, norms, and the fundamental subspaces.</p>
        <p><strong>Forward Connections:</strong> The definition of convex problems relies on convex functions and sets (Lectures 03-06). The modeling techniques introduced here are used throughout the course. Duality theory (Lecture 09) generalizes the optimality conditions proven here.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Define Convex Optimization Problems:</b> State the three-part definition (convex objective, convex inequalities, affine equalities) and distinguish convex from nonconvex problems.</li>
        <li><b>Prove the Fundamental Theorem:</b> Show that every local minimum of a convex problem is a global minimum.</li>
        <li><b>Understand the Problem Hierarchy:</b> Identify and classify problems as Linear Programs (LP), Quadratic Programs (QP), Second-Order Cone Programs (SOCP), or Semidefinite Programs (SDP).</li>
        <li><b>Apply the Loss + Regularizer Paradigm:</b> Formulate machine learning problems using the "loss + regularizer + constraints" template.</li>
        <li><b>Transform to Standard Forms:</b> Apply reformulation techniques to convert non-standard terms (norms, max functions) into standard convex forms.</li>
        <li><b>Verify Problem Convexity:</b> Perform sanity checks for feasibility and verify convexity using composition rules.</li>
        <li><b>Understand Solver Architecture:</b> Grasp the "model → canonicalize → solve" pipeline used by tools like CVXPY.</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
        <h2>1. Why Convexity Matters: The Watershed</h2>

        <h3>1.1 The Great Divide</h3>
        <p>For decades, the field of optimization was divided into "linear" and "nonlinear". This taxonomy proved insufficient. The true watershed in optimization is between <b>convex</b> and <b>nonconvex</b> problems.
        <br><b>Convex problems</b> are tractable: we can solve them efficiently (in polynomial time), reliably (finding the global optimum), and at scale (millions of variables). The theory provides strong guarantees: if a solution exists, we can find it; if not, we can prove infeasibility.
        <br><b>Nonconvex problems</b> are intractable: they are generally NP-hard. Algorithms may get stuck in local minima, depend heavily on initialization, and offer no guarantees of optimality or running time. Solving a nonconvex problem often feels like "art," whereas solving a convex problem is "technology."</p>

        <h3>1.2 The Local-to-Global Principle</h3>
        <p>What makes convexity special? It is the <b>local-to-global principle</b>: local information about a convex function (like its gradient) tells us something about its global behavior. If a convex function is flat at a point, it is the global minimum. If we are feasible locally, we can move towards the optimum without hitting "dead ends." This property is what enables efficient algorithms like Gradient Descent and Newton's Method to converge to the true solution.</p>

        <h3>1.3 When Convexity is Lost</h3>
        <p>Certain features act as "kryptonite" to convexity, turning a tractable problem into a hard one:</p>
        <ul>
          <li><b>Integer constraints ($x \in \mathbb{Z}$):</b> Discreteness breaks the connectivity of the feasible set, destroying gradient information.</li>
          <li><b>Equality constraints $h(x)=0$ (nonlinear):</b> Unless $h$ is affine, the set $\{x \mid h(x)=0\}$ is generally curved (like a sphere), preventing convexity.</li>
          <li><b>Maximizing a convex function:</b> The maximum of a convex function occurs at the boundary, requiring a combinatorial search of vertices.</li>
        </ul>
      </section>

      <section class="section-card" id="section-2">
        <h2>2. What is a Convex Optimization Problem?</h2>

        <h3>2.1 Formal Definition</h3>
        <p>A mathematical optimization problem is <b>convex</b> if it can be written in the following standard form:</p>
        $$
        \begin{aligned}
        \min_{x \in \mathbb{R}^n} \quad & f_0(x) \\
        \text{subject to} \quad & f_i(x) \le 0, \quad i=1,\dots,m \\
        & A x = b
        \end{aligned}
        $$
        <p>where three specific conditions must be met:</p>
        <ol>
          <li><b>Convex Objective:</b> The function $f_0: \mathcal{D} \to \mathbb{R}$ must be convex. Geometrically, its graph is bowl-shaped.</li>
          <li><b>Convex Inequality Constraints:</b> The functions $f_i$ must be convex. The constraint $f_i(x) \le 0$ defines a sublevel set, which is a convex set. The intersection of these sets forms a convex feasible region.</li>
          <li><b>Affine Equality Constraints:</b> The equality constraints must be linear/affine ($Ax=b$). Geometrically, this restricts the search to a flat plane (a subspace or affine set), which is the only type of surface that is both convex and concave.</li>
        </ol>

        <h3>2.2 The Feasible Set</h3>
        <p>The <b>feasible set</b> is the intersection of the domains of all functions and the sets defined by the constraints:
        $$ \mathcal{F} = \{x \in \mathcal{D} \mid f_i(x) \le 0, Ax=b\} $$
        Since the intersection of convex sets is convex, a convex optimization problem minimizes a convex function over a convex set. This geometric structure is the key to its solvability.</p>
      </section>

      <section class="section-card" id="section-3">
        <h2>3. The Fundamental Theorem: Local = Global</h2>

        <h3>3.1 The Theorem</h3>
        <p>The most important theoretical result in this course is the guarantee of global optimality.</p>
        <div class="theorem-box">
          <h4>Theorem: Global Optimality</h4>
          <p>For a convex optimization problem, any <b>local minimum</b> is also a <b>global minimum</b>. If the objective function is <i>strictly</i> convex, the global minimum is unique.</p>
        </div>

        <h3>3.2 The Proof (Straight Line Argument)</h3>
        <p>The proof relies on the definition of convexity: the line segment connecting any two feasible points lies entirely within the feasible set (geometry) and above the function graph (algebra).</p>
        <div class="proof-box">
          <h4>Proof by Contradiction</h4>
          <div class="proof-step">
            <strong>1. Assumption:</strong> Suppose $x^*$ is a local minimum, but <i>not</i> global. Then there exists a feasible point $y$ (potentially far away) such that $f_0(y) < f_0(x^*)$.
          </div>
          <div class="proof-step">
            <strong>2. The Path:</strong> Consider the line segment connecting $x^*$ and $y$: $z(\theta) = (1-\theta)x^* + \theta y$ for $\theta \in [0, 1]$. Since the feasible set is convex, every point $z(\theta)$ is feasible.
          </div>
          <div class="proof-step">
            <strong>3. The Value:</strong> By the convexity of $f_0$:
            $$ f_0(z(\theta)) \le (1-\theta)f_0(x^*) + \theta f_0(y) $$
            Since $f_0(y) < f_0(x^*)$, we have:
            $$ f_0(z(\theta)) < (1-\theta)f_0(x^*) + \theta f_0(x^*) = f_0(x^*) $$
            Thus, for <i>any</i> $\theta > 0$, the point $z(\theta)$ has a strictly lower objective value than $x^*$.
          </div>
          <div class="proof-step">
            <strong>4. Contradiction:</strong> For very small $\theta$, $z(\theta)$ is arbitrarily close to $x^*$. This contradicts the assumption that $x^*$ is a <i>local</i> minimum (which requires $f_0(x^*) \le f_0(z)$ for all neighbors $z$).
            <br><b>Conclusion:</b> No such better point $y$ can exist. $x^*$ must be globally optimal.
          </div>
        </div>
      </section>

      <section class="section-card" id="section-4">
        <h2>4. The Hierarchy of Convex Problems</h2>

        <p>Convex problems form a nested hierarchy. Specialized solvers exist for each class, exploiting their specific structure for efficiency.</p>

        <h3>4.1 Linear Programming (LP)</h3>
        <p><b>Linear Programs</b> minimize a linear objective subject to linear equality and inequality constraints. The feasible set is a polyhedron (a shape with flat facets, like a diamond or cube). LPs are the workhorses of operations research, used for resource allocation, network flow, and supply chain management.</p>

        <h3>4.2 Quadratic Programming (QP)</h3>
        <p><b>Quadratic Programs</b> minimize a convex quadratic function (elliptical contours) over a polyhedron. The quadratic term $x^\top Q x$ adds curvature to the objective ($Q \succeq 0$). QPs appear in portfolio optimization (minimizing variance), least squares regression, and control theory (LQR).</p>

        <h3>4.3 Second-Order Cone Programming (SOCP)</h3>
        <p><b>SOCPs</b> allow constraints involving the Euclidean norm ($\|Ax+b\|_2 \le c^\top x + d$). The feasible set is an intersection of affine sets and "ice-cream cones." SOCPs generalize QPs and are used in robust optimization and antenna array design.</p>

        <h3>4.4 Semidefinite Programming (SDP)</h3>
        <p><b>Semidefinite Programs</b> optimize a linear function over the cone of positive semidefinite matrices. The variable is a matrix $X \in \mathbb{S}^n$, and constraints are Linear Matrix Inequalities ($X \succeq 0$). SDPs are the most general class in this hierarchy, appearing in control, combinatorial relaxations, and quantum information.</p>

        <p><b>The Hierarchy:</b> $\text{LP} \subset \text{QP} \subset \text{SOCP} \subset \text{SDP}$. Every LP is a QP (with zero quadratic term), every QP can be cast as an SOCP, and every SOCP can be cast as an SDP.</p>
      </section>

      <section class="section-card" id="section-5">
        <h2>5. The Loss + Regularizer Modeling Paradigm</h2>

        <p>A vast number of problems in machine learning and signal processing follow a unified template:</p>
        $$ \min_{x} \quad \underbrace{\mathcal{L}(x; \text{data})}_{\text{Loss}} + \lambda \underbrace{\mathcal{R}(x)}_{\text{Regularizer}} $$
        <p>This formulation balances two competing goals: <b>fidelity</b> to the data (minimizing loss) and <b>simplicity</b> of the model (minimizing the regularizer).</p>
        <ul>
          <li><b>Loss $\mathcal{L}$:</b> Typically a convex function like squared error $\|Ax-b\|_2^2$ (Regression), logistic loss $\sum \log(1+e^{-y_i x^\top a_i})$ (Classification), or hinge loss (SVM).</li>
          <li><b>Regularizer $\mathcal{R}$:</b> Typically a convex norm. $\ell_2$ regularization (Ridge) encourages small, diffuse coefficients. $\ell_1$ regularization (LASSO) encourages <b>sparse</b> solutions, effectively performing feature selection.</li>
        </ul>
        <p>Because the sum of convex functions is convex, combining any convex loss with any convex regularizer yields a tractable convex problem.</p>
      </section>

      <section class="section-card" id="section-6">
        <h2>6. Standard Form Transformations</h2>

        <p>Real-world problems rarely arrive in standard form. We use transformation tricks to rewrite them into a format solvers accept.</p>

        <h3>6.1 Epigraph Form (Minimizing Max)</h3>
        <p>To minimize a function $f(x) = \max_i f_i(x)$, we introduce a scalar variable $t$ representing the "ceiling" or upper bound. The problem becomes minimizing $t$ subject to $f_i(x) \le t$ for all $i$. This "lifts" the non-smooth max function into a smooth, higher-dimensional constraint set.</p>

        <h3>6.2 Converting Norms</h3>
        <p>Norm minimization problems like $\min \|Ax-b\|_1$ are reformulated by expanding the definition. Since $|u| \le t \iff -t \le u \le t$, we replace the norm with a slack variable and linear constraints. Similarly, $\ell_\infty$ minimization becomes a set of "box" constraints.</p>

        <h3>6.3 Linear-Fractional Programming</h3>
        <p>Minimizing a ratio of linear functions $\frac{c^\top x + d}{e^\top x + f}$ is quasiconvex, not convex. However, we can transform it into a linear program using the Charnes-Cooper transformation (homogenization), introducing a variable $z = 1/(e^\top x + f)$ and solving for $y = xz$ and $z$.</p>
      </section>

      <section class="section-card" id="section-7">
        <h2>7. Disciplined Convex Programming (DCP)</h2>
        <p><b>Disciplined Convex Programming</b> is a methodology that imposes a set of construction rules (a "grammar") for building convex problems. If a problem is constructed according to these rules (e.g., "convex + convex", "affine composition"), it is guaranteed to be convex by construction. Modeling languages like CVXPY use DCP to automatically verify convexity and convert the user's high-level code into standard conic forms for the solver.</p>
      </section>

      <section class="section-card" id="section-8">
        <h2>8. Verifying Convexity: A Practical Checklist</h2>
        <p>Before attempting to solve a problem, perform these sanity checks:</p>
        <ol>
          <li><b>Feasibility:</b> Is the feasible set non-empty? Are constraints consistent?</li>
          <li><b>Objective:</b> Is the objective function convex (for min) or concave (for max)? Check second derivatives (PSD Hessian) or build it from convex atoms.</li>
          <li><b>Inequalities:</b> Are all inequality constraints of the form $\text{convex} \le 0$?</li>
          <li><b>Equalities:</b> Are all equality constraints affine ($Ax=b$)?</li>
        </ol>
      </section>

      <section class="section-card" id="section-9">
        <h2>9. The Optimization Workflow</h2>
        <p>Solving a convex problem in practice involves a standard pipeline:</p>
        <ol>
          <li><b>Model:</b> Formulate the real-world problem mathematically.</li>
          <li><b>Transform:</b> Apply rewrites (epigraphs, slack variables) to fit standard forms.</li>
          <li><b>Canonicalize:</b> (Usually done by software) Convert to conic form (LP/SOCP/SDP).</li>
          <li><b>Solve:</b> Invoke a numerical solver (e.g., ECOS, SCS, MOSEK).</li>
          <li><b>Verify:</b> Check KKT conditions and feasibility of the result.</li>
        </ol>
      </section>

      <section class="section-card" id="section-review">
        <h2>10. Review & Cheat Sheet</h2>
        <table class="data-table">
          <thead>
            <tr>
              <th>Problem Type</th>
              <th>Objective</th>
              <th>Constraints</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><b>LP</b></td>
              <td>Linear</td>
              <td>Linear inequalities</td>
            </tr>
            <tr>
              <td><b>QP</b></td>
              <td>Convex Quadratic</td>
              <td>Linear inequalities</td>
            </tr>
            <tr>
              <td><b>SOCP</b></td>
              <td>Linear</td>
              <td>Norm cones ($\|Ax+b\|_2 \le c^\top x + d$)</td>
            </tr>
            <tr>
              <td><b>SDP</b></td>
              <td>Linear</td>
              <td>Linear Matrix Inequalities (LMI)</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section class="section-card" id="section-exercises">
        <h2>11. Exercises</h2>
        <div class="problem">
          <h3>P2.1 — Classifying Problems</h3>
          <p>Determine whether the following are convex optimization problems:</p>
          <ol>
            <li>Minimize $\|Ax-b\|_2^2$ subject to $\|x\|_1 \le 1$.</li>
            <li>Minimize $- \sum \log(x_i)$ subject to $Ax=b, x \ge 0$.</li>
            <li>Minimize $x^\top Q x$ subject to $\|x\|_2 = 1$ (where $Q \succeq 0$).</li>
          </ol>
          <div class="solution-box">
            <h4>Solution</h4>
            <ol>
              <li><b>Convex.</b> Objective is convex quadratic (least squares). Constraint is convex (norm ball).</li>
              <li><b>Convex.</b> Objective is $-\sum \log x_i$, which is convex (since $\log$ is concave). Constraints are linear.</li>
              <li><b>Not Convex.</b> The equality constraint $\|x\|_2 = 1$ defines a sphere surface (not a convex set). This is a nonconvex QCQP.</li>
            </ol>
          </div>
        </div>
      </section>

    </article>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="../../static/lib/pyodide/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/resizable.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexCombination } from './widgets/js/convex-combination.js';
    initConvexCombination('widget-convex-combination');
  </script>
  <script type="module">
    import { initOptimizationLandscape } from './widgets/js/optimization-landscape.js';
    initOptimizationLandscape('widget-optimization-landscape');
  </script>
  <script type="module">
    import { initConvergenceComparison } from './widgets/js/convergence-comparison.js';
    initConvergenceComparison('widget-convergence-comparison');
  </script>
  <script type="module">
    import { initProblemFlowchart } from './widgets/js/problem-flowchart.js';
    initProblemFlowchart('widget-problem-flowchart');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
