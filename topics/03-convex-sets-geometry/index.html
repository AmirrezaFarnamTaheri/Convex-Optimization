<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>03. Convex Sets: Geometry — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/dist/katex.min.js"></script>
  <script defer src="../../static/lib/katex/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/dist/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "../../static/lib/three/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../02-introduction/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../04-convex-sets-cones/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>03. Convex Sets: Geometry</h1>
      <div class="lecture-summary">
        <p>This lecture builds the geometric foundation of convex optimization. We formalize affine and convex sets, construct key examples (halfspaces, norm balls, polyhedra, and the PSD cone), and learn a small set of operations that generate most convex sets used in practice. We end with a topological toolkit (closure, interior, relative interior) that becomes essential later for constraint qualifications and duality.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00</a> (vectors, inner products, norms) and <a href="../02-introduction/index.html">Lecture 02</a> (convexity motivation).</p>
        <p><strong>Forward Connections:</strong> Cone geometry and separation (<a href="../04-convex-sets-cones/index.html">Lecture 04</a>) and convex functions (<a href="../05-convex-functions-basics/index.html">Lecture 05</a>) build directly on these set constructions. Standard and conic problem classes (<a href="../07-convex-problems-standard/index.html">Lecture 07</a>, <a href="../08-convex-problems-conic/index.html">Lecture 08</a>) are largely “recognize the set.”</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Distinguish affine vs. convex geometry:</b> Tell apart affine hulls, convex hulls, and their geometric meaning (lines vs. line segments).</li>
        <li><b>Recognize canonical convex sets:</b> Identify halfspaces, hyperplanes, norm balls, ellipsoids, polyhedra/polytopes, and the PSD cone as “building blocks.”</li>
        <li><b>Generate new convex sets:</b> Use intersection, affine images/preimages, perspective, and linear-fractional maps to prove convexity quickly.</li>
        <li><b>Use relative interior correctly:</b> Work with $\mathrm{ri}(C)$ when $\mathrm{int}(C)$ is empty and connect this to feasibility notions used in duality.</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
        <h2>1. Affine and Convex Sets: Definitions and Basic Properties</h2>

        <p>The geometry of optimization is built on understanding how points combine. Two fundamental operations define the landscape:</p>

        <h3>1.1 Affine Combinations and Affine Sets</h3>

        <p>An <a href="#" class="definition-link">affine combination</a> is a linear combination of points where the coefficients sum to exactly one. Geometrically, this operation generates lines, planes, and hyperplanes passing through the given points, without reference to the origin.</p>
        $$
        \sum_{i=1}^k \theta_i x_i \quad \text{where} \quad \sum_{i=1}^k \theta_i = 1
        $$

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/layer3_32_subspace_affine.gif"
               alt="Animation distinguishing Subspaces from Affine Sets"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Animation:</i> A <b>Subspace</b> (left) is tied to the origin (infinite sheet through 0). An <b>Affine Set</b> (right) is a translated subspace (infinite sheet floating in space). Affine combinations let you travel anywhere on the sheet, but never off it.</figcaption>
        </figure>

        <h4>Two-point affine combination = full line</h4>
        <p>For two points $x, y \in \mathbb{R}^n$, the affine combination $z = \theta x + (1-\theta)y = y + \theta(x-y)$ traces the <b>entire line</b> through $y$ in the direction $x-y$ as $\theta$ ranges over all of $\mathbb{R}$.</p>
        <ul>
            <li>$\theta \in [0, 1]$: Segment between $y$ and $x$ (interpolation).</li>
            <li>$\theta > 1$: Extrapolation past $x$.</li>
            <li>$\theta < 0$: Extrapolation past $y$.</li>
        </ul>

        <h4>Why "sum to one" works (Translation Invariance)</h4>
        <p>The condition $\sum \theta_i = 1$ ensures that the operation commutes with translation. If you shift all points by $t$, the affine combination shifts by exactly $t$:
        $$ \sum \theta_i (x_i + t) = \sum \theta_i x_i + (\sum \theta_i)t = \sum \theta_i x_i + t $$
        This is why affine geometry is "linear geometry without a fixed origin."</p>

        <div class="theorem-box">
          <h4>Definition (Affine Set)</h4>
          <p>A set $C$ is <a href="#" class="definition-link">affine</a> if it contains all affine combinations of its points. Equivalently, the <b>infinite line</b> passing through any two points in $C$ lies entirely in $C$.</p>
          $$ x_1, x_2 \in C, \ \theta \in \mathbb{R} \implies \theta x_1 + (1-\theta)x_2 \in C $$
        </div>

        <h4>Affine Sets as Translated Subspaces</h4>
        <p>Geometrically, every affine set is a linear subspace that has been shifted (translated) away from the origin.</p>

        <div class="theorem-box">
          <h4>Theorem: Affine Set $\iff$ Translated Subspace</h4>
          <p>A set $C \subseteq \mathbb{R}^n$ is affine if and only if it can be written as:</p>
          $$
          C = x_0 + V = \{x_0 + v \mid v \in V\}
          $$
          <p>where $x_0$ is any specific point in $C$, and $V$ is a linear subspace of $\mathbb{R}^n$.</p>
          <div class="proof-box">
            <h4>Proof: The Geometric Shift</h4>
            <div class="proof-step">
              <strong>($\Leftarrow$ Subspace $\to$ Affine):</strong> Suppose $C = x_0 + V$. We check if $C$ contains the line through any two points $x_1, x_2 \in C$.
              <br>Since $x_1, x_2 \in C$, we can write $x_1 = x_0 + v_1$ and $x_2 = x_0 + v_2$ for some $v_1, v_2 \in V$.
              <br>Form the affine combination for any $\theta \in \mathbb{R}$:
              $$
              z = \theta x_1 + (1-\theta)x_2 = \theta(x_0 + v_1) + (1-\theta)(x_0 + v_2)
              $$
              Distribute and regroup terms relative to $x_0$:
              $$ z = \theta x_0 + \theta v_1 + (1-\theta)x_0 + (1-\theta)v_2 = (\theta + 1 - \theta)x_0 + \theta v_1 + (1-\theta)v_2 = x_0 + (\theta v_1 + (1-\theta)v_2) $$
              Since $V$ is a subspace, it is closed under scalar multiplication and vector addition.
              <ul>
                <li>$\theta v_1 \in V$ and $(1-\theta)v_2 \in V$.</li>
                <li>Their sum $v' = \theta v_1 + (1-\theta)v_2$ is in $V$.</li>
              </ul>
              <br>Therefore, $z = x_0 + v' \in x_0 + V = C$.
            </div>
            <div class="proof-step">
              <strong>($\Rightarrow$ Affine $\to$ Subspace):</strong> Suppose $C$ is affine. Pick a base point $x_0 \in C$. Define the set of displacements $V = C - x_0 = \{c - x_0 \mid c \in C\}$. We must prove $V$ is a linear subspace.
              <ul>
                <li><b>Contains zero:</b> Since $x_0 \in C$, $x_0 - x_0 = 0 \in V$.</li>
                <li><b>Closed under scaling:</b> Let $v \in V$ and $\alpha \in \mathbb{R}$. We want $\alpha v \in V$.
                <br>Since $v \in V$, $x_0 + v \in C$. Since $C$ is affine, the affine combination of $(x_0 + v)$ and $x_0$ with weights $\alpha$ and $(1-\alpha)$ is in $C$:
                $$ \alpha(x_0 + v) + (1-\alpha)x_0 = \alpha x_0 + \alpha v + x_0 - \alpha x_0 = x_0 + \alpha v $$
                Since $x_0 + \alpha v \in C$, by definition $\alpha v \in V$.</li>
                <li><b>Closed under addition:</b> Let $u, w \in V$. We want $u+w \in V$.
                <br>Since $u, w \in V$, $x_0+u \in C$ and $x_0+w \in C$.
                <br>Since $C$ is affine, the midpoint is in $C$:
                $$ m = \frac{1}{2}(x_0+u) + \frac{1}{2}(x_0+w) = x_0 + \frac{1}{2}(u+w) $$
                Thus $\frac{1}{2}(u+w) \in V$.
                <br>Since $V$ is closed under scaling (proven above), $2 \cdot \frac{1}{2}(u+w) = u+w \in V$.</li>
              </ul>
              Since $V$ contains 0 and is closed under addition and scaling, it is a subspace.
            </div>
          </div>
        </div>


        <!-- Moved P3.20 to Section 6 -->

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/affine-subspace.png"
               alt="An affine set C shown as a red plane parallel to a blue subspace V passing through the origin"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 1:</i> An affine set $C$ (red) is a subspace $V$ (blue) translated by a vector $x_0$. Note that $V$ passes through the origin, while $C$ does not necessarily.</figcaption>
        </figure>

        <div class="example">
          <h4>Examples of Affine Sets</h4>
          <ul>
            <li><b>Solution set of linear equations:</b> $C = \{x \mid Ax = b\}$. If $x_0$ is a particular solution ($Ax_0 = b$), then $C = x_0 + \mathcal{N}(A)$, where the nullspace $\mathcal{N}(A)$ is the associated subspace.</li>
            <li><b>$\mathbb{R}^n$ itself:</b> The entire space is affine by definition ($V = \mathbb{R}^n, x_0 = 0$).</li>
            <li><b>Single point $\{x_0\}$:</b> Affine ($V = \{0\}$).</li>
          </ul>
        </div>

        <h3>1.2 Convex Combinations and Convex Sets</h3>

        <p>A <a href="#" class="definition-link">convex combination</a> is an affine combination with the additional constraint that all weights are nonnegative:</p>
        $$
        \sum_{i=1}^k \theta_i x_i \quad \text{where} \quad \theta_i \ge 0, \ \sum_{i=1}^k \theta_i = 1
        $$

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/layer3_33_span_generators.gif"
               alt="Animation of Convex Combinations vs Linear Span"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Animation:</i> <b>Convex Hull</b> vs <b>Affine Hull</b> vs <b>Linear Span</b>.
          <br>$\bullet$ <b>Span (Blue):</b> Infinite sheet through origin (Linear combinations).
          <br>$\bullet$ <b>Affine (Green):</b> Infinite sheet through points (Affine combinations).
          <br>$\bullet$ <b>Convex (Orange):</b> The finite region strictly "between" the points (Convex combinations).</figcaption>
        </figure>

        <h4>Mixing vs Scaling</h4>
        <p>Convex combinations represent "mixing" (interpolation) without "scaling" (size change).
        <br>Consider a linear combination $z = \alpha x + \beta y$.
        <ul>
            <li>If $\alpha+\beta \neq 1$, the result includes a global scaling factor. For example, $x+y$ (where weights are 1 and 1) is not a mix; it is a mix scaled by 2 ($2 \cdot (0.5x + 0.5y)$).</li>
            <li>If $\alpha < 0$, the result includes extrapolation (subtraction).</li>
        </ul>
        Convexity restricts operations to pure mixing: you cannot go outside the segment between $x$ and $y$.</p>

        <div class="insight">
            <h4>Comparison: Affine vs. Convex Combinations</h4>
            <table class="data-table">
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Affine Combination</th>
                        <th>Convex Combination</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><b>Formula</b></td>
                        <td>$\sum \theta_i x_i$ with $\sum \theta_i = 1$</td>
                        <td>$\sum \theta_i x_i$ with $\sum \theta_i = 1, \theta_i \ge 0$</td>
                    </tr>
                    <tr>
                        <td><b>Geometric Result (2 points)</b></td>
                        <td>The infinite <b>line</b> through $x_1, x_2$.</td>
                        <td>The finite <b>line segment</b> between $x_1, x_2$.</td>
                    </tr>
                    <tr>
                        <td><b>Geometric Result (3 points)</b></td>
                        <td>The infinite <b>plane</b> containing the triangle.</td>
                        <td>The finite <b>triangle</b> (including interior).</td>
                    </tr>
                    <tr>
                        <td><b>Intuition</b></td>
                        <td>"Interpolation + Extrapolation" (Unbounded)</td>
                        <td>"Interpolation / Mixing" (Bounded)</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p><b>Intuition (The Elastic Sheet):</b> The constraints $\theta_i \ge 0$ prevent us from "extrapolating" beyond the points. We are restricted to "interpolating" between them. Think of a rubber band stretched around nails; the convex hull is the area inside the band.
        <br>For two points, $z = \theta x + (1-\theta)y = y + \theta(x-y)$. As $\theta$ runs from 0 to 1, we trace the segment from $y$ to $x$. If $\theta < 0$ or $\theta > 1$, we extrapolate.</p>

        <div class="theorem-box">
          <h4>Definition (Convex Set)</h4>
          <p>A set $C \subseteq \mathbb{R}^n$ is <a href="#" class="definition-link" data-term="convex set">convex</a> if for any two points $x, y \in C$ and any $\theta \in [0,1]$:</p>
          $$
          \theta x + (1-\theta)y \in C
          $$
          <p><b>Geometric meaning:</b> The line segment connecting any two points in the set lies entirely within the set—no "dents" or "holes."
          <br><b>Quantifiers matter:</b> This must hold for <b>all</b> pairs $x,y \in C$ and <b>all</b> $\theta \in [0,1]$. A single counterexample (one leaking segment) disproves convexity.</p>

          <h4 style="margin-top: 15px;">Theorem (Pairwise Closure implies Finite Closure)</h4>
          <p>If a set $C$ is closed under convex combinations of <b>pairs</b> of points, then it is closed under convex combinations of <b>any finite number</b> of points.
          <br><i>Proof:</i> We prove by induction on $k$.
          <br><b>Base case $k=2$:</b> True by definition.
          <br><b>Inductive step:</b> Assume true for $k-1$. Let $z = \sum_{i=1}^k \theta_i x_i$ with $\theta_i \ge 0, \sum \theta_i = 1$.
          If $\theta_k = 1$, then $z=x_k \in C$. If $\theta_k < 1$, define $\bar{\theta}_i = \theta_i / (1-\theta_k)$ for $i < k$. Note $\sum \bar{\theta}_i = 1$.
          Let $y = \sum_{i=1}^{k-1} \bar{\theta}_i x_i$. By hypothesis, $y \in C$.
          Now observe $z = (1-\theta_k)y + \theta_k x_k$. This is a convex combination of two points in $C$. Thus $z \in C$.</p>
        </div>

        <h4>Operations that Preserve Convexity</h4>
        <p>The following operations preserve convexity. This is the "calculus" of convex sets.</p>
        <ul>
          <li><b>Intersection:</b> $\cap_i C_i$. (Corresponds to logical AND).</li>
          <li><b>Affine Image:</b> $f(C)$ where $f(x)=Ax+b$.</li>
          <li><b>Affine Preimage:</b> $f^{-1}(D)$ where $D$ is convex and $f(x)=Ax+b$. (This is the most used constraint form).</li>
          <li><b>Cartesian Product:</b> $C \times D$.</li>
          <li><b>Minkowski Sum:</b> $C + D$.</li>
        </ul>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/convex-vs-nonconvex.png"
               alt="Visual definition of convex and non-convex sets"
               style="max-width: 800px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 2:</i> In a convex set (right), the line segment between any two points lies entirely within the set. In a non-convex set (left), some line segments (red) cross outside the set boundaries.</figcaption>
        </figure>

        <h3>1.3 Convex Hull</h3>

        <p>The <a href="#" class="definition-link">convex hull</a> of a set $S$ is the smallest convex set that contains $S$. Intuitively, it is the shape formed by "shrink-wrapping" the set $S$ or stretching a rubber band around it. Mathematically, it is the set of all possible convex combinations of points in $S$:</p>
        $$
        \mathrm{conv}(S) = \left\{\sum_{i=1}^k \theta_i x_i \ \bigg| \ x_i \in S, \ \theta_i \ge 0, \ \sum_{i=1}^k \theta_i = 1, \ k \in \mathbb{N}\right\}
        $$
        <p><b>Probability Interpretation:</b> If $\theta_i$ are probabilities ($\theta_i \ge 0, \sum \theta_i = 1$), the convex combination is the expectation $\mathbb{E}[X]$ of a random variable $X$ taking values in $S$. Thus, the convex hull is the set of all possible averages.</p>
        <p><b>Minimality:</b> $\operatorname{conv}(S)$ is the unique smallest convex set containing $S$. Any convex set containing $S$ must contain all convex combinations of points in $S$.</p>

        <h4>Recession Cone (Asymptotic Directions)</h4>
        <p>For unbounded convex sets, it is useful to characterize directions in which the set extends infinitely. The <b>recession cone</b> of a convex set $C$ is defined as:</p>
        $$ R_C = \{v \in \mathbb{R}^n \mid x + tv \in C \text{ for all } x \in C, t \ge 0\} $$
        <p>Intuitively, if you look at $C$ from "infinitely far away", it looks like $R_C$.
        <br><b>Key Properties:</b>
        <ul>
            <li>$R_C$ is always a closed convex cone.</li>
            <li>A closed convex set is <b>bounded</b> if and only if its recession cone is trivial ($R_C = \{0\}$).</li>
            <li>If $C = \{x \mid Ax \le b\}$, then $R_C = \{v \mid Av \le 0\}$. The recession directions are those that keep you feasible as you move infinitely far.</li>
        </ul>
        This concept is crucial for proving the existence of solutions: if a problem is unbounded, the improving direction lives in the recession cone.</p>

        <div class="theorem-box">
          <h4>Theorem (Carathéodory's Theorem)</h4>
          <p>If $S \subseteq \mathbb{R}^n$, then every point in $\mathrm{conv}(S)$ can be written as a convex combination of at most $n+1$ points from $S$.</p>
          <p><b>Implication:</b> To describe any point in the convex hull, we never need more than $n+1$ points, regardless of how large $S$ is!</p>
          <div class="example">
            <h4>Example: $\mathbb{R}^2$</h4>
            <p>In 2D, $n=2$, so we need at most $n+1=3$ points.
            <br>Consider a cloud of 100 points. The convex hull is a polygon. Any point <i>inside</i> that polygon can be covered by a triangle formed by just 3 of the original vertices. You don't need all 100.</p>
          </div>
        </div>

        <div class="proof-box">
          <h4>Proof of Carathéodory's Theorem (Step-by-Step Reduction)</h4>
          <p><b>Goal:</b> Show that if a point $\mathbf{x}$ is a convex combination of $k$ points where $k > n+1$, we can rewrite it as a convex combination of $k-1$ points. By repeating this, we stop at $n+1$.</p>

          <div class="proof-step">
            <strong>Step 1: Setup.</strong>
            Suppose $\mathbf{x} = \sum_{i=1}^k \theta_i \mathbf{x}_i$ with $\sum \theta_i = 1$ and $\theta_i > 0$. Assume $k > n+1$.
            The $k$ points are <b>affinely dependent</b>. This means we can find coefficients $\alpha_1, \dots, \alpha_k$ (not all zero) that sum to zero, such that the weighted sum of vectors is also zero:
            $$ \sum_{i=1}^k \alpha_i \mathbf{x}_i = 0, \quad \sum_{i=1}^k \alpha_i = 0 $$
            Since $\sum \alpha_i = 0$, some $\alpha_i$ must be positive and some negative.
          </div>

          <div class="proof-step">
            <strong>Step 2: The Perturbation Strategy.</strong>
            We want to change the weights $\theta_i$ to eliminate one point while keeping the sum valid.
            Consider new weights $\theta'_i(\lambda) = \theta_i - \lambda \alpha_i$.
            We need to verify two things:
            <ol>
                <li><b>Affine Sum is Preserved:</b> The weights still sum to 1.
                $$ \sum \theta'_i = \sum (\theta_i - \lambda \alpha_i) = \sum \theta_i - \lambda \sum \alpha_i = 1 - \lambda(0) = 1 $$
                </li>
                <li><b>Point is Preserved:</b> The combination still produces $\mathbf{x}$.
                $$ \sum \theta'_i \mathbf{x}_i = \sum (\theta_i - \lambda \alpha_i)\mathbf{x}_i = \sum \theta_i \mathbf{x}_i - \lambda \sum \alpha_i \mathbf{x}_i = \mathbf{x} - \lambda(0) = \mathbf{x} $$
                </li>
            </ol>
            Thus, for any $\lambda$, the new combination is a valid affine representation of $\mathbf{x}$. Our goal is to choose $\lambda$ so it becomes a valid <i>convex</i> representation (non-negative weights) with fewer non-zero terms.
          </div>

          <div class="proof-step">
            <strong>Step 3: Moving to the Boundary.</strong>
            We need to choose $\lambda$ so that all new weights $\theta'_i(\lambda) = \theta_i - \lambda \alpha_i$ remain non-negative, but at least one becomes exactly 0.
            <br>Analyze the condition $\theta_i - \lambda \alpha_i \ge 0$ for each index $i$:
            <ul>
                <li><b>Case 1: $\alpha_i \le 0$.</b> Since $\lambda > 0$, the term $-\lambda \alpha_i \ge 0$. The weight $\theta'_i$ increases or stays constant. Since we started with $\theta_i > 0$, we are safe.</li>
                <li><b>Case 2: $\alpha_i > 0$.</b> Here, the weight decreases as $\lambda$ increases. To keep $\theta'_i \ge 0$, we require $\theta_i \ge \lambda \alpha_i$, or $\lambda \le \frac{\theta_i}{\alpha_i}$.</li>
            </ul>
            To ensure <i>all</i> weights remain non-negative, $\lambda$ must satisfy this upper bound for all $i$ where $\alpha_i > 0$. To eliminate at least one point, we push $\lambda$ to the tightest such bound:
            $$ \lambda^* = \min_{\{i \mid \alpha_i > 0\}} \frac{\theta_i}{\alpha_i} $$
            Let $j$ be an index that achieves this minimum. By construction, $\theta'_j(\lambda^*) = 0$ and $\theta'_i(\lambda^*) \ge 0$ for all $i$.
          </div>

          <div class="proof-step">
            <strong>Step 4: Reduction.</strong>
            Set the new weights $\theta'_i = \theta_i - \lambda^* \alpha_i$.
            <ul>
                <li>For all $i$, $\theta'_i \ge 0$ by construction.</li>
                <li>For the index $j$, $\theta'_j = 0$.</li>
                <li>The point $\mathbf{x}$ is now a convex combination of the remaining $k-1$ points (excluding $\mathbf{x}_j$).</li>
            </ul>
          </div>

          <div class="proof-step">
            <strong>Conclusion:</strong> We reduced the count from $k$ to $k-1$. We can repeat this process as long as $k-1 > n+1$. Thus, we eventually express $\mathbf{x}$ using at most $n+1$ points.
          </div>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/convex-hull.png"
               alt="Illustration of a convex hull"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 3:</i> The convex hull of a set of points (black dots) acts like a rubber band (blue polygon) snapped tight around them.</figcaption>
        </figure>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0; flex-wrap: wrap;">
            <figure style="text-align: center; flex: 1; min-width: 280px;">
              <img src="assets/convex-hull-nonconvex.png"
                   alt="Convex hull of a crescent shape"
                   style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 4:</i> The convex hull of a non-convex set (dark grey crescent) fills in the "gaps" (light blue), creating the smallest convex superset.</figcaption>
            </figure>
            <figure style="text-align: center; flex: 1; min-width: 280px;">
              <img src="assets/caratheodory.png"
                   alt="Caratheodory's theorem visualization"
                   style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 5:</i> Carathéodory's Theorem: Any point $\mathbf{x}$ in the hull of many points (black) can be formed by a convex combination of $n+1$ of them (red triangle in $\mathbb{R}^2$).</figcaption>
            </figure>
        </div>

        <h3>1.4 Sets Defined by Functions</h3>

        <p>A powerful way to generate convex sets is to derive them from convex functions (<a href="../05-convex-functions-basics/index.html">Lecture 05</a>). Two primary objects bridge the gap between sets and functions: sublevel sets and epigraphs.</p>

        <h4>(a) Sublevel Sets</h4>
        <p>The $\alpha$-<a href="#" class="definition-link">sublevel set</a> of a function $f : \mathbb{R}^n \to \mathbb{R}$ is the set of all points where the function value is below a threshold $\alpha$:</p>
        $$
        C_\alpha = \{x \in \mathrm{dom} f \mid f(x) \le \alpha\}
        $$
        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f$ is a convex function, then its sublevel set $C_\alpha$ is a convex set for any $\alpha \in \mathbb{R}$.</p>
          <p><i>Proof:</i> Let $x, y \in C_\alpha$ and $\theta \in [0,1]$. Then $f(x) \le \alpha$ and $f(y) \le \alpha$. By convexity, $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) \le \theta \alpha + (1-\theta)\alpha = \alpha$. Thus the combination is in $C_\alpha$.</p>
          <p><i>Note:</i> The converse is false. Functions whose sublevel sets are convex but who are not themselves convex are called <b>quasiconvex</b>.</p>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
             <img src="assets/indicator-function.png"
                  alt="3D plot of an indicator function"
                  style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
             <figcaption><i>Figure 6:</i> The sublevel set of a convex function corresponds to a convex set. For example, the unit ball is the 1-sublevel set of the norm function $f(x) = \|x\|$.</figcaption>
        </figure>

        <h4>(b) Epigraphs</h4>
        <p>The <a href="#" class="definition-link">epigraph</a> of a function $f$ is the set of points lying on or above its graph in $\mathbb{R}^{n+1}$:</p>
        $$
        \mathrm{epi}(f) = \{(x, t) \in \mathbb{R}^{n+1} \mid x \in \mathrm{dom} f, \ f(x) \le t\}
        $$
        <p>This object is the "Rosetta Stone" connecting the algebra of functions to the geometry of sets.</p>

        <div class="theorem-box">
          <h4>Fundamental Bridge Theorem</h4>
          <p>A function $f$ is <b>convex</b> if and only if its <b>epigraph</b> $\mathrm{epi}(f)$ is a <b>convex set</b>.</p>
          <p><b>Why is this useful?</b> This equivalence allows us to translate analytic properties of functions into geometric properties of sets. For example, why is the maximum of two convex functions also convex?
          <br> Analytically, proving $f(x) = \max(f_1(x), f_2(x))$ is convex requires checking inequalities.
          <br> Geometrically, $\mathrm{epi}(f) = \mathrm{epi}(f_1) \cap \mathrm{epi}(f_2)$. Since the intersection of convex sets is convex, the result is immediate.</p>
        </div>

        <h3>1.5 Key Properties</h3>

        <ul>
          <li>Every affine set is convex (but not vice versa).</li>
          <li>The intersection of any collection of convex sets is convex (proven in Section 3).</li>
          <li>The union of convex sets is generally <b>not</b> convex.</li>
        </ul>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Tool: Convex Set Checker</h3>
          <p><b>Test Convexity Interactively:</b> Use the drawing tools below to create shapes and verify their convexity. Any "dent" will be flagged as a violation of the line segment property.</p>
          <!-- Note: The standalone drawing widget has been merged into the lab below for a unified experience. -->
        </div>
      </section>


      <section class="section-card" id="section-2">
        <h2>2. Canonical Convex Sets: Building Blocks</h2>

        <p>These fundamental convex sets appear repeatedly in optimization formulations. Recognizing them is essential for problem classification.</p>

        <h3>2.1 Hyperplanes and Halfspaces</h3>

        <p>A <a href="#" class="definition-link">hyperplane</a> is a set of the form:</p>
        $$
        H = \{x \in \mathbb{R}^n \mid a^\top x = b\}
        $$
        <p>where $a \in \mathbb{R}^n \setminus \{0\}$ and $b \in \mathbb{R}$. The vector $a$ is the <b>normal vector</b> (perpendicular to the hyperplane).
        <br><b>Constraint Interpretation:</b> $H$ is the solution set of a single linear equation.
        <br><b>Affine Set Interpretation:</b> $H$ is a subspace ($\ker a^\top$) shifted by any point $x_0$ such that $a^\top x_0 = b$. It is an affine set of codimension 1.
        <br><b>Particular + Nullspace:</b> $H = x_0 + \{s \mid a^\top s = 0\}$.</p>

        <div class="insight">
          <h4>Geometric Intuition: The Level Set View</h4>
          <p>Consider the function $f(x) = a^\top x$. This function measures "how far" you are in direction $a$.
          <br>$\bullet$ The hyperplane $H$ is the level set where $f(x) = b$.
          <br>$\bullet$ The normal vector $a$ points in the direction of increasing value.
          <br>$\bullet$ Changing $b$ shifts the hyperplane parallel to itself along the direction $a$.
          </p>
        </div>

        <div class="theorem-box">
          <h4>Signed Distance to a Hyperplane</h4>
          <p>The signed distance of a point $y$ to $H$ is:
          $$ \frac{a^\top y - b}{\|a\|_2} $$
          <br><b>Derivation:</b> The closest point $x^*$ to $y$ on the hyperplane lies along the normal direction. $x^* = y - \lambda a$.
          Enforcing $a^\top x^* = b \implies a^\top(y-\lambda a) = b \implies \lambda = \frac{a^\top y - b}{\|a\|^2}$.
          The distance is $\|y-x^*\| = \|\lambda a\| = |\lambda| \|a\| = \frac{|a^\top y - b|}{\|a\|}$.</p>
        </div>

        <p>A <a href="#" class="definition-link">halfspace</a> is a set of the form:</p>
        $$
        H^- = \{x \in \mathbb{R}^n \mid a^\top x \le b\}
        $$
        <p><b>Convexity:</b> Halfspaces are convex.
        <br><i>Proof:</i> If $a^\top x \le b$ and $a^\top y \le b$, then $a^\top(\theta x + (1-\theta)y) \le \theta b + (1-\theta)b = b$ for $\theta \in [0,1]$.
        <br>Note that halfspaces are generally <b>not affine</b> (extrapolation can leave the set).</p>

        <div class="proof-box">
          <h4>Proof: Hyperplanes and Halfspaces are Convex</h4>

          <div class="proof-step">
            <strong>Hyperplane:</strong> Take $x_1, x_2 \in H$ and $\theta \in [0,1]$. Then:
            $$
            a^\top(\theta x_1 + (1-\theta)x_2) = \theta a^\top x_1 + (1-\theta)a^\top x_2 = \theta b + (1-\theta)b = b
            $$
            So the entire segment lies in $H$.
          </div>

          <div class="proof-step">
            <strong>Halfspace:</strong> Take $x_1, x_2 \in H^-$ (so $a^\top x_i \le b$) and $\theta \in [0,1]$. Then:
            $$
            a^\top(\theta x_1 + (1-\theta)x_2) = \theta a^\top x_1 + (1-\theta)a^\top x_2 \le \theta b + (1-\theta)b = b
            $$
            So the segment lies in $H^-$.
          </div>

          <div class="proof-step">
            <strong>Alternate View (Preimage):</strong> Define the linear function $f(x) = a^\top x$. Then $H^- = f^{-1}((-\infty, b])$. Since $(-\infty, b]$ is a convex interval in $\mathbb{R}$ and preimages of convex sets under affine maps are convex, $H^-$ is convex.
          </div>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/hyperplane-halfspace.png"
               alt="A 3D visualization of a hyperplane dividing space into two halfspaces"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 7:</i> A hyperplane (transparent plane) divides $\mathbb{R}^3$ into two halfspaces. The normal vector $a$ determines the orientation.</figcaption>
        </figure>

        <h3>2.2 Norm Balls: The Geometric Machine</h3>

        <p>A <a href="#" class="definition-link" data-term="norm ball">norm ball</a> centered at $x_c$ with radius $r$ is:</p>
        $$
        B(x_c, r) = \{x \in \mathbb{R}^n \mid \|x - x_c\| \le r\}
        $$
        <p>where $\|\cdot\|$ is any norm. All norm balls are convex (by the triangle inequality). In fact, the geometry of the unit ball $B = \{x : \|x\| \le 1\}$ completely determines the norm and its optimization properties.</p>

        <h4>The Unit Ball Completely Characterizes the Norm</h4>
        <p>There is a fundamental equivalence between norms and certain convex sets. If I hand you the unit ball $B$, I’ve essentially handed you the norm. You recover $\|x\|$ by asking: "How much must I scale the unit ball to catch the vector $x$?"</p>
        <p>This is formalized via the <b>Minkowski functional</b> (or gauge):</p>
        $$ \|x\| = \inf\{t > 0 \mid x \in tB\} $$

        <div class="proof-box">
          <h4>Proof: Unit Ball Determines Norm</h4>
          <p>We prove $\|x\| = \inf\{t > 0 \mid x \in tB\}$ using only the norm axioms.</p>
          <div class="proof-step">
            <strong>Step 1: Lower Bound ($t \ge \|x\|$).</strong>
            Assume $x \in tB$. By definition, $x = tb$ for some $b \in B$ (where $\|b\| \le 1$).
            Using homogeneity: $\|x\| = \|tb\| = t\|b\| \le t$.
            Thus, every feasible scaling factor $t$ must be at least $\|x\|$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Achievability ($t = \|x\|$ works).</strong>
            If $x \neq 0$, let $b = x/\|x\|$. Then $\|b\|=1$, so $b \in B$.
            We can write $x = \|x\| b$, which means $x \in \|x\|B$.
            So $t=\|x\|$ is a valid scaling factor.
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> Since $\|x\|$ is a lower bound for the set of valid $t$'s and is itself a valid $t$, the infimum is exactly $\|x\|$.
          </div>
        </div>

        <div class="theorem-box">
          <h4>Theorem: What Geometric Properties Must a Norm Ball Have?</h4>
          <p>A set $B \subset \mathbb{R}^n$ is the unit ball of <i>some</i> norm if and only if it satisfies four geometric properties. Each property corresponds to a specific norm axiom:</p>
          <ol>
            <li><b>Convexity (from Triangle Inequality):</b> $B$ must be convex.
                <br><i>Why:</i> $\| \theta x + (1-\theta)y \| \le \theta \|x\| + (1-\theta)\|y\| \le \theta(1) + (1-\theta)(1) = 1$. The ball contains the segment between any two points. No dents.</li>
            <li><b>Central Symmetry (from Homogeneity):</b> $B$ is symmetric around the origin ($x \in B \iff -x \in B$).
                <br><i>Why:</i> $\|-x\| = |-1|\|x\| = \|x\|$. Every point has an opposite.</li>
            <li><b>Absorbing (from Finiteness):</b> For any $x$, there exists $t>0$ such that $x \in tB$.
                <br><i>Why:</i> The ball is "solid" around the origin; it is not trapped in a lower-dimensional subspace.</li>
            <li><b>Boundedness and Closure (from Definiteness):</b> The ball does not extend to infinity, and contains its boundary.
                <br><i>Why:</i> Essential for $\|\cdot\|$ to be a valid function on $\mathbb{R}^n$ with $\|x\|=0 \iff x=0$.</li>
          </ol>
          <p><b>The Inverse:</b> Any set $C$ satisfying these four properties defines a norm via the Minkowski functional $\gamma_C(x) = \inf\{t>0 : x \in tC\}$.</p>
        </div>

        <h4>Geometry of Specific Norms: Smooth vs. Pointy vs. Flat</h4>
        <p>The boundary structure of the unit ball—whether it is smooth, pointy, or flat—predicts how the norm behaves in optimization. This is not folklore; it is geometry.</p>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0;">
            <figure style="text-align: center; flex: 1;">
              <img src="assets/norm-balls.png"
                   alt="Comparison of unit balls for L1, L2, and L-infinity norms"
                   style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 8:</i> The shapes of unit balls in $\mathbb{R}^2$.</figcaption>
            </figure>
        </div>

        <ul>
            <li><b>$\ell_2$ (Euclidean): Circle/Sphere.</b>
            <br><i>Geometry:</i> Rotationally symmetric and <b>smooth</b> everywhere.
            <br><i>Optimization Consequence:</i> <b>Uniqueness.</b> When minimizing a linear function $c^\top x$ over the $\ell_2$ ball, there is a unique boundary point where the normal vector aligns with $-c$. Solutions vary continuously with data.</li>

            <li><b>$\ell_\infty$ (Max Norm): Square/Box.</b>
            <br><i>Geometry:</i> <b>Flat faces</b> and corners. The unit ball is the hypercube $\{x : -1 \le x_i \le 1\}$.
            <br><i>Optimization Consequence:</i> <b>Non-uniqueness.</b> Linear functionals are maximized on entire faces. If the gradient points "mostly" in the $x_1$ direction, the solution is the entire face $x_1=1$. This corresponds to "box constraints" in modeling.</li>

            <li><b>$\ell_1$ (Sum Norm): Diamond/Cross-Polytope.</b>
            <br><i>Geometry:</i> <b>Pointy corners</b> on the axes. The vertices are $\pm e_i$.
            <br><i>Optimization Consequence:</i> <b>Sparsity.</b> Because the "pointiest" parts of the ball lie on the coordinate axes, linear optimization tends to land exactly on a vertex (where $n-1$ coordinates are zero). This is the geometric engine behind LASSO and compressed sensing.</li>
        </ul>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/norm-balls.png"
               alt="Comparison of unit balls for L1, L2, and L-infinity norms"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 8:</i> Unit balls for different norms in $\mathbb{R}^2$: $L_1$ (diamond), $L_2$ (circle), and $L_\infty$ (square).</figcaption>
        </figure>

        <h3>2.3 Dual Norms: The Shadow Ruler</h3>

        <p>The <b>dual norm</b> $\|\cdot\|_*$ is defined by maximizing the inner product over the primal unit ball:</p>
        $$ \boxed{\ \|u\|_* := \sup_{\|x\| \le 1} u^\top x\ } $$

        <p><b>What it measures:</b> If the primal norm $\|x\|$ measures the "size" of a vector, the dual norm $\|u\|_*$ measures the "measuring power" of the linear functional $f(x) = u^\top x$. Geometrically, it is the distance to the supporting hyperplane of the unit ball in the direction $u$.</p>

        <div class="theorem-box">
            <h4>Theorem: Properties of the Dual Norm</h4>
            <ol>
                <li><b>It is a Norm:</b> $\|\cdot\|_*$ satisfies all three norm axioms (positivity, homogeneity, triangle inequality).</li>
                <li><b>Generalized Hölder Inequality:</b> For any $u, x$:
                $$ u^\top x \le \|u\|_* \|x\| $$
                This is the universal bound for linear functionals.</li>
                <li><b>Dual of Dual:</b> The dual of the dual is the primal: $(\|\cdot\|_* )_* = \|\cdot\|$.</li>
            </ol>
        </div>

        <div class="proof-box">
          <h4>Proof: The Dual Norm is a Valid Norm</h4>
          <p>We verify the axioms for $\|u\|_* = \sup_{x \in B} u^\top x$.</p>
          <div class="proof-step">
            <strong>(N1) Definiteness ($u=0 \iff \|u\|_*=0$).</strong>
            If $u=0$, $\sup 0 = 0$.
            Conversely, if $\sup_{x \in B} u^\top x = 0$, then $u^\top x \le 0$ for all $x \in B$. Since $B$ is symmetric (contains $-x$), $u^\top (-x) \le 0 \implies u^\top x \ge 0$.
            Thus $u^\top x = 0$ for all $x \in B$. Since $B$ spans $\mathbb{R}^n$, $u$ must be 0.
          </div>
          <div class="proof-step">
            <strong>(N2) Homogeneity ($\|\alpha u\|_* = |\alpha|\|u\|_*$).</strong>
            For $\alpha \ge 0$, $\sup (\alpha u)^\top x = \alpha \sup u^\top x$.
            For $\alpha < 0$, let $\alpha = -|\alpha|$. $\sup (-|\alpha| u)^\top x = |\alpha| \sup u^\top (-x)$.
            Since $B$ is symmetric ($x \in B \iff -x \in B$), maximizing over $-x$ is the same as maximizing over $x$.
          </div>
          <div class="proof-step">
            <strong>(N3) Triangle Inequality.</strong>
            $\|u+v\|_* = \sup_{x \in B} (u+v)^\top x = \sup_{x \in B} (u^\top x + v^\top x) \le \sup_{x \in B} u^\top x + \sup_{x \in B} v^\top x = \|u\|_* + \|v\|_*$.
          </div>
        </div>

        <h4>Canonical Dual Pairs</h4>
        <p>We derive the dual norms for the standard $\ell_p$ norms.</p>
        <ul>
            <li><b>$\ell_2$ is Self-Dual:</b> $\|\mathbf{y}\|_{2,*} = \|\mathbf{y}\|_2$.
            <br><i>Reason:</i> The Cauchy-Schwarz inequality $u^\top x \le \|u\|_2 \|x\|_2$ gives the upper bound. Equality is achieved by choosing $x$ parallel to $u$.</li>

            <li><b>$\ell_1 \leftrightarrow \ell_\infty$:</b> The dual of $\ell_1$ is $\ell_\infty$.
            <br><i>Reason:</i> We want to maximize $\sum u_i x_i$ subject to $\sum |x_i| \le 1$. The best strategy is to put all the "mass" of $x$ (magnitude 1) onto the index $k$ where $|u_k|$ is largest. Then the sum becomes $|u_k| \cdot 1 = \|u\|_\infty$.</li>

            <li><b>$\ell_\infty \leftrightarrow \ell_1$:</b> The dual of $\ell_\infty$ is $\ell_1$.
            <br><i>Reason:</i> We want to maximize $\sum u_i x_i$ subject to $|x_i| \le 1$ for all $i$. To maximize the sum, we should simply match the signs: set $x_i = \mathrm{sign}(u_i)$. Then the sum becomes $\sum u_i \mathrm{sign}(u_i) = \sum |u_i| = \|u\|_1$.</li>
        </ul>

        <div class="proof-box">
          <h4>Complete Proof: Dual of $\ell_\infty$ is $\ell_1$</h4>
          <div class="proof-step">
            <strong>Goal:</strong> Show $\sup_{\|x\|_\infty \le 1} u^\top x = \|u\|_1 = \sum_i |u_i|$.
          </div>
          <div class="proof-step">
            <strong>Upper Bound:</strong> For any $x$ with $\|x\|_\infty \le 1$, we have $|x_i| \le 1$ for all $i$.
            We use the standard inequality $a \cdot b \le |a| \cdot |b|$:
            $$ u^\top x = \sum_{i=1}^n u_i x_i \le \sum_{i=1}^n |u_i x_i| = \sum_{i=1}^n |u_i| |x_i| $$
            Since $|x_i| \le 1$ and $|u_i| \ge 0$, we have $|u_i| |x_i| \le |u_i|$. Thus:
            $$ \sum_{i=1}^n |u_i| |x_i| \le \sum_{i=1}^n |u_i| = \|u\|_1 $$
            So $u^\top x \le \|u\|_1$ for all feasible $x$.
          </div>
          <div class="proof-step">
            <strong>Achievability:</strong> Choose $x_i = \text{sign}(u_i)$ for each $i$. Since each component is $\pm 1$ or 0, we have $\|x\|_\infty \le 1$.
            $$ u^\top x = \sum_i u_i \cdot \text{sign}(u_i) = \sum_i |u_i| = \|u\|_1 $$
            Since the upper bound is achievable, it is the supremum.
          </div>
        </div>

        <h4>Geometry: Dual Ball = Polar of Primal Ball</h4>
        <p>The unit ball of the dual norm is exactly the <b>polar set</b> of the primal unit ball:</p>
        $$ B_* = \{u : \|u\|_* \le 1\} = \{u : u^\top x \le 1 \ \forall x \in B\} = B^\circ $$
        <p>This explains the duality of shapes:
        <ul>
            <li><b>$\ell_1 \leftrightarrow \ell_\infty$:</b> The polar of the Diamond ($\ell_1$) is the Square ($\ell_\infty$).
            <br><i>Why:</i> To keep $u^\top x \le 1$ for all axis vertices $\pm e_i$, we need $|u_i| \le 1$ (the box).</li>
            <li><b>$\ell_2 \leftrightarrow \ell_2$:</b> The polar of the Sphere is the Sphere. ($\ell_2$ is self-dual).</li>
        </ul>
        </p>

        <h3>2.4 Robustness: The Support Function View</h3>
        <p>This is the high-leverage identity for modern optimization (Robust & Stochastic). It turns a "worst-case" supremum over a ball into a deterministic norm.</p>

        <div class="theorem-box">
            <h4>The "Supremum = Dual Norm" Lemma</h4>
            <p>For any norm $\|\cdot\|$ and radius $\rho \ge 0$:</p>
            $$ \boxed{\ \sup_{\|u\| \le \rho} u^\top y = \rho \|y\|_* \ } $$
            <p><b>Proof:</b> Let $u = \rho v$ where $\|v\| \le 1$.
            $$ \sup_{\|u\| \le \rho} u^\top y = \sup_{\|v\| \le 1} (\rho v)^\top y = \rho \sup_{\|v\| \le 1} v^\top y = \rho \|y\|_* $$
            <b>The Rule:</b> The norm on the <i>uncertainty set</i> determines the dual norm on the <i>coefficient vector</i>.
            <br>$\bullet$ Uncertainty in $\ell_2$ ball $\to$ Dual term is $\ell_2$ norm.
            <br>$\bullet$ Uncertainty in $\ell_\infty$ box $\to$ Dual term is $\ell_1$ norm.
            </p>
        </div>

        <div class="example">
            <h4>Application: Robust Linear Constraint</h4>
            <p>Consider a constraint $a^\top x \le b$ where $a$ is uncertain: $a = \bar{a} + u$, with perturbation $\|u\| \le \rho$.
            We require the constraint to hold for <i>all</i> perturbations:
            $$ \sup_{\|u\| \le \rho} (\bar{a} + u)^\top x \le b $$
            Using the lemma ($y=x$ here):
            $$ \bar{a}^\top x + \sup_{\|u\| \le \rho} u^\top x \le b \iff \bar{a}^\top x + \rho \|x\|_* \le b $$
            <b>Result:</b> Robustifying a linear constraint adds a <b>dual norm regularization term</b>.
            <br>This explains why Tikhonov regularization ($\|x\|_2^2$) corresponds to robustness against $\ell_2$ noise.</p>
        </div>

        <h3>2.4 Ellipsoids</h3>
        <p>An <a href="#" class="definition-link">ellipsoid</a> is the image of a Euclidean ball under an invertible affine map. It is defined as:</p>
        $$
        \mathcal{E} = \{x \in \mathbb{R}^n \mid (x - x_c)^\top P^{-1} (x - x_c) \le 1\}
        $$
        <p>where $P \in \mathbb{S}^n_{++}$ (symmetric positive definite). $P$ determines the shape and orientation.</p>

        <div class="insight">
          <h4>Geometric Interpretation via Eigendecomposition</h4>
          <p>Let $P = Q \Lambda Q^\top$ be the eigendecomposition of $P$, where $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$. The semi-axes of the ellipsoid are aligned with the eigenvectors $q_i$ (columns of $Q$) and have lengths $\sqrt{\lambda_i}$.</p>
          <p>Alternatively, an ellipsoid is the image of the unit Euclidean ball under an affine mapping: $\mathcal{E} = f(B(0,1))$ where $f(u) = P^{1/2}u + x_c$. Since affine maps preserve convexity, ellipsoids are convex.</p>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/ellipsoid-axes.png"
               alt="Anatomy of an ellipsoid showing principal axes aligned with eigenvectors"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 9:</i> An ellipsoid is defined by a PSD matrix $P$. Its principal axes align with the eigenvectors of $P$, and their lengths are the square roots of the eigenvalues.</figcaption>
        </figure>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Explorer: Ellipsoid Geometry</h3>
          <p><b>See How PSD Matrices Define Ellipsoids:</b> An ellipsoid is defined by $\{x \mid (x-x_c)^\top P^{-1} (x-x_c) \le 1\}$ where $P \succ 0$. This tool lets you:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Adjust matrix P:</b> Modify the PSD matrix entries and watch the ellipsoid reshape in real-time</li>
            <li><b>Visualize eigenvectors:</b> The principal axes align with eigenvectors of $P$</li>
            <li><b>Observe eigenvalue effects:</b> Axis lengths are proportional to $\sqrt{\lambda_i}$ where $\lambda_i$ are eigenvalues</li>
          </ul>
          <div id="widget-ellipsoid-explorer" style="width: 100%; height: 400px; position: relative; max-width: 800px; margin: 0 auto; border-radius: 8px;"></div>
        </div>

        <h3>2.5 Polyhedra</h3>

        <p>A <a href="#" class="definition-link">polyhedron</a> is the solution set of finitely many linear inequalities and equalities:</p>
        $$
        \mathcal{P} = \{x \in \mathbb{R}^n \mid Ax \le b, \ Cx = d\}
        $$
        <p>Geometrically, a polyhedron is the intersection of a finite number of halfspaces and hyperplanes. This description is known as the <b>H-representation</b> (Hyperplane representation).
        <br><b>Convexity:</b> Since halfspaces and hyperplanes are convex, and intersections preserve convexity, polyhedra are always convex.</p>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/polyhedron-construction.png"
               alt="A polyhedron formed by the intersection of multiple halfspaces"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 10:</i> A polyhedron (central solid region) is formed by intersecting multiple halfspaces. Each face corresponds to one linear inequality constraint.</figcaption>
        </figure>

        <div class="proof-box">
          <h4>Convexity of Polyhedra</h4>
          <p>Since halfspaces and hyperplanes are convex sets, and the intersection of any collection of convex sets is convex (see Section 3), a polyhedron is convex.</p>
        </div>

        <h3>Polytope vs. Polyhedron: H-Rep and V-Rep</h3>
        <p>A <a href="#" class="definition-link">polytope</a> is a bounded polyhedron.</p>
        <ul>
            <li><b>H-Representation (Intersection):</b> $\mathcal{P} = \{x \mid Ax \le b\}$. This describes the set by its "walls" (facets). It is efficient for checking if a point is in the set ($Ax \le b$?) but hard for generating points.</li>
            <li><b>V-Representation (Hull):</b> $\mathcal{P} = \mathrm{conv}\{v_1, \dots, v_k\}$. This describes the set by its "corners" (vertices). It is efficient for generating points (just mix the vertices) but hard for checking membership.</li>
        </ul>
        <div class="theorem-box">
            <h4>Minkowski-Weyl Theorem</h4>
            <p>A set $\mathcal{P}$ is a polyhedron if and only if it is the sum of a convex hull of a finite set of points and a conical hull of a finite set of directions (rays):
            $$ \mathcal{P} = \mathrm{conv}\{v_1, \dots, v_k\} + \mathrm{cone}\{r_1, \dots, r_m\} $$
            If the set is bounded (a polytope), the conical part is zero, and $\mathcal{P} = \mathrm{conv}\{v_1, \dots, v_k\}$.</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Visualizer: Polyhedron Builder</h3>
          <p><b>Build Feasible Regions from Constraints:</b> A polyhedron is the intersection of finitely many halfspaces. This tool brings LP feasible sets to life:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Add constraints one at a time:</b> Drag to define linear inequalities $a^\top x \le b$</li>
            <li><b>Visualize normals:</b> See the normal vector $a$ pointing <i>out</i> of the feasible region</li>
            <li><b>Watch the intersection:</b> As you add constraints, see how the feasible region is carved out of the plane</li>
          </ul>
          <div id="widget-polyhedron-visualizer" style="width: 100%; height: 520px; position: relative; max-width: 800px; margin: 0 auto; border-radius: 8px;"></div>
        </div>

        <h3>2.6 The Positive Semidefinite (PSD) Cone</h3>

        <p>The set of symmetric positive semidefinite matrices forms a convex cone, denoted $\mathbb{S}^n_+$. This object is the engine of <b>Semidefinite Programming (SDP)</b>.</p>
        $$
        \mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid X \succeq 0\}
        $$
        <p><b>Generalized Inequalities:</b> As we will detail in <a href="../04-convex-sets-cones/index.html">Lecture 04</a>, a convex cone $K$ induces a partial ordering on the vector space. For the PSD cone, we write $X \succeq Y$ to mean $X - Y \in \mathbb{S}^n_+$ (i.e., $X-Y$ is PSD). This generalizes the standard inequality $\ge$ on real numbers.</p>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/psd-cone-3d.png"
               alt="Visualization of the 2x2 PSD cone in 3D space"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 11:</i> The cone of $2 \times 2$ PSD matrices visualized in 3D space (axes are the matrix entries $x, y, z$). It is a convex cone with a specific "ice-cream" like shape but with a flat boundary structure.</figcaption>
        </figure>

        <div class="proof-box">
          <h4>Proof: The PSD Cone is Convex</h4>
          <p>We need to show that if $A, B \in \mathbb{S}^n_+$ and $\theta \in [0,1]$, then $\theta A + (1-\theta)B \in \mathbb{S}^n_+$.</p>
          <div class="proof-step">
            <strong>Definition Check:</strong> Let $\mathbf{z} \in \mathbb{R}^n$ be any vector. We examine the quadratic form of the convex combination.
            First, recall that the quadratic form $Q_M(z) = z^\top M z$ is <b>linear</b> in the matrix $M$.
            $$
            \mathbf{z}^\top (\theta A + (1-\theta)B) \mathbf{z} = \theta (\mathbf{z}^\top A \mathbf{z}) + (1-\theta) (\mathbf{z}^\top B \mathbf{z})
            $$
          </div>
          <div class="proof-step">
            <strong>Sign Analysis:</strong> Since $A, B \succeq 0$, we know their "energy" is non-negative: $\mathbf{z}^\top A \mathbf{z} \ge 0$ and $\mathbf{z}^\top B \mathbf{z} \ge 0$ for all $\mathbf{z}$.
            Since $\theta \in [0,1]$, the mixing weights $\theta$ and $1-\theta$ are non-negative.
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> We have a non-negative weighted sum of non-negative numbers.
            Thus $\mathbf{z}^\top (\theta A + (1-\theta)B) \mathbf{z} \ge 0$ for all $\mathbf{z}$.
            This satisfies the definition of positive semidefiniteness, so the combination is in $\mathbb{S}^n_+$.
          </div>
        </div>

        <div class="example">
            <h4>Spectrahedra: The Shape of SDP</h4>
            <p>The intersection of the PSD cone with an affine subspace is called a <a href="#" class="definition-link">spectrahedron</a>. These are the feasible sets for Semidefinite Programs (SDPs). Unlike polyhedra, they have smooth, curved boundaries.</p>
            <figure style="text-align: center; margin: 24px 0;">
              <img src="assets/spectrahedron.png"
                   alt="A spectrahedron (feasible set of an LMI)"
                   style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 12:</i> A spectrahedron looks like a "puffy" polygon. Its faces are flat where the affine space cuts the cone's boundary, but its edges and corners can be smooth curves.</figcaption>
            </figure>
        </div>
      </section>


      <section class="section-card" id="section-3">
        <h2>3. Operations that Preserve Convexity</h2>

        <p>We can prove that a complex set is convex by building it from simpler convex sets using operations that preserve convexity. This is the "calculus" of convex sets.</p>


        <h3>3.1 Intersection</h3>

        <div class="theorem-box">
          <h4>Theorem (Intersection Preserves Convexity)</h4>
          <p>The intersection of any collection (finite or infinite) of convex sets is convex.</p>
          $$ C = \bigcap_{i \in I} C_i $$
          <p><i>Modeling Meaning:</i> Every time you write multiple constraints ($f_1(x) \le 0, f_2(x) \le 0, Ax=b$), you are taking an intersection. Convex feasibility is stable under "adding more convex constraints."</p>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/convex-intersection.png"
               alt="Venn diagram showing the intersection of two convex sets is convex"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 13:</i> The intersection of two convex sets (blue and yellow) is the green region. Note that while the union is not necessarily convex, the intersection always is.</figcaption>
        </figure>

        <div class="proof-box">
          <h4>Proof</h4>
          <div class="proof-step">
            Let $x, y \in C$. By definition, $x \in C_i$ and $y \in C_i$ for all $i \in I$.
          </div>
          <div class="proof-step">
            Since each $C_i$ is convex, $\theta x + (1-\theta)y \in C_i$ for all $i \in I$.
          </div>
          <div class="proof-step">
            Therefore, $\theta x + (1-\theta)y \in \bigcap_{i \in I} C_i = C$.
          </div>
        </div>

        <div class="example">
          <h4>Advanced Example: Trigonometric Polynomials</h4>
          <p>Consider the set of coefficients $x \in \mathbb{R}^m$ such that the trigonometric polynomial $p_x(t) = \sum_{k=1}^m x_k \cos(kt)$ is bounded by 1 on an interval:</p>
          $$
          S = \{x \in \mathbb{R}^m \mid |p_x(t)| \le 1 \text{ for all } |t| \le \pi/3\}
          $$
          <p>This set can be written as an infinite intersection of halfspaces:</p>
          $$
          S = \bigcap_{|t| \le \pi/3} \{x \mid -1 \le c(t)^\top x \le 1\}
          $$
          <p>where $c(t) = (\cos(t), \dots, \cos(mt))$. Since each constraint defines a convex "slab" (intersection of two halfspaces), the infinite intersection $S$ is convex.</p>
        </div>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0; flex-wrap: wrap;">
            <figure style="text-align: center; flex: 1; min-width: 280px;">
              <img src="assets/trig-poly-intersection.png"
                   alt="Plot of the convex set of bounded trigonometric polynomials"
                   style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 14:</i> The set $S$ defined by infinite constraints.</figcaption>
            </figure>
            <figure style="text-align: center; flex: 1; min-width: 280px;">
              <img src="assets/gonzo-shape.png"
                   alt="Visualizing the intersection of infinite halfspaces"
                   style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 15:</i> The smooth convex "safe zone" formed by tangent hyperplanes.</figcaption>
            </figure>
        </div>
<h3>3.2 Affine Functions Preserve Convexity</h3>

        <p>Let $f : \mathbb{R}^n \to \mathbb{R}^m$ be an affine function, $f(x) = Ax + b$.</p>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0; flex-wrap: wrap;">
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/affine-image-projection.png"
                    alt="Projection of a polyhedron onto 2D space"
                    style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 16:</i> The image of a convex set (polyhedron) under an affine map (projection) is convex.</figcaption>
             </figure>
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/affine-preimage-cone.png"
                    alt="Slice of a cone by a plane"
                    style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 17:</i> The preimage of a convex cone (intersection with a plane) is a convex set (an ellipse).</figcaption>
             </figure>
        </div>

        <div class="theorem-box">
          <h4>Theorem (Affine Image and Preimage)</h4>
          <ul>
            <li><b>Image:</b> If $C \subseteq \mathbb{R}^n$ is convex, then $f(C) = \{Ax + b \mid x \in C\} \subseteq \mathbb{R}^m$ is convex.
            <br><i>Modeling Meaning:</i> Taking linear measurements preserves convexity. Projections are affine images.</li>
            <li><b>Preimage (Inverse Image):</b> If $D \subseteq \mathbb{R}^m$ is convex, then $f^{-1}(D) = \{x \in \mathbb{R}^n \mid Ax + b \in D\}$ is convex.
            <br><i>Modeling Meaning:</i> This is fundamental to convex modeling. Constraints like $Ax \le b$, $\|Ax-b\| \le t$, and LMIs are all affine preimages of simple convex sets (orthants, norm balls, PSD cones).</li>
          </ul>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>

          <div class="proof-step">
            <strong>Image:</strong> Take $y_1, y_2 \in f(C)$, so $y_1 = Ax_1 + b$ and $y_2 = Ax_2 + b$ for some $x_1, x_2 \in C$. For $\theta \in [0,1]$:
            $$
            \theta y_1 + (1-\theta)y_2 = \theta(Ax_1 + b) + (1-\theta)(Ax_2 + b) = A(\theta x_1 + (1-\theta)x_2) + b
            $$
            Since $C$ is convex, $\theta x_1 + (1-\theta)x_2 \in C$, so $\theta y_1 + (1-\theta)y_2 \in f(C)$.
          </div>

          <div class="proof-step">
            <strong>Preimage:</strong> Take $x_1, x_2 \in f^{-1}(D)$, so $Ax_1 + b \in D$ and $Ax_2 + b \in D$. For $\theta \in [0,1]$:
            $$
            A(\theta x_1 + (1-\theta)x_2) + b = \theta(Ax_1 + b) + (1-\theta)(Ax_2 + b)
            $$
            Since $D$ is convex, the right side is in $D$, so $\theta x_1 + (1-\theta)x_2 \in f^{-1}(D)$.
          </div>
        </div>

        <div class="example">
          <h4>Application: Ellipsoids are Convex</h4>
          <p>An ellipsoid $\mathcal{E} = \{x \mid (x - x_c)^\top P^{-1} (x - x_c) \le 1\}$ where $P \succ 0$ can be written as:</p>
          $$
          \mathcal{E} = \{x \mid \|P^{-1/2}(x - x_c)\|_2 \le 1\}
          $$
          <p>This is the preimage of the Euclidean ball $\{z \mid \|z\|_2 \le 1\}$ (convex) under the affine map $f(x) = P^{-1/2}(x - x_c)$. Since preimages preserve convexity, $\mathcal{E}$ is convex.</p>
          <div class="insight">
            <h4>Strict vs Non-Strict Separation</h4>
            <p>The <b>Separating Hyperplane Theorem</b> states that disjoint convex sets $C, D$ can be separated by a hyperplane ($a^\top x \le b$ for $x \in C$, $a^\top x \ge b$ for $x \in D$).
            <ul>
                <li><b>Non-Strict:</b> Always possible if $C, D$ are disjoint and convex. But they might touch at the boundary.</li>
                <li><b>Strict:</b> Possible if $C$ is closed and $D$ is compact (closed & bounded). Then there exists $\epsilon > 0$ such that the gap is strictly positive.</li>
            </ul></p>
          </div>
          <figure style="text-align: center; margin: 24px 0;">
               <img src="assets/image-vs-preimage-ball.png"
                    alt="Comparison of image vs preimage of a ball"
                    style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 18:</i> The image of a ball is an ellipsoid (left). The preimage of a ball (slab) is also convex (right).</figcaption>
          </figure>
        </div>

        <div class="insight">
            <h4>Counterexample: Non-Affine Maps</h4>
            <p>Convexity is fragile. Non-affine maps, such as $f(x) = 1/x$ or $f(x) = x^2$, do not generally preserve convexity of sets.</p>
            <figure style="text-align: center; margin: 24px 0;">
               <img src="assets/division-counterexample.png"
                    alt="Division function mapping a convex set to non-convex"
                    style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 19:</i> The "division" map $f(x,y) = x/y$ can tear a convex square into two disjoint (non-convex) pieces.</figcaption>
            </figure>
        </div>

        <h3>3.3 Perspective and Linear-Fractional Functions</h3>

        <p>The <b>perspective function</b> $P : \mathbb{R}^{n+1} \to \mathbb{R}^n$ is defined by:</p>
        $$
        P(x, t) = x/t, \quad \mathrm{dom}\, P = \{(x, t) \mid t > 0\}
        $$
        <p>Geometrically, this corresponds to a pinhole camera projection: points $(x,t)$ are projected onto the plane $t=1$ via lines through the origin.</p>

        <div class="insight">
            <h4>Why does Perspective Preserve Convexity?</h4>
            <p>Imagine a convex object (like a solid ellipsoid) floating in 3D space. Place a light source at the origin (0,0,0). The shadow cast by this object onto a screen (the plane $t=1$) is also convex.
            <br>The perspective function maps points to their shadows. Since the "cone of light" generated by a convex object is a convex cone, its slice by the screen is a convex set.</p>
        </div>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0; flex-wrap: wrap;">
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/pinhole-camera.png"
                    alt="Geometric intuition of perspective projection"
                    style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 20:</i> The perspective map $P(x,t) = x/t$ projects 3D points onto the $t=1$ plane. The image of a convex object (ellipsoid) is a convex shadow.</figcaption>
             </figure>
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/perspective-domain.png"
                    alt="Domain of the perspective map"
                    style="width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 21:</i> The domain is restricted to $t > 0$. As $t \to 0$, the projected point shoots to infinity.</figcaption>
             </figure>
        </div>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>The perspective function preserves convexity: if $C \subseteq \mathbb{R}^{n+1}$ is convex, then $P(C)$ is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Proof: Perspective Preserves Convexity (Step-by-Step)</h4>
          <p>We want to show that if $u, v$ are in the perspective image $P(C)$, then the entire line segment connecting them is also in $P(C)$.</p>

          <div class="proof-step">
            <strong>Step 1: Lift to the source set (Pre-image).</strong>
            Since $u \in P(C)$, there exists $(y, t) \in C$ with $t>0$ such that $u = y/t$.
            Since $v \in P(C)$, there exists $(y', t') \in C$ with $t'>0$ such that $v = y'/t'$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Target point (Image).</strong>
            Consider a point on the segment in the image space: $w = \alpha u + (1-\alpha)v$ for $\alpha \in [0, 1]$.
            $$ w = \alpha \frac{y}{t} + (1-\alpha) \frac{y'}{t'} $$
          </div>

          <div class="proof-step">
            <strong>Step 3: Construct the convex combination in the domain.</strong>
            We need to find a point in $C$ that maps to $\mathbf{w}$. This point will be a convex combination of $(\mathbf{y},t)$ and $(\mathbf{y}',t')$.
            <br><b>The Weight Trick:</b> We cannot simply use $\alpha$ as the mixing weight because the perspective map $P$ scales inputs by $1/t$. We need to "compensate" for the different denominators $t$ and $t'$.
            <br>Define the domain mixing weight $\mu$ proportional to the denominators:
            $$ \mu = \frac{\alpha t'}{\alpha t' + (1-\alpha)t} $$
            Observe that since $t, t' > 0$ and $\alpha \in [0,1]$, the denominator is positive and $\mu \in [0, 1]$.
            <br>Consider the point $\mathbf{z} = \mu (\mathbf{y}, t) + (1-\mu) (\mathbf{y}', t')$. Since $C$ is convex, $\mathbf{z} \in C$.
          </div>

          <div class="proof-step">
            <strong>Step 4: Verify mapping.</strong>
            We calculate $P(\mathbf{z}) = Y_{num} / D$, where $Y_{num}$ is the vector part and $D$ is the scalar part of $\mathbf{z}$.
            <br>The scalar denominator is $D = \mu t + (1-\mu)t'$.
            <br>The vector numerator is $Y_{num} = \mu \mathbf{y} + (1-\mu) \mathbf{y}'$.
            <br>The projection is:
            $$ P(\mathbf{z}) = \frac{\mu}{D} \mathbf{y} + \frac{1-\mu}{D} \mathbf{y}' $$
            Let's simplify the coefficient $\frac{\mu}{D}$:
            $$ \frac{\mu}{D} = \frac{\mu}{\mu t + (1-\mu)t'} $$
            Divide numerator and denominator by $\mu$:
            $$ \frac{\mu}{D} = \frac{1}{t + \frac{1-\mu}{\mu} t'} $$
            From the definition of $\mu$, we know the ratio $\frac{1-\mu}{\mu} = \frac{(1-\alpha)t}{\alpha t'}$. Substituting this back:
            $$ \frac{\mu}{D} = \frac{1}{t + \frac{(1-\alpha)t}{\alpha t'} t'} = \frac{1}{t + \frac{1-\alpha}{\alpha} t} = \frac{1}{t (1 + \frac{1-\alpha}{\alpha})} = \frac{1}{t (\frac{1}{\alpha})} = \frac{\alpha}{t} $$
            Similarly, we can show $\frac{1-\mu}{D} = \frac{1-\alpha}{t'}$.
            <br>Substituting these coefficients back into the expression for $P(\mathbf{z})$:
            $$ P(\mathbf{z}) = \frac{\alpha}{t} \mathbf{y} + \frac{1-\alpha}{t'} \mathbf{y}' = \alpha \left(\frac{\mathbf{y}}{t}\right) + (1-\alpha) \left(\frac{\mathbf{y}'}{t'}\right) = \alpha u + (1-\alpha) v = w $$
          </div>

          <div class="proof-step">
            <strong>Conclusion:</strong>
            We found a point $z \in C$ such that $P(z) = w$. Thus $w \in P(C)$.
          </div>
        </div>

        <p>A <a href="#" class="definition-link">linear-fractional function</a> is a composition of perspective with an affine function:</p>
        $$
        f(x) = \frac{Ax + b}{c^\top x + d}
        $$
        <p>Since it is composed of operations that preserve convexity (Affine $\to$ Perspective), linear-fractional functions also preserve convexity.</p>

        <figure style="text-align: center; margin: 24px 0;">
             <img src="assets/composition-maps.png"
                  alt="Block diagram of function composition"
                  style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
             <figcaption><i>Figure 22:</i> Linear-fractional functions are built by composing an affine map $g(x)$ with the perspective map $P$.</figcaption>
        </figure>

        <div class="example">
             <h4>Visualizing Projective Geometry</h4>
             <p>Linear-fractional maps can distort space significantly, turning parallel lines into converging ones (like train tracks on a horizon), yet they strictly preserve the convexity of sets.</p>
             <figure style="text-align: center; margin: 24px 0;">
                  <img src="assets/warping-grid.png"
                       alt="Grid warped by a linear-fractional map"
                       style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
                  <figcaption><i>Figure 23:</i> A regular grid (left) is warped by a linear-fractional transformation (right). Squares become "conic" quadrilaterals, but they remain convex sets.</figcaption>
             </figure>
        </div>

        <h3>3.4 Minkowski Sum</h3>
        <p>The <a href="#" class="definition-link">Minkowski sum</a> of two sets $C, D \subseteq \mathbb{R}^n$ is the set of all vector sums:</p>
        $$ C + D = \{x + y \mid x \in C, y \in D\} $$

        <div class="intuition-box">
          <p><b>Geometric picture:</b> Build $C+D$ by taking every point $c \in C$, shifting the whole set $D$ by $c$, and then taking the union of all these shifted copies: $C+D=\bigcup_{c\in C}(c+D)$. The result is a “thickened” version of $C$ whose boundary is smoothed/expanded according to the shape of $D$.</p>
          <p><b>Special case:</b> If $D$ is a ball of radius $\varepsilon$, then $C + D$ is the $\varepsilon$-offset of $C$: all points within distance $\varepsilon$ of $C$.</p>
        </div>

        <div class="interpretation-box">
          <p><b>Modeling view:</b> Minkowski sum is “resource aggregation.” If $C$ is what you can do with resource bundle 1 and $D$ is what you can do with resource bundle 2, then $C+D$ is what you can do with both. In robust modeling, adding an uncertainty set $U$ often turns “nominal” constraints into constraints that hold for all perturbations in $U$; geometrically this corresponds to offsetting or shrinking feasible sets (sum/difference).</p>
        </div>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $C$ and $D$ are convex, then $C+D$ is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <p>Let $u, v \in C+D$. Then $u = c_1 + d_1$ and $v = c_2 + d_2$ for some $c_i \in C, d_i \in D$.
          For any $\theta \in [0,1]$:</p>
          $$
          \theta u + (1-\theta)v = \theta(c_1 + d_1) + (1-\theta)(c_2 + d_2)
          = [\theta c_1 + (1-\theta)c_2] + [\theta d_1 + (1-\theta)d_2]
          $$
          <p>Since $C$ is convex, the first bracket is in $C$. Since $D$ is convex, the second is in $D$. Thus the sum is in $C+D$.</p>
        </div>

        <h3>3.5 Minkowski Difference (Erosion)</h3>
        <p>There is a related operation that “shrinks” a set. The <b>Minkowski difference</b> (also called <b>erosion</b>) of $C$ by $D$ is defined as:</p>
        $$
        C \ominus D := \{\mathbf{x} \in \mathbb{R}^n \mid \mathbf{x} + D \subseteq C\}
        $$
        <p>In words: $\mathbf{x}$ is in $C \ominus D$ if, when you place a translated copy of $D$ centered at $\mathbf{x}$, it still fits entirely inside $C$. This makes "safety margins" precise.</p>
        <p>A useful equivalent form is an intersection of translations:</p>
        $$
        C \ominus D = \bigcap_{d \in D} (C - d)
        $$
        <p>because $x+D \subseteq C \iff x \in C-d$ for every $d\in D$.</p>

        <div class="theorem-box">
          <h4>Convexity of Minkowski Difference (when nonempty)</h4>
          <p>If $C$ and $D$ are convex, then $C \ominus D$ is convex (and it may be empty if $D$ is “too large” to fit inside $C$ anywhere).</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <p>For each fixed $d \in D$, the translate $C-d = \{x \mid x+d \in C\}$ is convex because translation preserves convexity. Since $C\ominus D$ is the intersection $\bigcap_{d\in D}(C-d)$, and intersections of convex sets are convex, $C\ominus D$ is convex.</p>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/minkowski-sum-difference.png"
               alt="Minkowski sum and Minkowski difference (erosion) of two convex sets"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 24:</i> Left: Minkowski sum $S_1+S_2$ “adds” shapes by combining all pointwise sums. Right: Minkowski difference $S_1\ominus S_2$ keeps the points where a translated copy of $S_2$ still fits inside $S_1$.</figcaption>
        </figure>

        <h3>3.6 Cartesian Product: Building Higher-Dimensional Convex Sets</h3>

        <div class="definition-box">
          <h4>Definition: Cartesian Product</h4>
          <p>The <b>Cartesian product</b> of two sets $C \subseteq \mathbb{R}^n$ and $D \subseteq \mathbb{R}^m$ is the set of all ordered pairs:
          $$ C \times D = \{(\mathbf{x}, \mathbf{y}) \in \mathbb{R}^{n+m} \mid \mathbf{x} \in C, \ \mathbf{y} \in D\} $$
          Geometrically, $C \times D$ lives in the $(n+m)$-dimensional product space $\mathbb{R}^n \times \mathbb{R}^m \cong \mathbb{R}^{n+m}$, where the first $n$ coordinates come from $C$ and the last $m$ coordinates come from $D$.</p>
        </div>

        <div class="theorem-box">
          <h4>Theorem: Cartesian Product Preserves Convexity</h4>
          <p>If $C \subseteq \mathbb{R}^n$ and $D \subseteq \mathbb{R}^m$ are convex sets, then their Cartesian product $C \times D \subseteq \mathbb{R}^{n+m}$ is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <div class="proof-step">
            <strong>Setup:</strong> Let $(\mathbf{x}_1, \mathbf{y}_1), (\mathbf{x}_2, \mathbf{y}_2) \in C \times D$ be two arbitrary points in the product. By definition, this means $\mathbf{x}_1, \mathbf{x}_2 \in C$ and $\mathbf{y}_1, \mathbf{y}_2 \in D$. We must show that for any $\theta \in [0,1]$, the convex combination
            $$ \theta (\mathbf{x}_1, \mathbf{y}_1) + (1-\theta)(\mathbf{x}_2, \mathbf{y}_2) $$
            also lies in $C \times D$.
          </div>
          <div class="proof-step">
            <strong>Component-wise Convex Combination:</strong> The convex combination in the product space decomposes into separate convex combinations in each coordinate block:
            $$ \theta (\mathbf{x}_1, \mathbf{y}_1) + (1-\theta)(\mathbf{x}_2, \mathbf{y}_2) = \left( \theta \mathbf{x}_1 + (1-\theta)\mathbf{x}_2, \ \theta \mathbf{y}_1 + (1-\theta)\mathbf{y}_2 \right) $$
          </div>
          <div class="proof-step">
            <strong>Apply Convexity of $C$ and $D$:</strong> Since $C$ is convex and $\mathbf{x}_1, \mathbf{x}_2 \in C$, we have:
            $$ \theta \mathbf{x}_1 + (1-\theta)\mathbf{x}_2 \in C $$
            Similarly, since $D$ is convex and $\mathbf{y}_1, \mathbf{y}_2 \in D$, we have:
            $$ \theta \mathbf{y}_1 + (1-\theta)\mathbf{y}_2 \in D $$
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> By definition of the Cartesian product, if the first $n$ coordinates lie in $C$ and the last $m$ coordinates lie in $D$, then the combined point $(\mathbf{x}, \mathbf{y})$ lies in $C \times D$. Thus:
            $$ \left( \theta \mathbf{x}_1 + (1-\theta)\mathbf{x}_2, \ \theta \mathbf{y}_1 + (1-\theta)\mathbf{y}_2 \right) \in C \times D $$
            This proves that $C \times D$ is convex.
          </div>
        </div>

        <h4>Examples</h4>

        <div class="example">
          <h4>Example 1: Product of Intervals</h4>
          <p>Consider two closed intervals on the real line: $C = [a, b] \subseteq \mathbb{R}$ and $D = [c, d] \subseteq \mathbb{R}$. Their Cartesian product is:
          $$ C \times D = [a, b] \times [c, d] = \{(x, y) \in \mathbb{R}^2 \mid a \le x \le b, \ c \le y \le d\} $$
          This is a <b>rectangle</b> (or "box") in $\mathbb{R}^2$ with corners at $(a, c), (a, d), (b, c), (b, d)$. Since intervals are convex (in fact, they are line segments), the theorem guarantees that this rectangle is convex—which is geometrically obvious.</p>
        </div>

        <div class="example">
          <h4>Example 2: Box in $\mathbb{R}^n$</h4>
          <p>Generalizing the rectangle, the product of $n$ intervals:
          $$ \prod_{i=1}^n [a_i, b_i] = [a_1, b_1] \times [a_2, b_2] \times \cdots \times [a_n, b_n] $$
          is an $n$-dimensional <b>hyperrectangle</b> or <b>box</b> in $\mathbb{R}^n$. This is the set of all points $\mathbf{x} = (x_1, \dots, x_n)$ satisfying $a_i \le x_i \le b_i$ for each $i$. Boxes are fundamental in optimization for defining simple bound constraints.</p>
        </div>

        <div class="example">
          <h4>Example 3: Product of Simplices</h4>
          <p>Let $\Delta_n = \{\mathbf{x} \in \mathbb{R}^n \mid \mathbf{x} \ge 0, \ \sum x_i = 1\}$ be the standard $(n-1)$-simplex (the probability simplex) in $\mathbb{R}^n$. The product of two simplices:
          $$ \Delta_2 \times \Delta_3 \subseteq \mathbb{R}^5 $$
          consists of all pairs $(\mathbf{p}, \mathbf{q})$ where $\mathbf{p} = (p_1, p_2) \in \Delta_2$ (a probability distribution over 2 outcomes) and $\mathbf{q} = (q_1, q_2, q_3) \in \Delta_3$ (a probability distribution over 3 outcomes). This higher-dimensional convex set arises in multi-stage decision problems or probabilistic models with multiple independent random variables.</p>
        </div>

        <div class="insight">
          <h4>Connection to Separable Optimization Problems</h4>
          <p>Cartesian products naturally arise in <b>separable</b> or <b>decomposable</b> optimization problems where variables can be partitioned into independent groups. Consider the problem:
          $$ \min_{\mathbf{x} \in C, \mathbf{y} \in D} f(\mathbf{x}) + g(\mathbf{y}) $$
          Here, the objective function is the sum of two functions, each depending on disjoint variable sets. The feasible region is the Cartesian product $C \times D$. Such problems can often be solved by decomposition: optimize over $\mathbf{x} \in C$ and $\mathbf{y} \in D$ <i>separately</i>, then combine the solutions. This is the foundation of <b>block coordinate descent</b> and <b>alternating minimization</b> algorithms.</p>
        </div>

        <h4>Generalization to Multiple Sets</h4>
        <p>The Cartesian product extends naturally to any finite number of sets. If $C_1, C_2, \dots, C_k$ are convex sets in spaces $\mathbb{R}^{n_1}, \mathbb{R}^{n_2}, \dots, \mathbb{R}^{n_k}$ respectively, then:
        $$ C_1 \times C_2 \times \cdots \times C_k = \{(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_k) \mid \mathbf{x}_i \in C_i \ \forall i\} \subseteq \mathbb{R}^{n_1 + n_2 + \cdots + n_k} $$
        is convex. The proof follows the same component-wise argument: convex combinations are taken independently in each coordinate block, and since each $C_i$ is convex, the componentwise convex combinations remain inside each $C_i$.</p>

        <div class="example">
          <h4>Example 4: Polyhedra as Products</h4>
          <p>In optimization, box constraints like:
          $$ 0 \le x_1 \le 5, \quad -2 \le x_2 \le 3, \quad 1 \le x_3 \le 4 $$
          define the feasible region as the Cartesian product:
          $$ [0, 5] \times [-2, 3] \times [1, 4] \subseteq \mathbb{R}^3 $$
          This is a rectangular box in 3D space. Many optimization solvers exploit this product structure for efficient constraint handling.</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Laboratory: Convex Geometry</h3>
          <p><b>Draw, Combine, and Verify:</b> This unified workspace lets you experiment with convex sets and operations. It combines drawing capabilities with set algebra:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Draw Custom Sets:</b> Use the pen tool to draw polygons. Double-click to close.</li>
            <li><b>Add Primitives:</b> Quickly add standard convex sets like circles and squares.</li>
            <li><b>Apply Operations:</b> Select multiple sets (Shift+Click) and apply operations like <b>Intersection</b>, <b>Convex Hull</b>, or <b>Minkowski Sum</b>.</li>
            <li><b>Verify Convexity:</b> The lab automatically labels sets as Convex (C) or Non-Convex (NC).</li>
          </ul>
          <p><i>Key Concept:</i> Notice how the intersection of any two sets (even non-convex ones) doesn't guarantee convexity, but the intersection of <i>convex</i> sets is always convex!</p>
          <div id="widget-convex-geometry-lab" style="width: 100%; height: 600px; position: relative; max-width: 900px; margin: 0 auto; border-radius: 8px;"></div>
        </div>
      </section>


    <section class="section-card" id="section-4">
      <h2>4. Separation and Supporting Hyperplanes (Preview)</h2>
      <p>A fundamental property of convex sets is that disjoint ones can be separated by a flat surface (hyperplane). This geometric fact is the basis for <b>duality</b> and <b>optimality conditions</b>.</p>

      <div class="theorem-box">
        <h4>The Separating Hyperplane Theorem (Intuition)</h4>
        <p>If two convex sets $C$ and $D$ do not intersect, you can slide a sheet of paper (hyperplane) between them.</p>
        $$ \exists a \neq 0, b : \quad a^\top x \le b \quad \forall x \in C \quad \text{and} \quad a^\top x \ge b \quad \forall x \in D $$
      </div>

      <figure style="text-align: center; margin: 24px 0;">
          <img src="assets/separating-hyperplane.png"
               alt="A hyperplane separating two disjoint convex sets"
               style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 25:</i> Separation allows us to certify that two sets don't intersect.</figcaption>
      </figure>

      <p>This theorem has a rigorous constructive proof based on <b>projection</b> (finding the shortest distance between sets). We will cover the detailed proofs, strict separation, and the connection to dual cones in <b><a href="../04-convex-sets-cones/index.html#section-2">Lecture 04: Cones and Separation</a></b>.</p>

      <h4>Supporting Hyperplanes</h4>
      <p>If a point $x_0$ is on the boundary of a convex set $C$, there exists a hyperplane passing through $x_0$ that contains $C$ entirely on one side. This is a <b>supporting hyperplane</b>.
      <br><i>Significance:</i> In optimization, the optimal point is supported by the "cost gradient" hyperplane. This geometric tangency condition is the origin of KKT conditions.</p>


    </section>
<section class="section-card" id="section-5">
        <h2>5. Topological Toolkit: Closure, Interior, Relative Interior</h2>

        <p>To work precisely with convex sets and optimality conditions, we need clear notions of "inside," "edge," and "outside."</p>

        <h3>5.1 Basic Topological Definitions</h3>

        <p>Let $C \subseteq \mathbb{R}^n$ be any set.</p>

        <ul>
          <li><b>Closure</b> $\mathrm{cl}(C)$: The set of all limits of convergent sequences in $C$. It's the smallest closed set containing $C$.</li>
          <li><b>Interior</b> $\mathrm{int}(C)$: Points with some open ball fully contained in $C$. Intuitively, "strictly inside" $C$.</li>
          <li><b>Boundary</b> $\partial C = \mathrm{cl}(C) \setminus \mathrm{int}(C)$: The "edge" of $C$.</li>
          <li><b>Affine hull</b> $\mathrm{aff}(C)$: The smallest affine set containing $C$.</li>
          <li><b>Relative interior</b> $\mathrm{ri}(C)$: The interior of $C$ <i>relative to</i> $\mathrm{aff}(C)$. Points strictly inside when viewed in the affine hull.</li>
        </ul>

        <div class="example">
          <h4>Example 1: Line Segment in $\mathbb{R}^2$</h4>
          <p>Consider $C = \{(t, 0) \mid 0 \le t \le 1\}$ (a line segment on the $x$-axis).</p>
          <ul>
            <li>$\mathrm{int}(C) = \emptyset$ (no open ball in $\mathbb{R}^2$ fits inside a line)</li>
            <li>$\mathrm{aff}(C) = \{(t, 0) \mid t \in \mathbb{R}\}$ (the entire $x$-axis)</li>
            <li>$\mathrm{ri}(C) = \{(t, 0) \mid 0 < t < 1\}$ (interior relative to the $x$-axis)</li>
          </ul>
        </div>

        <div class="example">
          <h4>Example 1b: A Flat Disk in 3D (The "Frisbee" Intuition)</h4>
          <p>Imagine a Frisbee floating in the air. Mathematically, this is the set $C = \{(x, y, 0) \mid x^2 + y^2 \le 1\} \subset \mathbb{R}^3$.</p>
          <ul>
            <li><b>Interior $\mathrm{int}(C) = \emptyset$:</b> If you try to fit a solid 3D ball inside the Frisbee, you fail because the Frisbee has zero thickness. Thus, strictly speaking, it has no interior points in $\mathbb{R}^3$.</li>
            <li><b>Affine Hull $\mathrm{aff}(C)$:</b> The "minimal universe" containing the Frisbee is the flat 2D plane $z=0$. This is its affine hull.</li>
            <li><b>Relative Interior $\mathrm{ri}(C)$:</b> If we restrict our view to just that 2D plane, the Frisbee <i>does</i> have an interior! It is the set of points strictly inside the rim: $\{(x, y, 0) \mid x^2 + y^2 < 1\}$. This is the relative interior.</li>
          </ul>
          <p><i>Why this matters:</i> Optimization solvers (like interior-point methods) move through the <i>relative</i> interior of the feasible set. They don't care that the set has no volume in the full space, as long as it has volume in its "native" subspace.</p>
        </div>

        <div class="example">
          <h4>Example 2: Standard Simplex</h4>
          <p>$\Delta^n = \{x \in \mathbb{R}^n \mid x \ge 0, \mathbf{1}^\top x = 1\}$.</p>
          <ul>
            <li>$\mathrm{int}(\Delta^n) = \emptyset$ (lies in a hyperplane)</li>
            <li>$\mathrm{aff}(\Delta^n) = \{x \mid \mathbf{1}^\top x = 1\}$ (the hyperplane)</li>
            <li>$\mathrm{ri}(\Delta^n) = \{x > 0 \mid \mathbf{1}^\top x = 1\}$ (all coordinates strictly positive)</li>
          </ul>
        </div>

        <h3>5.2 Key Properties for Convex Sets</h3>

        <div class="theorem-box">
          <h4>Facts About Convex Sets</h4>
          <ol>
            <li><b>Closure is Convex:</b> $\mathrm{cl}(C)$ is convex.
              <br><i>Proof:</i> Let $a, b \in \mathrm{cl}(C)$. Then there exist sequences $a_k \to a, b_k \to b$ with $a_k, b_k \in C$. For any $\theta$, the sequence $z_k = \theta a_k + (1-\theta)b_k$ lies in $C$ (by convexity) and converges to $\theta a + (1-\theta)b$. Thus the limit is in $\mathrm{cl}(C)$.
            </li>
            <li>$\mathrm{int}(C)$ (if nonempty) and $\mathrm{ri}(C)$ are convex.</li>
            <li>If $C$ is convex with nonempty interior, then $\mathrm{cl}(C) = \mathrm{cl}(\mathrm{int}(C))$ and $\mathrm{int}(C) = \mathrm{int}(\mathrm{cl}(C))$.</li>
            <li><b>Relative interior of intersection:</b> If $\mathrm{ri}(C) \cap \mathrm{ri}(D) \neq \emptyset$, then:
              $$
              \mathrm{ri}(C \cap D) = \mathrm{ri}(C) \cap \mathrm{ri}(D)
              $$
            </li>
            <li><b>Affine functions:</b> If $f(x) = Ax + b$, then $f(\mathrm{ri}(C)) = \mathrm{ri}(f(C))$ for convex $C$.</li>
          </ol>
        </div>

        <h3>5.3 Why Relative Interior Matters</h3>

        <div class="insight">
          <h4>The Problem with Ordinary Interior</h4>
          <p>Many convex sets in optimization problems have <b>empty ordinary interior</b>. Consider:</p>
          <ul>
            <li>A line segment in $\mathbb{R}^2$: $\{(t, 0) \mid 0 \le t \le 1\}$ has $\mathrm{int}(C) = \emptyset$.</li>
            <li>The unit simplex $\Delta^n = \{x \in \mathbb{R}^{n+1} \mid \sum x_i = 1, x \ge 0\}$ lives in a hyperplane, so $\mathrm{int}(\Delta^n) = \emptyset$ in $\mathbb{R}^{n+1}$.</li>
            <li>The epigraph of a function $f: \mathbb{R}^n \to \mathbb{R}$ lives in $\mathbb{R}^{n+1}$ but often sits on a lower-dimensional surface.</li>
          </ul>
          <p>If we require points in the ordinary interior for theorems, they would fail for these fundamental sets. Relative interior <b>rescues</b> these theorems by restricting attention to the affine hull.</p>
        </div>

        <p>Relative interior is crucial in three major areas:</p>

        <div class="subsection">
          <h4>1. Constraint Qualifications (Slater's Condition)</h4>
          <p>The cornerstone result in convex duality is <b>Slater's theorem</b> (Lecture 09): <i>if there exists a strictly feasible point</i>, then strong duality holds (primal optimal value equals dual optimal value). But what does "strictly feasible" mean?</p>

          <p>For a convex problem $\min f_0(x)$ subject to $f_i(x) \le 0$ and $Ax = b$, a point $\tilde{x}$ is <b>strictly feasible</b> (satisfies Slater's condition) if:
          $$ f_i(\tilde{x}) < 0 \text{ for all } i, \quad A\tilde{x} = b, \quad \tilde{x} \in \mathrm{ri}(\mathrm{dom}(f_0)) $$
          The requirement $\tilde{x} \in \mathrm{ri}(\mathrm{dom}(f_0))$ (not $\mathrm{int}(\mathrm{dom}(f_0))$) is essential because the domain may be lower-dimensional.</p>

          <div class="example-box">
            <h4>Example: Portfolio Optimization with Budget Constraint</h4>
            <p>Consider the problem:
            $$ \min \frac{1}{2} x^\top \Sigma x \quad \text{subject to} \quad \sum_{i=1}^n x_i = 1, \quad x \ge 0 $$
            The feasible set is the simplex $\Delta^n$, which has <b>empty ordinary interior</b> in $\mathbb{R}^n$ (it lives in the hyperplane $\sum x_i = 1$).
            <br><br>
            However, $\mathrm{ri}(\Delta^n) = \{x \mid \sum x_i = 1, x > 0\}$ (strictly positive entries) is nonempty. A strictly feasible point is any $\tilde{x}$ with $\tilde{x}_i > 0$ for all $i$ and $\sum \tilde{x}_i = 1$ (e.g., $\tilde{x} = (1/n, \ldots, 1/n)$). Slater's condition is satisfied, guaranteeing strong duality.</p>
          </div>
        </div>

        <div class="subsection">
          <h4>2. Subdifferential Calculus and Optimality Conditions</h4>
          <p>The <b>subdifferential</b> $\partial f(x)$ of a convex function $f$ at $x$ is well-behaved when $x \in \mathrm{ri}(\mathrm{dom}(f))$. Key results include:</p>
          <ul>
            <li><b>Nonemptiness:</b> If $f$ is convex and $x \in \mathrm{ri}(\mathrm{dom}(f))$, then $\partial f(x) \neq \emptyset$. (This fails at boundary points!)</li>
            <li><b>Sum rule:</b> $\partial (f+g)(x) = \partial f(x) + \partial g(x)$ holds <i>when</i> $\mathrm{ri}(\mathrm{dom}(f)) \cap \mathrm{ri}(\mathrm{dom}(g)) \neq \emptyset$.</li>
            <li><b>Optimality:</b> For an unconstrained problem $\min f(x)$, the condition $0 \in \partial f(x^*)$ is necessary <i>and sufficient</i> for optimality if $x^* \in \mathrm{ri}(\mathrm{dom}(f))$.</li>
          </ul>
        </div>

        <div class="subsection">
          <h4>3. Interior-Point Methods</h4>
          <p>Modern convex optimization solvers (CVX, MOSEK, ECOS) rely on <b>primal-dual interior-point methods</b> that:</p>
          <ul>
            <li>Start from a point in the <b>relative interior</b> of the feasible set</li>
            <li>Follow the <b>central path</b> $\{x(\tau) \mid \nabla f_0(x) + \sum \tau_i \nabla f_i(x) = 0, f_i(x) < 0\}$ as $\tau \to 0$</li>
            <li>Stay in $\mathrm{ri}(C)$ throughout, approaching the boundary only at convergence</li>
          </ul>
          <p>For problems with equality constraints $Ax = b$ (e.g., standard form LP), the central path lives in a <b>lower-dimensional affine subspace</b>. The algorithm operates in $\mathrm{ri}(\{x \mid Ax = b\})$, not $\mathrm{int}(\{x \mid Ax = b\}) = \emptyset$.</p>
        </div>

        <div class="insight">
          <h4>Key Takeaway: Relative Interior = "Interior Where It Counts"</h4>
          <p>The relative interior generalizes the intuitive notion of "points strictly inside the set" to lower-dimensional sets. It is the <b>right</b> notion of interior for:</p>
          <ul>
            <li>Constraint qualifications (Slater's condition)</li>
            <li>Subdifferential calculus (nonemptiness, sum rules)</li>
            <li>Interior-point algorithms (feasibility and strict decrease)</li>
          </ul>
          <p>Whenever you see $\mathrm{ri}(C)$ in a theorem, it's rescuing a result that would fail with $\mathrm{int}(C)$ due to empty interiors.
          <br><b>Visualizing the Rescue:</b> Think of the 3D Frisbee again. The theorem says "you can move around freely in the interior."
          <br>$\bullet$ With $\mathrm{int}(C)$: "You can move up, down, left, right, forward, backward." (False, you can't move up/down off the Frisbee).
          <br>$\bullet$ With $\mathrm{ri}(C)$: "You can move freely <i>within the plane of the Frisbee</i>." (True!).</p>
        </div>

      </section>


      <!-- SECTION 6: REVIEW -->
      <section class="section-card" id="section-6">
        <h2>6. Review & Cheat Sheet</h2>
        <h3>Key Definitions</h3>
        <ul>
          <li><b>Affine Set:</b> Contains the line through any two points. ($C = x_0 + V$)</li>
          <li><b>Convex Set:</b> Contains the line segment between any two points.</li>
          <li><b>Convex Hull:</b> Smallest convex set containing $S$. ($\text{conv}(S)$)</li>
          <li><b>Cone:</b> Closed under positive scaling ($\theta x \in C$ for $\theta \ge 0$).</li>
        </ul>

        <h3>Operations Preserving Convexity</h3>
        <ul>
          <li>Intersection ($\bigcap C_i$)</li>
          <li>Affine map ($f(x) = Ax+b$) and inverse map ($f^{-1}(D)$)</li>
          <li>Perspective function ($P(x,t) = x/t$)</li>
          <li>Linear-fractional function ($f(x) = \frac{Ax+b}{c^\top x+d}$)</li>
        </ul>
      </section>

      <!-- SECTION 5.5: LOOKING AHEAD -->
      <section class="section-card">
        <h3>6.1 Looking Ahead: From Sets to Operations to Functions</h3>

        <div class="insight">
          <h4>The Convexity Trifecta: A Three-Lecture Arc</h4>
          <p>This lecture (Lecture 03) introduced <b>convex sets</b>—the geometric objects that form the foundation of convex optimization. We answered: <b>WHAT are convex sets?</b> We saw cones, polyhedra, ellipsoids, and norm balls.</p>

          <p>But convex optimization is not just about sets—it's about <b>operations</b> and <b>functions</b>. The next two lectures complete the "convexity trifecta":</p>

          <ol>
            <li><b><a href="../04-convex-sets-cones/index.html">Lecture 04 (Cones and Operations)</a>:</b> What OPERATIONS preserve convexity?
              <ul style="margin-top: 0.5rem;">
                <li>We'll study cones—sets with special algebraic structure ($x \in K \implies \lambda x \in K$)</li>
                <li>Cones enable duality theory (polar cones, dual cones)</li>
                <li>Key operations: proper cones, second-order cone, PSD cone</li>
                <li><i>Why cones matter:</i> They're the building blocks for conic programming (SOCP, SDP) in <a href="../08-convex-problems-conic/index.html">Lecture 08</a></li>
              </ul>
            </li>

            <li><b><a href="../05-convex-functions-basics/index.html">Lecture 05 (Convex Functions)</a>:</b> What FUNCTIONS have convex graphs (epigraphs)?
              <ul style="margin-top: 0.5rem;">
                <li>We'll define convex functions via Jensen's inequality and the epigraph</li>
                <li>We'll verify convexity using first-order (gradient) and second-order (Hessian) conditions</li>
                <li>We'll build complex convex functions from simple ones using composition rules</li>
                <li><i>Why functions matter:</i> They're the objectives and constraints in <a href="../07-convex-problems-standard/index.html">Lecture 07</a> (standard problems) and beyond</li>
              </ul>
            </li>
          </ol>

          <p><b>The Arc:</b> <b>Sets</b> (L03: geometry) → <b>Cones</b> (L04: algebra + duality) → <b>Functions</b> (L05: analysis + calculus) → <b>Problems</b> (L07-09: optimization)</p>

          <p><b>Key Insight:</b> Every convex function defines a convex set (its epigraph). Every convex set can be described by convex inequality constraints. This interplay between sets and functions is the DNA of convex optimization.</p>
        </div>

        <h4>Preview of Key Connections</h4>
        <ul>
          <li><b>Separation theorems (§3.3):</b> Will underpin Lagrangian duality in <a href="../09-duality/index.html">Lecture 09</a></li>
          <li><b>Cones (L04):</b> Enable the second-order cone (SOCP) and semidefinite cone (SDP) problem classes</li>
          <li><b>Epigraph characterization:</b> The bridge from sets (L03) to functions (L05)</li>
          <li><b>Convexity-preserving operations (L05):</b> Let you construct all DCP-compatible functions</li>
        </ul>

        <p style="margin-top: 1.5rem;"><b>What's Next:</b> In <a href="../04-convex-sets-cones/index.html">Lecture 04</a>, we'll study cones—sets that close under positive scaling. Cones have rich algebraic structure (duals, polars) and enable powerful duality theories. Then in <a href="../05-convex-functions-basics/index.html">Lecture 05</a>, we'll shift from geometry to calculus, studying convex functions via gradients and Hessians.</p>
      </section>

      <!-- SECTION 7: EXERCISES -->
      <section class="section-card" id="section-7">
        <h2><i data-feather="edit-3"></i> 7. Exercises</h2>

        <div class="problem">
          <h3>P3.1 — Voronoi Regions</h3>
          <p><b>Problem:</b> Let $x_1, \dots, x_k \in \mathbb{R}^n$ be a set of points. The Voronoi region associated with $x_i$ is defined as:
          $$ V_i = \{x \in \mathbb{R}^n \mid \|x - x_i\|_2 \le \|x - x_j\|_2, \ \forall j \neq i\} $$
          Prove that $V_i$ is a polyhedron (and thus a convex set).</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Polyhedron Definition:</b> A polyhedron is the intersection of a finite number of halfspaces. The goal is to show $V_i$ fits the form $\{x \mid Ax \le b\}$.</li>
                <li><b>Geometry of Distance:</b> The condition $\|x - a\| \le \|x - b\|$ is equivalent to a linear inequality $2(b-a)^\top x \le \|b\|^2 - \|a\|^2$. This defines a halfspace bounded by the perpendicular bisector of $a$ and $b$.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Step 1: Expand the inequality.</strong>
              The condition $\|x - x_i\|_2 \le \|x - x_j\|_2$ is equivalent to $\|x - x_i\|_2^2 \le \|x - x_j\|_2^2$.
              $$ x^\top x - 2x_i^\top x + x_i^\top x_i \le x^\top x - 2x_j^\top x + x_j^\top x_j $$
            </div>
            <div class="proof-step">
              <strong>Step 2: Linearize.</strong>
              Notice that $\|x-a\|^2 = \|x\|^2 - 2a^\top x + \|a\|^2$.
              Substituting this into the inequality:
              $$ \|x\|^2 - 2x_i^\top x + \|x_i\|^2 \le \|x\|^2 - 2x_j^\top x + \|x_j\|^2 $$
              The quadratic term $\|x\|^2$ cancels out from both sides (this is the key step). Rearranging terms:
              $$ 2(x_j - x_i)^\top x \le \|x_j\|_2^2 - \|x_i\|_2^2 $$
              This is a linear inequality $a_{ij}^\top x \le b_{ij}$ defining a closed halfspace $H_{ij}$.
            </div>
            <div class="proof-step">
              <strong>Step 3: Intersection.</strong>
              The Voronoi region is the intersection of these halfspaces for all $j \neq i$:
              $$ V_i = \bigcap_{j \neq i} H_{ij} $$
              Since $V_i$ is the intersection of a finite number ($k-1$) of halfspaces, it is a polyhedron. Since halfspaces are convex and intersections preserve convexity, $V_i$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.2 — Midpoint Convexity</h3>
          <p><b>Problem:</b> A set $C$ is <b>midpoint convex</b> if $\frac{1}{2}(x+y) \in C$ whenever $x, y \in C$. Prove that if $C$ is closed and midpoint convex, then $C$ is convex.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Dyadic Rationals:</b> Fractions of the form $k/2^n$. These are dense in the real numbers.</li>
                <li><b>Jensen's Logic:</b> If a property holds for midpoints, it holds for quarters, eighths, etc.</li>
                <li><b>Role of Topology:</b> Midpoint convexity implies convexity only for closed (or open) sets. Counter-example: The set of rational numbers $\mathbb{Q}$ is midpoint convex but not convex (contains holes).</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Step 1: Dyadic Combinations.</strong>
              By induction, we show $\lambda x + (1-\lambda)y \in C$ for all dyadic rationals $\lambda = k/2^n \in [0,1]$.
              Base case ($n=1$): $\lambda=1/2$, true by assumption.
              Inductive step: If true for $n$, any dyadic at $n+1$ is the midpoint of two dyadics at level $n$. Since $C$ is midpoint convex, the property holds.
            </div>
            <div class="proof-step">
              <strong>Step 2: Density Argument.</strong>
              Let $\theta \in [0, 1]$ be any real number. Since dyadic rationals are dense in $[0, 1]$, there exists a sequence $\lambda_k \in \{m/2^n\}$ such that $\lambda_k \to \theta$.
            </div>
            <div class="proof-step">
              <strong>Step 3: Limit via Closure.</strong>
              Let $z_k = \lambda_k x + (1-\lambda_k)y$. From Step 1, $z_k \in C$ for all $k$.
              The sequence converges: $z_k \to z = \theta x + (1-\theta)y$.
              Since $C$ is closed, it must contain all its limit points. Thus $z \in C$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.3 — Convex Hull of a Union</h3>
          <p><b>Problem:</b> Let $S_1, S_2$ be convex sets. Show that $\mathrm{conv}(S_1 \cup S_2)$ consists exactly of all line segments connecting points in $S_1$ to points in $S_2$. i.e., $x \in \mathrm{conv}(S_1 \cup S_2) \iff x = \theta y + (1-\theta)z$ for $y \in S_1, z \in S_2, \theta \in [0,1]$.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Join of Sets:</b> The convex hull of a union $A \cup B$ is called the "join". Geometrically, it fills the space between $A$ and $B$.</li>
                <li><b>Reduction:</b> A generic convex combination of points in $S_1 \cup S_2$ can be grouped into a "weighted center" for $S_1$ and a "weighted center" for $S_2$.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>($\Leftarrow$):</strong> Let $x = \theta y + (1-\theta)z$ with $y \in S_1, z \in S_2$. Since $S_1, S_2 \subseteq S_1 \cup S_2$, both $y$ and $z$ are in the union. Since the convex hull contains all convex combinations, $x \in \mathrm{conv}(S_1 \cup S_2)$.
            </div>
            <div class="proof-step">
              <strong>($\Rightarrow$):</strong> Let $x \in \mathrm{conv}(S_1 \cup S_2)$. By definition, $x = \sum_{i=1}^k \lambda_i u_i$ with $u_i \in S_1 \cup S_2$.
              Reorder indices so $u_1, \dots, u_m \in S_1$ and $u_{m+1}, \dots, u_k \in S_2$.
              $$ x = \sum_{i=1}^m \lambda_i u_i + \sum_{j=m+1}^k \lambda_j u_j $$
              Let $\theta = \sum_{i=1}^m \lambda_i$. If $\theta=0$, $x$ is a convex combination of points only in $S_2$, so $x \in S_2$ (convex). Same for $\theta=1$.
              If $0 < \theta < 1$, define:
              $$ y = \sum_{i=1}^m \frac{\lambda_i}{\theta} u_i, \quad z = \sum_{j=m+1}^k \frac{\lambda_j}{1-\theta} u_j $$
              These coefficients sum to 1, so $y \in S_1$ and $z \in S_2$ (by convexity of $S_1, S_2$).
              Substituting back, $x = \theta y + (1-\theta)z$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.4 — Linear-Fractional Functions</h3>
          <p><b>Problem:</b> Show that the range of a linear-fractional function $f(x) = \frac{Ax+b}{c^\top x+d}$ on a convex domain $C$ (where $c^\top x + d > 0$) is a convex set.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Perspective Composition:</b> Linear-fractional functions are the composition of an affine map with the perspective function $P(u, v) = u/v$.</li>
                <li><b>Preservation Rules:</b> Affine maps preserve convexity. The perspective map preserves convexity. Therefore, their composition preserves convexity. This is a purely structural proof avoiding messy algebra.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Step 1: Identify components.</strong>
              Define the affine map $g(x) = (Ax+b, c^\top x+d) \in \mathbb{R}^{m+1}$.
              Define the perspective map $P(y, t) = y/t$.
              Then $f(x) = P(g(x))$.
            </div>
            <div class="proof-step">
              <strong>Step 2: Apply preservation theorems.</strong>
              Since $C$ is convex, its image under the affine map, $D = g(C)$, is convex.
              The range of $f$ is $P(D)$. Since the perspective function preserves convexity (Theorem in 3.3), $P(D)$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.5 — Convexity of Quadratic Sublevel Set</h3>
          <p>Show that the set $C = \{x \in \mathbb{R}^n \mid x^\top A x + b^\top x + c \le 0\}$ is convex when $A \in \mathbb{S}^n_+$ (positive semidefinite).</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Algebraic Convexity:</b> We prove convexity directly from the definition $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$.</li>
                <li><b>Correction Term:</b> The value of a quadratic at an average is the average of values <i>minus</i> a term depending on the variance: $f(\bar{x}) = \overline{f(x)} - \text{variance}$. For convex functions ($A \succeq 0$), this 'variance' term is non-positive.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Step 1: Expand the Quadratic Form.</strong>
              Let $z = \theta x + (1-\theta)y$. We want to compute $z^\top A z$.
              $$
              z^\top A z = \theta^2 x^\top A x + (1-\theta)^2 y^\top A y + 2\theta(1-\theta)x^\top A y
              $$
            </div>
            <div class="proof-step">
              <strong>Step 2: The Key Identity.</strong>
              Using $\theta^2 = \theta - \theta(1-\theta)$, we derive:
              $$
              z^\top A z = \theta x^\top A x + (1-\theta)y^\top A y - \theta(1-\theta)(x-y)^\top A (x-y)
              $$
            </div>
            <div class="proof-step">
              <strong>Step 3: Combine with Linear Terms.</strong>
              Let $f(u) = u^\top A u + b^\top u + c$. By linearity of the other terms:
              $$
              f(z) = \theta f(x) + (1-\theta)f(y) - \theta(1-\theta)(x-y)^\top A (x-y)
              $$
            </div>
            <div class="proof-step">
              <strong>Step 4: Apply PSD Condition.</strong>
              Since $A \succeq 0$, $(x-y)^\top A (x-y) \ge 0$. Thus:
              $$ f(z) \le \theta f(x) + (1-\theta)f(y) $$
              Since $f(x) \le 0$ and $f(y) \le 0$, we have $f(z) \le 0$. Thus $C$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.6 — Relative Interior of the Simplex</h3>
          <p>Let $\Delta^n = \{x \in \mathbb{R}^n \mid x \ge 0, \mathbf{1}^\top x = 1\}$. Show that:
          $$ \mathrm{ri}(\Delta^n) = \{x \in \mathbb{R}^n \mid x > 0, \mathbf{1}^\top x = 1\} $$</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Relative Interior:</b> The interior relative to the affine hull. It excludes "edges" that lie on the relative boundary.</li>
                <li><b>Simplex Geometry:</b> The simplex lives in the hyperplane $\sum x_i = 1$. Its relative interior consists of strictly positive probability distributions.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Step 1: Identify affine hull.</strong> $\mathrm{aff}(\Delta^n) = \{x \in \mathbb{R}^n \mid \mathbf{1}^\top x = 1\}$ (the hyperplane).
            </div>
            <div class="proof-step">
              <strong>Step 2: Necessity ($\Rightarrow$).</strong> If any $x_i = 0$, any ball in the affine hull centered at $x$ contains points with negative coordinates. Thus $x \notin \mathrm{ri}(\Delta^n)$.
            </div>
            <div class="proof-step">
              <strong>Step 3: Sufficiency ($\Leftarrow$).</strong> If $x > 0$ and $\mathbf{1}^\top x = 1$, let $\varepsilon = \min x_i > 0$. Any point $y$ in the affine hull with $\|y-x\|_\infty < \varepsilon$ satisfies $y > 0$, hence $y \in \Delta^n$. Thus $x \in \mathrm{ri}(\Delta^n)$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.7 — Convex Hull Characterizations</h3>
          <p>Given points $x_1, \dots, x_k \in \mathbb{R}^n$, show that the convex hull equals the intersection of all convex sets containing the points.</p>

          <div class="recap-box">
             <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
             <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                 <li><b>Constructive vs. Intersection:</b> The convex hull can be built 'from the inside' (combinations) or 'from the outside' (intersections).</li>
                 <li><b>Minimality:</b> The intersection definition proves that the convex hull is the <i>smallest</i> convex set containing $S$.</li>
             </ul>
           </div>

           <div class="solution-box">
             <h4>Solution</h4>
             <div class="proof-step">
               Let $C_{comb}$ be the set of convex combinations, and $C_{int} = \bigcap_{S \subseteq K, K \text{ convex}} K$.
             </div>
             <div class="proof-step">
               <strong>($C_{comb} \subseteq C_{int}$):</strong> Let $x \in C_{comb}$. Then $x = \sum \theta_i x_i$. Any convex set $K$ containing $\{x_i\}$ must contain $x$ by definition. Thus $x$ is in the intersection of all such sets.
             </div>
             <div class="proof-step">
               <strong>($C_{int} \subseteq C_{comb}$):</strong> $C_{comb}$ is itself a convex set containing the points. Therefore, it is one of the sets $K$ in the intersection. Thus the intersection is contained in $C_{comb}$.
             </div>
           </div>
        </div>

        <div class="problem">
          <h3>P3.8 — Minkowski Sum of Sets</h3>
          <p>The Minkowski sum is $X + Y = \{x+y \mid x \in X, y \in Y\}$. Show that if $X, Y$ are convex, $X+Y$ is convex.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Linear Maps:</b> $X+Y$ is the image of $X \times Y$ under the linear map $f(x,y)=x+y$. Linear maps preserve convexity.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              Let $u, v \in X+Y$. Then $u = x_1+y_1$ and $v = x_2+y_2$ with $x_i \in X, y_i \in Y$.
              For $\lambda \in [0,1]$:
              $$ \lambda u + (1-\lambda)v = \lambda(x_1+y_1) + (1-\lambda)(x_2+y_2) = (\lambda x_1 + (1-\lambda)x_2) + (\lambda y_1 + (1-\lambda)y_2) $$
              By convexity of $X$ and $Y$, the terms in parentheses are in $X$ and $Y$ respectively. Thus the sum is in $X+Y$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.9 — Convexity of Thickened Sets</h3>
          <p>Let $C \subseteq \mathbb{R}^n$ be a closed convex set. Define $C_\varepsilon = \{y \mid \mathrm{dist}(y, C) \le \varepsilon\}$. Show that $C_\varepsilon$ is convex.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Minkowski Sum Interpretation:</b> The thickened set $C_\varepsilon$ is exactly $C + B(0, \varepsilon)$.</li>
                <li><b>Convexity Preservation:</b> Since $C$ is convex and the ball $B(0, \varepsilon)$ is convex, their Minkowski sum is convex.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              We observe that $C_\varepsilon = C + \{z \mid \|z\| \le \varepsilon\}$.
              The set $\{z \mid \|z\| \le \varepsilon\}$ is a norm ball, which is convex.
              The Minkowski sum of two convex sets is convex (Problem 3.8). Thus $C_\varepsilon$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.10 — Closure of a Convex Set</h3>
          <p>Show that if $C$ is convex, its closure $\mathrm{cl}(C)$ is convex.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Sequences:</b> Limit points preserve non-strict inequalities like $\theta x + (1-\theta)y \in \dots$.</li>
                <li><b>Intuition:</b> You cannot "bend" the boundary of a convex shape inwards (making it non-convex) without affecting the interior points.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              Let $a, b \in \mathrm{cl}(C)$. By definition, there exist sequences $x_k \to a, y_k \to b$ with $x_k, y_k \in C$.
            </div>
            <div class="proof-step">
              Let $\lambda \in [0,1]$. By convexity, $z_k = \lambda x_k + (1-\lambda)y_k \in C$.
            </div>
            <div class="proof-step">
              Taking limits: $z_k \to \lambda a + (1-\lambda)b$. Since limits of sequences in $C$ are in $\mathrm{cl}(C)$, the combination is in $\mathrm{cl}(C)$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.11 — Product of Convex Sets</h3>
          <p>Show that if $X, Y$ are convex, then $X \times Y$ is convex.</p>

          <div class="recap-box">
             <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
             <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                 <li><b>Component-wise Convexity:</b> The Cartesian product stacks convexity conditions. A segment in the product space projects to segments in the components.</li>
             </ul>
           </div>

           <div class="solution-box">
             <h4>Solution</h4>
             <div class="proof-step">
               Let $u_1 = (x_1, y_1), u_2 = (x_2, y_2) \in X \times Y$.
               $$ \lambda u_1 + (1-\lambda)u_2 = (\lambda x_1 + (1-\lambda)x_2, \lambda y_1 + (1-\lambda)y_2) $$
               Since $X$ is convex, the first component is in $X$. Since $Y$ is convex, the second is in $Y$. Thus the pair is in $X \times Y$.
             </div>
           </div>
        </div>

        <div class="problem">
          <h3>P3.12 — Finite Convex Combinations (Jensen's for Sets)</h3>
          <p>Prove by induction that if $C$ is convex, then any convex combination of $k$ points from $C$ lies in $C$.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Recursive Structure:</b> Any $k$-point combination can be viewed as a 2-point combination of one point and a $(k-1)$-point combination.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Base Case ($k=2$):</strong> Definition of convexity.
            </div>
            <div class="proof-step">
              <strong>Inductive Step:</strong> Assume true for $k$. Consider $x = \sum_{i=1}^{k+1} \theta_i x_i$. Let $\alpha = \sum_{i=1}^k \theta_i$. If $\alpha=0$, the statement holds. Otherwise, write:
              $$ x = \alpha \sum_{i=1}^k \frac{\theta_i}{\alpha} x_i + \theta_{k+1} x_{k+1} $$
              The sum term is a convex combination of $k$ points (in $C$ by hypothesis). Let's call it $y$.
              Then $x = \alpha y + (1-\alpha)x_{k+1}$, which is a convex combination of 2 points. Thus $x \in C$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.13 — Convexity via Line Intersections</h3>
          <p>Prove that $C$ is convex if and only if its intersection with every line is convex.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Reduction to 1D:</b> This allows checking convexity by 'slicing' the set.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>($\Rightarrow$):</strong> Suppose $C$ is convex. A line $L$ is an affine set, hence convex. Since the intersection of any collection of convex sets is convex (Theorem 3.1), the intersection $C \cap L$ is convex.
            </div>
            <div class="proof-step">
              <strong>($\Leftarrow$):</strong> Assume the intersection of $C$ with every line is convex. For any two points $x, y \in C$, let $L$ be the unique line passing through them. By assumption, the set $S = C \cap L$ is convex. Since $x, y \in S$, the convexity of $S$ implies the segment $[x, y] \subseteq S$. Since $S \subseteq C$, it follows that $[x, y] \subseteq C$. Thus, $C$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.14 — Hyperbolic Sets</h3>
          <p>Show that $C = \{ x \in \mathbb{R}^n_+ \mid \prod x_i \ge 1 \}$ is convex.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>AM-GM Power:</b> $\prod z_i \ge \prod (x_i^\theta y_i^{1-\theta})$ allows component-wise analysis.</li>
                <li><b>Superlevel Set:</b> Equivalently $\sum \log x_i \ge 0$, a superlevel set of a concave function.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              Let $z = \theta x + (1-\theta)y$. We want to show $\prod z_i \ge 1$.
              Apply the weighted AM-GM inequality to each component $i$:
              $$ z_i = \theta x_i + (1-\theta)y_i \ge x_i^\theta y_i^{1-\theta} $$
              (Here we treat $\theta$ and $1-\theta$ as the weights. This is valid since they sum to 1 and are non-negative).
            </div>
            <div class="proof-step">
              Taking the product over all $i$:
              $$ \prod z_i \ge \prod (x_i^\theta y_i^{1-\theta}) = \left(\prod x_i\right)^\theta \left(\prod y_i\right)^{1-\theta} $$
            </div>
            <div class="proof-step">
              Since $x, y \in C$, $\prod x_i \ge 1$ and $\prod y_i \ge 1$. Thus:
              $$ \prod z_i \ge 1^\theta \cdot 1^{1-\theta} = 1 $$
              Thus $z \in C$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.15 — Set Classification Challenge</h3>
          <p>Determine whether the following are convex: (a) Slab, (b) Rectangle, (c) Wedge, (d) Apollonius Set $\{x \mid \|x-a\| \le \theta \|x-b\|\}$.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Intersection Principle:</b> Slabs, rectangles, wedges are intersections of halfspaces.</li>
                <li><b>Quadratic Inequalities:</b> Apollonius set reduces to a quadratic inequality.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <ul>
              <li>(a-c) <b>Yes</b>, they are intersections of halfspaces (polyhedra).</li>
              <li>(d) <b>Yes, if $\theta \le 1$.</b>
              <br>Squaring the condition $\|x-a\| \le \theta \|x-b\|$ gives:
              $$ \|x\|^2 - 2a^\top x + \|a\|^2 \le \theta^2 (\|x\|^2 - 2b^\top x + \|b\|^2) $$
              Rearranging terms to group quadratics:
              $$ (1-\theta^2)\|x\|^2 - 2(a - \theta^2 b)^\top x + (\|a\|^2 - \theta^2 \|b\|^2) \le 0 $$
              <ul>
                <li>If $\theta = 1$: The quadratic term vanishes. We get a linear inequality (halfspace), which is convex.</li>
                <li>If $\theta < 1$: The coefficient $(1-\theta^2)$ is positive. This defines a sublevel set of a convex quadratic (a ball), which is convex.</li>
                <li>If $\theta > 1$: The coefficient is negative. This defines a superlevel set of a convex quadratic (the region exterior to a ball), which is <b>not convex</b> (it has a hole).</li>
              </ul>
              </li>
            </ul>
          </div>
        </div>

        <div class="problem">
          <h3>P3.16 — Partial Sum of Convex Sets</h3>
          <p>Show that $S = \{(x, y_1+y_2) \mid (x, y_1) \in S_1, (x, y_2) \in S_2\}$ is convex.</p>

          <div class="recap-box">
             <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
             <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                 <li><b>Fiber-wise Sum:</b> For fixed $x$, the set is $S_1(x) + S_2(x)$.</li>
             </ul>
           </div>

           <div class="solution-box">
             <h4>Solution</h4>
             <div class="proof-step">
               Let $z^a, z^b \in S$. Then $y^a = y_1^a + y_2^a$.
               Consider $\theta z^a + (1-\theta)z^b$. The $x$-component is $x^\theta$. The $y$-component is $y^\theta$.
               We can decompose $y^\theta$ into $\tilde{y}_1 + \tilde{y}_2$ where $\tilde{y}_1$ is the combination of $y_1$'s.
               Since $(x^\theta, \tilde{y}_1) \in S_1$ (by convexity) and $(x^\theta, \tilde{y}_2) \in S_2$, the sum is in $S$.
             </div>
           </div>
        </div>

        <div class="problem">
          <h3>P3.17 — Perspective of Polyhedral Sets</h3>
          <p>Determine the image of a convex hull $C = \mathrm{conv}\{(v_i, t_i)\}$ under $P(v,t) = v/t$.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Projective Map:</b> Perspective maps convex hulls to convex hulls.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              $P(C) = \mathrm{conv}\{v_i/t_i\}$.
              Any point in $P(C)$ is $\frac{\sum \theta_i v_i}{\sum \theta_i t_i}$. This is a weighted average of the points $v_i/t_i$ with weights $\lambda_i \propto \theta_i t_i$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.18 — Invertible Linear-Fractional Functions</h3>
          <p>Find the inverse of $f(x) = (Ax+b)/(c^\top x + d)$ assuming matrix $Q$ is nonsingular.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Matrix Representation:</b> Linear-fractional maps correspond to matrix multiplication in homogeneous coordinates. The inverse map corresponds to the inverse matrix.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              The function corresponds to $y \propto Q x_{hom}$. Inverting, $x_{hom} \propto Q^{-1} y_{hom}$.
              If $Q^{-1} = \begin{bmatrix} E & f \\ g^\top & h \end{bmatrix}$, then $f^{-1}(y) = \frac{Ey + f}{g^\top y + h}$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.19 — Projection onto a Convex Set</h3>
          <p>Let $C$ be a closed convex set. The projection $P_C(x)$ is the unique point in $C$ minimizing $\|z-x\|_2$. Prove the optimality condition: $\langle x - P_C(x), z - P_C(x) \rangle \le 0$ for all $z \in C$.</p>
          <div class="solution-box">
            <h4>Solution (Geometric Proof)</h4>
            <div class="proof-step">
              Let $y = P_C(x)$ be the projection. We want to show $\langle x-y, z-y \rangle \le 0$ for all $z \in C$.
              <br><b>Proof by Contradiction:</b>
              Suppose there exists a point $z \in C$ such that the angle is acute: $\langle x-y, z-y \rangle > 0$.
              <br>Consider a point $w$ on the segment between $y$ and $z$: $w = (1-\theta)y + \theta z = y + \theta(z-y)$ for small $\theta \in (0,1)$. Since $C$ is convex, $w \in C$.
            </div>
            <div class="proof-step">
              Check the distance squared from $x$ to $w$:
              $$ \|x - w\|^2 = \|(x - y) - \theta(z - y)\|^2 = \|x - y\|^2 - 2\theta \langle x - y, z - y \rangle + \theta^2 \|z - y\|^2 $$
            </div>
            <div class="proof-step">
              We assume for contradiction that the inner product is positive: $S = \langle x - y, z - y \rangle > 0$.
              We analyze the squared distance $\|x-w\|^2$ as a function of $\theta$:
              $$ g(\theta) = \|x-w\|^2 = \|x-y\|^2 - 2\theta S + \theta^2 \|z-y\|^2 $$
              We want to show that for small $\theta > 0$, $g(\theta) < g(0) = \|x-y\|^2$.
              <br>The derivative at 0 is $g'(0) = -2S < 0$. Since the function has a negative slope at 0, it must decrease immediately.
              Specifically, for any $\theta$ in the range $0 < \theta < \frac{2S}{\|z-y\|^2}$, we have $-2\theta S + \theta^2 \|z-y\|^2 < 0$, so $\|x-w\|^2 < \|x-y\|^2$.
              <br>Since such a $\theta$ exists (and can be chosen $\le 1$), we found a point $w \in C$ closer to $x$ than $y$. This contradicts the definition of projection.
              <br><b>Conclusion:</b> The assumption $S > 0$ must be false. Thus $\langle x-y, z-y \rangle \le 0$.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.20 — Preimages under Linear-Fractional Maps</h3>
          <p>Find the inverse image of a halfspace, polyhedron, and ellipsoid under a linear-fractional map.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Complexity Preservation:</b> Polyhedra map to polyhedra. Ellipsoids map to sets defined by quadratic inequalities (related to SOCs).</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <strong>Setup (multiply by the positive denominator).</strong>
              Let $f(x) = \dfrac{Ax+b}{c^\top x + d}$ with domain $\{x \mid c^\top x + d > 0\}$.
              For any target set $S$ in the output space,
              $$ f^{-1}(S) = \{x \mid f(x)\in S,\; c^\top x + d > 0\}. $$
              The key simplification is that on the domain $c^\top x + d$ is strictly positive, so multiplying an inequality by $c^\top x + d$ does not flip the inequality direction.
            </div>

            <div class="proof-step">
              <strong>(a) Halfspace.</strong>
              Let $H = \{y \mid a^\top y \le \beta\}$. Then $f(x)\in H$ iff
              $$ a^\top \frac{Ax+b}{c^\top x + d} \le \beta. $$
              Multiply by $c^\top x + d > 0$ and rearrange:
              $$ a^\top(Ax+b) \le \beta(c^\top x + d)\;\;\Longleftrightarrow\;\;(a^\top A - \beta c^\top)x \le \beta d - a^\top b. $$
              Therefore $f^{-1}(H)$ is a halfspace intersected with the domain constraint $c^\top x + d > 0$.
            </div>

            <div class="proof-step">
              <strong>(b) Polyhedron.</strong>
              Any polyhedron can be written as $P = \{y \mid My \le h\}$, i.e., an intersection of finitely many halfspaces.
              Apply the halfspace computation to each row inequality $m_i^\top y \le h_i$.
              The preimage is an intersection of finitely many halfspaces in $x$ (plus $c^\top x + d > 0$), hence a polyhedron.
            </div>

            <div class="proof-step">
              <strong>(c) Ellipsoid (SOC form).</strong>
              Write the ellipsoid as $E = \{y \mid \|P^{-1/2}(y-y_0)\|_2 \le 1\}$ with $P \succ 0$.
              Then $x \in f^{-1}(E)$ if and only if:
              $$ \left\|P^{-1/2}\left(\frac{Ax+b}{c^\top x+d}-y_0\right)\right\|_2 \le 1 \quad \text{and} \quad c^\top x + d > 0 $$
              Since the denominator $t = c^\top x + d$ is positive, we can multiply the inequality by $t$ without changing its direction.
              Inside the norm, we multiply by $t$:
              $$ \|P^{-1/2}(Ax+b) - P^{-1/2}y_0(c^\top x + d)\|_2 \le c^\top x + d $$
              Let $A' = P^{-1/2}A$, $b' = P^{-1/2}b$, and $v = P^{-1/2}y_0$. The condition becomes:
              $$ \|A'x + b' - v(c^\top x + d)\|_2 \le c^\top x + d $$
              This inequality describes a convex set. Specifically, it is the intersection of a <b>second-order cone constraint</b> (of the form $\|Mz + q\|_2 \le \ell^\top z + r$) and the open halfspace $c^\top x + d > 0$.
              Thus, the preimage of an ellipsoid under a linear-fractional map is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P3.21 — Projection onto the Probability Simplex</h3>
          <p>The probability simplex is $\Delta_n = \{x \in \mathbb{R}^n \mid x \ge 0, \mathbf{1}^\top x = 1\}$. Find the projection of a vector $z \in \mathbb{R}^n$ onto $\Delta_n$. This is the solution to $\min_x \frac{1}{2}\|x-z\|_2^2$ s.t. $x \in \Delta_n$.</p>
          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              This problem asks for the projection of $z$ onto the simplex $\Delta_n$.
              The optimality condition for projection (from P3.19) is $\langle z - x^*, y - x^* \rangle \le 0$ for all $y \in \Delta_n$.
              This is equivalent to minimizing the Lagrangian $\mathcal{L}(x, \nu, \lambda) = \frac{1}{2}\|x-z\|^2 + \nu (\mathbf{1}^\top x - 1) - \lambda^\top x$ with $\lambda \ge 0$.
            </div>
            <div class="proof-step">
              The stationarity condition $\nabla_x \mathcal{L} = 0$ gives:
              $$ x - z + \nu \mathbf{1} - \lambda = 0 \implies x_i = z_i - \nu + \lambda_i $$
              Complementary slackness requires $\lambda_i x_i = 0$ and $\lambda_i \ge 0, x_i \ge 0$.
              <ul>
                  <li>If $x_i > 0$, then $\lambda_i = 0$, so $x_i = z_i - \nu$.</li>
                  <li>If $x_i = 0$, then $\lambda_i \ge 0$, so $z_i - \nu = -\lambda_i \le 0$, i.e., $z_i \le \nu$.</li>
              </ul>
              Combining these, we get the structure:
              $$ x_i = \max(z_i - \nu, 0) $$
            </div>
            <div class="proof-step">
              We must choose the scalar $\nu$ such that $\sum x_i = \sum \max(z_i - \nu, 0) = 1$.
              Let $g(\nu) = \sum_{i=1}^n \max(z_i - \nu, 0)$.
              This function is continuous, strictly decreasing (when non-zero), $g(-\infty) = \infty$, and $g(\infty) = 0$.
              Thus, there is a unique $\nu^*$ satisfying the equation.
            </div>
            <div class="proof-step">
              <b>Water-filling Algorithm (Step-by-Step Construction):</b>
              The condition is $x_i = \max(z_i - \nu, 0)$ and $\sum x_i = 1$.
              This means $\sum_{i=1}^n \max(z_i - \nu, 0) = 1$.
              Imagine "pouring water" into channels of height $z_i$ up to level $\nu$.
              <br>1. <b>Sort:</b> Sort $z$ in descending order: $z_{(1)} \ge z_{(2)} \ge \dots \ge z_{(n)}$.
              <br>2. <b>Active Set Search:</b> We need to find how many components are non-zero. Let $k$ be the number of active components ($x_i > 0$).
              If $k$ components are active, they are the $k$ largest ones. For these, $z_i > \nu$, so $x_i = z_i - \nu$.
              The sum constraint becomes:
              $$ \sum_{j=1}^k (z_{(j)} - \nu) = 1 \implies \sum_{j=1}^k z_{(j)} - k\nu = 1 \implies \nu = \frac{1}{k}\left(\sum_{j=1}^k z_{(j)} - 1\right) $$
              We need to find the largest $k$ (call it $\rho$) such that the consistency check holds: $z_{(\rho)} > \nu$.
              <br>3. <b>Compute Threshold:</b> Set $\nu = \frac{1}{\rho}(\sum_{j=1}^\rho z_{(j)} - 1)$.
              <br>4. <b>Result:</b> Set $x_i = \max(z_i - \nu, 0)$.
            </div>
          </div>
        </div>

      </section>

      <section class="section-card" id="section-8">
        <h2>8. Recap &amp; What's Next</h2>
        <div class="recap-box">
          <ul style="margin: 0 0 0 20px;">
            <li><b>Affine vs. convex:</b> affine sets contain whole lines; convex sets contain line segments.</li>
            <li><b>Canonical convex sets:</b> halfspaces/hyperplanes, norm balls, ellipsoids, polyhedra/polytopes, and the PSD cone appear repeatedly as feasible regions.</li>
            <li><b>Convexity-preserving operations:</b> intersections, affine images/preimages, perspective, and linear-fractional maps are the fastest way to prove a set is convex.</li>
            <li><b>Topology that matters:</b> when $\mathrm{int}(C)=\emptyset$ (common in constrained problems), the right notion is $\mathrm{ri}(C)$, the interior inside the affine hull.</li>
          </ul>
        </div>
        <div class="interpretation-box">
          <p style="margin: 0;"><b>Forward look:</b> In <a href="../04-convex-sets-cones/index.html">Lecture 04</a>, cones and separating hyperplanes explain why convex problems admit powerful certificates (duals). In <a href="../05-convex-functions-basics/index.html">Lecture 05</a>, convex functions are introduced largely by studying the geometry of sets like epigraphs and sublevel sets.</p>
        </div>
      </section>
    </article>

    <section class="section-card" id="section-8">
      <h2>8. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, Chapter 2 (Convex Sets).</li>
        <li><strong>Supplementary:</strong> Rockafellar, <em>Convex Analysis</em>, Sections 1-3.</li>
        <li><strong>Advanced:</strong> Bertsekas, <em>Convex Optimization Theory</em>, Chapter 1.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/resizable.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initEllipsoidExplorer } from './widgets/js/ellipsoid-explorer.js';
    initEllipsoidExplorer('widget-ellipsoid-explorer');
  </script>
  <script type="module">
    import { initPolyhedronVisualizer } from './widgets/js/polyhedron-visualizer.js';
    initPolyhedronVisualizer('widget-polyhedron-visualizer');
  </script>
  <script type="module">
    import { initConvexGeometryLab } from './widgets/js/convex-geometry-lab.js';
    initConvexGeometryLab('widget-convex-geometry-lab');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
