<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>13. Algorithms I: Unconstrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="section-card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">13. Algorithms I: Unconstrained Minimization</h1>
      <div class="meta">
        Date: 2025-12-16 · Duration: 90 min · Tags: algorithms, unconstrained, strong-convexity, newton-method
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> A rigorous "zero-to-hero" analysis of descent methods for unconstrained convex optimization. Covers gradient descent, steepest descent, and Newton's method with full convergence proofs, including detailed derivations of strong convexity implications and self-concordance.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05</a></p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you should understand:</p>
      <ul style="line-height: 1.8;">
        <li>The rigorous derivations of convergence rates for gradient descent under strong convexity and smoothness using Taylor's theorem.</li>
        <li>How the condition number $\kappa$ and the geometry of sublevel sets (width) dictate convergence speed.</li>
        <li>The duality between choosing a norm for steepest descent and choosing a preconditioner.</li>
        <li>Newton's method as steepest descent in the Hessian geometry, and its two-phase convergence behavior.</li>
        <li>How to implement and benchmark these algorithms (Gradient Descent, Newton's Method) on problems like Analytic Centering.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>0. Notation and the "Model"</h3>
      <p>
        We study the problem of minimizing a function
        $$ p^\star = \inf_{x\in \mathrm{dom}\,f} f(x) \tag{9.1} $$
        under the following assumptions:
      </p>
      <ul>
        <li>$f$ is <strong>convex</strong> on $\mathrm{dom}\,f$ (meaning the domain is a convex set and the function satisfies the Jensen's inequality).</li>
        <li>$f$ is <strong>twice continuously differentiable</strong> ($f \in C^2(\mathrm{dom}\,f)$). We need this to use the Gradient $\nabla f$ and Hessian $\nabla^2 f$ in our algorithms.</li>
        <li>$\mathrm{dom}\,f$ is <strong>open</strong>. This is critical. It ensures that for any point $x$ in the domain, we can take a small step in <em>any</em> direction without leaving the set. This allows derivatives to be well-defined in all directions.</li>
        <li>The problem is <strong>solvable</strong>: the optimal value $p^\star = \min_x f(x)$ is finite and attained at some $x^\star$. Not all convex problems are solvable (e.g., $e^{-x}$ on $\mathbb{R}$ is bounded below by 0 but never reaches it).</li>
      </ul>
      <p>
        <strong>Important Subtlety:</strong> If $f: \mathbb{R}^n \to \mathbb{R}$, then $\mathrm{dom}\,f = \mathbb{R}^n$ is automatically open. In the broader convex-analysis setting, we often allow <strong>extended-valued</strong> convex functions $f:\mathbb{R}^n\to\mathbb{R}\cup\{+\infty\}$, where $\mathrm{dom}\,f=\{x:f(x)<+\infty\}$ can be a strict open subset. For example, the logarithmic barrier $f(x) = -\sum \log(x_i)$ is only defined on the open positive orthant $\mathbb{R}^n_{++}$. Twice differentiability forces us to work on an open set.
      </p>
      <p>
        The analysis repeatedly refers to a set $S$. In this chapter, $S$ is typically a <strong>sublevel set</strong> that contains all iterates of the algorithm:
        $$ S=\{x\in\mathrm{dom}\,f \mid f(x)\le f(x^{(0)})\} \tag{9.3} $$
        <strong>Why do we care about $S$?</strong>
        <br>
        Most "global" convergence proofs actually rely on local properties. We don't need the function to be strongly convex <em>everywhere</em> in the universe. We only need it to be well-behaved (strongly convex, smooth) in the region where the algorithm actually travels. Since descent algorithms decrease the objective value $f(x^{(k)})$ at every step, the sequence of iterates $x^{(0)}, x^{(1)}, \dots$ will remain trapped inside the initial sublevel set $S$. Thus, bounds that hold on $S$ are sufficient for convergence.
      </p>

      <h3>1. The Core Equivalence (Optimality Conditions)</h3>
      <p>
        The unconstrained problem is:
        $$ \min_x f(x) \tag{9.1} $$
        "Unconstrained" means no explicit constraints (like $Ax=b$) are written, but constraints are implicit in the domain $\mathrm{dom}\,f$. Stepping outside the domain is forbidden (function value is effectively $+\infty$).
      </p>
      <h4>1.1 Why Convexity + Differentiability Makes Optimality Global</h4>
      <p>
        The necessary and sufficient condition for optimality is:
        $$ \nabla f(x^\star)=0 \tag{9.2} $$
        In standard nonconvex calculus, $\nabla f=0$ is merely a <em>stationary point</em> condition—it could be a local minimum, a local maximum, or a saddle point. However, <strong>convexity upgrades "stationary point" to "global minimizer."</strong>
        This relies on the <strong>first-order characterization of convexity</strong>:
        $$ f(y) \ge f(x) + \nabla f(x)^T (y-x) \qquad (\star) $$
        Geometrically, this inequality says that the first-order Taylor approximation (the tangent hyperplane) is a <strong>global underestimator</strong> of the function. The graph of the function lies entirely <em>above</em> its tangent plane at any point $x$.
      </p>
      <p><strong>Detailed Proof of $(\star)$:</strong></p>
      <ul>
          <li><strong>Definition of Convexity:</strong> By definition, for any $t \in [0,1]$, the function segment lies below the chord:
              $$ f(x+t(y-x)) \le (1-t)f(x)+t f(y) $$
          </li>
          <li><strong>Isolate $f(y)$:</strong> Rearrange the inequality to isolate the terms involving $f(y)$:
              $$ f(x+t(y-x)) - f(x) \le t(f(y)-f(x)) $$
          </li>
          <li><strong>Form Difference Quotient:</strong> Divide by $t$ (assuming $t > 0$):
              $$ \frac{f(x+t(y-x))-f(x)}{t} \le f(y)-f(x) $$
          </li>
          <li><strong>Limit $t \to 0$:</strong> Take the limit as $t \downarrow 0$. By definition of the gradient, the LHS converges to the directional derivative $\nabla f(x)^T(y-x)$.
              $$ \nabla f(x)^T(y-x) \le f(y) - f(x) $$
              Rearranging gives the result: $f(y) \ge f(x) + \nabla f(x)^T (y-x)$.
          </li>
      </ul>
      <p><strong>Proof of Equivalence ($\nabla f(x^\star)=0 \iff \text{Global Min}$):</strong></p>
      <ul>
          <li><strong>Sufficiency ($\nabla f(x^\star)=0 \Rightarrow$ Min):</strong>
              Suppose $\nabla f(x^\star) = 0$. We plug this into the convexity inequality $(\star)$. For any feasible point $y$:
              $$ f(y) \ge f(x^\star) + \underbrace{\nabla f(x^\star)^T}_{0} (y-x^\star) = f(x^\star) $$
              Since $f(y) \ge f(x^\star)$ for all $y$, $x^\star$ is a global minimizer.
          </li>
          <li><strong>Necessity (Min $\Rightarrow \nabla f(x^\star)=0$):</strong>
              Assume $x^\star$ is a local minimizer. Since $\mathrm{dom}\,f$ is <strong>open</strong>, for any direction vector $v$, the point $x^\star + tv$ is inside the domain for small enough $t$.
              Define the scalar function $\phi(t)=f(x^\star+tv)$. Since $x^\star$ minimizes $f$, $t=0$ must minimize $\phi(t)$.
              From single-variable calculus, if $t=0$ is an interior minimum, then $\phi'(0)=0$.
              By the chain rule: $\phi'(0)=\nabla f(x^\star)^T v = 0$.
              Since the dot product $\nabla f(x^\star)^T v$ is zero for <em>every</em> possible direction vector $v$ (we can choose basis vectors $e_1, \dots, e_n$), the gradient vector itself must be zero: $\nabla f(x^\star)=0$.
          </li>
      </ul>

      <h3>2. Motivating Examples</h3>
      <p>Before diving into the proofs, let's look at the canonical problems we are trying to solve. These examples will serve as our testbed for algorithms.</p>
      <h4>2.1 Quadratic Minimization</h4>
      <p>
        The most basic convex optimization problem is the unconstrained quadratic program:
        $$ \min_x \ f(x) = \frac12 x^T P x + q^T x + r,\qquad P\in S^n_+ \tag{9.4} $$
        where $P$ is symmetric positive semidefinite ($P \succeq 0$).
        <br>
        <strong>Derivation of Gradient:</strong>
        Using the fact that $\nabla_x (x^T P x) = 2Px$ (for symmetric $P$) and $\nabla_x (q^T x) = q$:
        $$ \nabla f(x)=Px+q $$
        Setting the gradient to zero gives the linear system:
        $$ Px^\star = -q $$
      </p>
      <ul>
          <li><strong>Case 1: $P \succ 0$ (Positive Definite).</strong> The matrix $P$ is invertible. There is a <strong>unique global minimizer</strong>: $x^\star = -P^{-1}q$. The level sets are ellipsoids, and the function looks like a bowl.</li>
          <li><strong>Case 2: $P \succeq 0$ but Singular.</strong> The matrix $P$ has a non-trivial nullspace.
            <ul>
                <li>If $-q \in \mathrm{range}(P)$, there are infinitely many solutions (a linear subspace of solutions).</li>
                <li>If $-q \notin \mathrm{range}(P)$, the system is inconsistent. The function is unbounded below (decreases to $-\infty$ along directions in the nullspace).</li>
            </ul>
          </li>
      </ul>

      <h4>2.2 Least Squares</h4>
      <p>
        $$ \min_x \|Ax-b\|_2^2 $$
        We can expand the squared norm to reveal it is a quadratic function:
        $$ f(x) = (Ax-b)^T (Ax-b) = x^T A^T A x - 2b^T A x + b^T b $$
        Comparing this to the standard form in 2.1:
        <ul>
            <li>$P = 2A^T A$ (Always symmetric positive semidefinite).</li>
            <li>$q = -2A^T b$.</li>
        </ul>
        The optimality condition $\nabla f(x) = 0$ becomes the famous <strong>Normal Equations</strong>:
        $$ 2A^T A x - 2A^T b = 0 \implies A^T A x = A^T b $$
      </p>

      <h4>2.3 Log-Sum-Exp (Softmax)</h4>
      <p>
        This function approximates the "max" function ($\max_i (a_i^T x + b_i)$) but is smooth and convex:
        $$ f(x)=\log\Big(\sum_{i=1}^m e^{a_i^T x+b_i}\Big) $$
        <strong>Gradient Interpretation:</strong> The gradient is a probability-weighted sum of the vectors $a_i$:
        $$ \nabla f(x) = \sum_{i=1}^m w_i(x) a_i, \quad \text{where } w_i(x) = \frac{e^{a_i^T x+b_i}}{\sum_j e^{a_j^T x+b_j}} $$
        The weights $w_i(x)$ sum to 1. This is effectively the expected value of $a_i$ under the "softmax" distribution.
        <br>
        <strong>Hessian:</strong> The Hessian turns out to be the covariance matrix of the vectors $a_i$ under this distribution. Since covariance matrices are always PSD, $\nabla^2 f(x) \succeq 0$, proving convexity.
      </p>

      <h4>2.4 Analytic Centers (Logarithmic Barriers)</h4>
      <p>
        $$ f(x) = -\sum_{i=1}^m \log(b_i - a_i^T x) $$
        defined on the polyhedron $\mathcal{P} = \{x \mid a_i^T x < b_i \forall i\}$.
        <br>
        <strong>Intuition:</strong> The term $-\log(b_i - a_i^T x)$ approaches $+\infty$ as $a_i^T x \to b_i$. This acts as a <strong>repulsive force field</strong> from the boundary of the feasible set.
        Minimizing $f(x)$ finds a point $x^\star$ that is "maximally inside" the set, balancing the repulsion from all walls. This point is called the <strong>Analytic Center</strong>. This concept is the engine behind <strong>Interior Point Methods</strong>, which solve constrained problems by following a path of analytic centers.
      </p>

      <h3>3. Strong Convexity: The Engine of Convergence</h3>
      <p>
        We assume $f$ is <strong>strongly convex on $S$</strong>. This is a condition on the curvature of the function. Formally, there exists a scalar $m>0$ such that:
        $$ \nabla^2 f(x)\succeq mI,\qquad \forall x\in S. \tag{9.7} $$
        <strong>Meaning:</strong> The eigenvalues of the Hessian are all at least $m$. For every non-zero vector $v$, the curvature along direction $v$ satisfies $v^T\nabla^2 f(x)v \ge m\|v\|_2^2$.
        <br>
        Intuitively, the function is "at least as curved as a quadratic bowl with curvature $m$." It cannot contain any flat regions.
      </p>

      <h4>3.1 From Taylor’s Theorem to the Quadratic Lower Bound</h4>
      <p>
        Let's derive the most important consequence of strong convexity.
        Consider two points $x,y\in S$. Taylor’s theorem with Lagrange remainder states that:
        $$ f(y)=f(x)+\nabla f(x)^T(y-x)+\frac12 (y-x)^T\nabla^2 f(z)(y-x) $$
        where $z$ is some point on the line segment connecting $x$ and $y$.
        <br>
        Since $z \in S$ (by convexity of $S$), we can apply our strong convexity assumption ($\nabla^2 f(z) \succeq mI$):
        $$ (y-x)^T\nabla^2 f(z)(y-x)\ \ge\ m\|y-x\|_2^2 $$
        Substituting this back into the Taylor expansion yields the <strong>Quadratic Lower Bound</strong>:
        $$ f(y)\ \ge\ f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}\|y-x\|_2^2 \tag{9.8} $$
        <strong>Geometric Interpretation:</strong>
        <ul>
            <li>For a standard convex function ($m=0$), the graph sits above the tangent plane: $f(y) \ge f(x) + \nabla f(x)^T(y-x)$.</li>
            <li>For a strongly convex function, the graph sits above a <strong>paraboloid</strong> that touches the graph at $x$. This paraboloid grows quadratically, forcing the function to rise steeply as we move away from the minimum.</li>
        </ul>
      </p>

      <h4>3.2 Bounding Suboptimality and Distance to Optimizer</h4>
      <p>
        The quadratic lower bound allows us to bound how far we are from the solution, both in terms of function value and distance in $\mathbb{R}^n$.
        <br>
        <strong>1. Suboptimality Bound ($f(x) - p^\star$):</strong>
        Fix $x$. The right-hand side of (9.8) is a function of $y$: $g(y) = f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|^2$.
        Since $f(y) \ge g(y)$ for all $y$, it must be that $\min_y f(y) \ge \min_y g(y)$, so $p^\star \ge \min_y g(y)$.
        The function $g(y)$ is a simple quadratic. Its minimum occurs where its gradient is zero:
        $$ \nabla_y g(y) = \nabla f(x) + m(y-x) = 0 \implies \tilde{y} = x - \frac{1}{m}\nabla f(x) $$
        Plugging $\tilde{y}$ back into $g(y)$:
        $$ \min_y g(y) = f(x) - \frac{1}{m}\|\nabla f(x)\|^2 + \frac{m}{2}\frac{1}{m^2}\|\nabla f(x)\|^2 = f(x) - \frac{1}{2m}\|\nabla f(x)\|^2 $$
        Therefore:
        $$ p^\*\ \ge\ f(x)-\frac{1}{2m}\|\nabla f(x)\|_2^2 \implies f(x)-p^\*\ \le\ \frac{1}{2m}\|\nabla f(x)\|_2^2 \tag{9.9} $$
        This is incredibly useful. It tells us that if the gradient is small, the function value is close to optimal. We can use $\|\nabla f(x)\|_2 \le \sqrt{2m\epsilon}$ as a rigorous stopping criterion.
      </p>
      <p>
        <strong>2. Distance to Optimizer ($\|x - x^\star\|$):</strong>
        Applying the master inequality (9.8) with $y=x^\star$:
        $$ p^\* \ge f(x)+\nabla f(x)^T(x^\*-x)+\frac{m}{2}\|x^\*-x\|^2 $$
        Since $p^\star \le f(x)$, we have:
        $$ 0 \ge \nabla f(x)^T(x^\*-x)+\frac{m}{2}\|x^\*-x\|^2 $$
        $$ -\nabla f(x)^T(x^\*-x) \ge \frac{m}{2}\|x^\*-x\|^2 $$
        Using Cauchy-Schwarz ($a^Tb \le \|a\|\|b\|$):
        $$ \|\nabla f(x)\|_2 \|x^\*-x\|_2 \ge \frac{m}{2}\|x^\*-x\|_2^2 $$
        Dividing by $\|x^\star - x\|_2$ (assuming $x \neq x^\star$):
        $$ \|x-x^\*\|_2 \le \frac{2}{m}\|\nabla f(x)\|_2 \tag{9.11} $$
        This implies the optimizer $x^\star$ is <strong>unique</strong>.
      </p>

      <h3>4. Smoothness: The Guarantee of Descent</h3>
      <p>
        Complementing the lower bound, we need an upper bound on curvature.
        Since $S$ is bounded (which is implied by strong convexity) and closed, and $f$ is twice continuously differentiable, the maximum eigenvalue of the Hessian must be bounded above by some constant $M$ on $S$:
        $$ \nabla^2 f(x)\preceq MI,\qquad \forall x\in S \tag{9.12} $$
        This property is called <strong>M-smoothness</strong> (or Lipschitz continuity of the gradient with constant $M$).
      </p>
      <h4>4.1 Quadratic Upper Bound</h4>
      <p>
        Using Taylor's theorem in the exact same way as before, but using the upper bound on the Hessian:
        $$ f(y)\ \le\ f(x)+\nabla f(x)^T(y-x)+\frac{M}{2}\|y-x\|_2^2 \tag{9.13} $$
        <strong>The Sandwich Principle:</strong> Combining (9.8) and (9.13), we see that the function $f(y)$ is <strong>sandwiched</strong> between two quadratic bowls anchored at $x$:
        $$ f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|^2 \le f(y) \le f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}\|y-x\|^2 $$
        <br>
        <strong>Implication for Descent:</strong> The upper bound guarantees that if we take a small enough step opposite to the gradient, the function value <em>must</em> decrease.
        Minimizing the quadratic upper bound (RHS of 9.13) with respect to $y$ gives $y = x - \frac{1}{M}\nabla f(x)$. Plugging this specific $y$ into the inequality:
        $$ f\left(x - \frac{1}{M}\nabla f(x)\right) \le f(x) - \frac{1}{M}\|\nabla f(x)\|^2 + \frac{M}{2}\frac{1}{M^2}\|\nabla f(x)\|^2 = f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 $$
        This implies:
        $$ p^\*\ \le\ f(x)-\frac{1}{2M}\|\nabla f(x)\|_2^2 \tag{9.14} $$
        So, a single gradient step reduces the function value by at least $\frac{1}{2M}\|\nabla f(x)\|^2$.
      </p>
      <p>
        <strong>Summary of Bounds:</strong>
        $$ \frac{1}{2M}\|\nabla f(x)\|^2 \ \le\ f(x)-p^\* \ \le\ \frac{1}{2m}\|\nabla f(x)\|^2 $$
      </p>

      <h3>5. Condition Number of Sublevel Sets</h3>
      <p>
        From (9.7) and (9.12), the eigenvalues of the Hessian are bounded: $mI \preceq \nabla^2 f(x) \preceq MI$ for all $x \in S$.
        The ratio $\kappa = M/m$ bounds the condition number of the Hessian matrix at any point:
        $$ \kappa(\nabla^2 f(x))=\frac{\lambda_{\max}(\nabla^2 f(x))}{\lambda_{\min}(\nabla^2 f(x))}\le \frac{M}{m} $$
        We call this upper bound $\kappa$ the condition number of the problem (on $S$).
      </p>
      <h4>5.1 Geometric Meaning: Eccentricity and "Skinniness"</h4>
      <p>
        The condition number $\kappa$ measures the <strong>anisotropy</strong> (direction-dependence) of the function's curvature.
        <ul>
            <li><strong>$\kappa \approx 1$:</strong> The function is shaped like a nice, round bowl (spherical level sets). The gradient points straight to the minimum.</li>
            <li><strong>$\kappa \gg 1$:</strong> The function is shaped like a narrow, deep valley (elongated ellipsoidal level sets). The gradient is nearly orthogonal to the direction of the minimum.</li>
        </ul>
        Geometrically, any sublevel set $C_\alpha = \{x : f(x) \le \alpha\}$ contains a ball of radius $r$ and is contained in a ball of radius $R$. The ratio $R^2/r^2$ is bounded by $\kappa = M/m$. Thus, $\sqrt{\kappa}$ represents the maximum "aspect ratio" or eccentricity of the level sets.
      </p>
      <div style="background: var(--code-bg); padding: 16px; border-radius: 8px; border-left: 4px solid var(--accent); margin-bottom: 16px;">
        <strong>Example: Width of an Ellipsoid.</strong> Let $\mathcal{E}=\{x\mid (x-x_0)^T A^{-1}(x-x_0)\le 1\}$ with $A \in S^n_{++}$.
        The "width" of the ellipsoid varies with direction.
        The condition number of the set is exactly the condition number of the matrix $A$:
        $$ \mathrm{cond}(\mathcal{E}) = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)} = \kappa(A) $$
        If $\kappa = 100$, the ellipsoid is 10 times longer in one direction than another ($\sqrt{100}=10$). Gradient descent will "bounce" back and forth across the narrow valley rather than moving down it.
      </div>

      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/kappa_effect_paths.gif" alt="Gradient descent zig-zag worsening with condition number" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;"><strong>The Condition Number Tax:</strong> As $\kappa = M/m$ grows (ellipses become thinner), gradient descent zig-zags significantly, slowing convergence linearly with $\kappa$.</figcaption>
      </figure>

      <h3>6. Descent Methods</h3>
      <p>
        Most unconstrained optimization algorithms follow a simple iterative structure. We produce a sequence of points $x^{(k)}, k=1,\dots$ such that $f(x^{(k)}) \to p^\star$.
        <br>
        <strong>General Algorithm:</strong>
        <pre><code class="language-python">
def descent_method(f, gradient, x0, tol=1e-6):
    x = x0
    while True:
        # 1. Compute Descent Direction
        delta_x = compute_direction(x)  # e.g., -gradient(x)

        # 2. Check Stopping Criterion
        if -gradient(x).dot(delta_x) / 2 <= tol:
            break

        # 3. Line Search (Step Size)
        t = line_search(f, x, delta_x)

        # 4. Update
        x = x + t * delta_x
    return x
        </code></pre>
        The search direction $\Delta x$ must be a <strong>descent direction</strong>: $\nabla f(x)^T \Delta x < 0$. This ensures that for small enough step sizes, the objective function decreases.
      </p>

      <h4>6.1 Exact Line Search: A Rigorous Proof of Linear Convergence</h4>
      <p>
        Suppose we choose step size $t$ to minimize $f$ exactly along the ray. How fast does the error decay?
        <br>
        <strong>Proof Step 1: Guaranteed Decrease.</strong>
        From the smoothness upper bound (9.13), taking step $t = 1/M$ along the gradient direction $g = \nabla f(x)$ gives:
        $$ f(x - \frac{1}{M}g) \le f(x) - \frac{1}{2M}\|g\|^2 $$
        Since exact line search finds the <em>optimal</em> $t$, it must do at least as well as this specific choice:
        $$ f(x^+) \le f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 $$
        <br>
        <strong>Proof Step 2: Relating Gradient to Optimality Gap.</strong>
        Recall strong convexity (9.9): $\|\nabla f(x)\|^2 \ge 2m(f(x) - p^\star)$. Substituting this into the inequality:
        $$ f(x^+) \le f(x) - \frac{1}{2M} \left( 2m(f(x) - p^\star) \right) = f(x) - \frac{m}{M}(f(x) - p^\star) $$
        <br>
        <strong>Proof Step 3: Subtracting $p^\star$.</strong>
        Subtract $p^\star$ from both sides to see the evolution of the error $\epsilon_k = f(x^{(k)}) - p^\star$:
        $$ f(x^+) - p^\star \le (f(x) - p^\star) - \frac{m}{M}(f(x) - p^\star) $$
        $$ \epsilon_{k+1} \le \left(1 - \frac{m}{M}\right) \epsilon_k $$
        This is <strong>Linear Convergence</strong>. The error contracts by a fixed factor $c = 1 - m/M < 1$ at every step.
        (Note: $m/M = 1/\kappa$, so the rate depends on the condition number. If $\kappa$ is large, $c \approx 1$, and convergence is slow).
      </p>

      <h4>6.2 Backtracking (Armijo) Line Search</h4>
      <p>
        Exact minimization is usually too expensive. Instead, we use <strong>Backtracking Line Search</strong>. It guarantees a "sufficient decrease" without finding the exact minimum.
        <br>
        <strong>The Algorithm:</strong>
        <pre><code class="language-python">
def backtracking_line_search(f, x, delta_x, gradient, alpha=0.3, beta=0.8):
    """
    alpha: Fraction of decrease we demand (0 < alpha < 0.5)
           Usually 0.01 to 0.3.
    beta:  Shrink factor (0 < beta < 1).
           Usually 0.1 to 0.8.
    """
    t = 1.0  # Start with full step
    current_f = f(x)
    linear_term = alpha * gradient.dot(delta_x)

    # While function is above the "Armijo Line"
    while f(x + t * delta_x) > current_f + t * linear_term:
        t = t * beta  # Reduce step size

    return t
        </code></pre>
        <strong>Geometric Interpretation:</strong>
        The condition $f(x+t\Delta x) \le f(x) + \alpha t \nabla f^T \Delta x$ forces the function value to lie below a linear ray starting at $f(x)$ with slope $\alpha \nabla f^T \Delta x$.
        Since the actual slope of the function at $t=0$ is $\nabla f^T \Delta x$ (which is steeper/more negative than $\alpha \nabla f^T \Delta x$ because $\alpha < 1$), the condition <strong>must</strong> hold for sufficiently small $t$. The loop is guaranteed to terminate.
      </p>
      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/backtracking_armijo.gif" alt="Backtracking line search visualization" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Backtracking reduces step size $t$ until the function value drops below the linear extrapolation (Armijo line).</figcaption>
      </figure>

      <h3>7. Steepest Descent</h3>
      <p>
        Standard gradient descent follows the negative gradient. But why? This is the direction of steepest descent <strong>only if distance is measured in the Euclidean norm ($\ell_2$)</strong>.
        <br>
        <strong>Definition:</strong> The normalized steepest descent direction $\Delta x_{\mathrm{nsd}}$ solves:
        $$ \Delta x_{\mathrm{nsd}} = \text{argmin}_{v} \{ \nabla f(x)^T v \mid \|v\| \le 1 \} $$
        It is the direction that yields the most negative directional derivative per unit of "length."
        <br>
        <strong>Dual Norm Derivation:</strong>
        By the definition of the dual norm $\|z\|_* = \sup \{z^T x \mid \|x\| \le 1\}$, the optimal value of the minimization problem above is simply $-\|\nabla f(x)\|_*$.
      </p>

      <h4>7.1 Unnormalized Step and Cases</h4>
      <p>
        For convenience, we define the <strong>unnormalized</strong> steepest descent step as:
        $$ \Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_* \Delta x_{\mathrm{nsd}} $$
        This scaling ensures that the directional derivative is:
        $$ \nabla f(x)^T \Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_* (\nabla f(x)^T \Delta x_{\mathrm{nsd}}) = \|\nabla f(x)\|_* (-\|\nabla f(x)\|_* ) = -\|\nabla f(x)\|_*^2 $$
      </p>

      <div class="card-grid" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 20px;">

        <!-- Case 1: L2 Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">1. Euclidean Norm ($\ell_2$)</h4>
            <p><strong>Norm:</strong> $\|v\|_2 = \sqrt{v^T v}$. <strong>Dual:</strong> $\|g\|_2$.</p>
            <p><strong>Geometry:</strong> The unit ball is a sphere.</p>
            <p><strong>Direction:</strong> $\Delta x_{\mathrm{sd}} = -\nabla f(x)$.</p>
            <p><strong>Result:</strong> Standard Gradient Descent. The gradient is perpendicular to the tangent plane of the level set.</p>
        </div>

        <!-- Case 2: Quadratic Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">2. Quadratic Norm (Preconditioning)</h4>
            <p><strong>Norm:</strong> $\|v\|_P = \sqrt{v^T P v}$ ($P \in S^n_{++}$). <strong>Dual:</strong> $\|g\|_{P^{-1}}$.</p>
            <p><strong>Geometry:</strong> The unit ball is an ellipsoid $\{v \mid v^T P v \le 1\}$.</p>
            <p><strong>Direction:</strong> $\Delta x_{\mathrm{sd}} = -P^{-1}\nabla f(x)$.</p>
            <p><strong>Result:</strong> Preconditioned Gradient Descent. By choosing $P \approx \nabla^2 f(x)$, we align the "metric" with the function's curvature, turning ellipses into circles.</p>
            <figure style="margin: 12px 0; text-align: center;">
                <img src="assets/preconditioning_comparison.gif" alt="Preconditioned GD vs GD" style="width: 100%; border-radius: 4px;">
            </figure>
        </div>

        <!-- Case 3: L1 Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">3. $\ell_1$ Norm (Manhattan)</h4>
            <p><strong>Norm:</strong> $\|v\|_1 = \sum |v_i|$. <strong>Dual:</strong> $\|g\|_\infty = \max_i |g_i|$.</p>
            <p><strong>Geometry:</strong> The unit ball is a diamond (polytope).</p>
            <p><strong>Direction:</strong> Let $i$ be the index where $|(\nabla f)_i|$ is maximal.
            $$ \Delta x_{\mathrm{sd}} = -\text{sign}\left( \frac{\partial f}{\partial x_i} \right) e_i $$
            </p>
            <p><strong>Result:</strong> Coordinate Descent (Greedy). We only update the single variable with the largest partial derivative.</p>
            <figure style="margin: 12px 0; text-align: center;">
                <img src="assets/l1_coordinate_descent_vs_gd.gif" alt="L1 Coordinate Descent vs GD" style="width: 100%; border-radius: 4px;">
            </figure>
        </div>
      </div>

      <h3>8. Newton's Method</h3>
      <p>
        Newton's method is the gold standard for unconstrained minimization. It uses curvature information (the Hessian) to take much smarter steps than gradient descent.
        $$ \Delta x_{\mathrm{nt}} = -\nabla^2 f(x)^{-1} \nabla f(x) $$
      </p>
      <h4>8.1 Three Interpretations</h4>
      <ol>
          <li><strong>Minimizing the Quadratic Model:</strong>
          We approximate $f$ near $x$ by its second-order Taylor expansion:
          $$ \hat{f}(x+v) = f(x) + \nabla f(x)^T v + \frac{1}{2}v^T \nabla^2 f(x) v $$
          To find the best step $v$, we minimize this quadratic. Setting the derivative with respect to $v$ to zero:
          $$ \nabla f(x) + \nabla^2 f(x) v = 0 \implies v = -\nabla^2 f(x)^{-1} \nabla f(x) $$
          <figure style="margin: 12px 0; text-align: center;">
            <img src="assets/newton_quadratic_model.gif" alt="Newton step minimizes local quadratic model" style="width: 100%; max-width: 500px; border-radius: 8px; border: 1px solid #ddd;">
            <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Newton's method jumps directly to the minimum of the local quadratic approximation.</figcaption>
          </figure>
          </li>
          <li><strong>Steepest Descent in Hessian Norm:</strong>
          It is exactly steepest descent with respect to the norm $\|v\|_H = \sqrt{v^T \nabla^2 f(x) v}$.
          This means Newton's method is "Geometry-Adaptive." It resizes the unit ball at every step to match the local curvature of the function.
          </li>
          <li><strong>Affine Invariance:</strong>
          A crucial property. If we change coordinates $x = Ty$ (where $T$ is invertible), the algorithm produces the exact same sequence of points (mapped through $T$).
          <br>
          <strong>Proof of Affine Invariance:</strong>
          Let $g(y) = f(Ty)$. Then:
          $$ \nabla g(y) = T^T \nabla f(x), \quad \nabla^2 g(y) = T^T \nabla^2 f(x) T $$
          The Newton step for $g$ at $y$ is:
          $$ \Delta y_{\mathrm{nt}} = -(\nabla^2 g(y))^{-1} \nabla g(y) = -(T^T \nabla^2 f(x) T)^{-1} (T^T \nabla f(x)) $$
          $$ = -T^{-1} (\nabla^2 f(x))^{-1} (T^{-T} T^T) \nabla f(x) = -T^{-1} (\nabla^2 f(x))^{-1} \nabla f(x) = T^{-1} \Delta x_{\mathrm{nt}} $$
          Since $\Delta x = T \Delta y$, the steps correspond perfectly. Gradient descent <strong>does not</strong> have this property.
          </li>
      </ol>

      <h4>8.2 The Newton Decrement $\lambda(x)$</h4>
      <p>
        The quantity $\lambda(x) = (\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x))^{1/2}$ is called the <strong>Newton Decrement</strong>.
        It plays a dual role:
        <ul>
            <li><strong>Stopping Criterion:</strong> It is an affine-invariant measure of the proximity to optimality. $\frac{1}{2}\lambda(x)^2$ is exactly the difference between $f(x)$ and the minimum of the local quadratic model. Stopping when $\lambda(x)^2/2 \le \epsilon$ is a rigorous standard.</li>
            <li><strong>Directional Derivative:</strong> The slope in the Newton direction is $-\lambda(x)^2$.
            $$ \nabla f(x)^T \Delta x_{\mathrm{nt}} = -\nabla f(x)^T (\nabla^2 f)^{-1} \nabla f(x) = -\lambda(x)^2 $$
            </li>
        </ul>
      </p>

      <h4>8.3 Implementation</h4>
      <pre><code class="language-python">
def newton_step(f, grad, hess, x):
    g = grad(x)
    H = hess(x)

    # Solve linear system H * dx = -g
    # More stable than inv(H)
    delta_x = np.linalg.solve(H, -g)

    # Compute Newton Decrement
    lambda_sq = g.dot(np.linalg.solve(H, g)) # g^T H^-1 g

    return delta_x, lambda_sq
      </code></pre>

      <h3>9. Convergence Analysis & Self-Concordance</h3>
      <p>
        Newton's method has a unique two-phase convergence behavior.
      </p>
      <h4>9.1 The Two Phases</h4>
      <ul>
          <li><strong>Damped Phase (Far from $x^\star$):</strong>
          When gradient is large, we must use a step size $t < 1$ (via backtracking). The function value decreases by a constant amount at each step. This phase is linear, but transient.
          </li>
          <li><strong>Pure Newton Phase (Close to $x^\star$):</strong>
          Once we get close enough (specifically, when $\lambda(x) < 0.68$), the backtracking search will always accept $t=1$.
          Convergence becomes <strong>Quadratic</strong>. The number of correct digits doubles at every iteration.
          $$ \frac{\epsilon_{k+1}}{\epsilon_k^2} \le C \quad \implies \quad \epsilon_{k+1} \approx \epsilon_k^2 $$
          In practice, this means you go from 1 digit of accuracy to 2, then 4, then 8, then 16 (machine precision) in just 4-5 steps.
          </li>
      </ul>

      <h4>9.2 Self-Concordance: Theoretical Rigor</h4>
      <p>
        Classical convergence proofs rely on unknown constants $m, M, L$. <strong>Self-Concordance</strong> is a scale-invariant property that limits how fast the Hessian changes:
        $$ |f'''(x)[h,h,h]| \le 2 (f''(x)[h,h])^{3/2} $$
        Functions like $-\log(x)$ and linear/quadratic functions satisfy this.
        <br>
        <strong>Why it matters:</strong> It allows us to prove complexity bounds that depend <strong>only</strong> on the problem structure (e.g., number of constraints) and not on the data values. It is the theoretical foundation of Interior Point Methods.
      </p>

      <h4>9.3 Quasi-Newton Methods (BFGS)</h4>
      <p>
        If $N$ is large ($> 1000$), computing $\nabla^2 f^{-1}$ ($O(N^3)$) is too slow.
        <strong>Quasi-Newton methods</strong> update an approximation of the Hessian $B \approx \nabla^2 f$ using the change in gradients ($\Delta g = B \Delta x$).
        <br>
        <strong>BFGS Update:</strong> A rank-2 update rule that preserves symmetry and positive definiteness. It converges <strong>superlinearly</strong> (faster than gradient descent, but no quadratic doubling).
      </p>

    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Gradient Descent Visualizer</h3>
        <p>Visualize the trajectory of gradient descent for a 2D quadratic function.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- Widget 4: Norm Steepest Descent -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Steepest Descent Geometry</h3>
        <p>Visualizing how the norm determines the "steepest" direction. L2 steepest opposes the gradient. L1 steepest snaps to an axis (coordinate descent).</p>
        <div id="widget-norm-steepest" style="width: 100%; height: auto; position: relative; text-align: center;"></div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">GD vs. Newton Race</h3>
        <p>Compare the convergence of gradient descent and Newton's method.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- Widget 3 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Convergence Rate Comparison</h3>
        <p>Plots the convergence rates of different first-order methods.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Solved Exercises & Example Problems</h2>

      <div class="example-problem">
        <h3>Exercise 9.1: Minimizing a Quadratic Function (Detailed)</h3>
        <p><strong>Problem:</strong> Consider minimizing $f(x)=\frac{1}{2}x^T P x+q^T x+r$ where $P \in S^n$.
        <br>(a) Show that if $P \not\succeq 0$, the problem is unbounded below.
        <br>(b) Suppose $P \succeq 0$ but $Px = -q$ has no solution. Show it is unbounded below.</p>

        <p><strong>Step-by-Step Solution:</strong></p>
        <p><strong>Part (a): $P \not\succeq 0$.</strong>
        <br>1. <strong>Definition:</strong> If $P$ is not positive semidefinite, there exists at least one eigenvector $v$ with a negative eigenvalue $\lambda < 0$. Thus, $v^T P v = \lambda \|v\|^2 < 0$.
        <br>2. <strong>Construct Ray:</strong> Consider the path $x(t) = tv$ for $t > 0$.
        <br>3. <strong>Evaluate Function:</strong>
        $$ f(tv) = \frac{1}{2}t^2 (v^T P v) + t(q^T v) + r $$
        <br>4. <strong>Limit:</strong> As $t \to \infty$, the term $t^2 (v^T P v)$ dominates because it grows quadratically. Since its coefficient is negative, $f(tv) \to -\infty$.
        </p>

        <p><strong>Part (b): $P \succeq 0$ but $q \notin \mathcal{R}(P)$.</strong>
        <br>1. <strong>Linear Algebra Fact:</strong> Since $P$ is symmetric, the nullspace $\mathcal{N}(P)$ is orthogonal to the range $\mathcal{R}(P)$. Any vector $q$ can be decomposed into $q_{\text{range}} + q_{\text{null}}$. If $q \notin \mathcal{R}(P)$, then $q_{\text{null}} \neq 0$.
        <br>2. <strong>Pick Direction:</strong> Let $z = -q_{\text{null}}$. Note that $Pz = 0$ (by definition of nullspace).
        <br>3. <strong>Evaluate Function along $x(t) = tz$:</strong>
        $$ f(tz) = \frac{1}{2}t^2 (z^T P z) + t(q^T z) + r = 0 + t(q^T z) + r $$
        <br>4. <strong>Analyze Slope:</strong> $q^T z = (q_{\text{range}} + q_{\text{null}})^T (-q_{\text{null}}) = -\|q_{\text{null}}\|^2 < 0$.
        <br>5. <strong>Limit:</strong> As $t \to \infty$, $f(tz) \to -\infty$ linearly.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.5: Backtracking Line Search Proof (Rigorous)</h3>
        <p><strong>Problem:</strong> Prove that if $0 < t \le -\frac{\nabla f(x)^T \Delta x}{M\|\Delta x\|_2^2}$, the backtracking condition $f(x+t\Delta x) \le f(x) + \alpha t \nabla f^T \Delta x$ holds for $\alpha \le 0.5$.</p>
        <p><strong>Proof:</strong></p>
        <ol>
            <li><strong>Smoothness Upper Bound:</strong> Start with the M-smoothness bound (9.13) along direction $\Delta x$:
            $$ f(x+t\Delta x) \le f(x) + t\nabla f^T \Delta x + \frac{M}{2}t^2\|\Delta x\|^2 $$
            </li>
            <li><strong>Substitute $t$ bound:</strong> We are given $t \le \frac{-\nabla f^T \Delta x}{M\|\Delta x\|^2}$. Multiply by $\frac{M}{2}t\|\Delta x\|^2$:
            $$ \frac{M}{2}t^2\|\Delta x\|^2 \le \frac{1}{2}t (-\nabla f^T \Delta x) $$
            </li>
            <li><strong>Plug into Upper Bound:</strong>
            $$ f(x+t\Delta x) \le f(x) + t\nabla f^T \Delta x - \frac{1}{2}t \nabla f^T \Delta x = f(x) + \frac{1}{2}t\nabla f^T \Delta x $$
            </li>
            <li><strong>Compare to Armijo:</strong> The condition requires slope $\alpha$. Since we proved it holds for slope $0.5$, and $\nabla f^T \Delta x < 0$ (descent), the condition holds for any $\alpha \le 0.5$ (since a smaller $\alpha$ makes the required drop <em>smaller</em>/easier).
            </li>
        </ol>
      </div>

      <div class="example-problem">
        <h3>Example 13.2: Newton's Method for Logistic Regression</h3>
        <p><strong>Problem:</strong> Derive the Newton step for Logistic Regression explicitly.</p>
        <p><strong>Solution:</strong></p>
        <p><strong>Objective:</strong> $f(w) = \sum_{i=1}^m \log(1 + \exp(w^T x_i)) - y_i w^T x_i$.
        <br>Let $z_i = w^T x_i$. The sigmoid function is $p_i(w) = \sigma(z_i) = \frac{1}{1+e^{-z_i}}$.
        <br><strong>Gradient:</strong>
        $$ \frac{\partial f}{\partial w} = \sum_{i=1}^m (p_i - y_i) x_i = X^T (p - y) $$
        <br><strong>Hessian:</strong>
        $$ \frac{\partial^2 f}{\partial w^2} = \sum_{i=1}^m \frac{\partial p_i}{\partial z_i} x_i x_i^T = \sum_{i=1}^m p_i(1-p_i) x_i x_i^T = X^T D X $$
        where $D$ is diagonal with $D_{ii} = p_i(1-p_i)$.
        <br><strong>Newton Step:</strong>
        $$ \Delta w = -(X^T D X)^{-1} X^T (p - y) $$
        This formula is the basis for the <strong>Iteratively Reweighted Least Squares (IRLS)</strong> algorithm used by standard statistical packages.
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.3: Steepest Descent in $\ell_\infty$ Norm</h3>
        <p><strong>Problem:</strong> Explicitly derive the steepest descent direction for the $\ell_\infty$ norm.</p>
        <p><strong>Solution:</strong></p>
        <p>We solve $\min \nabla f^T v$ s.t. $\|v\|_\infty \le 1$.
        The objective is $\sum_{i=1}^n (\nabla f)_i v_i$.
        Since the variables $v_i$ are constrained independently ($|v_i| \le 1$), we can minimize the sum by minimizing each term separately.
        To minimize $c_i v_i$ subject to $v_i \in [-1, 1]$:
        <ul>
            <li>If $c_i > 0$, set $v_i = -1$.</li>
            <li>If $c_i < 0$, set $v_i = +1$.</li>
            <li>If $c_i = 0$, $v_i$ is arbitrary (usually 0).</li>
        </ul>
        Thus, $v_i = -\text{sign}((\nabla f)_i)$.
        <strong>Geometric Insight:</strong> In the $\ell_\infty$ "box" world, the fastest way down is to go to a corner of the hypercube.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.15: Proving Self-Concordance</h3>
        <p><strong>Problem:</strong> Prove $f(x,y) = -\log(y^2 - x^T x)$ is self-concordant.</p>
        <p><strong>Solution Strategy:</strong></p>
        <p>We use the <strong>barrier method</strong> logic.
        1. <strong>Restrict to a line:</strong> $x(t) = \hat{x} + tv$, $y(t) = \hat{y} + tw$.
        2. <strong>Composition with Log:</strong> Use the property that $-\log(-g(t)) - \log t$ is self-concordant if $|g'''| \le 3g''/t$.
        3. <strong>Algebra:</strong> Factor $y^2 - x^T x$ into geometric components.
        For $f(x,y)$, restricting to a line yields a function of the form $-\log(\text{quadratic})$.
        Since $-\log(y)$ is SC and affine transformations preserve SC, the result holds.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.30: Analytic Center (Gradient vs Newton)</h3>
        <p><strong>Problem:</strong> Compute the analytic center of $a_i^T x \le 1, |x_i| \le 1$ using Gradient Descent and Newton's Method. Compare convergence.</p>
        <p><strong>Objective:</strong> $f(x) = -\sum_{i=1}^m \log(1-a_i^T x) - \sum_{j=1}^n \log(1-x_j^2)$.</p>

        <p><strong>Python Implementation:</strong></p>
<pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

def solve_analytic_center(m=200, n=50):
    # Generate data
    np.random.seed(1)
    A = np.random.randn(m, n) * 0.5  # Scale for feasibility at 0

    # Objective and Derivatives
    def f_val(x):
        s = 1.0 - A @ x
        t = 1.0 - x**2
        if np.any(s <= 0) or np.any(t <= 0): return np.inf
        return -np.sum(np.log(s)) - np.sum(np.log(t))

    def grad_hess(x):
        s = 1.0 - A @ x
        t = 1.0 - x**2
        # Gradient
        g = A.T @ (1/s) + 2*x/t
        # Hessian
        w = 1/(s**2)
        H = (A.T * w) @ A + np.diag(2*(1+x**2)/(t**2))
        return g, H

    # Backtracking Line Search
    def line_search(x, dx, g, alpha=0.1, beta=0.5):
        t = 1.0
        fx = f_val(x)
        g_dot_dx = g @ dx
        while f_val(x + t*dx) > fx + alpha * t * g_dot_dx:
            t *= beta
        return t

    # Newton's Method Loop
    x = np.zeros(n)
    history = []
    for iter in range(50):
        fx = f_val(x)
        g, H = grad_hess(x)
        lambda_sq = g @ np.linalg.solve(H, g) # Newton decrement
        history.append(lambda_sq)

        if lambda_sq < 1e-10: break

        dx = np.linalg.solve(H, -g)
        t = line_search(x, dx, g, alpha=0.1)
        x += t * dx

    return history

# Run and visualize
hist = solve_analytic_center()
# In a real plot: plt.semilogy(hist); plt.show()
</code></pre>
        <p><strong>Observation:</strong> Gradient descent will take thousands of iterations (linear convergence). Newton's method typically converges in 10-20 iterations (quadratic convergence), often accepting step size $t=1$ in the final phase.</p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.31: Approximate Newton Methods</h3>
        <p><strong>Problem:</strong> Compare full Newton with (a) Re-using Hessian every $N$ steps, and (b) Diagonal approximation.</p>
        <p><strong>Analysis:</strong></p>
        <ul>
          <li><strong>Reuse Hessian:</strong> Saves $O(n^3)$ factorization cost but uses "stale" curvature. Convergence becomes linear or superlinear but not quadratic. Good if Hessian evaluation is very expensive.</li>
          <li><strong>Diagonal Approximation:</strong> Only uses $H_{ii}$. Cost is $O(n)$. Becomes a scaled gradient descent. Effective if variables are loosely coupled (matrix $A$ is sparse or near-diagonal), but poor for highly correlated data.</li>
        </ul>
        <p><strong>Code Snippet (Diagonal Newton):</strong></p>
<pre><code class="language-python">
# Diagonal Hessian Approximation
H_diag = np.sum((A**2).T * (1/s**2), axis=1) + 2*(1+x**2)/(t**2)
dx = -g / H_diag  # Element-wise division
</code></pre>
      </div>

      <div class="example-problem">
        <h3>Example 13.5: BFGS Update Rank</h3>
        <p><strong>Problem:</strong> Show that the BFGS update is a rank-2 update.</p>
        <p><strong>Solution:</strong></p>
        <p>Update: $B_{k+1} = B_k + \frac{y y^\top}{y^\top s} - \frac{B s s^\top B}{s^\top B s}$.
        This adds two rank-1 matrices.
        It preserves positive definiteness if $y^\top s > 0$ (curvature condition).
        Avoids $O(n^3)$ cost of inverting Hessian.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.6: Convergence Rate Comparison</h3>
        <p><strong>Problem:</strong> Compare GD and Newton on $f(x) = x^4$.</p>
        <p><strong>Solution:</strong></p>
        <p>GD: $x_{k+1} = x_k - t 4 x_k^3$. For convergence, need small $t$. Rate is sublinear (slow).
        Newton: $\Delta x = -f'/f'' = -4x^3 / 12x^2 = -x/3$.
        $x_{k+1} = x_k - x_k/3 = (2/3)x_k$.
        Linear convergence with rate $2/3$.
        Note: $x^4$ is strictly convex but not strongly convex at 0 ($f''(0)=0$). Newton is not quadratic here!</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.7: Nesterov Acceleration</h3>
        <p><strong>Problem:</strong> Write the update equations for Nesterov's accelerated gradient.</p>
        <p><strong>Solution:</strong></p>
        <p>$x_k$: current position, $y_k$: momentum variable.
        1. $x_{k+1} = y_k - t \nabla f(y_k)$
        2. $y_{k+1} = x_{k+1} + \frac{k-1}{k+2} (x_{k+1} - x_k)$
        Step 1 is standard GD step from the lookahead position $y_k$.
        Step 2 adds momentum.
        Converges as $1/k^2$ for smooth convex functions.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.8: Newton Decrement Property</h3>
        <p><strong>Problem:</strong> Show $\lambda(x)^2 = \nabla f(x)^\top \Delta x_{nt}$.</p>
        <p><strong>Solution:</strong></p>
        <p>$\Delta x_{nt} = -H^{-1} g$.
        $\lambda(x)^2 = g^\top H^{-1} g$.
        Inner product: $g^\top \Delta x_{nt} = g^\top (-H^{-1} g) = -g^\top H^{-1} g = -\lambda(x)^2$.
        So $\lambda^2 = -g^\top \Delta x_{nt}$ (directional derivative in Newton direction is $-\lambda^2$).</p>
      </div>

    </section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 9 — Unconstrained Minimization</li>
        <li><strong>Detailed Lecture Notes:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">PDF (Chapter 9)</a></li>
      </ul>
    </section>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initGradientDescentVisualizer } from './widgets/js/gradient-descent-visualizer.js';
    initGradientDescentVisualizer('widget-1');
  </script>
  <script type="module">
    import { initGDvsNewton } from './widgets/js/gd-vs-newton.js';
    initGDvsNewton('widget-2');
  </script>
  <script type="module">
    import { initConvergenceRate } from './widgets/js/convergence-rate.js';
    initConvergenceRate('widget-3');
  </script>
  <script type="module">
    import { initNormSteepest } from './widgets/js/norm-steepest.js';
    initNormSteepest('widget-norm-steepest');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
