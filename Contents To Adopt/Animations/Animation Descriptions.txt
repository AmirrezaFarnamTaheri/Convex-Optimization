15_projection_animation
Animation GIF: moving 
ğ‘£
v and projecting onto span
{ğ‘¢}
{u} (residual stays orthogonal)


05_linear_combinations_lattice
Linear combinations 
ğ‘ğ‘£+ğ‘ğ‘¤
av+bw shown as an integer-combo lattice


2ï¸âƒ£ 2.1 Dot product = projection (shadow length)


What you see

Black arrow: fixed direction 
ğ‘¢
u

Blue arrow: moving vector 
ğ‘¥
x

Red dashed segment + red dot: projection of 
ğ‘¥
x onto 
ğ‘¢
u

The number displayed is 
ğ‘¥
âŠ¤
ğ‘¢
x
âŠ¤
u.

Concept it locks in
ğ‘¥
âŠ¤
ğ‘¢
=
âˆ¥
ğ‘¢
âˆ¥
Ã—
(
signed length of the shadow of 
ğ‘¥
 on 
ğ‘¢
)
x
âŠ¤
u=âˆ¥uâˆ¥Ã—(signed length of the shadow of x on u)

This animation eliminates:

â€œDot product is coordinate multiplicationâ€

â€œDot product is mysterious algebraâ€

After this, Cauchyâ€“Schwarz is inevitable, not magical.



2ï¸âƒ£ 2.3 Orthogonality and projection decomposition

Download: layer2_23_orthogonality_projection.gif

What you see

Black arrow: fixed 
ğ‘¢
u

Blue arrow: 
ğ‘£
v

Red arrow: 
proj
ğ‘¢
(
ğ‘£
)
proj
u
	â€‹

(v)

Green arrow: residual 
ğ‘£
âˆ’
proj
ğ‘¢
(
ğ‘£
)
vâˆ’proj
u
	â€‹

(v)

The displayed number is:

âŸ¨
residual
,
ğ‘¢
âŸ©
â‰ˆ
0
âŸ¨residual,uâŸ©â‰ˆ0
Concept it locks in
ğ‘£
=
proj
ğ‘¢
(
ğ‘£
)
+
orthogonal residual
v=proj
u
	â€‹

(v)+orthogonal residual

This is the core geometric fact behind:

Least squares

Normal equations

Gramâ€“Schmidt

Orthogonal decompositions

If this animation is understood, least squares later feels obvious.



3ï¸âƒ£ 3.1 Vector space axioms are universal (same rules, different worlds)

Download: layer3_31_universality.gif

What you see

Three panels evolving in sync:

â„Â² vectors

Polynomials 
ğ‘
+
ğ‘
ğ‘¥
a+bx

Functions 
ğ‘
sin
â¡
ğ‘¡
+
ğ‘
cos
â¡
ğ‘¡
asint+bcost

The same coefficients 
ğ‘
(
ğ‘¡
)
,
ğ‘
(
ğ‘¡
)
a(t),b(t) drive all three simultaneously.

Concept it locks in

A vector space is not defined by geometry or arrows.

It is defined by:

addition

scalar multiplication

axioms

Once the axioms hold, everything you prove applies to all of them.

This animation prevents the classic failure mode:

â€œLinear algebra is about arrows in â„Â².â€



3ï¸âƒ£ 3.2 Subspace vs affine set (origin matters)

Download: layer3_32_subspace_affine.gif

What you see

Blue arrow: vector living in a subspace (line through the origin)

Red arrow: same vector added to a fixed offset â†’ affine set

Both move identically, but one always passes through 0 and the other never does.

Concept it locks in

Subspace = closed world containing 0

Affine set = translate of a subspace

This animation permanently clarifies:

Why 
ğ´
ğ‘¥
=
0
Ax=0 is a subspace

Why 
ğ´
ğ‘¥
=
ğ‘
Ax=b is not (unless 
ğ‘
=
0
b=0)

This is foundational for:

Solution sets

Least squares

Optimization geometry


3ï¸âƒ£ 3.3 Span and generating sets (redundancy revealed)

Download: layer3_33_span_generators.gif

What you see

ğ‘£
1
,
ğ‘£
2
v
1
	â€‹

,v
2
	â€‹

: dashed arrows

ğ‘£
3
v
3
	â€‹

: dotted arrow (redundant)

Red arrow: all linear combinations 
ğ›¼
ğ‘£
1
+
ğ›½
ğ‘£
2
Î±v
1
	â€‹

+Î²v
2
	â€‹


The red vector reaches everything 
ğ‘£
3
v
3
	â€‹

 ever could.

Concept it locks in

Span = set of all linear combinations

Redundancy = vectors that add no new reach

Basis = minimal spanning set

After this animation:

â€œLinear independenceâ€ becomes intuitive

Gaussian elimination later feels like systematic redundancy removal


4ï¸âƒ£ 4.3 Dimension = degrees of freedom

Download: layer4_43_dimension.gif

What you see

Two panels:

Left: line (1 degree of freedom)

Right: plane (2 degrees of freedom)

Independent â€œslidersâ€ drive reachability:

One slider â†’ line

Two sliders â†’ plane

Concept it locks in
dimension
=
number of independent parameters needed
dimension=number of independent parameters needed

This is the conceptual backbone behind:

Rank

Nullity

Degrees-of-freedom counting

Rankâ€“nullity theorem




Least squares via QR (single-column case 
ğ´
=
ğ‘
ğ‘Ÿ
A=qr)
Download: least_squares_via_qr.gif
Shows the stable computation: 
ğ‘¦
=
ğ‘
âŠ¤
ğ‘
y=q
âŠ¤
b, then 
ğ‘¥
\*
=
ğ‘¦
/
ğ‘Ÿ
x
\*
=y/r, and 
ğ´
ğ‘¥
\*
Ax
\*
 is the same projection result.

Next layer is SVD / â€œtrue geometryâ€ of any matrix (and then PCA falls out as a one-liner).




Least squares = projection of 
ğ‘
b onto 
C
o
l
(
ğ´
)
Col(A)
Download: least_squares_projection.gif
Shows 
ğ‘
b moving, its projection 
ğ‘
=
p
r
o
j
C
o
l
(
ğ´
)
(
ğ‘
)
p=proj
Col(A)
	â€‹

(b), and 
ğ´
ğ‘¥
\*
=
ğ‘
Ax
\*
=p. The residual length 
âˆ¥
ğ‘
âˆ’
ğ´
ğ‘¥
\*
âˆ¥
âˆ¥bâˆ’Ax
\*
âˆ¥ updates live.




.

ğŸ¬ Linear map = structure-preserving deformation

Download: linear_map_grid.gif

How to watch

Start with a square grid (pure structure).

The grid smoothly deforms under a linear map.

Straight lines remain straight.

Parallel lines remain parallel.

The origin never moves.

What this locks in

A linear map is not about coordinates or formulas.
It is exactly:

A transformation that preserves linear structure.

Everything later â€” eigenvectors, rank, SVD â€” is a refinement of this picture.




2ï¸âƒ£ 2.3 Orthogonality and projection decomposition

Download: layer2_23_orthogonality_projection.gif

What you see

Black arrow: fixed 
ğ‘¢
u

Blue arrow: 
ğ‘£
v

Red arrow: 
proj
ğ‘¢
(
ğ‘£
)
proj
u
	â€‹

(v)

Green arrow: residual 
ğ‘£
âˆ’
proj
ğ‘¢
(
ğ‘£
)
vâˆ’proj
u
	â€‹

(v)

The displayed number is:

âŸ¨
residual
,
ğ‘¢
âŸ©
â‰ˆ
0
âŸ¨residual,uâŸ©â‰ˆ0
Concept it locks in
ğ‘£
=
proj
ğ‘¢
(
ğ‘£
)
+
orthogonal residual
v=proj
u
	â€‹

(v)+orthogonal residual

This is the core geometric fact behind:

Least squares

Normal equations

Gramâ€“Schmidt

Orthogonal decompositions

If this animation is understood, least squares later feels obvious.




Orthonormal basis: coordinates are dot products
Download: orthonormal_coordinates.gif
Shows 
ğ‘£
=
(
ğ‘
1
âŠ¤
ğ‘£
)
ğ‘
1
+
(
ğ‘
2
âŠ¤
ğ‘£
)
ğ‘
2
v=(q
1
âŠ¤
	â€‹

v)q
1
	â€‹

+(q
2
âŠ¤
	â€‹

v)q
2
	â€‹

. When the basis is orthonormal, the coefficients are literally the inner products.




Row operations move the codomain picture but preserve the domain invariants
Download: row_ops_four_spaces.gif

What it shows:

We apply a changing invertible row operation 
ğ´
â€²
=
ğ¸
ğ´
A
â€²
=EA.

Domain: Row(A) and Null(A) stay put (same subspaces).

Codomain: Col(EA) moves (shears), and LeftNull(EA) moves with it (always the orthogonal complement).

A sample output vector always lies on the moving column-space line.

If you want the â€œLayer 8 finaleâ€ as a single unified animation (four spaces + rank/nullity bars updating live + a moving 
ğ‘
b with solvable/unsolvable indication), I can fuse these into one tight dashboard-style GIF.



Row reduction preserves the solution (same intersection)
Download: row_reduction_preserves_solution.gif

What it shows: two equations as lines; a row operation morphs equation 2 while the intersection point stays fixed. This is the geometric reason elimination doesnâ€™t change the solution set.




2) Supporting hyperplanes sliding over â„“â‚ / â„“âˆ in three panels (â„“â‚ only, â„“âˆ only, overlay)

Download: supporting_hyperplanes_l1_linf_three_panels.gif

How to watch it:

Panel 1 (diamond): supporting line is

ğ‘¢
âŠ¤
ğ‘¥
=
sup
â¡
âˆ¥
ğ‘¥
âˆ¥
1
â‰¤
1
ğ‘¢
âŠ¤
ğ‘¥
=
âˆ¥
ğ‘¢
âˆ¥
âˆ
.
u
âŠ¤
x=
âˆ¥xâˆ¥
1
	â€‹

â‰¤1
sup
	â€‹

u
âŠ¤
x=âˆ¥uâˆ¥
âˆ
	â€‹

.

Panel 2 (square): supporting line is

ğ‘¢
âŠ¤
ğ‘¥
=
sup
â¡
âˆ¥
ğ‘¥
âˆ¥
âˆ
â‰¤
1
ğ‘¢
âŠ¤
ğ‘¥
=
âˆ¥
ğ‘¢
âˆ¥
1
.
u
âŠ¤
x=
âˆ¥xâˆ¥
âˆ
	â€‹

â‰¤1
sup
	â€‹

u
âŠ¤
x=âˆ¥uâˆ¥
1
	â€‹

.

Panel 3 overlays both: you literally see two different â€œshadow lengthsâ€ for the same direction 
ğ‘¢
u, depending on whether the ball is diamond or square.

If you want the hyperplanes to be shown as halfspaces (shaded feasible side) instead of just the boundary line, I can generate that version too (it makes the â€œintersection = robustified setâ€ even more visually obvious).



Trace as â€œsum of eigenvaluesâ€ (with constant trace while eigenvalues trade off)
Download: trace_sum_eigenvalues_constant.gif
Youâ€™ll see a circle mapped to an ellipse while 
ğœ†
1
Î»
1
	â€‹

 increases and 
ğœ†
2
Î»
2
	â€‹

 decreases so that 
t
r
(
ğ´
)
=
ğœ†
1
+
ğœ†
2
tr(A)=Î»
1
	â€‹

+Î»
2
	â€‹

 stays fixed. This nails the intuition: trace is a â€œtotal scaling budgetâ€ across invariant directions.



2) Supporting hyperplanes sliding over â„“â‚ / â„“âˆ in three panels (â„“â‚ only, â„“âˆ only, overlay)

Download: supporting_hyperplanes_l1_linf_three_panels.gif

How to watch it:

Panel 1 (diamond): supporting line is

ğ‘¢
âŠ¤
ğ‘¥
=
sup
â¡
âˆ¥
ğ‘¥
âˆ¥
1
â‰¤
1
ğ‘¢
âŠ¤
ğ‘¥
=
âˆ¥
ğ‘¢
âˆ¥
âˆ
.
u
âŠ¤
x=
âˆ¥xâˆ¥
1
	â€‹

â‰¤1
sup
	â€‹

u
âŠ¤
x=âˆ¥uâˆ¥
âˆ
	â€‹

.

Panel 2 (square): supporting line is

ğ‘¢
âŠ¤
ğ‘¥
=
sup
â¡
âˆ¥
ğ‘¥
âˆ¥
âˆ
â‰¤
1
ğ‘¢
âŠ¤
ğ‘¥
=
âˆ¥
ğ‘¢
âˆ¥
1
.
u
âŠ¤
x=
âˆ¥xâˆ¥
âˆ
	â€‹

â‰¤1
sup
	â€‹

u
âŠ¤
x=âˆ¥uâˆ¥
1
	â€‹

.

Panel 3 overlays both: you literally see two different â€œshadow lengthsâ€ for the same direction 
ğ‘¢
u, depending on whether the ball is diamond or square.

If you want the hyperplanes to be shown as halfspaces (shaded feasible side) instead of just the boundary line, I can generate that version too (it makes the â€œintersection = robustified setâ€ even more visually obvious). 






Hereâ€™s a single unified animation that shows kernel + image + rank all at once (same map, two panels, with the rankâ€“nullity identity baked in).

Download: kernel_image_rank_unified.gif

How to watch it:

Left panel (domain): the moving input vector 
ğ‘£
v explores all directions; the dashed line is the kernel direction (inputs that collapse to zero).

Right panel (codomain): the output 
ğ´
ğ‘£
Av always lands on the dashed image line (all possible outputs live in a 1D subspace).

When 
ğ‘£
v aligns with the kernel line, the output snaps to (near) zero.

The caption reminds you: 
dim
â¡
(
ğ‘…
2
)
=
rank
(
ğ´
)
+
nullity
(
ğ´
)
=
1
+
1
dim(R
2
)=rank(A)+nullity(A)=1+1.

If you want the same â€œunifiedâ€ idea but with a rank drop animation (matrix smoothly degenerates from rank 2 â†’ rank 1 so the image collapses from plane â†’ line and the kernel â€œappearsâ€), I can generate that variant too.





Four fundamental subspaces under one map
Download: four_fundamental_subspaces.gif

What it shows (in one view):

Domain panel: Row(A) direction vs Null(A) direction.

Codomain panel: Col(A) direction vs LeftNull(A) direction.

Two different inputs 
ğ‘¥
1
,
ğ‘¥
2
x
1
	â€‹

,x
2
	â€‹

 differ only in their Null(A) component, yet their outputs overlap: 
ğ´
ğ‘¥
1
=
ğ´
ğ‘¥
2
Ax
1
	â€‹

=Ax
2
	â€‹

.
Thatâ€™s the kernel/row-space split in action, visually.



2) â„“âˆ epigraph via dual â„“â‚ (u* â€œone-hotâ€ concentrates on max coordinate)

Download: epigraph_dual_norm_linf_l1.gif

How to watch:

Left: square is 
âˆ¥
ğ‘¥
âˆ¥
âˆ
â‰¤
ğ›¾
âˆ¥xâˆ¥
âˆ
	â€‹

â‰¤Î³.

The red arrow 
ğ‘¢
\*
u
\*
 is a one-hot vector (all mass on the coordinate with largest 
âˆ£
ğ‘¥
ğ‘–
âˆ£
âˆ£x
i
	â€‹

âˆ£), satisfying 
âˆ¥
ğ‘¢
\*
âˆ¥
1
=
1
âˆ¥u
\*
âˆ¥
1
	â€‹

=1.

The text shows:

sup
â¡
âˆ¥
ğ‘¢
âˆ¥
1
â‰¤
1
ğ‘¢
âŠ¤
ğ‘¥
=
âˆ¥
ğ‘¥
âˆ¥
âˆ
.
âˆ¥uâˆ¥
1
	â€‹

â‰¤1
sup
	â€‹

u
âŠ¤
x=âˆ¥xâˆ¥
âˆ
	â€‹

.

Right: epigraph view of 
ğ‘¡
â‰¥
âˆ¥
ğ‘¥
âˆ¥
âˆ
tâ‰¥âˆ¥xâˆ¥
âˆ
	â€‹

, again checking feasibility via 
ğ›¾
â‰¥
âˆ¥
ğ‘¥
âˆ¥
âˆ
Î³â‰¥âˆ¥xâˆ¥
âˆ
	â€‹

.

Youâ€™ll see 
ğ‘¢
\*
u
\*
 jump when the max-absolute coordinate switches.






1) â„“â‚ epigraph via dual â„“âˆ (u* jumps by quadrants/faces)

Download: epigraph_dual_norm_l1_linf.gif

How to watch:

Left: diamond is 
âˆ¥
ğ‘¥
âˆ¥
1
â‰¤
ğ›¾
âˆ¥xâˆ¥
1
	â€‹

â‰¤Î³.

The red arrow is 
ğ‘¢
\*
=
sign
(
ğ‘¥
)
u
\*
=sign(x), which always satisfies 
âˆ¥
ğ‘¢
\*
âˆ¥
âˆ
â‰¤
1
âˆ¥u
\*
âˆ¥
âˆ
	â€‹

â‰¤1.

The text shows the key identity in real time:

sup
â¡
âˆ¥
ğ‘¢
âˆ¥
âˆ
â‰¤
1
ğ‘¢
âŠ¤
ğ‘¥
=
âˆ¥
ğ‘¥
âˆ¥
1
.
âˆ¥uâˆ¥
âˆ
	â€‹

â‰¤1
sup
	â€‹

u
âŠ¤
x=âˆ¥xâˆ¥
1
	â€‹

.

Right: epigraph view of 
ğ‘¡
â‰¥
âˆ¥
ğ‘¥
âˆ¥
1
tâ‰¥âˆ¥xâˆ¥
1
	â€‹

. With 
ğ‘¡
=
ğ›¾
t=Î³, feasibility is exactly 
ğ›¾
â‰¥
âˆ¥
ğ‘¥
âˆ¥
1
Î³â‰¥âˆ¥xâˆ¥
1
	â€‹

.

Youâ€™ll see 
ğ‘¢
\*
u
\*
 snap when 
ğ‘¥
x crosses an axis (face-switching).

2) â„“âˆ epigraph via dual â„“â‚ (u* â€œone-hotâ€ concentrates on max coordinate)

Download: epigraph_dual_norm_linf_l1.gif

How to watch:

Left: square is 
âˆ¥
ğ‘¥
âˆ¥
âˆ
â‰¤
ğ›¾
âˆ¥xâˆ¥
âˆ
	â€‹

â‰¤Î³.

The red arrow 
ğ‘¢
\*
u
\*
 is a one-hot vector (all mass on the coordinate with largest 
âˆ£
ğ‘¥
ğ‘–
âˆ£
âˆ£x
i
	â€‹

âˆ£), satisfying 
âˆ¥
ğ‘¢
\*
âˆ¥
1
=
1
âˆ¥u
\*
âˆ¥
1
	â€‹

=1.

The text shows:

sup
â¡
âˆ¥
ğ‘¢
âˆ¥
1
â‰¤
1
ğ‘¢
âŠ¤
ğ‘¥
=
âˆ¥
ğ‘¥
âˆ¥
âˆ
.
âˆ¥uâˆ¥
1
	â€‹

â‰¤1
sup
	â€‹

u
âŠ¤
x=âˆ¥xâˆ¥
âˆ
	â€‹

.

Right: epigraph view of 
ğ‘¡
â‰¥
âˆ¥
ğ‘¥
âˆ¥
âˆ
tâ‰¥âˆ¥xâˆ¥
âˆ
	â€‹

, again checking feasibility via 
ğ›¾
â‰¥
âˆ¥
ğ‘¥
âˆ¥
âˆ
Î³â‰¥âˆ¥xâˆ¥
âˆ
	â€‹

.

Youâ€™ll see 
ğ‘¢
\*
u
\*
 jump when the max-absolute coordinate switches.



ğŸ¬ Dimension â€” degrees of freedom

Download: dimension_degrees_of_freedom.gif

How to read this animation

Two panels:

Left

Motion constrained to a line.

One parameter controls everything.

Right

Motion fills the plane.

Two independent parameters are required.

What this locks in
dimension
=
number of independent parameters needed to describe all vectors
.
dimension=number of independent parameters needed to describe all vectors.

This makes later results feel inevitable:

rank

nullity

rankâ€“nullity

â€œhow many constraints really matter?â€



Diagonalization = in the eigenbasis the map is just axis scaling
Download: diagonalization_eigenbasis.gif
Left: in standard coordinates, a circle becomes an ellipse.
Right: in eigen-coordinates, the same transformation becomes pure independent scaling along axes.



Determinant goes to 0 when columns become dependent
Download: determinant_column_dependence.gif

What youâ€™ll see: as the two columns become nearly collinear, the parallelogram flattens and the determinant shrinks to ~0. This is the clean geometric reason:

det
â¡
(
ğ´
)
=
0
â€…â€Š
âŸº
â€…â€Š
columns are linearly dependent
â€…â€Š
âŸº
â€…â€Š
ğ´
 is not invertible.
det(A)=0âŸºcolumns are linearly dependentâŸºA is not invertible.



Determinant = signed area scaling + orientation
Download: determinant_area_orientation.gif

What youâ€™ll see: the unit square becomes a parallelogram.

âˆ£
det
â¡
(
ğ´
)
âˆ£
âˆ£det(A)âˆ£ is the area scale factor. The sign tells you whether orientation is preserved or flipped. When 
det
â¡
(
ğ´
)
â‰ˆ
0
det(A)â‰ˆ0, the parallelogram collapses (singular map).


ğŸ¬ Basis & Coordinates â€” representation is unique

Download: basis_coordinates.gif

How to read this animation

Black dotted grid: standard coordinates (reference only).

Red skewed grid: a new basis 
{
ğ‘
1
,
ğ‘
2
}
{b
1
	â€‹

,b
2
	â€‹

}.

Blue vector 
ğ‘£
v: fixed geometric object.

The coefficients 
(
ğ‘
1
,
ğ‘
2
)
(c
1
	â€‹

,c
2
	â€‹

) update live so that

ğ‘£
=
ğ‘
1
ğ‘
1
+
ğ‘
2
ğ‘
2
.
v=c
1
	â€‹

b
1
	â€‹

+c
2
	â€‹

b
2
	â€‹

.
What this locks in

A basis is a coordinate system.

Coordinates depend on the basis.

Uniqueness of coordinates is the defining feature of a basis.

This animation is the conceptual bridge between:

linear independence (4.1),

matrices as representations (Layer 6),

change of basis.





