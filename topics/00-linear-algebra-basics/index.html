<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. Linear Algebra Basics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "../../static/lib/three/three.module.js"
      }
    }
  </script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-linear-algebra-advanced/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>00. Linear Algebra Basics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, review, linear-algebra, foundational</span>
      </div>
      <div class="lecture-summary">
        <p>Optimization is the art of making the best choice from a set of alternatives, and linear algebra is the language we use to describe that set. This lecture establishes the mathematical foundations required for convex optimization, moving beyond mechanical matrix operations to a geometric understanding of high-dimensional spaces. We begin by defining the atomic units of our "type system"—scalars, vectors, and matrices—and explore how they interact through linear maps. We then delve into the geometry of subspaces, using the Rank-Nullity Theorem to understand the conservation of information. A rigorous treatment of inner products and norms follows, allowing us to measure lengths and angles, which are prerequisites for defining convergence and optimality. We also explore Positive Semidefinite (PSD) matrices, the matrix generalization of non-negative numbers that underpins convexity. Finally, we derive the method of least squares not just as a formula, but as a geometric consequence of orthogonal projection. This material provides the necessary vocabulary to describe convex sets, functions, and optimization problems with precision.</p>
        <p><strong>Prerequisites:</strong> Basic multivariable calculus and familiarity with standard matrix notation.</p>
        <p><strong>Forward Connections:</strong> The projection techniques introduced here are essential for the geometric interpretation of constrained optimization. PSD matrices are the cornerstone of convex quadratic programs (QP) and Semidefinite Programming (SDP) covered in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The four fundamental subspaces provide the geometric intuition for duality theory (<a href="../09-duality/index.html">Lecture 09</a>). Advanced topics such as QR factorization, SVD, and pseudoinverses are covered in <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Analyze Fundamental Subspaces:</b> Identify and compute the dimensions of the four fundamental subspaces of a matrix using the Rank-Nullity Theorem.</li>
        <li><b>Manipulate Norms and Inner Products:</b> Apply standard and generalized inner products and vector norms ($\ell_1, \ell_2, \ell_\infty$) to establish inequalities.</li>
        <li><b>Perform Geometric Operations:</b> Construct orthonormal bases via Gram-Schmidt and compute orthogonal projections onto subspaces and affine sets.</li>
        <li><b>Characterize PSD Matrices:</b> Determine positive semidefiniteness using eigenvalues, variational forms, and the Schur Complement lemma.</li>
        <li><b>Solve Least Squares:</b> Derive and solve the normal equations for overdetermined systems and interpret the solution geometrically.</li>
        <li><b>Apply Matrix Calculus:</b> Compute gradients and Hessians for linear, quadratic, and log-determinant functions.</li>
      </ul>
    </section>

    <!-- SECTION 1: NOTATION -->
    <section class="section-card" id="section-notation">
      <h2>1. Mathematical Atoms: From Fields to Functions</h2>

      <h3>1.1 The Optimization Type System</h3>
      <p>Optimization is "glue math": we compose maps, stack constraints, take gradients, and build Lagrangians. To do this correctly, we must treat mathematics as a <b>typed language</b>. Just as a compiler checks that you don't add a string to an integer, rigorous optimization requires us to respect the types of our mathematical objects. Most "conceptual" errors in optimization are actually simple type errors—mistaking a row vector for a column vector, or treating a set of points as a subspace.</p>

      <p>Every symbol in an optimization problem has a declared type. <b>Scalars</b> ($a \in \mathbb{R}$) are the atomic units, acting as weights or objective values. <b>Vectors</b> ($x \in \mathbb{R}^n$) are ordered lists of $n$ scalars, representing points in space or directions. <b>Matrices</b> ($A \in \mathbb{R}^{m \times n}$) are typed operators that transform vectors from one space ($\mathbb{R}^n$) to another ($\mathbb{R}^m$). <b>Sets</b> ($C \subseteq \mathbb{R}^n$) define the valid search space, acting as boolean predicates for feasibility. Finally, <b>Functions</b> ($f: D \to Y$) map inputs to outputs, with domains playing a crucial role in defining the problem's scope.</p>

      <h4>Type Checking Rules</h4>
      <p>Operations are only defined when types match. You may only add vectors of the same dimension. Matrix-vector multiplication $Ax$ is defined only if the inner dimensions match ($m \times n$ times $n \times 1$). This "shape discipline" prevents silent errors. For instance, the expression $Ax=b$ asserts that the transformed vector $Ax$ lives in the same space as $b$. Similarly, adding a gradient vector to a decision variable requires them to live in the same (or dual) space.</p>

      <div class="example-box">
        <h4>Example: The Silent Killer</h4>
        <p>Consider the gradient update $\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \nabla f(\mathbf{x}_k)$.
        <br>If $\mathbf{x} \in \mathbb{R}^n$ is a column vector, mathematically $\nabla f(\mathbf{x})$ is often treated as a row vector (a linear functional in the dual space). However, in implementation (NumPy/PyTorch), adding a shape $(n,)$ array to a shape $(1, n)$ array might trigger broadcasting, resulting in an $(n, n)$ matrix rather than a vector update!
        <br><b>Rigorous Typing:</b> Always enforce that gradients are explicitly reshaped to match the primal variable's shape before addition.</p>
      </div>

      <h3>1.2 Scalars: The Rulebook</h3>
      <p>A <b>scalar</b> is an element of a field, usually the real numbers $\mathbb{R}$. Scalars play two primary roles in optimization: as coefficients (weights) that scale vectors, and as values of the objective function we seek to minimize. While we typically work with real numbers, the rules governing scalars are defined by the abstract axioms of a <b>field</b>.</p>

      <p>A field is a set equipped with two operations—addition and multiplication—that behave "nicely." Addition forms an abelian group, meaning it is closed, associative, commutative, has an identity ($0$), and every element has an inverse. Multiplication (excluding zero) also forms an abelian group with identity ($1$). The distributive law connects these two operations, ensuring that $a(b+c) = ab + ac$. These axioms guarantee that we can solve linear equations, since we can add, subtract, multiply, and divide (by non-zero elements) without leaving the system.</p>

      <div class="proof-box">
        <h4>Fundamental Field Lemmas</h4>
        <p>These proofs demonstrate how algebraic rigor underpins "obvious" arithmetic.</p>
        <div class="proof-step">
          <strong>Lemma 1: $0 \cdot a = 0$ for all $a$.</strong>
          <p>This isn't an axiom; it's a consequence of the additive identity and distributivity.</p>
          $$ 0\cdot a = (0+0)\cdot a = 0\cdot a + 0\cdot a \quad \text{(Distributivity)} $$
          Now, we have an equation $x = x + x$ where $x = 0 \cdot a$. To solve for $x$, we add its additive inverse $-(0\cdot a)$ to both sides (cancellation law):
          $$ 0 = 0\cdot a $$
        </div>
        <div class="proof-step">
          <strong>Lemma 2: $(-1)a = -a$.</strong>
          We must show that $(-1)a$ acts as the additive inverse of $a$.
          $$ a + (-1)a = 1\cdot a + (-1)\cdot a = (1 + (-1))a = 0\cdot a = 0 $$
          By uniqueness of the additive inverse, $(-1)a = -a$.
        </div>
        <div class="proof-step">
          <strong>Lemma 3: No Zero Divisors.</strong>
          If $ab=0$ and $a \neq 0$, then $b=0$.
          <br><i>Proof:</i> Since $a \neq 0$, $a^{-1}$ exists. Multiply both sides by $a^{-1}$:
          $$ a^{-1}(ab) = a^{-1}(0) \implies (a^{-1}a)b = 0 \implies 1\cdot b = 0 \implies b=0 $$
          This property fails in rings like $\mathbb{Z}_6$ (where $2 \cdot 3 = 0$), which is why linear algebra requires a field.
        </div>
      </div>

      <p><b>Extended Reals:</b> In optimization, we often extend the field of real numbers to include $+\infty$ (and sometimes $-\infty$). The set $\overline{\mathbb{R}} = \mathbb{R} \cup \{+\infty\}$ allows us to define the value of a constrained objective function outside the feasible set as $+\infty$. This "indicator function" trick unifies constrained and unconstrained optimization into a single framework.</p>

      <h3>1.3 Vectors: Elements of $\mathbb{R}^n$</h3>
      <p>A <b>vector</b> $x \in \mathbb{R}^n$ is an ordered $n$-tuple of scalars. We universally treat vectors as <b>columns</b> unless specified otherwise. The notation $x \in \mathbb{R}^n$ serves as a type tag asserting two things: the vector has exactly $n$ coordinates, and it lives in a vector space where addition and scalar multiplication are defined. Equality between vectors is defined coordinate-wise: $x=y$ if and only if $x_i = y_i$ for all $i$. This definition bridges the gap between $n$ scalar equations and a single vector equation.</p>

      <p>Operations on vectors are defined by consistency. Vector addition $(x+y)_i = x_i + y_i$ and scalar multiplication $(ax)_i = a x_i$ are defined component-wise. This structure satisfies the eight axioms of a vector space, ensuring that vectors behave like "arrows" that can be stretched and added tip-to-tail. The standard basis vectors $e_i$ (with a 1 in the $i$-th position and 0 elsewhere) allow us to write any vector as a linear combination of basis elements, effectively identifying the vector with its coordinates.</p>

      <h3>1.4 Matrices: Typed Operators</h3>
      <p>A <b>matrix</b> $A \in \mathbb{R}^{m \times n}$ is best understood not as a table of numbers, but as a linear map $A: \mathbb{R}^n \to \mathbb{R}^m$. It is a typed operator that takes an input vector of size $n$ and produces an output vector of size $m$. The definition of matrix-vector multiplication, $(Ax)_i = \sum_{j=1}^n A_{ij} x_j$, serves three purposes simultaneously: it enforces type safety (mapping $n$-space to $m$-space), it ensures linearity ($A(ax+by) = aAx + bAy$), and it allows for function composition via matrix multiplication.</p>

      <p>We can view matrix multiplication in two essential ways. The <b>Row View</b> treats each row as a measurement vector or linear functional; $(Ax)_i$ is the dot product of the $i$-th row with the input $x$. The <b>Column View</b> treats the operation as synthesis; $Ax$ is a linear combination of the columns of $A$, weighted by the entries of $x$. This column perspective is crucial for understanding the range and span of a matrix.</p>

      <h3>1.5 Sets and Functions: The Logic Layer</h3>
      <p>Optimization problems ask us to minimize a function over a feasible set. Constraints are fundamentally membership tests defining a set. An inequality like $g(x) \le 0$ is shorthand for $x \in \{z \in \mathbb{R}^n : g(z) \le 0\}$. Feasibility requires the simultaneous satisfaction of multiple constraints, which corresponds to the <b>intersection</b> of sets (logical AND). Infeasibility corresponds to the empty set.</p>

      <p>Modeling often involves <b>preimages</b>. If $g: \mathbb{R}^n \to \mathbb{R}^m$ is a function and $D \subseteq \mathbb{R}^m$ is a set, the preimage $g^{-1}(D) = \{x \in \mathbb{R}^n : g(x) \in D\}$ is the set of inputs that map into $D$. For example, the constraint $Ax=b$ is the preimage of the singleton set $\{b\}$ under the linear map $A$. A crucial property for convex optimization is that convexity is preserved under affine preimages: if $D$ is convex and $g$ is affine, then $g^{-1}(D)$ is convex.</p>

      <p>Functions $f: D \to Y$ consist of a domain, a codomain, and a mapping rule. In optimization, the domain is a hard constraint. For instance, the function $f(x) = -\log x$ is only defined for $x > 0$. We can also view functions geometrically via their <b>epigraphs</b>. The epigraph of $f$ is the set of points lying on or above its graph: $\mathrm{epi}(f) = \{(x, t) : f(x) \le t\}$. A function is convex if and only if its epigraph is a convex set, providing a bridge between the analysis of functions and the geometry of sets.</p>

      <h3>1.6 Linear Combinations: The Fundamental Constructor</h3>
      <p>Linear algebra is built on one atomic operation: the <b>finite linear combination</b>. Every concept in the course—span, independence, basis, matrix multiplication—reduces to sums of the form $\mathbf{y} = \sum_{i=1}^k \alpha_i \mathbf{v}_i$.</p>

      <div style="display: flex; gap: 16px; flex-wrap: wrap; margin: 24px 0;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/05_linear_combinations_lattice.png"
               alt="A lattice of linear combinations generated by two vectors"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Figure 2:</i> The span of two vectors creates a lattice of reachable points. Every intersection is a linear combination with integer coefficients.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/layer1_13_linear_combinations.gif"
               alt="Animation showing how varying coefficients sweeps out the plane"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> By varying coefficients $\alpha_1, \alpha_2$, the linear combination $\alpha_1 v_1 + \alpha_2 v_2$ can reach any point in the plane (the span).</figcaption>
        </figure>
      </div>

      <p>Solving a linear system $A\mathbf{x} = \mathbf{b}$ is essentially asking a question about linear combinations: "Is the vector $\mathbf{b}$ in the span of the columns of $A$?" If the answer is yes, the system is consistent; if no, it is inconsistent. Gaussian elimination (row reduction) is the algorithmic tool we use to answer this question, transforming the system into a simpler form without changing the solution set.</p>

      <p><b>Linear Independence</b> captures the concept of non-redundancy. A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the others. This is equivalent to saying that the only linear combination yielding the zero vector is the trivial one (all coefficients zero). Independence ensures that coordinates are unique: if a vector can be written as a linear combination of independent vectors, that representation is unique.</p>

      <p>A <b>basis</b> is a set of vectors that is both linearly independent (no redundancy) and spans the entire space (existence of representation). The number of vectors in a basis is invariant and defines the <b>dimension</b> of the vector space. The Rank-Nullity Theorem, which we will explore next, relates the dimensions of the fundamental subspaces associated with a matrix.</p>
    </section>

    <!-- SECTION 2: SUBSPACES -->
    <section class="section-card" id="section-subspaces">
      <h2>2. Subspaces and the Four Fundamental Spaces</h2>

      <p>A <a href="javascript:void(0)" class="definition-link">linear subspace</a> is a subset of a vector space that is closed under addition and scalar multiplication. Every linear map $T: V \to W$ decomposes the domain and codomain into fundamental subspaces based on <b>what information is lost</b> and <b>what information survives</b> the transformation.</p>

      <h3>2.1 Kernel and Image: Information Loss and Survival</h3>
      <p>For a linear map represented by matrix $A$, the <b>Kernel</b> (or Nullspace), $\mathcal{N}(A)$, consists of all input vectors $\mathbf{x}$ that map to the zero vector ($A\mathbf{x} = 0$). Geometrically, the kernel represents the directions in the input space that are "crushed" or "flattened" by the transformation. This corresponds to a loss of information: if two inputs $\mathbf{u}$ and $\mathbf{v}$ differ by a vector in the kernel, they produce the same output, making them indistinguishable from the perspective of the range.</p>

      <p>The <b>Image</b> (or Column Space), $\mathcal{R}(A)$, consists of all possible output vectors $A\mathbf{x}$. It represents the "reachable" part of the codomain—the information that survives the transformation. The system $A\mathbf{x} = \mathbf{b}$ is solvable if and only if $\mathbf{b}$ lies in the image of $A$. Both the kernel and the image are subspaces, closed under linear combinations.</p>

      <div class="theorem-box">
        <h4>Structure of Solution Sets: Affine Geometry</h4>
        <p>$$ \boxed{ \{\text{solutions of } A\mathbf{x}=\mathbf{b}\} = \mathbf{x}_p + \mathcal{N}(A) } $$
        <br>Every consistent linear system produces an affine solution set. If we find just <b>one</b> particular solution $\mathbf{x}_p$ (where $A\mathbf{x}_p = \mathbf{b}$), then <i>every</i> other solution can be found by adding a vector from the nullspace. Why? Because adding a nullspace vector $\mathbf{x}_h$ to $\mathbf{x}_p$ doesn't change the output: $A(\mathbf{x}_p + \mathbf{x}_h) = A\mathbf{x}_p + A\mathbf{x}_h = \mathbf{b} + 0 = \mathbf{b}$. Conversely, the difference between any two solutions lies in the nullspace. Thus, the solution set is a "shifted subspace"—an affine space parallel to the kernel.</p>
      </div>

      <h3>2.2 Rank-Nullity: Conservation of Degrees of Freedom</h3>
      <p>The <b>Rank-Nullity Theorem</b> acts as a conservation law for linear maps. It states that the dimension of the input space ($n$) is split exactly between the dimension of the image (Rank) and the dimension of the kernel (Nullity).</p>
      $$ \boxed{ n = \dim(\mathcal{N}(A)) + \dim(\mathcal{R}(A)) } $$
      <p>Think of the input vector $\mathbf{x} \in \mathbb{R}^n$ as carrying $n$ independent degrees of freedom. When the map $A$ acts on $\mathbf{x}$, some of these degrees of freedom are preserved and appear in the output (contributing to the Rank), while others are annihilated and sent to zero (contributing to the Nullity). The theorem asserts that degrees of freedom cannot simply vanish or appear out of nowhere; they must be accounted for in one of these two buckets. You cannot compress a high-dimensional space into a lower-dimensional one without losing information (nullity > 0), nor can you generate more independent outputs than you had inputs.</p>

      <h3>2.3 The Four Fundamental Subspaces</h3>
      <p>Every matrix $A \in \mathbb{R}^{m \times n}$ induces four intrinsic subspaces that describe its geometry. These spaces are paired by orthogonality, linking the domain and codomain.</p>

      <p>In the <b>Domain</b> ($\mathbb{R}^n$), we have the <b>Row Space</b> $\mathcal{R}(A^\top)$ and the <b>Nullspace</b> $\mathcal{N}(A)$. The row space is spanned by the rows of $A$ and represents the active measurement directions. The nullspace contains all vectors orthogonal to every row; these represent the "blind spots" of the matrix. The Fundamental Theorem of Linear Algebra states that these two spaces are orthogonal complements: $\mathbb{R}^n = \mathcal{R}(A^\top) \oplus \mathcal{N}(A)$. Every input vector can be uniquely decomposed into a component that is "seen" by the matrix and a component that is annihilated.</p>

      <p>In the <b>Codomain</b> ($\mathbb{R}^m$), we have the <b>Column Space</b> $\mathcal{R}(A)$ and the <b>Left Nullspace</b> $\mathcal{N}(A^\top)$. The column space is the range of the map—the set of reachable outputs. The left nullspace contains all vectors $\mathbf{y}$ such that $A^\top \mathbf{y} = 0$, meaning $\mathbf{y}$ is orthogonal to every column of $A$. These two spaces are also orthogonal complements: $\mathbb{R}^m = \mathcal{R}(A) \oplus \mathcal{N}(A^\top)$. This implies that for any vector $\mathbf{b}$ not in the column space, there is a "witness" vector in the left nullspace perpendicular to it—a fact central to the Fredholm Alternative and duality theory.</p>

      <div class="proof-box">
        <h4>Proof: Orthogonality of Nullspace and Row Space</h4>
        <p>We want to show that $\mathcal{N}(A) = \mathcal{R}(A^\top)^\perp$.
        <br><b>Step 1 ($\subseteq$):</b> Let $\mathbf{x} \in \mathcal{N}(A)$. This means $A\mathbf{x} = 0$. Writing $A$ in terms of its rows $r_1^\top, \dots, r_m^\top$, this equation implies $r_i^\top \mathbf{x} = 0$ for all $i$. Since $\mathbf{x}$ is orthogonal to every row, it must be orthogonal to any linear combination of the rows. Thus, $\mathbf{x}$ is orthogonal to the entire row space $\mathcal{R}(A^\top)$.
        <br><b>Step 2 ($\supseteq$):</b> Let $\mathbf{x} \in \mathcal{R}(A^\top)^\perp$. By definition, $\mathbf{x}$ is orthogonal to every vector in the row space, which includes each individual row $r_i$ of $A$. Therefore, $r_i^\top \mathbf{x} = 0$ for all $i$. Stacking these equations back into matrix form gives $A\mathbf{x} = 0$, so $\mathbf{x} \in \mathcal{N}(A)$.
        <br><b>Conclusion:</b> The nullspace consists exactly of the vectors perpendicular to the row space.</p>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/four_fundamental_subspaces.gif"
             alt="The four fundamental subspaces under a linear map"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> The four fundamental subspaces in action. Domain (left): Row(A) vs Null(A). Codomain (right): Col(A) vs LeftNull(A). Inputs differing only in the nullspace direction map to the same output.</figcaption>
      </figure>
    </section>

    <!-- SECTION 3: ALGEBRAIC INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>3. Algebraic Invariants: Determinant, Trace, and Eigenvalues</h2>

      <p>Matrices depend on the choice of basis, but the linear operators they represent have intrinsic properties that remain constant regardless of the coordinate system. These <b>invariants</b>—Determinant, Trace, and Eigenvalues—capture the geometric essence of the transformation.</p>

      <h3>3.1 Determinant: Volume and Orientation</h3>
      <p>The <b>Determinant</b> is a scalar value that characterizes how a linear map scales volume. For an $n \times n$ matrix $A$, $|\det(A)|$ represents the factor by which the volume of a region changes under the transformation $x \mapsto Ax$. If you map a unit hypercube through $A$, the volume of the resulting parallelepiped is exactly $|\det(A)|$. The sign of the determinant indicates orientation: a positive determinant preserves the "handedness" of the space, while a negative one reverses it (like a mirror reflection). Crucially, if $\det(A) = 0$, the map squashes the space into a lower dimension (volume becomes zero), implying the matrix is singular (non-invertible). This "Squash Test" is the ultimate check for invertibility.</p>

      <h3>3.2 Trace: The Sum of Diagonal Elements</h3>
      <p>The <b>Trace</b> of a matrix, denoted $\mathrm{tr}(A)$, is defined as the sum of its diagonal elements. While this definition seems tied to coordinates, the trace is actually a profound invariant: it equals the sum of the eigenvalues. The trace acts as a linear functional with the unique property of being cyclic: $\mathrm{tr}(AB) = \mathrm{tr}(BA)$. This cyclic property is vital in matrix calculus and implies that the trace is invariant under similarity transformations ($\mathrm{tr}(P^{-1}AP) = \mathrm{tr}(A)$). Geometrically, for infinitesimal transformations $I + \epsilon A$, the trace measures the rate of volume expansion (divergence).</p>

      <h3>3.3 Eigenvalues: Invariant Directions</h3>
      <p><b>Eigenvalues</b> are the roots of the characteristic polynomial $p_A(\lambda) = \det(A - \lambda I)$. An eigenvalue $\lambda$ corresponds to an eigenvector $\mathbf{v} \neq 0$ such that $A\mathbf{v} = \lambda \mathbf{v}$. Geometrically, eigenvectors represent directions that are invariant under the transformation—they are merely scaled by the factor $\lambda$, not rotated. The set of eigenvalues (the spectrum) governs the dynamic behavior of the operator. For symmetric matrices, the eigenvalues are always real, and they determine the shape of the quadratic form $\mathbf{x}^\top A \mathbf{x}$ (ellipsoid axes). The sum of eigenvalues equals the trace, and their product equals the determinant.</p>

      <div class="proof-box">
        <h4>The Spectral Shift</h4>
        <p>Adding a multiple of the identity matrix, $A + tI$, shifts all eigenvalues by $t$ without changing the eigenvectors. Why?
        <br>If $A\mathbf{v} = \lambda \mathbf{v}$, then $(A + tI)\mathbf{v} = A\mathbf{v} + t\mathbf{v} = \lambda \mathbf{v} + t\mathbf{v} = (\lambda + t)\mathbf{v}$.
        <br>This simple but powerful tool allows us to adjust the spectrum of a matrix—for example, to make a matrix positive definite or to improve its condition number—without altering its geometric eigenstructure.</p>
      </div>
    </section>

    <!-- SECTION 4: INNER PRODUCTS -->
    <section class="section-card" id="section-norms">
      <h2>4. Inner Products & Norms: Geometry from Algebra</h2>
      <p>To do geometry—to talk about lengths, angles, and distances—we need more than just a vector space. We need additional structure. Inner products and norms provide this structure, turning a "floppy" vector space into a rigid geometric object capable of supporting optimization.</p>

      <h3>4.1 Inner Products: The Source of Angles</h3>
      <p>An <b>inner product</b> $\langle \mathbf{x}, \mathbf{y} \rangle$ is a function that takes two vectors and produces a scalar. It generalizes the dot product. To be a valid inner product, it must satisfy three axioms: <b>Symmetry</b> ($\langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle$), <b>Linearity</b> in the first argument, and <b>Positive Definiteness</b> ($\langle \mathbf{x}, \mathbf{x} \rangle \ge 0$, with equality only if $\mathbf{x}=0$). The inner product allows us to define angles via the formula $\cos \theta = \frac{\langle \mathbf{x}, \mathbf{y} \rangle}{\|\mathbf{x}\| \|\mathbf{y}\|}$ and, crucially, defines <b>orthogonality</b>: two vectors are orthogonal if their inner product is zero.</p>

      <h3>4.2 Norms: Defining Length</h3>
      <p>A <b>norm</b> $\|\mathbf{x}\|$ is a function that measures the "size" or "length" of a vector. While norms can be induced by inner products ($\|\mathbf{x}\| = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}$), they can also be defined independently. A valid norm must satisfy three axioms:</p>
      <ul>
        <li><b>Positivity:</b> $\|\mathbf{x}\| \ge 0$, and $\|\mathbf{x}\| = 0$ iff $\mathbf{x}=0$. (No invisible non-zero vectors).</li>
        <li><b>Homogeneity:</b> $\|\alpha \mathbf{x}\| = |\alpha| \|\mathbf{x}\|$. (Scaling a vector scales its length).</li>
        <li><b>Triangle Inequality:</b> $\|\mathbf{x} + \mathbf{y}\| \le \|\mathbf{x}\| + \|\mathbf{y}\|$. (The straight line is the shortest path).</li>
      </ul>
      <p>The Triangle Inequality is the engine behind convexity; it ensures that norm balls are convex sets.</p>

      <h3>4.3 Standard Norms and Unit Balls</h3>
      <p>Different norms induce different geometries, visualized by their <b>unit balls</b> $\{ \mathbf{x} : \|\mathbf{x}\| \le 1 \}$.
      <br>The <b>Euclidean Norm ($\ell_2$)</b>, $\|\mathbf{x}\|_2 = \sqrt{\sum x_i^2}$, corresponds to standard physical distance. Its unit ball is a sphere, rotationally symmetric. It is "democratic," giving weight to all coordinates.
      <br>The <b>Manhattan Norm ($\ell_1$)</b>, $\|\mathbf{x}\|_1 = \sum |x_i|$, has a diamond-shaped unit ball (cross-polytope). It promotes sparsity because its "corners" lie on the coordinate axes.
      <br>The <b>Max Norm ($\ell_\infty$)</b>, $\|\mathbf{x}\|_\infty = \max |x_i|$, has a square/cube unit ball. It measures the worst-case coordinate.
      <br>In optimization, the choice of norm determines the "shape" of the solution we prefer (e.g., sparse vs. dense).</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/norm-balls.png"
             alt="Comparison of L1, L2, and Infinity norm unit balls"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 6:</i> The "unit ball" $\{\mathbf{x} \mid \|\mathbf{x}\| \le 1\}$ for the $\ell_1$ (diamond), $\ell_2$ (circle), and $\ell_\infty$ (square) norms. All are convex sets.</figcaption>
      </figure>
    </section>

    <!-- SECTION 5: PSD MATRICES -->
    <section class="section-card" id="section-psd">
      <h2>5. Positive Semidefinite (PSD) Matrices</h2>

      <p>Positive Semidefinite matrices are the matrix equivalent of non-negative numbers. They allow us to define convex quadratic forms and are central to modern optimization, specifically Semidefinite Programming (SDP).</p>

      <h3>5.1 Definitions: Energy and Eigenvalues</h3>
      <p>There are two equivalent ways to define a PSD matrix $A \in \mathbb{S}^n$:</p>
      <ul>
        <li><b>Variational Definition (Energy):</b> A symmetric matrix $A$ is PSD ($A \succeq 0$) if the quadratic form $\mathbf{x}^\top A \mathbf{x}$ is non-negative for all vectors $\mathbf{x} \in \mathbb{R}^n$. Geometrically, this means the function $f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$ forms a "bowl" shape that never dips below zero. It represents non-negative curvature in every direction.</li>
        <li><b>Eigenvalue Definition (Spectral):</b> A symmetric matrix $A$ is PSD if and only if all its eigenvalues are non-negative ($\lambda_i \ge 0$). Since eigenvalues represent scaling factors along principal axes, this ensures the matrix does not "flip" or negatively scale any direction.</li>
      </ul>
      <p><b>Positive Definite (PD)</b> matrices satisfy the stricter condition $\mathbf{x}^\top A \mathbf{x} > 0$ for all $\mathbf{x} \neq 0$ (or equivalently, $\lambda_i > 0$). These correspond to strictly convex bowls with a unique minimum at the origin.</p>

      <h3>5.2 Geometry: Ellipsoids</h3>
      <p>PSD matrices define ellipsoids. For a Positive Definite matrix $P$, the set $\{ \mathbf{x} : \mathbf{x}^\top P \mathbf{x} \le 1 \}$ is an ellipsoid centered at the origin. The axes of the ellipsoid align with the eigenvectors of $P$, and the lengths of the semi-axes are proportional to $1/\sqrt{\lambda_i}$. Thus, large eigenvalues correspond to short axes (steep curvature), and small eigenvalues correspond to long axes (flat curvature). This geometric link explains why "conditioning" (the ratio of largest to smallest eigenvalues) determines the difficulty of optimization problems.</p>

      <h3>5.3 The Schur Complement Lemma</h3>
      <p>The Schur Complement provides a powerful tool for converting nonlinear constraints into linear matrix inequalities (LMIs). For a block symmetric matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ with $A \succ 0$, the matrix $M$ is PSD if and only if the Schur complement $S = C - B^\top A^{-1} B$ is PSD. This lemma is the "universal translator" in convex optimization, allowing us to rewrite constraints like $\|B\mathbf{x}\|_2^2 \le c$ as LMI constraints $\begin{bmatrix} I & B\mathbf{x} \\ (B\mathbf{x})^\top & c \end{bmatrix} \succeq 0$.</p>
    </section>

    <!-- SECTION 6: PROJECTIONS -->
    <section class="section-card" id="section-projections">
      <h2>6. Projections onto Subspaces and Affine Sets</h2>

      <p>Optimization is often about finding the "closest" point in a feasible set. This is the geometric concept of projection.</p>

      <h3>6.1 The Projection Theorem</h3>
      <p>Let $S$ be a subspace of $\mathbb{R}^n$. For any vector $\mathbf{b}$, the <b>orthogonal projection</b> of $\mathbf{b}$ onto $S$ is the unique vector $\mathbf{p} \in S$ that is closest to $\mathbf{b}$ in Euclidean distance. The <b>Projection Theorem</b> characterizes this point by a simple geometric condition: the error vector (residual) $\mathbf{r} = \mathbf{b} - \mathbf{p}$ must be <b>orthogonal</b> to the subspace $S$. In other words, the shortest path from a point to a plane is the straight line perpendicular to the plane. This orthogonality condition, $\mathbf{r} \perp S$, uniquely determines $\mathbf{p}$.</p>

      <h3>6.2 Subspace vs. Affine Projection</h3>
      <p>While projecting onto a subspace (which contains the origin) is a linear operation $\mathbf{p} = P\mathbf{b}$, projecting onto an <b>affine set</b> (like a line or plane not passing through the origin) involves a translation. To project onto the affine set $\mathcal{A} = \{ \mathbf{x} : F\mathbf{x} = \mathbf{g} \}$, we first translate the set to the origin to form a subspace, project the shifted vector, and then translate back. Alternatively, we can use the explicit formula derived via Lagrange multipliers or least squares: $\mathbf{p} = \mathbf{b} - F^\top (FF^\top)^{-1}(F\mathbf{b} - \mathbf{g})$.</p>
    </section>

    <!-- SECTION 7: LEAST SQUARES -->
    <section class="section-card" id="section-least-squares">
      <h2>7. The Method of Least Squares</h2>

      <p>Least squares is the prototypical convex optimization problem. When we have an overdetermined system $A\mathbf{x} = \mathbf{b}$ (more equations than unknowns), an exact solution usually doesn't exist. Instead, we seek the "best approximate" solution that minimizes the squared error $\|\mathbf{b} - A\mathbf{x}\|_2^2$.</p>

      <h3>7.1 Geometric Interpretation</h3>
      <p>Geometrically, the term $A\mathbf{x}$ represents a linear combination of the columns of $A$. Thus, minimizing the error corresponds to finding the point $\mathbf{p} = A\mathbf{x}$ in the <b>Column Space</b> $\mathcal{R}(A)$ that is closest to the target vector $\mathbf{b}$. By the Projection Theorem, this optimal point $\mathbf{p}$ is the orthogonal projection of $\mathbf{b}$ onto $\mathcal{R}(A)$.</p>

      <h3>7.2 The Normal Equations</h3>
      <p>The geometric condition for optimality is that the residual vector $\mathbf{r} = \mathbf{b} - A\mathbf{x}$ must be orthogonal to the column space of $A$. This means $\mathbf{r}$ must be orthogonal to every column of $A$. In matrix notation, this orthogonality condition is $A^\top \mathbf{r} = 0$, or $A^\top (\mathbf{b} - A\mathbf{x}) = 0$. Rearranging this gives the famous <b>Normal Equations</b>:
      $$ A^\top A \mathbf{x} = A^\top \mathbf{b} $$
      This system transforms the rectangular, unsolvable system $A\mathbf{x}=\mathbf{b}$ into a square, solvable system. If the columns of $A$ are linearly independent, $A^\top A$ is invertible, and the unique solution is $\mathbf{x} = (A^\top A)^{-1} A^\top \mathbf{b}$. This derivation shows that least squares is not just a statistical trick, but a direct consequence of Euclidean geometry.</p>
    </section>

    <!-- SECTION 8: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>8. Review & Cheat Sheet</h2>
      <div class="lecture-summary" style="margin-bottom: 20px;">
        <p>This section condenses the lecture into a quick-reference format for definitions, properties, and standard results.</p>
      </div>

      <h3>Definitions</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>Definition</th>
            <th>Key Properties</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>Subspace</b></td>
            <td>Set $S$ closed under addition and scalar multiplication</td>
            <td>Contains $\mathbf{0}$. Intersection of subspaces is a subspace.</td>
          </tr>
          <tr>
            <td><b>Inner Product</b></td>
            <td>$\langle \mathbf{x}, \mathbf{y} \rangle$</td>
            <td>Bilinear, Symmetric, Positive Definite. Enables angles and projection.</td>
          </tr>
          <tr>
            <td><b>Norm</b></td>
            <td>$\|x\|$</td>
            <td>Positivity, Homogeneity, Triangle Inequality.</td>
          </tr>
          <tr>
            <td><b>PSD Matrix</b></td>
            <td>$A \succeq 0$</td>
            <td>$\mathbf{x}^\top A \mathbf{x} \ge 0 \ \forall \mathbf{x}$. Eigenvalues $\lambda_i \ge 0$.</td>
          </tr>
          <tr>
            <td><b>Orthogonal Matrix</b></td>
            <td>$Q^\top Q = I$</td>
            <td>Preserves norms/angles. Columns are orthonormal.</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Theorems</h3>
      <ul>
        <li><b>Rank-Nullity:</b> $\dim \mathcal{R}(A) + \dim \mathcal{N}(A) = n$ (for $A \in \mathbb{R}^{m \times n}$).</li>
        <li><b>Spectral Theorem:</b> Real symmetric matrices have real eigenvalues and orthogonal eigenvectors.</li>
        <li><b>Schur Complement:</b> $M \succeq 0 \iff D \succ 0$ and $A - BD^{-1}B^\top \succeq 0$.</li>
        <li><b>Fundamental Theorem of Linear Algebra:</b> $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ and $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.</li>
      </ul>

      <h3>Standard Formulas</h3>
      <ul>
        <li><b>Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$</li>
        <li><b>Projection onto Subspace:</b> $P = A(A^\top A)^{-1}A^\top$ (if $A$ full rank)</li>
        <li><b>Cauchy-Schwarz:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$</li>
        <li><b>Gradient of Quadratic:</b> $\nabla (\mathbf{x}^\top A \mathbf{x}) = (A+A^\top)\mathbf{x}$</li>
      </ul>
    </section>

    <!-- SECTION 9: EXERCISES -->
    <section class="section-card" id="section-exercises">
      <h2><i data-feather="edit-3"></i> 9. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises reinforce the foundational tools of linear algebra used in optimization. Focus on the geometry of subspaces, the calculus of gradients (crucial for finding optimality conditions), and the properties of PSD matrices (essential for convexity).</p>
      </div>
      <h3>P0.1 — Linear Independence</h3>
      <p>Determine whether the following sets of vectors are linearly independent. If dependent, exhibit a linear combination summing to zero.</p>
      <ol type="a">
        <li>$v_1 = (1, 2, 3)^\top, v_2 = (4, 5, 6)^\top, v_3 = (7, 8, 9)^\top$.</li>
        <li>$v_1 = (1, 0, 0)^\top, v_2 = (1, 1, 0)^\top, v_3 = (1, 1, 1)^\top$.</li>
        <li>The columns of an upper triangular matrix with non-zero diagonal entries.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution (Step-by-Step)</h4>
        <ol type="a">
          <li><b>Status: Linearly Dependent.</b>
          <br><i>Walkthrough:</i>
          <br>1. <b>Observation:</b> Look at the vectors $(1,2,3), (4,5,6), (7,8,9)$. The components increase by constant steps. This suggests an arithmetic progression.
          <br>2. <b>Check differences:</b>
             $$ v_2 - v_1 = (3, 3, 3)^\top $$
             $$ v_3 - v_2 = (3, 3, 3)^\top $$
          <br>3. <b>Formulate relation:</b> Since the differences are equal, $v_2 - v_1 = v_3 - v_2$.
          <br>4. <b>Rearrange to standard form:</b> Move everything to one side to find the linear combination summing to zero.
             $$ v_1 - 2v_2 + v_3 = 0 $$
          <br>5. <b>Conclusion:</b> We found non-zero coefficients $(1, -2, 1)$ that annihilate the vectors. Thus, they are dependent.
          </li>
          <li><b>Status: Linearly Independent.</b>
          <br><i>Walkthrough:</i>
          <br>1. <b>Matrix Method:</b> Stack the vectors as columns of a matrix $A$:
             $$ A = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix} $$
          <br>2. <b>Analyze Structure:</b> The matrix is <b>upper triangular</b> (zeros below the diagonal).
          <br>3. <b>Determinant Test:</b> For triangular matrices, the determinant is the product of diagonal entries.
             $$ \det(A) = 1 \cdot 1 \cdot 1 = 1 $$
          <br>4. <b>Conclusion:</b> Since $\det(A) \neq 0$, the matrix is invertible, and its columns are linearly independent.
          </li>
          <li><b>Status: Linearly Independent.</b>
          <br><i>Reasoning:</i> This generalizes part (b). Any upper triangular matrix $U$ with non-zero diagonal entries $u_{ii} \neq 0$ has determinant $\det(U) = \prod u_{ii} \neq 0$. Therefore, its columns form a basis for $\mathbb{R}^n$.
          </li>
        </ol>
      </div>

      <h3>P0.2 — The Rank-Nullity Theorem</h3>
      <p>Let $A$ be a $10 \times 15$ matrix.</p>
      <ol type="a">
        <li>What is the maximum possible rank of $A$?</li>
        <li>If the rank of $A$ is 8, what is the dimension of the nullspace $\mathcal{N}(A)$?</li>
        <li>If $A \mathbf{x} = 0$ has only the solution $\mathbf{x}=0$, is this possible? Explain.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>The rank is bounded by the dimensions: $\mathrm{rank}(A) \le \min(m, n) = \min(10, 15) = 10$.</li>
          <li>By the Rank-Nullity Theorem, $\dim(\mathcal{N}(A)) + \mathrm{rank}(A) = n$. Here $n=15$ (number of columns). So $\dim(\mathcal{N}(A)) = 15 - 8 = 7$.</li>
          <li>The condition "only the solution $\mathbf{x}=0$" means $\mathcal{N}(A) = \{0\}$, so $\dim(\mathcal{N}(A)) = 0$. By Rank-Nullity, this would imply $\mathrm{rank}(A) = 15 - 0 = 15$. However, we established in (a) that the maximum rank is 10. Thus, this is <b>impossible</b>. An underdetermined system ($m < n$) always has a non-zero nullspace.</li>
        </ol>
      </div>

      <h3>P0.3 — Trace and Determinant</h3>
      <ol type="a">
        <li>Show that $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$ but generally $\mathrm{tr}(ABC) \neq \mathrm{tr}(BAC)$. Construct a $2 \times 2$ counterexample.</li>
        <li>Let $A \in \mathbb{R}^{n \times n}$ be skew-symmetric ($A^\top = -A$). Show that $\mathbf{x}^\top A \mathbf{x} = 0$ for all $\mathbf{x}$.</li>
        <li>Use the result from (b) to prove that if $n$ is odd, $\det(A) = 0$. (Hint: $\det(A^\top) = \det(-A)$).</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Cyclic Property:</b> Let $X = AB$. Then $\mathrm{tr}(XC) = \mathrm{tr}(CX)$ (basic cyclic property). Substituting $X=AB$, we get $\mathrm{tr}((AB)C) = \mathrm{tr}(C(AB)) = \mathrm{tr}(CAB)$. Applying it again gives $\mathrm{tr}(BCA)$.
          <br><b>Counterexample for Non-Cyclic:</b> Let $A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$, $B = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$, $C = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$.
          <br>$ABC = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \implies \mathrm{tr}=1$.
          <br>$BAC = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \mathbf{0} \implies \mathrm{tr}=0$.
          </li>
          <li><b>Skew-Symmetric Form:</b> The scalar $\mathbf{x}^\top A \mathbf{x}$ is its own transpose.
          $$ \mathbf{x}^\top A \mathbf{x} = (\mathbf{x}^\top A \mathbf{x})^\top = \mathbf{x}^\top A^\top \mathbf{x} $$
          Since $A^\top = -A$, we have $\mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top (-A) \mathbf{x} = -\mathbf{x}^\top A \mathbf{x}$.
          The only number equal to its negative is 0. Thus $\mathbf{x}^\top A \mathbf{x} = 0$.
          </li>
          <li><b>Determinant of Skew-Symmetric:</b>
          $$ \det(A) = \det(A^\top) = \det(-A) = (-1)^n \det(A) $$
          If $n$ is odd, $(-1)^n = -1$. So $\det(A) = -\det(A)$, which implies $2\det(A) = 0 \implies \det(A) = 0$.
          </li>
        </ol>
      </div>

      <h3>P0.4 — Norm Equivalence</h3>
      <p>In finite dimensions, all norms are equivalent. For $\mathbf{x} \in \mathbb{R}^n$, prove the following inequalities:</p>
      <ol type="a">
        <li>$\|x\|_\infty \le \|\mathbf{x}\|_2 \le \sqrt{n} \|\mathbf{x}\|_\infty$</li>
        <li>$\|x\|_2 \le \|\mathbf{x}\|_1 \le \sqrt{n} \|\mathbf{x}\|_2$</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Left inequality:</b> $\|x\|_\infty^2 = \max_i |x_i|^2 \le \sum_i x_i^2 = \|\mathbf{x}\|_2^2$. Taking square roots gives $\|x\|_\infty \le \|\mathbf{x}\|_2$.
          <br><b>Right inequality:</b> $\|x\|_2^2 = \sum_i x_i^2 \le \sum_i (\max_j |x_j|)^2 = \sum_i \|\mathbf{x}\|_\infty^2 = n \|\mathbf{x}\|_\infty^2$. Taking square roots gives $\|x\|_2 \le \sqrt{n}\|\mathbf{x}\|_\infty$.
          </li>
          <li><b>Left inequality:</b> Square $\|x\|_1$: $\|x\|_1^2 = (\sum |x_i|)^2 = \sum x_i^2 + \sum_{i \ne j} |x_i||x_j| = \|\mathbf{x}\|_2^2 + \text{non-negative terms} \ge \|\mathbf{x}\|_2^2$. Thus $\|x\|_1 \ge \|\mathbf{x}\|_2$.
          <br><b>Right inequality:</b> Use Cauchy-Schwarz with the vector of ones $\mathbf{1}$ and the vector $|\mathbf{x}| = (|x_1|, \dots, |x_n|)$.
          $$ \|\mathbf{x}\|_1 = \sum |x_i| \cdot 1 = |\mathbf{x}|^\top \mathbf{1} \le \||\mathbf{x}|\|_2 \|\mathbf{1}\|_2 = \|\mathbf{x}\|_2 \sqrt{n} $$
          </li>
        </ol>
      </div>

      <h3>P0.5 — Least Squares from Scratch</h3>
      <p>Consider the function $f(\mathbf{x}) = \frac{1}{2} \|A\mathbf{x} - \mathbf{b}\|_2^2$.</p>
      <ol type="a">
        <li>Expand the squared norm into terms involving $\mathbf{x}^\top A^\top A \mathbf{x}$, etc.</li>
        <li>Compute the gradient $\nabla f(\mathbf{x})$ step-by-step.</li>
        <li>Set the gradient to zero to derive the Normal Equations.</li>
        <li>Show that if $\mathcal{N}(A) = \{0\}$, the Hessian is positive definite, ensuring a unique global minimum.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>$f(\mathbf{x}) = \frac{1}{2}(A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = \frac{1}{2}(\mathbf{x}^\top A^\top - \mathbf{b}^\top)(A\mathbf{x} - \mathbf{b}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \frac{1}{2} \mathbf{x}^\top A^\top \mathbf{b} - \frac{1}{2} \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b}$.
          Since scalar transpose is identity ($\mathbf{x}^\top A^\top \mathbf{b} = \mathbf{b}^\top A \mathbf{x}$), we simplify to:
          $$ f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b} $$
          </li>
          <li><b>Gradient Derivation (Term-by-Term):</b>
            <br>We compute the gradient of each term in the expanded expression $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b}$.
            <br><b>Term 1:</b> $\nabla (\frac{1}{2} \mathbf{x}^\top (A^\top A) \mathbf{x}) = A^\top A \mathbf{x}$ (since $A^\top A$ is symmetric).
            <br><b>Term 2:</b> $\nabla (-\mathbf{b}^\top A \mathbf{x}) = \nabla (-(A^\top \mathbf{b})^\top \mathbf{x}) = -A^\top \mathbf{b}$.
            <br><b>Term 3:</b> $\nabla (\text{constant}) = 0$.
            <br><b>Result:</b> $\nabla f(\mathbf{x}) = A^\top A \mathbf{x} - A^\top \mathbf{b}$.
          </li>
          <li>$\nabla f(\mathbf{x}) = 0 \implies A^\top A \mathbf{x} - A^\top \mathbf{b} = 0 \implies A^\top A \mathbf{x} = A^\top \mathbf{b}$. These are the Normal Equations.
          </li>
          <li>$\nabla^2 f(\mathbf{x}) = A^\top A$. If $\mathcal{N}(A) = \{0\}$, then for any $\mathbf{v} \neq 0$, $A\mathbf{v} \neq 0$.
          $$ \mathbf{v}^\top A^\top A \mathbf{v} = (A\mathbf{v})^\top (A\mathbf{v}) = \|A\mathbf{v}\|_2^2 > 0 $$
          Thus the Hessian is positive definite, which guarantees strict convexity and a unique global minimum.
          </li>
        </ol>
      </div>

      <h3>P0.6 — Matrix Calculus Practice</h3>
      <p>Compute the gradient with respect to $X \in \mathbb{R}^{n \times n}$ for the following functions:</p>
      <ol type="a">
        <li>$f(X) = \mathrm{tr}(A X B)$, where $A, B$ are constant matrices.</li>
        <li>$f(X) = \mathrm{tr}(X^\top X)$.</li>
        <li>$f(X) = a^\top X \mathbf{b}$, where $a, \mathbf{b}$ are constant vectors.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>$\nabla_X f(X) = A^\top B^\top = (BA)^\top$. (Using $\mathrm{tr}(M X) \to M^\top$).</li>
          <li>$f(X) = \|X\|_F^2$. $\nabla_X f(X) = 2X$.</li>
          <li>$f(X) = \mathrm{tr}(a^\top X \mathbf{b}) = \mathrm{tr}(\mathbf{b} a^\top X)$. Gradient is $(\mathbf{b} a^\top)^\top = a \mathbf{b}^\top$.</li>
        </ol>
      </div>

      <h3>P0.7 — Hessian of a Cubic</h3>
      <p>Let $f(\mathbf{x}) = x_1^3 + x_2^3 + 2x_1 x_2$. Compute the gradient and Hessian. For which $\mathbf{x}$ is the Hessian PSD?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Gradient: $\nabla f = [3x_1^2 + 2x_2, \ 3x_2^2 + 2x_1]^\top$.
        <br>Hessian: $\nabla^2 f = \begin{bmatrix} 6x_1 & 2 \\ 2 & 6x_2 \end{bmatrix}$.
        <br>PSD Condition: Trace $6(x_1+x_2) \ge 0$ and Det $36x_1 x_2 - 4 \ge 0$.
        This requires $x_1+x_2 \ge 0$ and $x_1 x_2 \ge 1/9$. This region (hyperbola branches in 1st quadrant) is where $f$ is locally convex.</p>
      </div>

      <h3>P0.8 — Testing PSD</h3>
      <p>Determine the definiteness of: (a) $\begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$, (b) $\begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$, (c) $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>(a) PD (Det=3, Tr=4). (b) Indefinite (Det=-3 < 0). (c) PSD (Eigenvalues 1, 0).</p>
      </div>

      <h3>P0.9 — Schur Complement</h3>
      <p>Find the range of $x$ for which $M(x) = \begin{bmatrix} x & 1 \\ 1 & x \end{bmatrix} \succeq 0$.
      <br>Using Schur complement: $x > 0$ and $x - 1(1/x)1 \ge 0 \implies x^2 \ge 1 \implies x \ge 1$.
      <br>(Check: Eigenvalues are $x \pm 1$. Need $x-1 \ge 0$).</p>
    </section>

    <!-- SECTION 10: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>Readings & Resources</h2>
      <ul>
        <li>
          <b>Boyd & Vandenberghe, Convex Optimization:</b>
          <ul>
            <li>Appendix A: Mathematical Background (Norms, Analysis, Functions)</li>
            <li>Appendix C: Numerical Linear Algebra (Operations, Factorizations)</li>
          </ul>
        </li>
        <li>
          <b>Gilbert Strang, Introduction to Linear Algebra:</b>
          <ul>
            <li>Chapter 2: Vector Spaces</li>
            <li>Chapter 3: Orthogonality</li>
            <li>Chapter 6: Eigenvalues and Eigenvectors</li>
          </ul>
        </li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="../../static/lib/pyodide/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/resizable.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
