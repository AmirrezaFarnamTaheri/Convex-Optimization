| **1**  | **Gram–Schmidt Orthogonalization**| A step‑by‑step geometric illustration of the Gram–Schmidt process: start with linearly independent vectors (v_1,v_2,v_3), subtract projections to produce orthogonal vectors (u_1,u_2,u_3), and then normalize to obtain an orthonormal basis.  The diagram should show how each new vector is formed by subtracting its projection onto the span of the previous ones.  This fills a gap: existing images in the linear algebra primer cover norms, eigenvalues, rank/nullity and quadratic forms, but there is no visual treatment of orthogonalization.  It will help students understand how one constructs an orthonormal basis in inner‑product spaces, as described in the Gram–Schmidt article. | Gram–Schmidt constructs an orthonormal basis by orthogonalizing and normalizing a linearly independent set.                                     |
| **2**  | **Singular‑Value Decomposition Ellipsoid**| A diagram showing how a matrix (A) acts on the unit sphere, mapping it to an ellipsoid.  The singular values are the lengths of the ellipsoid’s semi‑axes and the singular vectors give the directions.  This geometric view clarifies the meaning of the SVD and complements the existing algebraic diagrams.  The SVD page notes that the singular values correspond to ellipsoid semi‑axes and singular vectors determine the directions.                                                                                                                                                                                                                                                            | The geometric interpretation of SVD links singular values to ellipsoid axis lengths and singular vectors to directions.                         |
| **3**  | **Minkowski Sum and Difference**| Two subfigures: one showing Minkowski addition of convex sets (e.g., adding two squares or a square and a disk), and the other showing Minkowski difference/erosion.  Annotations should explain that the sum of sets (S_1+S_2={x_1+x_2\mid x_1\in S_1,x_2\in S_2}) and illustrate that the Minkowski sum of two compact convex sets is compact and commutes with taking convex hulls.  The current lecture covers convex sets, but there is no visual for Minkowski sums/differences.                                                                                                                                                                                                                  | Minkowski sum definition and example (adding two squares) and properties such as commutativity with convex hulls.                               |
| **4**  | **Projection onto a Convex Set**| A figure illustrating the nearest‑point projection: given a point outside a convex set, draw the perpendicular to the boundary that lands at the unique closest point.  Annotate distances and normal vectors to illustrate why projections minimize Euclidean distance.  There is no dedicated diagram for projections, yet they are fundamental in optimization (later used in proximal methods).                                                                                                                                                                                                                                                                                                     |                                                                                                                                                 |
