<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. Linear Algebra Basics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "../../static/lib/three/three.module.js"
      }
    }
  </script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-linear-algebra-advanced/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>00. Linear Algebra Basics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, review, linear-algebra, foundational</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture establishes the linear algebra foundations required for convex optimization. We begin by defining the core objects—vectors, matrices, and their operations—and progress to the geometry of high-dimensional spaces through inner products, norms, and orthogonality. We then explore the crucial concept of Positive Semidefinite (PSD) matrices, which generalize non-negative numbers to the matrix setting and underpin convexity. Finally, we derive the method of least squares from a geometric perspective via orthogonal projections. This material provides the necessary language to describe convex sets, functions, and optimization problems with precision.</p>
        <p><strong>Prerequisites:</strong> Basic multivariable calculus and familiarity with standard matrix notation.</p>
        <p><strong>Forward Connections:</strong> The projection techniques introduced here are essential for the geometric interpretation of constrained optimization. PSD matrices are the cornerstone of convex quadratic programs (QP) and Semidefinite Programming (SDP) covered in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The four fundamental subspaces provide the geometric intuition for duality theory (<a href="../09-duality/index.html">Lecture 09</a>). Advanced topics such as QR factorization, SVD, and pseudoinverses are covered in <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Analyze Fundamental Subspaces:</b> Identify and compute the dimensions of the four fundamental subspaces of a matrix using the Rank-Nullity Theorem.</li>
        <li><b>Manipulate Norms and Inner Products:</b> Apply standard and generalized inner products and vector norms ($\ell_1, \ell_2, \ell_\infty$) to establish inequalities.</li>
        <li><b>Perform Geometric Operations:</b> Construct orthonormal bases via Gram-Schmidt and compute orthogonal projections onto subspaces and affine sets.</li>
        <li><b>Characterize PSD Matrices:</b> Determine positive semidefiniteness using eigenvalues, variational forms, and the Schur Complement lemma.</li>
        <li><b>Solve Least Squares:</b> Derive and solve the normal equations for overdetermined systems and interpret the solution geometrically.</li>
        <li><b>Apply Matrix Calculus:</b> Compute gradients and Hessians for linear, quadratic, and log-determinant functions.</li>
      </ul>
    </section>

    <!-- SECTION 1: NOTATION -->
    <section class="section-card" id="section-notation">
      <h2>1. Mathematical Atoms: From Fields to Functions</h2>

      <h3>1.1 The Optimization Type System</h3>
      <p>Optimization combines analysis, linear algebra, and geometry: we compose maps, stack constraints, take gradients, and build Lagrangians. To do this effectively, we must treat mathematics as a <b>typed language</b>. Every object has a declared type, and operations are only defined when types match. Most errors in convex optimization are not conceptual—they are <b>type errors</b> disguised as mathematics.</p>

      <h4>The Type System of Optimization</h4>
      <p>Think like a compiler. Every symbol has a declared type.</p>
      <ul>
        <li><b>Scalar:</b> $a \in \mathbb{R}$. Simple numbers used to scale vectors and matrices.</li>
        <li><b>Vector:</b> $x \in \mathbb{R}^n$. An ordered list of $n$ scalars, interpreted as a point in space or a direction.</li>
        <li><b>Matrix:</b> $A \in \mathbb{R}^{m \times n}$. A typed operator that transforms vectors from $\mathbb{R}^n$ to $\mathbb{R}^m$.</li>
        <li><b>Set:</b> $C \subseteq \mathbb{R}^n$. A boolean predicate (membership test) on the space. Constraints define sets.</li>
        <li><b>Function:</b> $f: D \to Y$. A rule assigning values to inputs. Domains are mandatory, not optional.</li>
      </ul>

      <h4>Type Checking Rules</h4>
      <p>Rigorous typing prevents silent errors. You may only add vectors of the same dimension. Matrix-vector multiplication $Ax$ is valid only if the inner dimensions match ($m \times n$ times $n \times 1$). This "shape discipline" is critical when translating math to code (e.g., NumPy or PyTorch), where broadcasting can mask dimensional mismatches.</p>

      <div class="example-box">
        <h4>Example: The Silent Killer</h4>
        <p>Consider the gradient update $\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \nabla f(\mathbf{x}_k)$.
        <br>If $\mathbf{x} \in \mathbb{R}^n$ is a column vector, mathematically $\nabla f(\mathbf{x})$ is often treated as a row vector (living in the dual space). However, in code, adding a shape $(n,)$ array to a shape $(1, n)$ array might trigger broadcasting, resulting in an $(n, n)$ matrix instead of a vector update!
        <br><b>Rigorous Typing:</b> Always enforce that gradients are column vectors in implementation to match the primal variable's shape.</p>
      </div>

      <h3>1.2 Scalars: The Rulebook</h3>
      <p>A <b>scalar</b> is an element of a field, usually $\mathbb{R}$. A <b>field</b> $\mathbb{F}$ is a set equipped with two operations—addition ($+$) and multiplication ($\cdot$)—that behave like standard arithmetic. While most linear algebra courses focus on $\mathbb{R}$ or $\mathbb{C}$, the definitions apply to any field.</p>

      <h4>The Field Axioms</h4>
      <p>A field must satisfy three groups of axioms:</p>
      <ol>
        <li><b>Addition ($\mathbb{F}, +$ is an abelian group):</b> Addition is closed, associative, and commutative. There exists an additive identity $0$ (such that $a+0=a$) and an additive inverse $-a$ (such that $a+(-a)=0$).</li>
        <li><b>Multiplication ($\mathbb{F} \setminus \{0\}, \cdot$ is an abelian group):</b> Multiplication is closed, associative, and commutative. There exists a multiplicative identity $1 \neq 0$ (such that $a \cdot 1 = a$) and a multiplicative inverse $a^{-1}$ for every non-zero $a$ (such that $a a^{-1} = 1$).</li>
        <li><b>Distributivity:</b> Multiplication distributes over addition: $a(b+c) = ab + ac$.</li>
      </ol>

      <div class="proof-box">
        <h4>Fundamental Field Lemmas</h4>
        <p>These proofs demonstrate how algebraic rigor underpins "obvious" arithmetic.</p>
        <div class="proof-step">
          <strong>Lemma 1: $0 \cdot a = 0$ for all $a$.</strong>
          <p>This isn't an axiom; it's a consequence.
          $$ 0\cdot a = (0+0)\cdot a = 0\cdot a + 0\cdot a \quad \text{(Distributivity)} $$
          Subtracting $0 \cdot a$ from both sides yields $0 = 0 \cdot a$.</p>
        </div>
        <div class="proof-step">
          <strong>Lemma 2: $(-1)a = -a$.</strong>
          We check if $(-1)a$ acts as the additive inverse of $a$.
          $$ a + (-1)a = 1\cdot a + (-1)\cdot a = (1 + (-1))a = 0\cdot a = 0 $$
          By uniqueness of the additive inverse, $(-1)a = -a$.
        </div>
        <div class="proof-step">
          <strong>Lemma 3: No Zero Divisors.</strong>
          If $ab=0$ and $a \neq 0$, then $b=0$.
          <br><i>Proof:</i> Since $a \neq 0$, $a^{-1}$ exists. Multiply both sides by $a^{-1}$:
          $$ a^{-1}(ab) = a^{-1}(0) \implies (a^{-1}a)b = 0 \implies 1\cdot b = 0 \implies b=0 $$
        </div>
      </div>

      <h4>Examples and Counterexamples</h4>
      <p>Understanding what <i>breaks</i> when axioms are missing clarifies their importance.</p>
      <ul>
        <li><b>Fields:</b> $\mathbb{Q}$ (rationals), $\mathbb{R}$ (reals), $\mathbb{C}$ (complex), and finite fields $\mathbb{F}_p$ (integers mod prime $p$). In $\mathbb{F}_5$, the inverse of 2 is 3 (since $2 \cdot 3 = 6 \equiv 1 \pmod 5$).</li>
        <li><b>Integers ($\mathbb{Z}$):</b> NOT a field (it is a <b>Ring</b>). The equation $2x=1$ has no solution in $\mathbb{Z}$ because 2 has no multiplicative inverse. Linear algebra over $\mathbb{Z}$ is "Module Theory," where bases are not guaranteed.</li>
        <li><b>Integers mod composite ($\mathbb{Z}_6$):</b> NOT a field.
            <br><i>Failure 1 (Zero Divisors):</i> $2 \cdot 3 = 0 \pmod 6$. Non-zero numbers multiply to zero, destroying the cancellation law ($2x=2y \not\implies x=y$).
            <br><i>Failure 2 (Singular Non-zeros):</i> The number 2 has no inverse; you cannot divide by it. Scaling by 2 is not an injective map.
        </li>
      </ul>

      <p><b>Extended Reals:</b> In optimization, we often use the <b>extended reals</b> $\overline{\mathbb{R}} := \mathbb{R} \cup \{+\infty\}$. This allows encoding constraints directly into the objective function (infinite cost for forbidden regions).</p>

      <h3>1.3 Vectors: Elements of $\mathbb{R}^n$</h3>
      <p>A vector $x \in \mathbb{R}^n$ is an ordered $n$-tuple of scalars, usually treated as a <b>column</b>:
      $$ x = \begin{bmatrix}x_1\\ \vdots\\ x_n\end{bmatrix} $$
      The notation "$\mathbb{R}^n$" asserts that $x$ has exactly $n$ coordinates and lives in a vector space where addition and scaling are defined. Equality is defined componentwise: $x=y$ if and only if $x_i=y_i$ for all $i$. This bridges $n$ scalar equalities into a single vector equation.</p>

      <h3>1.4 Vector Operations</h3>
      <p>Operations on $\mathbb{R}^n$ are defined to be consistent with scalar arithmetic:</p>
      <ol>
        <li><b>Addition:</b> $(x+y)_i := x_i + y_i$. This ensures addition commutes and reduces to scalar addition in each slot.</li>
        <li><b>Scalar Multiplication:</b> $(ax)_i := a x_i$. This ensures scaling distributes: $a(x+y) = ax + ay$.</li>
      </ol>
      <p>This structure satisfies all 8 vector space axioms because each axiom holds in $\mathbb{R}$ at every coordinate. For example, the zero vector $\mathbf{0}$ is the vector of all zeros, and the additive inverse $-\mathbf{x}$ consists of the negated coordinates.</p>

      <h3>1.5 Matrices: Typed Operators</h3>
      <p>A matrix $A \in \mathbb{R}^{m \times n}$ is best viewed as a <b>linear map</b> $A: \mathbb{R}^n \to \mathbb{R}^m$. The dimensions act as types: $n$ columns correspond to the input dimension, and $m$ rows correspond to the output dimension.</p>

      <p><b>Matrix-Vector Multiplication:</b>
      $$ (Ax)_i := \sum_{j=1}^n A_{ij} x_j $$
      This definition satisfies linearity $A(ax+by) = aAx + bAy$ and ensures compatibility with function composition ($A(Bx) = (AB)x$). It offers two essential perspectives:</p>

      <ul>
        <li><b>Row View (Measurement):</b> $(Ax)_i = \langle A_{i:}^\top, x \rangle$. Each row acts as a linear functional (a "sensor") measuring the input $x$.</li>
        <li><b>Column View (Synthesis):</b> $Ax = \sum_{j=1}^n x_j A_{:j}$. The output is a linear combination of the columns of $A$. This is the primary view for analyzing range, span, and feasibility.</li>
      </ul>

      <h3>1.6 The Abstract Vector Space</h3>
      <p>While $\mathbb{R}^n$ is our working model, linear algebra applies to <i>any</i> set that behaves like $\mathbb{R}^n$. This abstraction allows us to treat functions, polynomials, and matrices themselves as vectors.</p>
      <div class="definition-box">
        <h4>Definition: Vector Space</h4>
        <p>A vector space $V$ over a field $\mathbb{F}$ is a set equipped with addition and scalar multiplication satisfying:</p>
        <ul>
            <li><b>Addition:</b> Associative, commutative, has identity $\mathbf{0}$, and every element has an inverse.</li>
            <li><b>Scalar Multiplication:</b> Associative with field multiplication, has identity $1 \cdot u = u$.</li>
            <li><b>Distributivity:</b> Operations interact consistently: $\alpha(u+v) = \alpha u + \alpha v$ and $(\alpha+\beta)u = \alpha u + \beta u$.</li>
        </ul>
      </div>

      <h3>1.7 Sets: The Logic of Feasibility</h3>
      <p>Optimization minimizes a function over a <b>feasible set</b>. Constraints are membership tests defining this set.</p>

      <h4>Constraints as Sets</h4>
      <p>An inequality constraint $g(x) \le 0$ is shorthand for $x \in \{z \in \mathbb{R}^n : g(z) \le 0\}$. Feasibility requires satisfying <i>all</i> constraints simultaneously, which corresponds to the <b>intersection</b> of sets:
      $$ \mathcal{F} = \bigcap_{i=1}^m C_i $$
      Infeasibility means this intersection is empty. This geometric view is crucial because convexity is a property of sets: we prove a problem is convex by showing the intersection of its constraint sets is convex.</p>

      <h4>Preimages and Modeling</h4>
      <p>Most constraints take the form "an expression belongs to a set". Given a function $g: \mathbb{R}^n \to \mathbb{R}^m$ and a target set $D \subseteq \mathbb{R}^m$, the constraint $g(x) \in D$ defines the <b>preimage</b>:
      $$ g^{-1}(D) = \{x \in \mathbb{R}^n : g(x) \in D\} $$
      <b>Modeling Power:</b> If $D$ is convex and $g(x) = Ax+b$ is affine, then $g^{-1}(D)$ is convex. This single rule justifies why linear constraints ($Ax=b$), orthant constraints ($x \ge 0$), and norm constraints ($\|Ax\| \le 1$) all define convex sets.</p>

      <h3>1.8 Functions: Domain Definitions</h3>
      <p>A function $f: D \to Y$ is defined by its domain $D$, codomain $Y$, and mapping rule. In optimization, the domain is not optional—it is a strict constraint.</p>

      <p><b>Implicit Constraints:</b> For $f(x) = -\log x$, the domain $x > 0$ is implicit. An optimization problem minimizing $-\log x$ implicitly includes the constraint $x > 0$. Violating this domain is a logical error, not just a numerical one.</p>

      <p><b>Indicator Functions:</b> We can encode constraints into the objective using the extended reals. The indicator function of a set $C$ is $\delta_C(x) = 0$ if $x \in C$ and $+\infty$ otherwise. Minimizing $f(x)$ over $C$ is equivalent to minimizing the unconstrained function $f(x) + \delta_C(x)$.</p>

      <p><b>Epigraphs:</b> The epigraph $\mathrm{epi}(f) = \{(x, t) : f(x) \le t\}$ links functions to sets. A function is convex if and only if its epigraph is a convex set.</p>

      <h3>1.9 Affine Geometry: Points vs. Vectors</h3>
      <p>In classical geometry, points and vectors are distinct.
      <br><b>Vectors</b> are displacements; they can be added and scaled.
      <br><b>Points</b> are locations; they cannot be added intrinsically (what is "Paris + London"?). However, they can be subtracted to yield a vector ($q - p = v$).</p>

      <p><b>Affine Combinations:</b> While $p+q$ is undefined for points, the weighted sum $\sum \alpha_i p_i$ is a valid point <i>if and only if</i> $\sum \alpha_i = 1$. This restriction makes the result independent of the choice of origin.</p>

      <div class="proof-box">
        <h4>Proof: Affine Combinations are Origin-Independent</h4>
        <p>Let $p_1, \dots, p_k$ be points. Choose an origin $o$ and represent points as vectors $v_i = p_i - o$.
        The computed point is $p = o + \sum \alpha_i v_i$.
        <br>Now choose a different origin $o' = o + t$. The coordinates change to $v_i' = p_i - o' = v_i - t$.
        The computed point in the new system is:
        $$ p' = o' + \sum \alpha_i v_i' = (o+t) + \sum \alpha_i (v_i - t) $$
        $$ = o + t + \sum \alpha_i v_i - (\sum \alpha_i) t $$
        $$ = \left(o + \sum \alpha_i v_i\right) + t (1 - \sum \alpha_i) $$
        For $p'$ to equal $p$ (consistency), the term involving $t$ must vanish. This requires $1 - \sum \alpha_i = 0$, or $\sum \alpha_i = 1$.</p>
      </div>

      <p><b>Relevance:</b> The solution set to $Ax=0$ is a subspace (contains the origin). The solution set to $Ax=b$ (for $b \neq 0$) is an <b>affine set</b> (does not contain the origin). Affine sets are "shifted subspaces".</p>

      <h3>1.10 Linear Combinations: The Fundamental Constructor</h3>
      <p>The finite linear combination $\mathbf{y} = \sum_{i=1}^k \alpha_i \mathbf{v}_i$ is the atomic operation of linear algebra.</p>

      <div style="display: flex; gap: 16px; flex-wrap: wrap; margin: 24px 0;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/05_linear_combinations_lattice.png"
               alt="A lattice of linear combinations generated by two vectors"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Figure 2:</i> The span of two vectors creates a lattice of reachable points.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/layer1_13_linear_combinations.gif"
               alt="Animation showing how varying coefficients sweeps out the plane"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> Varying coefficients $\alpha_1, \alpha_2$ sweeps out the entire plane (the span).</figcaption>
        </figure>
      </div>

      <h4>Systems of Equations as Linear Maps</h4>
      <p>The equation $A\mathbf{x} = \mathbf{b}$ asks a fundamental geometric question: <b>Does $\mathbf{b}$ lie in the image of the linear map defined by $A$?</b></p>

      <ol>
        <li><b>Column Space Interpretation:</b> Using the column view of matrix multiplication, $A\mathbf{x} = x_1 \mathbf{a}_1 + \dots + x_n \mathbf{a}_n$. The equation asks if $\mathbf{b}$ can be synthesized as a linear combination of the columns of $A$.</li>
        <li><b>Consistency:</b> The system is solvable if and only if $\mathbf{b} \in \mathcal{R}(A) = \operatorname{span}\{\mathbf{a}_1, \dots, \mathbf{a}_n\}$. If $\mathbf{b}$ is outside the column space, the system is inconsistent.</li>
        <li><b>Row Reduction:</b> Gaussian elimination is equivalent to left-multiplying by an invertible matrix $E$. Since $E$ is invertible, the solution sets of $A\mathbf{x}=\mathbf{b}$ and $(EA)\mathbf{x} = E\mathbf{b}$ are identical. Row operations change the <i>coordinate representation</i> but preserve the solution geometry.</li>
      </ol>

      <div class="insight">
        <h4>The Mental Model</h4>
        <p>Do not think: <i>"I have equations and unknowns."</i></p>
        <p>Think: <i>"I have a linear map. What outputs can it produce?"</i></p>
        <p>Solving $A\mathbf{x}=\mathbf{b}$ is not algebra first and geometry later. It is <b>geometry first, algebra as a tool</b>.</p>
      </div>

      <h3>1.11 Affine vs Linear Mappings</h3>
      <p>A map $L: \mathbb{R}^n \to \mathbb{R}^m$ is <b>linear</b> if it preserves linear combinations: $L(\alpha x + \beta y) = \alpha L(x) + \beta L(y)$. Linear maps must fix the origin ($L(0)=0$).</p>
      <p>A map $f: \mathbb{R}^n \to \mathbb{R}^m$ is <b>affine</b> if it preserves affine combinations: $f(\theta x + (1-\theta)y) = \theta f(x) + (1-\theta)f(y)$ for all $\theta \in \mathbb{R}$. This implies the form $f(x) = Ax + b$. Affine maps allow for translation.</p>

      <h4>Convexity Invariance</h4>
      <p>Affine maps are the "structural backbone" of convex optimization because they preserve convexity.</p>
      <div class="theorem-box">
        <h4>Theorem: Preservation of Convexity</h4>
        <ul>
            <li><b>Image:</b> If $C \subseteq \mathbb{R}^n$ is convex and $f$ is affine, then the image $f(C)$ is convex.</li>
            <li><b>Preimage:</b> If $D \subseteq \mathbb{R}^m$ is convex and $f$ is affine, then the preimage $f^{-1}(D)$ is convex.</li>
        </ul>
      </div>

      <div class="proof-box">
        <h4>Proof: Preimage Convexity</h4>
        <p>Let $f(x) = Ax+b$ be an affine map and let $C = f^{-1}(D) = \{x \mid Ax+b \in D\}$, where $D$ is convex.
        <br>Take $x_1, x_2 \in C$ and $\theta \in [0,1]$.
        <br>We test if $x_\theta = \theta x_1 + (1-\theta)x_2$ is in $C$.
        $$ f(x_\theta) = A(\theta x_1 + (1-\theta)x_2) + b = \theta (Ax_1+b) + (1-\theta)(Ax_2+b) = \theta y_1 + (1-\theta)y_2 $$
        Since $x_1, x_2 \in C$, their images $y_1, y_2$ are in $D$. Since $D$ is convex, the combination $\theta y_1 + (1-\theta)y_2$ is in $D$.
        <br>Thus $f(x_\theta) \in D$, so $x_\theta \in C$. $C$ is convex.</p>
      </div>

      <h3>1.12 Matrix Algebra from Linear Maps</h3>
      <p>Every linear map $T: V \to W$ between finite-dimensional spaces can be represented by a matrix, once bases are chosen.</p>

      <h4>Matrix Representation Theorem</h4>
      <p>Given bases $\mathcal{B}=\{v_j\}$ for $V$ and $\mathcal{C}=\{w_i\}$ for $W$, there is a unique matrix $A$ such that $[T(x)]_\mathcal{C} = A [x]_\mathcal{B}$.
      <br><b>Construction:</b> The $j$-th column of $A$ is the coordinate vector of the image of the $j$-th basis vector: $\text{Column } j = [T(v_j)]_\mathcal{C}$.
      <br>This theorem bridges abstract operators and concrete arrays. Matrix multiplication corresponds to function composition: if $T \circ S$ represents doing $S$ then $T$, the matrix is $BA$ (where $B$ represents $T$ and $A$ represents $S$).</p>

      <h3>1.13 Independence, Basis, and Dimension</h3>
      <p>A set of vectors is <b>linearly independent</b> if no vector in the set is redundant (i.e., no vector lies in the span of the others). Independence ensures that coordinates are unique.</p>
      <p>A <b>basis</b> is a set that is both independent and spanning. It provides a perfect coordinate system for the space.
      <br><b>Dimension</b> is the number of vectors in a basis. It is an invariant property of the vector space: every basis for $V$ has the same number of elements.</p>

      <div class="theorem-box">
        <h4>Characterizations of Dimension $n$</h4>
        <p>For a vector space of dimension $n$:
        <ol>
            <li>Every basis has exactly $n$ vectors.</li>
            <li>Any linearly independent set has size $\le n$.</li>
            <li>Any spanning set has size $\ge n$.</li>
        </ol>
        </p>
      </div>

      <h3>1.14 Functions and Preimages</h3>
      <p>Recall that set operations are key to defining feasible regions.
      <br><b>Preimages preserve set logic:</b> $f^{-1}(A \cap B) = f^{-1}(A) \cap f^{-1}(B)$. This allows us to define regions by intersecting simple constraints. For example, the feasible set of a linear program is the intersection of halfspaces (preimages of $(-\infty, 0]$ under affine maps).</p>
    </section>

    <!-- SECTION 2: SUBSPACES -->
    <section class="section-card" id="section-subspaces">
      <h2>2. Subspaces and the Four Fundamental Spaces</h2>

      <p>A <a href="#" onclick="event.preventDefault()" class="definition-link">linear subspace</a> is a set of vectors that is closed under addition and scalar multiplication. When a linear map $T: V \to W$ acts, it decomposes the domain and codomain based on <b>what information is lost</b> and <b>what survives</b>.</p>

      <h3>2.1 Kernel and Image</h3>
      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: Kernel and Image</h3>
        <p>Visualize how the kernel (nullspace) and image (column space) relate. Vectors in the kernel map to zero. The image is the set of all reachable outputs.</p>
        <iframe sandbox="allow-scripts allow-same-origin" title="Kernel and Image Demo" src="widgets/la_batch2.html?mode=kernel" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1); width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>

      <div class="example-box">
        <h4>Example: Finding the Kernel</h4>
        <p>Let $A = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix}$. To find $\mathcal{N}(A)$, we solve $A\mathbf{x} = 0$.
        <br>The augmented matrix $\left[\begin{array}{ccc|c} 1 & 2 & 3 & 0 \\ 2 & 4 & 6 & 0 \end{array}\right]$ reduces to $\left[\begin{array}{ccc|c} 1 & 2 & 3 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right]$.
        <br>Here, $x_1$ is a pivot variable, while $x_2$ and $x_3$ are free. The equation $x_1 + 2x_2 + 3x_3 = 0$ implies $x_1 = -2x_2 - 3x_3$.
        <br>In parametric vector form:
        $$ \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = x_2 \begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix} + x_3 \begin{bmatrix} -3 \\ 0 \\ 1 \end{bmatrix} $$
        The kernel is the 2D plane spanned by $(-2, 1, 0)^\top$ and $(-3, 0, 1)^\top$.</p>
      </div>

      <p>For a linear map $T: \mathbb{R}^n \to \mathbb{R}^m$ represented by matrix $A$:</p>
      <ul>
        <li><b>Kernel (Nullspace):</b> $\mathcal{N}(A) = \{\mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = 0\}$.
          <br><i>Meaning:</i> <b>Information Loss.</b> Two inputs $\mathbf{u}, \mathbf{v}$ produce the same output ($A\mathbf{u}=A\mathbf{v}$) if and only if their difference lies in the kernel. It measures the indistinguishable inputs.
        </li>
        <li><b>Image (Column Space):</b> $\mathcal{R}(A) = \{A\mathbf{x} \mid \mathbf{x} \in \mathbb{R}^n\}$.
          <br><i>Meaning:</i> <b>Information Survival.</b> These are the reachable outputs. The equation $A\mathbf{x}=\mathbf{b}$ is solvable iff $\mathbf{b} \in \mathcal{R}(A)$.
        </li>
      </ul>

      <div class="theorem-box">
        <h4>Structure of Solution Sets</h4>
        <p>For a consistent linear system $A\mathbf{x}=\mathbf{b}$, the solution set is an affine subspace:
        $$ \{\text{solutions}\} = \mathbf{x}_p + \mathcal{N}(A) $$
        where $\mathbf{x}_p$ is any particular solution.
        <br><b>Proof intuition:</b>
        <br>1. <b>Closure:</b> If $\mathbf{x}_p$ works, then $\mathbf{x}_p + \mathbf{x}_n$ also works for any $\mathbf{x}_n \in \mathcal{N}(A)$, since $A(\mathbf{x}_p+\mathbf{x}_n) = \mathbf{b} + 0$.
        <br>2. <b>Exhaustion:</b> If $\mathbf{x}$ is any other solution, $A(\mathbf{x}-\mathbf{x}_p) = \mathbf{b}-\mathbf{b}=0$, so the difference must lie in the kernel.
        <br>This implies that $\dim(\text{solution set}) = \dim(\mathcal{N}(A))$. The nullspace controls the <b>degrees of freedom</b> of the solution.</p>

        <h4>Theorem: Injectivity $\iff$ Trivial Kernel</h4>
        <p>A linear map is injective (one-to-one) if and only if its kernel contains only the zero vector.</p>
      </div>

      <h3>2.2 Rank-Nullity: Conservation of Information</h3>
      <p>The <b>Rank-Nullity Theorem</b> is the fundamental conservation law of linear algebra. It states that a linear map cannot create or destroy dimension arbitrarily.</p>
      $$ \boxed{ n = \dim(\mathcal{N}(A)) + \dim(\mathcal{R}(A)) } $$
      <p>Input Information ($n$) = Lost Information (Nullity) + Surviving Information (Rank).
      <br>This implies you cannot map a high-dimensional space injectively into a low-dimensional one (nullity must be positive), nor surjectively map a low-dimensional space onto a high-dimensional one.</p>

      <h3>2.3 The Four Fundamental Subspaces</h3>
      <p>Every linear map $A: \mathbb{R}^n \to \mathbb{R}^m$ induces four intrinsic subspaces, paired by orthogonality.</p>

      <table class="data-table">
        <thead>
          <tr>
            <th>Space</th>
            <th>Notation</th>
            <th>Lives in</th>
            <th>Geometric Meaning</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Column Space</td>
            <td>$\mathcal{R}(A)$</td>
            <td>$\mathbb{R}^m$</td>
            <td>Reachable outputs</td>
          </tr>
          <tr>
            <td>Null Space</td>
            <td>$\mathcal{N}(A)$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Indistinguishable inputs</td>
          </tr>
          <tr>
            <td>Row Space</td>
            <td>$\mathcal{R}(A^\top)$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Active measurement directions</td>
          </tr>
          <tr>
            <td>Left Null Space</td>
            <td>$\mathcal{N}(A^\top)$</td>
            <td>$\mathbb{R}^m$</td>
            <td>Redundant constraints</td>
          </tr>
        </tbody>
      </table>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/four_fundamental_subspaces.gif"
             alt="The four fundamental subspaces under a linear map"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> The four fundamental subspaces. Inputs differing only in the nullspace direction map to the same output.</figcaption>
      </figure>

      <h3>2.4 Orthogonality Relations</h3>
      <p>The four subspaces are locked together by orthogonality. This is the geometric core of the Fundamental Theorem of Linear Algebra.</p>

      <div class="theorem-box">
        <h4>Theorem: Orthogonal Complements</h4>
        <ol>
            <li><b>Domain ($\mathbb{R}^n$):</b> $\mathcal{N}(A) = \mathcal{R}(A^\top)^\perp$. The nullspace is orthogonal to the row space.</li>
            <li><b>Codomain ($\mathbb{R}^m$):</b> $\mathcal{N}(A^\top) = \mathcal{R}(A)^\perp$. The left nullspace is orthogonal to the column space.</li>
        </ol>
      </div>

      <div class="proof-box">
        <h4>Proof: $\mathcal{N}(A) = \mathcal{R}(A^\top)^\perp$</h4>
        <div class="proof-step">
          <strong>Step 1: $\mathcal{N}(A) \subseteq \mathcal{R}(A^\top)^\perp$.</strong>
          Let $\mathbf{x} \in \mathcal{N}(A)$, so $A\mathbf{x} = 0$.
          Writing $A$ as rows $r_1^\top, \dots, r_m^\top$, this means $r_i^\top \mathbf{x} = 0$ for all $i$.
          Since $\mathbf{x}$ is orthogonal to every row, it is orthogonal to their span (the row space).
        </div>
        <div class="proof-step">
          <strong>Step 2: $\mathcal{R}(A^\top)^\perp \subseteq \mathcal{N}(A)$.</strong>
          Let $\mathbf{x}$ be orthogonal to the row space. Then $\mathbf{x}$ is orthogonal to each row $r_i$.
          The product $A\mathbf{x}$ is simply the vector of dot products $[r_1^\top \mathbf{x}, \dots, r_m^\top \mathbf{x}]^\top$.
          Since all dot products are zero, $A\mathbf{x}=0$, so $\mathbf{x} \in \mathcal{N}(A)$.
        </div>
      </div>

      <h4>The Fredholm Alternative</h4>
      <p>Because $\mathcal{R}(A)^\perp = \mathcal{N}(A^\top)$, a system $A\mathbf{x}=\mathbf{b}$ has a solution ($\mathbf{b} \in \mathcal{R}(A)$) if and only if $\mathbf{b}$ is orthogonal to every vector in the left nullspace ($\mathbf{b} \perp \mathcal{N}(A^\top)$).
      <br>This implies that for any $\mathbf{b}$, exactly one of the following holds:
      <ol>
        <li>$A\mathbf{x} = \mathbf{b}$ has a solution.</li>
        <li>There exists $\mathbf{y}$ such that $A^\top \mathbf{y} = 0$ and $\mathbf{y}^\top \mathbf{b} \neq 0$.</li>
      </ol>
      This "theorem of alternatives" is the basis for duality theory.</p>

      <div class="example-box">
        <h4>Geometric Example: $\mathbb{R}^3$</h4>
        <p>Let $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}$.
        <br><b>Column Space:</b> The $xy$-plane ($\mathbf{z}=0$).
        <br><b>Left Nullspace:</b> Solving $A^\top \mathbf{y} = 0$ yields $\mathbf{y} = (0, 0, y_3)^\top$, the $\mathbf{z}$-axis.
        <br><b>Orthogonality:</b> The $xy$-plane is orthogonal to the $\mathbf{z}$-axis.</p>
      </div>

      <div class="proof-box">
        <h4>Proof of the Rank-Nullity Theorem</h4>
        <p>We use the orthogonality to prove the dimension formula.</p>
        <div class="proof-step">
          <strong>Step 1: Basis Construction.</strong> Let $v_1, \dots, v_k$ be a basis for $\mathcal{N}(A)$ (dimension $k$). Extend this to a basis for $\mathbb{R}^n$ by adding $w_1, \dots, w_r$ (where $r = n-k$).
        </div>
        <div class="proof-step">
          <strong>Step 2: Image of Basis.</strong> Consider the vectors $Aw_1, \dots, Aw_r$. These lie in the column space.
          Any vector $\mathbf{x} \in \mathbb{R}^n$ is a combination of $v$'s and $w$'s. Since $Av_i = 0$, $A\mathbf{x}$ is a combination only of $Aw_j$'s. Thus $\{Aw_1, \dots, Aw_r\}$ spans $\mathcal{R}(A)$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Independence.</strong> Suppose $\sum c_j Aw_j = 0$. Then $A(\sum c_j w_j) = 0$.
          This means $\sum c_j w_j \in \mathcal{N}(A)$.
          But the only vector in the span of $w$'s that is also in the nullspace (span of $v$'s) is the zero vector (by basis construction).
          Thus $\sum c_j w_j = 0$. Since $w$'s are independent, all $c_j = 0$.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> $\{Aw_1, \dots, Aw_r\}$ is a basis for $\mathcal{R}(A)$.
          Rank = $r = n - k = n - \text{Nullity}$.
        </div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p><b>Explore the Four Fundamental Subspaces:</b> Define a matrix and visualize how its column space, row space, nullspace, and left nullspace relate. The tool shows the orthogonality relationships explicitly.</p>
        <iframe src="widgets/la_batch2.html?mode=kernel" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>
    </section>

    <!-- SECTION 3: ALGEBRAIC INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>3. Algebraic Invariants: Determinant, Trace, and Eigenvalues</h2>

      <p>While a matrix $A$ represents a linear map in a specific basis, we are often interested in properties that are intrinsic to the map itself, independent of the basis. These are called <b>invariants</b>.</p>

      <h3>3.1 Trace, Determinant, and Eigenvalues</h3>
      <p>For a square matrix $A \in \mathbb{R}^{n \times n}$ with eigenvalues $\lambda_1, \dots, \lambda_n$ (counted with algebraic multiplicity):</p>
      <ul>
        <li><b>Trace:</b> $\mathrm{tr}(A) = \sum_{i=1}^n A_{ii} = \sum_{i=1}^n \lambda_i$.</li>
        <li><b>Determinant:</b> $\det(A) = \prod_{i=1}^n \lambda_i$.</li>
        <li><b>Eigenvalues:</b> Roots of the characteristic polynomial $p_A(\lambda) = \det(A - \lambda I)$.
            <br><i>Definition:</i> $\lambda$ is an eigenvalue if there exists $v \neq 0$ such that $Av = \lambda v$.
            <br><i>Geometric Meaning:</i> $v$ is an <b>invariant direction</b>. The matrix $A$ maps the line spanned by $v$ into itself, scaling it by $\lambda$.
            <br><i>Dynamics Intuition:</i> If $x_{k+1} = Ax_k$, components of the state aligned with $v$ grow or decay according to $\lambda^k$. This determines the stability of the system.
        </li>
      </ul>

      <div class="definition-box">
        <h4>Diagonalization Criterion</h4>
        <p>A matrix $A$ is <b>diagonalizable</b> ($A = PDP^{-1}$) if and only if:
        <br>1. For every eigenvalue, the <b>geometric multiplicity</b> (dimension of eigenspace) equals the <b>algebraic multiplicity</b> (root multiplicity).
        <br>2. Equivalently, there exists a basis of eigenvectors.
        <br><i>Note:</i> Symmetric matrices are <b>always</b> diagonalizable (Spectral Theorem), and moreover, they are orthogonally diagonalizable ($A = Q \Lambda Q^\top$).</p>
      </div>

      <div class="insight">
        <h4>Geometric Interpretation of the Determinant</h4>
        <p>The determinant is a single number that measures three things at once:</p>
        <ol>
          <li><b>Volume Scaling:</b> $|\det(A)|$ is the factor by which the linear map multiplies $n$-dimensional volume. A unit cube becomes a parallelepiped with volume $|\det(A)|$.</li>
          <li><b>Orientation:</b> The sign of $\det(A)$ tells you if the map preserves "handedness" (positive) or flips it like a mirror (negative).</li>
          <li><b>Invertibility (The Squash Test):</b> $\det(A) = 0$ if and only if the map "squashes" the space into a lower dimension (volume becomes zero). This is the ultimate test for singularity.</li>
        </ol>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/determinant-volume-scaling.png"
             alt="A unit square mapped by a linear transformation to a parallelogram with area |det(A)|"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 5:</i> Determinant as volume scaling: a unit square maps to a parallelogram of area $|\det(A)|$; the sign of $\det(A)$ tracks whether orientation is preserved or flipped.</figcaption>
      </figure>

      <div style="display: flex; gap: 16px; flex-wrap: wrap; margin: 24px 0;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/determinant_area_orientation.gif"
               alt="Determinant as signed area scaling"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> The determinant as signed area: $|\det(A)|$ is the area scale factor; the sign indicates orientation.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/determinant_column_dependence.gif"
               alt="Determinant approaching zero as columns become dependent"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> As columns become dependent, the parallelogram flattens and $\det(A) \to 0$. This visually demonstrates the "Squash Test".</figcaption>
        </figure>
      </div>

      <div class="insight">
        <h4>Diagonalization and the Eigenbasis</h4>
        <p>When a matrix is diagonalizable, there exists a basis of eigenvectors. In this <b>eigenbasis</b>, the matrix acts as pure independent scaling along each coordinate axis. The transformation $A = PDP^{-1}$ separates "change of basis" from "scaling":</p>
        <ul>
          <li>$P^{-1}$: Convert input from standard to eigenbasis</li>
          <li>$D$: Scale each coordinate independently by its eigenvalue</li>
          <li>$P$: Convert back to standard basis</li>
        </ul>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/diagonalization_eigenbasis.gif"
             alt="In the eigenbasis, a linear map becomes pure axis scaling"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> Left: In standard coordinates, a circle becomes an ellipse. Right: In eigen-coordinates, the same map is pure axis scaling. Diagonalization reveals the hidden simplicity.</figcaption>
      </figure>

      <h3>3.2 The "Isotropic Scaling" Lemma</h3>
      <p>While eigenvalues describe the behavior of $A$ along specific directions, sometimes the matrix acts uniformly in all directions. A classic result connects this geometric isotropy to the algebraic structure of the matrix. We ask: what if <i>every</i> vector is an eigenvector?</p>
      <div class="theorem-box">
        <h4>Lemma: Scalar Matrices</h4>
        <p>If every non-zero vector $\mathbf{x} \in \mathbb{R}^n$ is an eigenvector of $A$ (i.e., $A\mathbf{x} = \lambda(\mathbf{x})\mathbf{x}$), then $A$ must be a scalar multiple of the identity: $A = \lambda I$.</p>
      </div>
      <div class="proof-box">
        <h4>Proof of the Isotropic Scaling Lemma</h4>
        <div class="proof-step">
            <strong>Step 1: Linearly Independent Vectors Share Eigenvalues.</strong>
            Let $\mathbf{u}, \mathbf{v}$ be linearly independent eigenvectors with values $\lambda_u, \lambda_v$.
            Consider $\mathbf{w} = \mathbf{u}+\mathbf{v}$. By assumption, $\mathbf{w}$ is an eigenvector with value $\lambda_w$:
            $$ A(\mathbf{u}+\mathbf{v}) = \lambda_w (\mathbf{u}+\mathbf{v}) = \lambda_w \mathbf{u} + \lambda_w \mathbf{v} $$
            By linearity:
            $$ A(\mathbf{u}+\mathbf{v}) = A\mathbf{u} + A\mathbf{v} = \lambda_u \mathbf{u} + \lambda_v \mathbf{v} $$
            Equating coefficients (unique by independence): $\lambda_w = \lambda_u$ and $\lambda_w = \lambda_v$. Thus $\lambda_u = \lambda_v$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Linearly Dependent Vectors Share Eigenvalues.</strong>
            Let $\mathbf{v} = c\mathbf{u}$ ($c \neq 0$). Pick a third vector $\mathbf{z}$ independent of $\mathbf{u}$ (exists if $n \ge 2$).
            From Step 1, $\lambda_u = \lambda_z$ and $\lambda_v = \lambda_z$. Thus $\lambda_u = \lambda_v$.
        </div>
        <div class="proof-step">
            <strong>Step 3: Conclusion.</strong>
            All vectors have the same eigenvalue $\lambda$. Thus $A\mathbf{x} = \lambda \mathbf{x} = (\lambda I)\mathbf{x}$ for all $\mathbf{x}$, so $A = \lambda I$.
        </div>
      </div>
      <p><b>Geometric Intuition:</b> A general symmetric matrix maps the unit sphere to an ellipsoid. The eigenvectors correspond to the axes of the ellipsoid. If <i>every</i> vector is an eigenvector, then every direction is a principal axis. The only ellipsoid with this symmetry is a sphere (isotropic scaling).</p>

      <h3>3.3 Algebraic Properties</h3>

      <h4>Linearity of Trace</h4>
      <p>The trace is a linear functional:
      $$ \mathrm{tr}(A + B) = \mathrm{tr}(A) + \mathrm{tr}(B) \quad \text{and} \quad \mathrm{tr}(cA) = c\,\mathrm{tr}(A). $$
      </p>

      <h4>Multiplicativity of Determinant</h4>
      <p>The determinant is multiplicative:
      $$ \det(AB) = \det(A)\det(B). $$
      This reflects the geometric fact that the volume scaling of a composite map is the product of the individual scalings. Note that $\det(AB) = \det(BA)$ even if $AB \neq BA$.
      </p>

      <h4>Trace of a Product (Cyclic Property)</h4>
      <p>In general, $\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$. However, the trace is <b>cyclic</b>:
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA). $$
      More generally, $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) = \mathrm{tr}(CAB)$. This property is crucial for deriving matrix gradients.
      </p>

      <div class="proof-box">
        <h4>Proof: $\mathrm{tr}(AB) = \mathrm{tr}(BA)$</h4>
        <p>
        $$ \mathrm{tr}(AB) = \sum_{i=1}^m (AB)_{ii} = \sum_{i=1}^m \sum_{k=1}^n a_{ik} b_{ki} $$
        Switching the order of summation (valid for finite sums):
        $$ = \sum_{k=1}^n \sum_{i=1}^m b_{ki} a_{ik} = \sum_{k=1}^n (BA)_{kk} = \mathrm{tr}(BA) $$
        </p>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/trace_sum_eigenvalues_constant.gif"
             alt="Trace as sum of eigenvalues: eigenvalues trade off while trace remains constant"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> The trace equals the sum of eigenvalues. As the matrix deforms, $\lambda_1$ increases and $\lambda_2$ decreases, but $\mathrm{tr}(A) = \lambda_1 + \lambda_2$ remains constant. The trace is a "total scaling budget".</figcaption>
      </figure>


      <div class="example-box">
        <h4>Worked Example: Trace and Determinant Caveats</h4>
        <p>While $\mathrm{tr}(A+B) = \mathrm{tr}(A) + \mathrm{tr}(B)$, other properties are subtler.</p>

        <p><b>1. Trace of Product:</b> $\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$.
        <br>Let $A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $B = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$.
        <br>$\mathrm{tr}(A)=1, \mathrm{tr}(B)=1 \implies \mathrm{tr}(A)\mathrm{tr}(B)=1$.
        <br>But $AB = 0$, so $\mathrm{tr}(AB) = 0$. Thus multiplicativity fails.</p>

        <p><b>2. Determinant of Sum:</b> $\det(A+B) \neq \det(A) + \det(B)$.
        <br>A nice conceptual counterexample: Take $A = I$ and $B = -I$ (for even $n$).
        <br>$\det(A) = 1$, $\det(B) = (-1)^n = 1$.
        <br>$\det(A+B) = \det(0) = 0$.
        <br>But $\det(A) + \det(B) = 1 + 1 = 2 \neq 0$.
        <br>This inequality is true generically; $\det$ is multiplicative, not additive.</p>
      </div>

      <h3>3.4 Similarity and Basis Independence</h3>
      <p>Two matrices $A$ and $B$ are <b>similar</b> if $B = P A P^{-1}$ for some invertible $P$. This represents the same linear operator in a different basis. The mechanics of this coordinate transformation and its connection to diagonalization are covered in depth in <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a>.</p>

      <div class="theorem-box">
        <h4>Theorem: Invariance under Similarity</h4>
        <p>If $A$ and $B$ are similar ($B = P A P^{-1}$), they share the same algebraic invariants:</p>
        <ul>
          <li>$\det(B) = \det(P)\det(A)\det(P^{-1}) = \det(A)$.</li>
          <li>$\mathrm{tr}(B) = \mathrm{tr}(AP^{-1}P) = \mathrm{tr}(A)$.</li>
          <li>$p_{B}(\lambda) = \det(P(A-\lambda I)P^{-1}) = p_A(\lambda)$.</li>
        </ul>
        <p>Thus, they have the <b>same eigenvalues</b>.</p>
      </div>

      <h3>3.5 Spectral Shift</h3>

      <p>Adding a scalar multiple of the identity to a matrix shifts all eigenvalues uniformly while preserving eigenvectors and their multiplicities. This simple operation has profound consequences for conditioning, positive definiteness, and algorithm design.</p>

      <div class="theorem-box">
        <h4>Theorem: Spectral Shift Formula</h4>
        <p>If $A\mathbf{x} = \lambda \mathbf{x}$, then:
        $$ (A + tI)\mathbf{x} = A\mathbf{x} + tI\mathbf{x} = (\lambda + t)\mathbf{x} $$
        Thus, the eigenvalues of $A+tI$ are $\{\lambda_i(A) + t\}_{i=1}^n$, and the eigenvectors remain unchanged.</p>
      </div>

      <h4>Applications and Consequences</h4>

      <div class="subsection">
        <h5>1. Ensuring Positive Definiteness</h5>
        <p>If $A$ is symmetric with minimum eigenvalue $\lambda_{\min}(A)$, then $A + tI$ is positive definite iff:
        $$ t > -\lambda_{\min}(A) $$
        This gives a simple way to "regularize" an indefinite or singular matrix. For example, <b>ridge regression</b> replaces $X^\top X$ (which may be singular) with $X^\top X + \lambda I$ (always invertible for $\lambda > 0$).</p>

        <div class="example-box">
          <h4>Example: Making a Gram Matrix Invertible</h4>
          <p>Consider the matrix $A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$. Its eigenvalues are $2$ and $0$. It is singular (non-invertible) because of the zero eigenvalue.
          <br><b>Goal:</b> We want to invert it to solve a system, but we can't.
          <br><b>Fix:</b> Add a "ridge" $\epsilon I$ with $\epsilon = 0.1$.
          $$ A + 0.1 I = \begin{bmatrix} 1.1 & 1 \\ 1 & 1.1 \end{bmatrix} $$
          The new eigenvalues are $2.1$ and $0.1$. Both are strictly positive. The matrix is now invertible, with determinant $1.1^2 - 1 = 0.21$.
          <br>This technique, known as <b>Tikhonov regularization</b> or "ridge regression," trades a small amount of bias (modifying the matrix) for numerical stability (invertibility).</p>
        </div>
      </div>

      <div class="subsection">
        <h5>2. Controlling Condition Number</h5>
        <p>The <b>condition number</b> $\kappa(A) = \frac{\lambda_{\max}}{\lambda_{\min}}$ measures numerical stability. Adding $tI$ modifies the condition number:
        $$ \kappa(A + tI) = \frac{\lambda_{\max}(A) + t}{\lambda_{\min}(A) + t} $$
        For $t > 0$, this <b>reduces</b> the condition number (bringing eigenvalues closer together), improving numerical stability at the cost of changing the problem.</p>

        <div class="example-box">
          <h4>Example: Improving Conditioning</h4>
          <p>Suppose $A$ has $\lambda_{\max} = 100$ and $\lambda_{\min} = 0.01$, so $\kappa(A) = 10{,}000$ (very ill-conditioned).
          <br>Adding $t = 10$: $\kappa(A + 10I) = \frac{110}{10.01} \approx 11$ (well-conditioned).</p>
        </div>
      </div>

      <div class="subsection">
        <h5>3. Proximal Algorithms and Augmented Lagrangians</h5>
        <p>In optimization, the <b>proximal operator</b> for a quadratic function $f(x) = \frac{1}{2}x^\top A x + b^\top x$ is:
        $$ \mathrm{prox}_{t f}(y) = \arg\min_x \left( f(x) + \frac{1}{2t}\|x - y\|^2 \right) $$
        Setting the gradient to zero gives $(A + \frac{1}{t}I)x = \frac{1}{t}y - b$. The spectral shift $A \to A + \frac{1}{t}I$ ensures the inverse exists and controls the step size.</p>
      </div>

      <div class="insight">
        <h4>Key Insight: Why Spectral Shift is Special</h4>
        <p>Unlike arbitrary matrix addition, $A + tI$ preserves:</p>
        <ul>
          <li><b>Eigenvectors:</b> The geometric structure (principal axes) remains unchanged</li>
          <li><b>Symmetry:</b> If $A \in \mathbb{S}^n$, then $A + tI \in \mathbb{S}^n$</li>
          <li><b>Positive semidefiniteness:</b> If $A \succeq 0$ and $t \ge 0$, then $A + tI \succeq 0$</li>
          <li><b>Simplicity:</b> The transformation $\lambda \to \lambda + t$ is trivial to analyze</li>
        </ul>
        <p>This makes spectral shift a "surgery with no side effects" - you can adjust eigenvalues without disturbing the eigen-structure.</p>
      </div>

      <h4>Warning: Eigenvalue Addition is Not Matrix Addition</h4>
      <p>The spectral shift property $\lambda(A+tI) = \lambda(A) + t$ does <b>not</b> generalize to arbitrary matrices:</p>
      <ul>
        <li>$\det(A+B) \neq \det(A) + \det(B)$ in general. (Example: $\det(I+I) = 2^n \neq 1+1 = 2$)</li>
        <li>$\lambda(A+B) \neq \lambda(A) + \lambda(B)$ in general. (The eigenvalues of a sum do not equal the sum of eigenvalues)</li>
        <li>Eigenvectors of $A+B$ generally differ from those of $A$ and $B$</li>
      </ul>
      <p>The identity matrix $I$ is the <b>unique</b> matrix that commutes with all matrices, making $A + tI$ preserve eigenvectors.</p>

    </section>


    <section class="section-card" id="section-norms">
      <h2>4. Inner Products & Norms: Geometry from Algebra</h2>
      <p>Inner products and norms bridge the gap between abstract vector spaces and geometry. They allow us to talk about <b>angles</b>, <b>lengths</b>, <b>distances</b>, and <b>projections</b>. Without these, a vector space is just a "floppy" elastic sheet; with them, it becomes a rigid structure capable of supporting optimization.</p>

      <h3>4.1 Inner Products</h3>
      <p>An <b>inner product</b> on a real vector space $V$ is a function $\langle \cdot, \cdot \rangle: V \times V \to \mathbb{R}$ satisfying three axioms for all $x, y, z \in V$ and $\alpha \in \mathbb{R}$:</p>
      <ol>
        <li><b>Linearity (in first argument):</b> $\langle \alpha x + y, z \rangle = \alpha \langle x, z \rangle + \langle y, z \rangle$.</li>
        <li><b>Symmetry:</b> $\langle x, y \rangle = \langle y, x \rangle$.</li>
        <li><b>Positive Definiteness:</b> $\langle x, x \rangle \ge 0$, and $\langle x, x \rangle = 0 \iff x = 0$.</li>
      </ol>
      <p>Note: Linearity + Symmetry implies bilinearity. Positive definiteness is what allows us to define "length".</p>

      <h4>Verification for the Dot Product</h4>
      <p>For $\langle x,y \rangle = x^\top y$:
      <br>1. <b>Symmetry:</b> $\sum x_i y_i = \sum y_i x_i$.
      <br>2. <b>Linearity:</b> $\sum (\alpha x_i + \beta z_i) y_i = \alpha \sum x_i y_i + \beta \sum z_i y_i$.
      <br>3. <b>Positivity:</b> $\sum x_i^2 \ge 0$. Sum of squares is zero iff every $x_i=0$.
      <br>Thus, the dot product transforms $\mathbb{R}^n$ into a <b>Euclidean Space</b>.</p>

      <h4>Weighted Inner Products ($P$-Geometry)</h4>
      <p>Any symmetric positive definite matrix $P \in \mathbb{S}^n_{++}$ defines a generalized inner product:
      $$ \langle x, y \rangle_P := x^\top P y $$
      <b>Induced Norm:</b> $\|x\|_P = \sqrt{x^\top P x}$.
      <br><b>Geometric Meaning:</b> If $P = L^\top L$ (Cholesky), then $\|x\|_P = \|Lx\|_2$. This is Euclidean geometry after a linear transformation $L$. The unit ball $\{x : x^\top P x \le 1\}$ is an ellipsoid.
      <br>If $P \succeq 0$ (PSD) but singular, we get a <b>semi-inner product</b> (lengths can be zero for non-zero vectors).</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/vector-addition-parallelogram.png"
             alt="Parallelogram law for vector addition"
             style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 3:</i> The parallelogram law, an identity satisfied only by norms induced by inner products.</figcaption>
      </figure>

      <h3>4.2 From Algebra to Geometry</h3>
      <p>Once you have an inner product, you define:</p>
      <ul>
        <li><b>Norm:</b> $\|x\| = \sqrt{\langle x, x \rangle}$.</li>
        <li><b>Orthogonality:</b> $x \perp y \iff \langle x, y \rangle = 0$.</li>
        <li><b>Angles:</b> $\cos \theta = \frac{\langle x, y \rangle}{\|x\| \|y\|}$ (valid by Cauchy-Schwarz).</li>
      </ul>

      <div class="proof-box">
        <h4>The Cauchy-Schwarz Inequality</h4>
        <p><b>Theorem:</b> $|\langle x, y \rangle| \le \|x\| \|y\|$. Equality holds iff $x$ and $y$ are collinear.
        <br><i>Proof:</i> Consider the function $\phi(t) = \|x - ty\|^2 = \langle x - ty, x - ty \rangle$. By positive definiteness, $\phi(t) \ge 0$ for all $t \in \mathbb{R}$.
        Expanding using bilinearity:
        $$ \phi(t) = \langle x, x \rangle - 2t\langle x, y \rangle + t^2\langle y, y \rangle = \|x\|^2 - 2t\langle x, y \rangle + t^2\|y\|^2 $$
        This is a quadratic polynomial $P(t) = at^2 + bt + c$ in $t$.
        Since it is non-negative for all $t$, its <b>discriminant must be non-positive</b> ($b^2 - 4ac \le 0$).
        $$ (-2\langle x, y \rangle)^2 - 4(\|y\|^2)(\|x\|^2) \le 0 $$
        $$ 4\langle x, y \rangle^2 \le 4\|x\|^2 \|y\|^2 \implies |\langle x, y \rangle| \le \|x\| \|y\| $$
        </p>
      </div>

      <h4>Derivation: Triangle Inequality</h4>
      <p>
      $$ \begin{aligned}
      \|x+y\|^2 &= \langle x+y, x+y \rangle = \|x\|^2 + 2\langle x, y \rangle + \|y\|^2 \\
      &\le \|x\|^2 + 2|\langle x, y \rangle| + \|y\|^2 \quad (\text{by Cauchy-Schwarz}) \\
      &\le \|x\|^2 + 2\|x\|\|y\| + \|y\|^2 = (\|x\| + \|y\|)^2
      \end{aligned} $$
      Taking square roots yields $\|x+y\| \le \|x\| + \|y\|$.
      </p>

      <h3>4.3 Norms: The Mental Model and Axioms</h3>

      <div class="insight">
        <h4>The Mental Model: Unit Balls and Shapes</h4>
        <p>In finite dimensions, a norm is completely determined by its <b>unit ball</b> $\mathcal{B} = \{x : \|x\| \le 1\}$. The norm tells you the "shape" of the space.</p>
        <ul>
            <li><b>$\ell_2$ (Euclidean):</b> The unit ball is a <b>sphere</b>. It has rotational symmetry. It measures "energy" or straight-line distance.</li>
            <li><b>$\ell_1$ (Manhattan):</b> The unit ball is a <b>diamond</b> (or cross-polytope). It has "corners" on the axes. This shape is why minimizing an $\ell_1$ norm encourages sparsity (solutions landing on corners).</li>
            <li><b>$\ell_\infty$ (Max):</b> The unit ball is a <b>square</b> (or hypercube). It has flat faces aligned with axes. It measures the "worst-case" coordinate.</li>
        </ul>
        <p>Optimization problems like "minimize $\|x\|$ subject to constraints" are geometrically equivalent to "find the smallest scaled copy of the unit ball that touches the feasible set."</p>
      </div>

      <p>These requirements are formalized in the definition of a norm $\|\cdot\|:\mathbb{R}^n\to\mathbb{R}$:</p>

      <div class="definition-box">
        <h4>(N1) Nonnegativity + Definiteness</h4>
        <p>1. $\|x\| \ge 0$.</p>
        <p>2. $\|x\| = 0 \iff x = 0$.</p>
        <p><i>What it enforces:</i> "Non-negative length" prevents physical nonsense. Definiteness forbids "invisible nonzero vectors"—if a nonzero vector had zero cost, optimization would break.</p>
      </div>

      <div class="definition-box">
        <h4>(N2) Positive Homogeneity</h4>
        <p>$$ \|\alpha x\| = |\alpha| \, \|x\| $$</p>
        <p><i>What it enforces:</i> If you double the vector, you double its size. The absolute value matters: direction reversal ($\alpha < 0$) should not change size.</p>
      </div>

      <div class="definition-box">
        <h4>(N3) Triangle Inequality</h4>
        <p>$$ \|x+y\| \le \|x\| + \|y\| $$</p>
        <p><i>What it enforces:</i> The "no cheating by zig-zagging" law. The straight path is always the shortest. This axiom is the main engine behind convexity.</p>
      </div>

      <h3>4.4 Core Lemmas: Reusable Proof Templates</h3>
      <p>From these three axioms, we can derive powerful properties that you will use constantly in proofs.</p>

      <h4>Lemma A: $\|0\|=0$</h4>
      <p>Take any $x$. Then $0 = 0 \cdot x$. By (N2), $\|0\| = \|0 \cdot x\| = |0| \, \|x\| = 0$. This doesn't need to be an axiom; it's a consequence.</p>

      <h4>Lemma 1: Symmetry $\|x\| = \|-x\|$</h4>
      <p>By (N2) with $\alpha = -1$: $\|-x\| = \|(-1)x\| = |-1| \, \|x\| = \|x\|$. Norms are centrally symmetric functions.</p>

      <h4>Lemma 2: Reverse Triangle Inequality</h4>
      <p>$$ \big| \|x\| - \|y\| \big| \le \|x - y\| $$</p>
      <p>This says that the norm function is <b>1-Lipschitz</b> with respect to itself. Norms cannot change faster than the distance they induce.</p>
      <div class="proof-box">
        <h4>Proof of Reverse Triangle Inequality</h4>
        <div class="proof-step">
          <strong>Step 1:</strong> Write $x = (x-y) + y$. Apply Triangle Inequality:
          $$ \|x\| = \|(x-y) + y\| \le \|x-y\| + \|y\| \implies \|x\| - \|y\| \le \|x-y\| $$
        </div>
        <div class="proof-step">
          <strong>Step 2:</strong> Swap $x$ and $y$.
          $$ \|y\| - \|x\| \le \|y-x\| = \|x-y\| $$
        </div>
        <div class="proof-step">
          <strong>Step 3:</strong> Combine.
          $$ -\|x-y\| \le \|x\| - \|y\| \le \|x-y\| \iff \big| \|x\| - \|y\| \big| \le \|x-y\| $$
        </div>
      </div>

      <h4>Lemma 3: "Two-point Jensen" (Convexity Inequality)</h4>
      <p>For $\theta \in [0, 1]$:</p>
      $$ \|\theta x + (1-\theta)y\| \le \theta \|x\| + (1-\theta) \|y\| $$
      <p>This proves that <b>all norms are convex functions</b>. The proof is purely algebraic:</p>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>$$ \|\theta x + (1-\theta)y\| \le \|\theta x\| + \|(1-\theta)y\| \quad \text{(Triangle Inequality)} $$</p>
        <p>$$ = |\theta| \|x\| + |1-\theta| \|y\| \quad \text{(Homogeneity)} $$</p>
        <p>Since $\theta \in [0, 1]$, $|\theta|=\theta$ and $|1-\theta|=1-\theta$. Thus:</p>
        <p>$$ = \theta \|x\| + (1-\theta) \|y\| $$</p>
      </div>

      <h3>4.5 Norms Induce Metrics</h3>
      <p>Given any norm $\|\cdot\|$, we can define a distance function (metric) $d(x, y) := \|x - y\|$. This metric inherits all required properties:</p>
      <ol>
        <li><b>Non-negativity:</b> $d(x,y) \ge 0$, and $0$ iff $x=y$ (from N1).</li>
        <li><b>Symmetry:</b> $d(x,y) = \|x-y\| = \|-(y-x)\| = \|y-x\| = d(y,x)$ (from N2).</li>
        <li><b>Triangle Inequality:</b> $d(x,z) = \|x-z\| = \|(x-y)+(y-z)\| \le \|x-y\| + \|y-z\| = d(x,y) + d(y,z)$ (from N3).</li>
      </ol>
      <p>So norms are not just "sizes"; they generate "distances." This is why we can talk about convergence, limits, and continuity in vector spaces.</p>

      <h3>4.6 Standard Norms ($\ell_2, \ell_1, \ell_\infty$)</h3>
      <p>The choice of $p$ in the $\ell_p$ norm corresponds to a choice of "spikiness sensitivity"—what kind of "badness" we are measuring.</p>

      <div class="insight">
        <h4>Deep Intuition: Sensitivity to Spikes</h4>
        <p>Consider a vector representing errors $x = (10, 1, 1, 1)$. How "big" is this error?</p>
        <ul>
            <li><b>$\ell_1$ (Sum of Magnitudes):</b> $\|x\|_1 = 10+1+1+1 = 13$. It treats every unit of error equally. It sees the "total mass" of the error. It is "democratic" in that every non-zero entry contributes.</li>
            <li><b>$\ell_2$ (Energy):</b> $\|x\|_2 = \sqrt{100+1+1+1} \approx 10.15$. The large coordinate (10) dominates because of the squaring. The small ones matter less. This norm is "aristocratic" — the largest entries have the most influence.</li>
            <li><b>$\ell_\infty$ (Max Error):</b> $\|x\|_\infty = 10$. It <i>only</i> cares about the worst coordinate. The small ones are invisible. This is the "dictator" norm.</li>
        </ul>
        <p>As $p$ increases, the norm becomes more sensitive to "spikes" (large outliers) and less sensitive to "noise" (small background values). Choosing a norm is choosing a philosophy of error.</p>
      </div>

      <div class="example-box">
        <h4>Example 1: Euclidean Norm ($\ell_2$)</h4>
        <p>$$ \|x\|_2 = \sqrt{\sum x_i^2} $$</p>
        <p><b>Unit Ball:</b> A sphere. Rotational symmetry.
        <br><b>Sensitivity:</b> "Democratic". Large coordinates dominate the sum, but all coordinates contribute.</p>
        <ul>
          <li><b>N3 (Triangle Inequality):</b> Follows from Cauchy-Schwarz.</li>
        </ul>
      </div>

      <div class="insight">
        <h4>Optimization View: Norms as Shapes</h4>
        <p>In optimization, the choice of norm determines the "shape" of the trust region or constraint set.
        <br><b>Minimizing a norm subject to constraints</b> is equivalent to finding the smallest scaled copy of the norm ball that touches the feasible set.</p>
        <ul>
            <li><b>$\ell_2$ (Sphere):</b> The contact point is usually generic (dense solution).</li>
            <li><b>$\ell_1$ (Diamond):</b> The "pointy" corners reach out further in coordinate directions. The contact point is likely to be a vertex, where many coordinates are zero (sparse solution). This is the geometric intuition behind LASSO.</li>
            <li><b>$\ell_\infty$ (Box):</b> The flat faces mean the solution is likely to have many coordinates "maxed out" at the same value.</li>
        </ul>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/layer2_21_dot_product_projection.gif" alt="Dot Product Projection" style="max-width: 900px; width: 100%; height: auto; border-radius: 8px; border: 1px solid var(--border); padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
        <figcaption><i>Animation:</i> Norms and Projections are linked. The dot product is the length of the shadow. Different norms measure this length differently.</figcaption>
      </figure>

      <div class="example-box">
        <h4>Example 2: Manhattan Norm ($\ell_1$)</h4>
        <p>$$ \|x\|_1 = \sum |x_i| $$</p>
        <p><b>Unit Ball:</b> A diamond (cross-polytope). The corners lie on the axes.
        <br><b>Sensitivity:</b> "Linear". Every coordinate contributes equally to the cost. This geometry promotes <b>sparsity</b> (solutions at corners).</p>
        <ul>
          <li><b>N3:</b> $|a+b| \le |a| + |b|$ coordinate-wise.</li>
        </ul>
      </div>

      <h3>4.7 Linear Functionals and Riesz Representation</h3>
      <p>A <b>linear functional</b> is a linear map from vector space $V$ to scalars $\mathbb{R}$.
      $$ f: \mathbb{R}^n \to \mathbb{R} $$
      Example: $f(x) = 3x_1 - 5x_2$.
      <br><b>Theorem (Riesz Representation):</b> In finite dimensions, <i>every</i> linear functional can be represented as an inner product with a unique vector $a$.
      $$ f(x) = \langle a, x \rangle $$
      <b>Why this matters:</b> This is why gradients are vectors. The derivative $Df(x)$ is technically a linear functional (it eats a direction and spits out a rate of change). The <b>gradient</b> $\nabla f(x)$ is the unique vector that represents this functional: $Df(x)[v] = \langle \nabla f(x), v \rangle$.</p>

      <h3>4.8 Dual Norms and Polar Sets</h3>
      <p>Given a norm $\|\cdot\|$, its <b>dual norm</b> is defined as:
      $$ \|\mathbf{y}\|_* := \sup_{\|\mathbf{x}\| \le 1} \mathbf{y}^\top \mathbf{x} $$
      <b>Interpretation (The "Shadow Norm"):</b> Think of $\|\mathbf{y}\|_*$ as the maximum "dot-product gain" you can get by aligning a unit vector $\mathbf{x}$ (measured in the primal norm) with $\mathbf{y}$. It tells you how "aligned" $\mathbf{y}$ is with the geometry of the primal unit ball.
      <br>This leads to the generalized Cauchy-Schwarz inequality (Hölder's inequality):
      $$ \mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\| \|\mathbf{y}\|_* $$
      </p>
      <ul>
        <li><b>$\ell_1$ (Manhattan):</b> $\|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|$. Unit ball is a diamond (polyhedron).</li>
        <li><b>$\ell_\infty$ (Chebyshev):</b> $\|\mathbf{x}\|_\infty = \max_{i} |x_i|$. Unit ball is a square (polyhedron).</li>
      </ul>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/norm-balls.png"
             alt="Comparison of L1, L2, and Infinity norm unit balls"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 6:</i> The "unit ball" $\{\mathbf{x} \mid \|\mathbf{x}\| \le 1\}$ for the $\ell_1$ (diamond), $\ell_2$ (circle), and $\ell_\infty$ (square) norms. All are convex sets.</figcaption>
      </figure>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: The Geometry of Norms</h3>
        <p><b>Visualize Norm Unit Balls:</b> Explore the unit balls for the $\ell_1$, $\ell_2$, and $\ell_\infty$ norms. Note that the $\ell_1$ ball has "corners" on the axes (promoting sparsity), while $\ell_2$ is isotropic.</p>
        <div id="widget-norm-geometry" style="width: 100%; max-width: 800px; height: 400px; position: relative; margin: 0 auto;"></div>
      </div>

      <h3>Dual Norms and Polar Sets</h3>
      <p>The <b>dual norm</b> provides a variational definition of size. For any norm $\|\cdot\|$, its dual $\|\cdot\|_*$ is:
      $$ \|\mathbf{y}\|_* = \sup_{\|\mathbf{x}\| \le 1} \mathbf{x}^\top \mathbf{y} $$
      This measures the maximum "correlation" $\mathbf{y}$ can have with any unit vector.</p>

      <div class="proof-box">
        <h4>Theorem: The Dual Norm is a Norm</h4>
        <div class="proof-step">
          <strong>1. Positive Definiteness:</strong> $\|\mathbf{y}\|_* \ge \mathbf{y}^\top 0 = 0$. If $\|\mathbf{y}\|_*=0$, then $\sup \mathbf{x}^\top \mathbf{y} = 0$. By taking $\mathbf{x} = \mathbf{y}/\|\mathbf{y}\|$ (if norm exists) or coordinate vectors, we see $\mathbf{y}$ must be 0.
        </div>
        <div class="proof-step">
          <strong>2. Homogeneity:</strong> $\|\alpha \mathbf{y}\|_* = \sup_{\|\mathbf{x}\|\le 1} \mathbf{x}^\top (\alpha \mathbf{y}) = \alpha \sup \mathbf{x}^\top \mathbf{y}$ (if $\alpha \ge 0$). Symmetry of the unit ball handles $\alpha < 0$.
        </div>
        <div class="proof-step">
          <strong>3. Triangle Inequality:</strong>
          $$ \|\mathbf{y}+\mathbf{z}\|_* = \sup_{\|\mathbf{x}\|\le 1} (\mathbf{y}+\mathbf{z})^\top \mathbf{x} \le \sup_{\|\mathbf{x}\|\le 1} \mathbf{y}^\top \mathbf{x} + \sup_{\|\mathbf{x}\|\le 1} \mathbf{z}^\top \mathbf{x} = \|\mathbf{y}\|_* + \|\mathbf{z}\|_*. $$
        </div>
      </div>

      <h4>Generalized Hölder's Inequality</h4>
      <p>From the definition, we immediately get the fundamental inequality:
      $$ \mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\| \|\mathbf{y}\|_* $$
      <i>Proof:</i> If $\mathbf{x}=0$, trivial. If $\mathbf{x} \neq 0$, let $\mathbf{u} = \mathbf{x}/\|\mathbf{x}\|$. Then $\|\mathbf{u}\|=1$, so $\mathbf{u}^\top \mathbf{y} \le \|\mathbf{y}\|_* \implies (\mathbf{x}/\|\mathbf{x}\|)^\top \mathbf{y} \le \|\mathbf{y}\|_* \implies \mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\| \|\mathbf{y}\|_*$.</p>

      <h4>Canonical Dual Pairs</h4>
      <ul>
        <li><b>$\ell_2$ is self-dual:</b> $\|\mathbf{y}\|_{2,*} = \|\mathbf{y}\|_2$. (Aligned by $\mathbf{x} = \mathbf{y}/\|\mathbf{y}\|_2$).</li>
        <li><b>$\ell_1$ dual is $\ell_\infty$:</b> $\|\mathbf{y}\|_{1,*} = \|\mathbf{y}\|_\infty$. (Maximize $\mathbf{x}^\top \mathbf{y}$ on diamond $\to$ pick corner).</li>
        <li><b>$\ell_\infty$ dual is $\ell_1$:</b> $\|y\|_{\infty,*} = \|\mathbf{y}\|_1$. (Maximize $\mathbf{x}^\top \mathbf{y}$ on box $\to$ match signs).</li>
      </ul>

      <div class="theorem-box">
        <h4>Lemma: Conjugate of a Norm</h4>
        <p>This lemma is crucial for dual derivations. Let $\|\cdot\|$ be any norm and $\|\cdot\|_*$ its dual norm. The conjugate of the scaled norm $f(\mathbf{x}) = \rho \|\mathbf{x}\|$ is the indicator function of the dual ball:
        $$ (\rho\|\mathbf{x}\|)^*(\mathbf{z}) = I_{\{\|\mathbf{z}\|_* \le \rho\}}(\mathbf{z}) = \begin{cases} 0 & \|\mathbf{z}\|_* \le \rho \\ \infty & \text{otherwise} \end{cases} $$
        <i>Proof:</i> We need to calculate the conjugate of $f(\mathbf{x}) = \rho \|\mathbf{x}\|$, which is $f^*(\mathbf{z}) = \sup_{\mathbf{x}} (\mathbf{z}^\top \mathbf{x} - \rho \|\mathbf{x}\|)$.
        <br><b>Case 1: $\|\mathbf{z}\|_* \le \rho$.</b> By definition of dual norm, $\mathbf{z}^\top \mathbf{x} \le \|\mathbf{z}\|_* \|\mathbf{x}\|$.
        $$ \mathbf{z}^\top \mathbf{x} - \rho \|\mathbf{x}\| \le (\|\mathbf{z}\|_* - \rho) \|\mathbf{x}\| $$
        Since $\|\mathbf{z}\|_* \le \rho$, the coefficient $(\|\mathbf{z}\|_* - \rho)$ is non-positive. Since $\|\mathbf{x}\| \ge 0$, the maximum value is 0 (achieved at $\mathbf{x}=0$).
        <br><b>Case 2: $\|\mathbf{z}\|_* > \rho$.</b> There exists a vector $\mathbf{x}_0$ such that $\|\mathbf{x}_0\| \le 1$ and $\mathbf{z}^\top \mathbf{x}_0 = \|\mathbf{z}\|_*$ (or arbitrarily close).
        Let $\mathbf{x} = t \mathbf{x}_0$ for $t > 0$.
        $$ \mathbf{z}^\top (t \mathbf{x}_0) - \rho \|t \mathbf{x}_0\| = t (\mathbf{z}^\top \mathbf{x}_0 - \rho \|\mathbf{x}_0\|) = t (\|\mathbf{z}\|_* - \rho) $$
        Since $\|\mathbf{z}\|_* > \rho$, this expression goes to $+\infty$ as $t \to \infty$.
        <br>Thus, the conjugate is the indicator of the dual ball.
        </p>
      </div>

      <h4>Polar Sets</h4>
      <p>The <b>polar</b> of a convex set $C$ is $C^\circ = \{\mathbf{y} : \mathbf{y}^\top \mathbf{x} \le 1 \ \forall \mathbf{x} \in C\}$.
      <br>For a unit ball $B = \{\mathbf{x} : \|\mathbf{x}\| \le 1\}$, the polar is exactly the dual unit ball:
      $$ B^\circ = \{\mathbf{y} : \|\mathbf{y}\|_* \le 1\} $$
      This geometric duality underpins the duality of optimization problems.</p>

      <div class="insight">
        <h4>Geometric Interpretation: Supporting Hyperplanes</h4>
        <p>The dual norm $\|\mathbf{y}\|_*$ has a direct geometric meaning: it measures how far the <b>supporting hyperplane</b> perpendicular to $\mathbf{y}$ can be pushed until it touches the unit ball $\{\mathbf{x}: \|\mathbf{x}\| \le 1\}$.</p>
        <p>Specifically, the constraint $\|\mathbf{y}\|_* \le \rho$ says: "the hyperplane $\mathbf{y}^\top \mathbf{x} = \rho$ is a supporting hyperplane of the $\rho$-scaled unit ball."</p>
        <ul>
          <li><b>For $\ell_1$:</b> The dual norm is $\ell_\infty$. The optimal direction $\mathbf{x}^*$ achieving $\sup_{\|\mathbf{x}\|_1 \le 1} \mathbf{y}^\top \mathbf{x}$ is a <b>vertex</b> of the diamond: $\mathbf{x}^* = \pm e_k$ where $k = \arg\max_i |y_i|$. This gives $\|\mathbf{y}\|_\infty = \max_i |y_i|$.</li>
          <li><b>For $\ell_\infty$:</b> The dual norm is $\ell_1$. The optimal $\mathbf{x}^*$ is a <b>corner</b> of the cube: $\mathbf{x}^* = \mathrm{sign}(\mathbf{y})$. This gives $\|\mathbf{y}\|_1 = \sum_i |y_i|$.</li>
          <li><b>For $\ell_2$:</b> The optimal $\mathbf{x}^*$ is the normalized vector $\mathbf{y}/\|\mathbf{y}\|_2$, giving the self-dual property $\|\mathbf{y}\|_{2,*} = \|\mathbf{y}\|_2$.</li>
        </ul>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/supporting_hyperplanes_l1_linf_three_panels.gif"
             alt="Supporting hyperplanes for L1 and L-infinity norms"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> Supporting hyperplanes $\mathbf{u}^\top \mathbf{x} = \|\mathbf{u}\|_*$ for the $\ell_1$ (diamond) and $\ell_\infty$ (square) balls. The dual norm determines how far the hyperplane can be pushed before touching the ball.</figcaption>
      </figure>

      <div class="proof-box">
        <h4>Proof: Dual of $\ell_1$ is $\ell_\infty$</h4>
        <div class="proof-step">
          <strong>Goal:</strong> We want to compute the dual norm $\|\mathbf{y}\|_{1,*} = \sup_{\|\mathbf{x}\|_1 \le 1} \mathbf{y}^\top \mathbf{x}$. We claim the result is $\|\mathbf{y}\|_\infty = \max_i |y_i|$.
        </div>
        <div class="proof-step">
          <strong>Part 1: The Upper Bound ($\le$).</strong>
          We must show that for <i>any</i> vector $\mathbf{x}$ inside the unit $\ell_1$ ball (i.e., $\sum |x_i| \le 1$), the inner product $\mathbf{y}^\top \mathbf{x}$ cannot exceed $\|\mathbf{y}\|_\infty$.
          <br>Expand the inner product:
          $$ \mathbf{y}^\top \mathbf{x} = \sum_{i=1}^n y_i x_i $$
          Apply the absolute value inequality ($z \le |z|$):
          $$ \sum_{i=1}^n y_i x_i \le \sum_{i=1}^n |y_i x_i| = \sum_{i=1}^n |y_i| |x_i| $$
          Now, replace each $|y_i|$ with the largest possible value, $\max_k |y_k| = \|\mathbf{y}\|_\infty$. This can only make the sum larger:
          $$ \sum_{i=1}^n |y_i| |x_i| \le \sum_{i=1}^n \|\mathbf{y}\|_\infty |x_i| = \|\mathbf{y}\|_\infty \left( \sum_{i=1}^n |x_i| \right) $$
          Since we assumed $\|\mathbf{x}\|_1 = \sum |x_i| \le 1$:
          $$ \|\mathbf{y}\|_\infty \left( \sum_{i=1}^n |x_i| \right) \le \|\mathbf{y}\|_\infty \cdot 1 = \|\mathbf{y}\|_\infty $$
          Thus, $\|\mathbf{y}\|_{1,*} \le \|\mathbf{y}\|_\infty$.
        </div>
        <div class="proof-step">
          <strong>Part 2: Achievability ($\ge$).</strong>
          To prove equality, we must find <i>at least one</i> specific vector $\mathbf{x}^*$ with $\|\mathbf{x}^*\|_1 \le 1$ that actually achieves this bound.
          <br>Let $k$ be the index where $\mathbf{y}$ achieves its maximum absolute value (i.e., $|y_k| = \|\mathbf{y}\|_\infty$).
          Construct the vector $\mathbf{x}^*$ to have a non-zero entry only at this position $k$, with a sign matching $y_k$:
          $$ \mathbf{x}^* = \mathrm{sign}(y_k) \mathbf{e}_k = (0, \dots, \underbrace{\mathrm{sign}(y_k)}_{k\text{-th pos}}, \dots, 0) $$
          Check the constraint: $\|\mathbf{x}^*\|_1 = |0| + \dots + |\mathrm{sign}(y_k)| + \dots = 1 \le 1$. It is feasible.
          <br>Check the value:
          $$ \mathbf{y}^\top \mathbf{x}^* = \sum y_i x^*_i = y_k \cdot \mathrm{sign}(y_k) = |y_k| = \|\mathbf{y}\|_\infty $$
          Since we found a valid vector achieving the bound, $\|\mathbf{y}\|_{1,*} \ge \|\mathbf{y}\|_\infty$.
        </div>
        <div class="proof-step">
            <strong>Conclusion:</strong> Since $\|\mathbf{y}\|_{1,*} \le \|\mathbf{y}\|_\infty$ and $\|\mathbf{y}\|_{1,*} \ge \|\mathbf{y}\|_\infty$, we have $\|\mathbf{y}\|_{1,*} = \|\mathbf{y}\|_\infty$.
        </div>
      </div>

      <h3>Spectral Norm and Convexity</h3>

      <h4>Induced Matrix Norms</h4>
      <p>The spectral norm is a specific instance of an <b>induced norm</b> (or operator norm). For any vector norms $\|\cdot\|_a$ on the domain and $\|\cdot\|_b$ on the codomain, the induced norm of a matrix $X$ is:
      $$ \|X\|_{a,\mathbf{b}} = \sup_{\mathbf{v} \neq 0} \frac{\|X\mathbf{v}\|_b}{\|\mathbf{v}\|_a} = \sup_{\|\mathbf{v}\|_a = 1} \|X\mathbf{v}\|_b $$
      This measures the maximum amplification of the $\mathbf{b}$-norm of the output relative to the $a$-norm of the input.</p>

      <h4>Common Operator Norms</h4>
      <p>When the domain and codomain are equipped with the same $\ell_p$ norm ($a=\mathbf{b}=\mathbf{p}$), we write $\|X\|_p$.</p>
      <ul>
        <li><b>Spectral Norm ($\mathbf{p}=2$):</b> $\|X\|_2 = \sigma_{\max}(X)$. (Max stretch of Euclidean length).</li>
        <li><b>Max Column Sum Norm ($\mathbf{p}=1$):</b> The operator norm induced by the $\ell_1$ norm is the maximum absolute column sum.
        $$ \|X\|_1 = \max_{j} \sum_{i} |X_{ij}| $$
        <i>Intuition:</i> The unit ball of $\ell_1$ is a diamond with vertices at $e_j$. The maximum stretch occurs at one of these vertices $X e_j$ (the $j$-th column).
        </li>
        <li><b>Max Row Sum Norm ($\mathbf{p}=\infty$):</b> The operator norm induced by the $\ell_\infty$ norm is the maximum absolute row sum.
        $$ \|X\|_\infty = \max_{i} \sum_{j} |X_{ij}| $$
        <i>Intuition:</i> This is the dual of the 1-norm case.
        </li>
      </ul>

      <p><b>Convexity:</b> All induced norms are convex functions of the matrix $X$. This follows because $\|X\|_{a,b} = \sup_{\|\mathbf{u}\|_{\mathbf{b}^*} \le 1, \|\mathbf{v}\|_a \le 1} \mathbf{u}^\top X \mathbf{v}$, which is a pointwise supremum of linear functions.</p>

      <h4>Spectral Norm Details</h4>
      <p>The <b>spectral norm</b> (or operator norm) of a matrix $X \in \mathbb{R}^{m \times n}$ is induced by the Euclidean vector norm:
      $$ \|X\|_2 = \sup_{\mathbf{v} \neq 0} \frac{\|X\mathbf{v}\|_2}{\|\mathbf{v}\|_2} = \sup_{\|\mathbf{v}\|_2=1} \|X\mathbf{v}\|_2 $$
      <p><b>Intuition: Worst-Case Stretch.</b> The operator norm measures the maximum factor by which the matrix $X$ can "stretch" a vector. It corresponds to the "Lipschitz constant" of the linear map. Geometrically, it is the length of the longest semi-axis of the ellipsoid formed by mapping the unit sphere.</p>
      It turns out this equals the maximum singular value:
      $$ \|X\|_2 = \sigma_{\max}(X) = \sqrt{\lambda_{\max}(X^\top X)} $$
      Since $X^\top X$ is always Positive Semidefinite ($\mathbf{v}^\top X^\top X \mathbf{v} = \|X\mathbf{v}\|_2^2 \ge 0$), its eigenvalues are non-negative.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/operator-norm-geometry.png"
             alt="A unit circle mapped to an ellipse illustrating the spectral norm as maximum stretch"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 7:</i> Operator-norm geometry: $\|A\|_2$ is the maximum stretch of the unit circle under $A$, equal to the longest semi-axis length (the largest singular value).</figcaption>
      </figure>

      <div class="proof-box">
        <h4>Proof: $\|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)}$</h4>
        <div class="proof-step">
          <strong>Step 1: Squared Norm.</strong>
          $$ \|X\|_2^2 = \sup_{\mathbf{v} \neq 0} \frac{\|X\mathbf{v}\|_2^2}{\|\mathbf{v}\|_2^2} = \sup_{\mathbf{v} \neq 0} \frac{\mathbf{v}^\top X^\top X \mathbf{v}}{\mathbf{v}^\top \mathbf{v}} $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Rayleigh Quotient.</strong> The expression $\frac{\mathbf{v}^\top M \mathbf{v}}{\mathbf{v}^\top \mathbf{v}}$ for a symmetric matrix $M$ is the Rayleigh Quotient. By the Spectral Theorem, we can diagonalize $M$ in an orthonormal basis of eigenvectors. The maximum value of the quotient is achieved when $\mathbf{v}$ is the eigenvector corresponding to the largest eigenvalue. Thus, the maximum is exactly $\lambda_{\max}(M)$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          $$ \|X\|_2^2 = \lambda_{\max}(X^\top X) \implies \|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)} $$
        </div>
      </div>

      <p><b>Dual Representation (Crucial for Convexity Proofs):</b> Similar to the max eigenvalue, the spectral norm can be written as a supremum:
      $$ \|X\|_2 = \sup_{\|\mathbf{u}\|_2=1, \|\mathbf{v}\|_2=1} \mathbf{u}^\top X \mathbf{v} $$
      Since $f(X) = \mathbf{u}^\top X \mathbf{v}$ is a <b>linear function</b> of $X$, and $\|X\|_2$ is the supremum of these linear functions, $\|X\|_2$ is a <b>convex function</b>.</p>

      <h3>Matrix Inner Product and Frobenius Norm</h3>
      <p>For matrices $X, Y \in \mathbb{R}^{m \times n}$, the <b>trace inner product</b> (also called the Hilbert-Schmidt inner product) is:
      $$ \langle X, Y \rangle = \mathrm{tr}(X^\top Y) = \sum_{i=1}^m \sum_{j=1}^n X_{ij} Y_{ij} $$
      The <b>Frobenius norm</b> (or Hilbert-Schmidt norm) is $\|X\|_F = \sqrt{\langle X, X \rangle}$.</p>

      <div class="proof-box">
        <h4>Derivation: Trace and Frobenius Norm</h4>
        <p>We show that the Frobenius norm $\|A\|_F = \sqrt{\sum_{i,j} A_{ij}^2}$ is induced by the trace inner product $\langle A, B \rangle = \mathrm{tr}(A^\top B)$.</p>
        <div class="proof-step">
            <strong>Step 1: Compute Diagonal Elements of $A^\top A$.</strong>
            Let $P = A^\top A$. By definition of matrix multiplication, the $(j, j)$ entry is the dot product of the $j$-th row of $A^\top$ and the $j$-th column of $A$.
            Since the $j$-th row of $A^\top$ is the transpose of the $j$-th column of $A$:
            $$ P_{jj} = \sum_{k=1}^m (A^\top)_{jk} A_{kj} = \sum_{k=1}^m A_{kj} A_{kj} = \sum_{k=1}^m A_{kj}^2 $$
            This represents the squared Euclidean norm of the $j$-th column of $A$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Sum the Diagonal (Trace).</strong>
            The trace is the sum of diagonal elements:
            $$ \mathrm{tr}(A^\top A) = \sum_{j=1}^n P_{jj} = \sum_{j=1}^n \left( \sum_{k=1}^m A_{kj}^2 \right) $$
        </div>
        <div class="proof-step">
            <strong>Step 3: Relate to Element-wise Sum.</strong>
            The double sum $\sum_{j=1}^n \sum_{k=1}^m A_{kj}^2$ simply iterates over every entry in the matrix, squaring it and adding it up.
            This is exactly the definition of the squared Frobenius norm:
            $$ \sum_{j=1}^n \sum_{k=1}^m A_{kj}^2 = \sum_{i,j} A_{ij}^2 = \|A\|_F^2 $$
            Thus, $\|A\|_F = \sqrt{\mathrm{tr}(A^\top A)}$.
        </div>
      </div>

      <h4>Submultiplicativity of the Frobenius Norm</h4>
      <p>For compatible matrices $A, B$, we have the inequality $\|AB\|_F \le \|A\|_F \|B\|_F$. This property is crucial for analyzing the convergence of matrix iterations.</p>

      <div class="proof-box">
        <h4>Proof via Column/Row Factorization</h4>
        <p>We prove this inequality by decomposing the matrix multiplication into vector inner products and applying Cauchy-Schwarz.</p>

        <div class="proof-step">
          <strong>Step 1: Setup.</strong>
          Let $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times \mathbf{p}}$. Let $C = AB$.
          <br>Denote the $i$-th <b>column</b> of $A^\top$ (which is the $i$-th row of $A$) as $A_i \in \mathbb{R}^n$.
          <br>Denote the $j$-th <b>column</b> of $B$ as $B^j \in \mathbb{R}^n$.
          <br>The $(i,j)$ entry of $C$ is the inner product: $C_{ij} = \langle A_i, B^j \rangle$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Expand Squared Norm.</strong>
          $$ \|C\|_F^2 = \sum_{i=1}^m \sum_{j=1}^p C_{ij}^2 = \sum_{i=1}^m \sum_{j=1}^p \langle A_i, B^j \rangle^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Apply Cauchy-Schwarz.</strong>
          For each term, $\langle A_i, B^j \rangle^2 \le \|A_i\|_2^2 \|B^j\|_2^2$.
          $$ \|C\|_F^2 \le \sum_{i=1}^m \sum_{j=1}^p \|A_i\|_2^2 \|B^j\|_2^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 4: Factorization Trick.</strong>
          Define $\alpha_i = \|A_i\|_2^2$ and $\beta_j = \|B^j\|_2^2$. The double sum splits because the terms are independent:
          $$ \sum_{i=1}^m \sum_{j=1}^p \alpha_i \beta_j = \left( \sum_{i=1}^m \alpha_i \right) \left( \sum_{j=1}^p \beta_j \right) $$
          Recognize that $\sum \alpha_i = \sum \|A_i\|_2^2 = \|A\|_F^2$ and $\sum \beta_j = \sum \|B^j\|_2^2 = \|B\|_F^2$.
          <br>Thus $\|AB\|_F^2 \le \|A\|_F^2 \|B\|_F^2$.
        </div>
      </div>

      <h4>Gradients with Respect to Matrices</h4>
      <p>For a matrix variable $X \in \mathbb{R}^{m \times n}$, the gradient is an $m \times n$ matrix. The definition is consistent with the scalar directional derivative.</p>
      <ul>
        <li><b>Rule 1: Linear Trace:</b> $\nabla_X \mathrm{tr}(A^\top X) = A$.
        <br><i>Proof:</i> $\mathrm{tr}(A^\top X) = \langle A, X \rangle$. The function is linear: $f(X+\Delta) = \langle A, X+\Delta \rangle = f(X) + \langle A, \Delta \rangle$. The gradient is the coefficient of $\Delta$, which is $A$.</li>
        <li><b>Rule 2: Quadratic Trace:</b> $\nabla_X \mathrm{tr}(X^\top A X) = (A + A^\top)X$. If $A$ is symmetric, this is $2AX$.
        <br><i>Note:</i> This corresponds to $\frac{d}{dx}(ax^2) = 2ax$. For vector $\mathbf{x}$, $\nabla (\mathbf{x}^\top A\mathbf{x}) = (A+A^\top)\mathbf{x}$. For matrix $X$, the columns transform independently if $A$ is scalar, but coupled if $A$ is a matrix.
        <br><i>Derivation:</i>
        $$ \begin{aligned}
        f(X+\Delta) &= \mathrm{tr}((X+\Delta)^\top A (X+\Delta)) \\
        &= \mathrm{tr}(X^\top AX + \Delta^\top AX + X^\top A\Delta + \Delta^\top A\Delta) \\
        &\approx f(X) + \mathrm{tr}(\Delta^\top AX) + \mathrm{tr}(X^\top A \Delta) \\
        &= f(X) + \mathrm{tr}(\Delta^\top AX) + \mathrm{tr}(\Delta^\top A^\top X) \quad \text{(using } \mathrm{tr}(M)=\mathrm{tr}(M^\top)) \\
        &= f(X) + \mathrm{tr}(\Delta^\top (A+A^\top)X) \\
        &= f(X) + \langle (A+A^\top)X, \Delta \rangle
        \end{aligned} $$
        Thus $\nabla f(X) = (A+A^\top)X$.
        </li>
        <li><b>Rule 3: Log Determinant:</b> $\nabla_X \log \det X = X^{-\top}$. If $X \in \mathbb{S}^n_{++}$ (symmetric positive definite), then $\nabla_X \log \det X = X^{-1}$.
        <br>
        <div class="proof-box">
          <h4>Derivation: Gradient of Log Determinant</h4>
          <p>We derive the gradient of $f(X) = \log \det X$ (defined for $\det X > 0$) by identifying the linear term in the Taylor expansion of $f(X+\Delta)$ around $X$. This step-by-step derivation uses the perturbation method, which is robust and general.</p>
          <div class="proof-step">
            <strong>Step 1: Factor out X.</strong>
            We want to isolate the perturbation $\Delta$. Since $X$ is invertible, we can write $X+\Delta = X(I + X^{-1}\Delta)$.
            $$ \log \det (X + \Delta) = \log \det (X(I + X^{-1}\Delta)) $$
            Using the multiplicative property of the determinant ($\det(AB) = \det(A)\det(B)$) and the additive property of the logarithm ($\log(ab) = \log a + \log b$):
            $$ = \log (\det X \cdot \det(I + X^{-1}\Delta)) = \log \det X + \log \det(I + X^{-1}\Delta) $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Relate Determinant to Trace via Eigenvalues.</strong>
            Let $E = X^{-1}\Delta$. We need to approximate $\log \det(I + E)$ for small $E$.
            Let $\lambda_i$ be the eigenvalues of $E$. The eigenvalues of $I+E$ are then $1+\lambda_i$.
            $$ \det(I + E) = \prod_{i=1}^n (1 + \lambda_i) $$
            Taking the natural logarithm turns the product into a sum:
            $$ \log \det(I + E) = \sum_{i=1}^n \log(1 + \lambda_i) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Linearize via Taylor Series.</strong>
            Recall the scalar Taylor series expansion for the natural logarithm near 1:
            $$ \log(1+z) \approx z \quad \text{for } z \approx 0 $$
            For small perturbations $E$, the eigenvalues $\lambda_i$ are small.
            Applying this approximation to each term:
            $$ \sum_{i=1}^n \log(1 + \lambda_i) \approx \sum_{i=1}^n \lambda_i $$
            We recognize the sum of eigenvalues $\sum \lambda_i$ as the <b>trace</b> of the matrix $E$.
            $$ \sum_{i=1}^n \lambda_i = \mathrm{tr}(E) = \mathrm{tr}(X^{-1}\Delta) $$
            Thus, to first order, the change in the log-determinant is exactly the trace of the relative change matrix $E$.
          </div>
          <div class="proof-step">
            <strong>Step 4: Extract the Gradient.</strong>
            Substituting back, the first-order approximation of the function is:
            $$ f(X+\Delta) \approx f(X) + \mathrm{tr}(X^{-1}\Delta) $$
            The gradient $\nabla f(X)$ is defined as the matrix $G$ such that the linear term is $\langle G, \Delta \rangle = \mathrm{tr}(G^\top \Delta)$.
            We rewrite our linear term to match this form:
            $$ \mathrm{tr}(X^{-1}\Delta) = \mathrm{tr}((X^{-\top})^\top \Delta) = \langle X^{-\top}, \Delta \rangle $$
            Comparing terms, we identify $G = X^{-\top}$.
            <br><b>Result:</b> $\nabla_X \log \det X = X^{-\top}$.
            <br>If $X$ is symmetric ($X=X^\top$), then $\nabla f(X) = X^{-1}$.
          </div>
        </div>
        </li>
      </ul>
      <div class="insight">
        <h4>Chain Rule for Matrix Functions</h4>
        <p>If $f(X) = g(h(X))$, then the differential is $df = \mathbf{g}'(h(X)) \circ dh$. For example, to differentiate $\|Ax - b\|_2^2$:</p>
        <ol>
          <li>Let $\mathbf{r} = A\mathbf{x} - \mathbf{b}$. Then $f = \mathbf{r}^\top \mathbf{r}$.</li>
          <li>$df = 2\mathbf{r}^\top dr$.</li>
          <li>$dr = A dx$.</li>
          <li>Substitute: $df = 2\mathbf{r}^\top A dx = (2A^\top \mathbf{r})^\top dx$.</li>
          <li>The gradient is the transpose of the coefficient of $dx^\top$ (or the vector multiplying $dx$), so $\nabla f = 2A^\top \mathbf{r} = 2A^\top(A\mathbf{x}-\mathbf{b})$.</li>
        </ol>
      </div>

      <div class="proof-box">
        <h4>Derivation: Hessian of Least Squares</h4>
        <p>Let $f(\mathbf{x}) = \|A\mathbf{x} - \mathbf{b}\|_2^2$. We show $\nabla^2 f(\mathbf{x}) = 2A^\top A$ and prove its convexity.</p>

        <div class="proof-step">
          <strong>Step 1: Expand.</strong>
          $$ f(\mathbf{x}) = (A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = \mathbf{x}^\top A^\top A \mathbf{x} - 2\mathbf{b}^\top A \mathbf{x} + \mathbf{b}^\top \mathbf{b} $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Gradient.</strong>
          Using the rules derived above:
          <ul>
            <li>$\nabla (\mathbf{x}^\top (A^\top A) \mathbf{x}) = 2A^\top A \mathbf{x}$ (since $A^\top A$ is symmetric).</li>
            <li>$\nabla (-2(A^\top \mathbf{b})^\top \mathbf{x}) = -2A^\top \mathbf{b}$.</li>
            <li>$\nabla (\mathbf{b}^\top \mathbf{b}) = 0$.</li>
          </ul>
          Summing these: $\nabla f(\mathbf{x}) = 2A^\top A \mathbf{x} - 2A^\top \mathbf{b} = 2A^\top (A\mathbf{x} - \mathbf{b})$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Hessian.</strong>
          Differentiating $\nabla f(\mathbf{x})$ with respect to $\mathbf{x}$:
          $$ \nabla^2 f(\mathbf{x}) = \nabla_x (2A^\top A \mathbf{x}) = 2A^\top A $$
          The Hessian is constant, characteristic of quadratic functions.
        </div>

        <div class="proof-step">
          <strong>Step 4: Convexity (PSD Check).</strong>
          For any vector $\mathbf{v} \in \mathbb{R}^n$:
          $$ \mathbf{v}^\top \nabla^2 f(\mathbf{x}) \mathbf{v} = \mathbf{v}^\top (2A^\top A) \mathbf{v} = 2(A\mathbf{v})^\top (A\mathbf{v}) = 2\|A\mathbf{v}\|_2^2 \ge 0 $$
          Since the quadratic form is always non-negative, the Hessian is Positive Semidefinite (PSD). Thus, the least squares objective is always <b>convex</b>.
        </div>
      </div>

      <div class="theorem-box">
        <h4>Lemma: Quadratic Minimization (Completion of the Square)</h4>
        <p>This result is fundamental to convex optimization derivation. Let $H \in \mathbb{S}_{++}^n$ (symmetric positive definite) and $\mathbf{g} \in \mathbb{R}^n$. Then the unique minimizer of the quadratic form:
        $$ \inf_{\mathbf{x}\in\mathbb R^n}\left\{\frac12 \mathbf{x}^\top H\mathbf{x} + \mathbf{g}^\top \mathbf{x}\right\} $$
        is given by $\boxed{\mathbf{x}^\star = -H^{-1}\mathbf{g}}$, and the optimal value is $\boxed{-\frac12 \mathbf{g}^\top H^{-1}\mathbf{g}}$.</p>
        <p><i>Proof:</i> Complete the square. Let $\mathbf{x} = \mathbf{z} - H^{-1}\mathbf{g}$.
        $$ \frac12 (\mathbf{z} - H^{-1}\mathbf{g})^\top H (\mathbf{z} - H^{-1}\mathbf{g}) + \mathbf{g}^\top (\mathbf{z} - H^{-1}\mathbf{g}) $$
        Expanding this yields $\frac12 \mathbf{z}^\top H \mathbf{z} - \frac12 \mathbf{g}^\top H^{-1}\mathbf{g}$. Since $H \succ 0$, the minimum is at $\mathbf{z}=0$.
        </p>
      </div>
    </section>
          $$ \sum_{i,j} \alpha_i \beta_j = \sum_i \alpha_i \left(\sum_j \beta_j\right) = \left(\sum_i \alpha_i\right) \left(\sum_j \beta_j\right) $$
          Substituting back:
          $$ \sum_{i,j} \|A_i\|_2^2 \|B^j\|_2^2 = \left(\sum_{i=1}^m \|A_i\|_2^2\right) \left(\sum_{j=1}^p \|B^j\|_2^2\right) $$
        </div>

        <div class="proof-step">
          <strong>Step 5: Identify Norms.</strong>
          The sum of squared norms of rows of $A$ is exactly $\|A\|_F^2$.
          The sum of squared norms of columns of $B$ is exactly $\|B\|_F^2$.
          $$ \|AB\|_F^2 \le \|A\|_F^2 \|B\|_F^2 $$
          Taking square roots gives the result: $\boxed{\|AB\|_F \le \|A\|_F \|B\|_F}$.
        </div>
      </div>
    </section>

    <!-- SECTION 5: ORTHOGONALITY -->
    <section class="section-card" id="section-orthogonality">
      <h2>5. Orthogonality and Projections</h2>

      <p>Inner products allow us to define <b>Orthogonality</b>, which is the geometric concept of "perpendicularity". This structure turns the abstract vector space into a tool for optimization, enabling us to solve the problem of finding the "closest" vector in a subspace.</p>

      <h3>5.1 Orthogonality and Complements</h3>
      <p><b>Definition:</b> Two vectors are orthogonal ($u \perp v$) if $\langle u, v \rangle = 0$.
      <br><b>Pythagorean Theorem:</b> If $u \perp v$, then $\|u+v\|^2 = \|u\|^2 + \|v\|^2$.
      <br><i>Proof:</i> $\|u+v\|^2 = \langle u+v, u+v \rangle = \|u\|^2 + 2\langle u, v \rangle + \|v\|^2$. Since the cross term vanishes, the identity holds.</p>

      <h4>Orthogonal Complement ($W^\perp$)</h4>
      <p>For a subspace $W \subseteq V$, the set of all vectors orthogonal to every vector in $W$ is called the orthogonal complement:
      $$ W^\perp := \{v \in V \mid \langle v, w \rangle = 0 \ \forall w \in W\} $$
      <b>Theorem:</b> $W^\perp$ is always a subspace of $V$.
      <br><i>Proof:</i>
      1. <b>Zero:</b> $\langle 0, w \rangle = 0$ for all $w$, so $0 \in W^\perp$.
      2. <b>Additivity:</b> If $v_1, v_2 \in W^\perp$, then $\langle v_1+v_2, w \rangle = \langle v_1, w \rangle + \langle v_2, w \rangle = 0+0=0$.
      3. <b>Scaling:</b> If $v \in W^\perp$, $\langle \alpha v, w \rangle = \alpha \langle v, w \rangle = 0$.
      </p>

      <h3>5.2 Orthogonal Projection: Preview</h3>
      <p>Orthogonal projection is the central problem of approximation theory: given a vector $\mathbf{x}$ and a subspace $W$, find the closest point $\mathbf{p} \in W$ to $\mathbf{x}$.
      <br><b>Intuition:</b> Think of shining a light perpendicular to the subspace $W$. The "shadow" cast by $\mathbf{x}$ onto $W$ is the projection $\mathbf{p}$. The vector difference $\mathbf{x} - \mathbf{p}$ is the "height" of $\mathbf{x}$ above the subspace.
      <br>This operation is so fundamental that we dedicate two later sections to it:
      <ul>
        <li><b>Section 8</b> covers the general theory for subspaces and affine sets.</li>
        <li><b>Section 9</b> applies it to solve the Least Squares problem.</li>
      </ul>
      For now, remember the golden rule: <b>The error vector is orthogonal to the subspace.</b></p>

      <div class="insight">
        <h4>Why Projections Matter for Optimization</h4>
        <p>Every constrained optimization problem involves finding a point that balances two competing objectives: minimizing a cost function while staying inside a feasible region. When the feasible region is a subspace or convex set, the solution often involves computing orthogonal projections. This geometric operation appears in:
        <ul style="margin-top: 0.5rem;">
          <li><b>Least Squares:</b> Projecting the target vector $\mathbf{b}$ onto the column space $\mathcal{R}(A)$ (Section 9).</li>
          <li><b>Constrained Optimization:</b> Projected gradient descent methods that enforce constraints by projecting iterates back onto the feasible set.</li>
          <li><b>Duality Theory:</b> The geometry of the Karush-Kuhn-Tucker (KKT) conditions involves decomposing gradients into components normal and tangent to constraint surfaces.</li>
        </ul>
        </p>
      </div>

      <h3>5.3 Orthonormal Bases: Geometry-Compatible Coordinates</h3>
      <p>An <b>orthonormal basis</b> is a coordinate system that preserves lengths, angles, and inner products exactly.
      <br><b>Definition:</b> A basis $\{q_1, \dots, q_n\}$ is orthonormal if $\langle q_i, q_j \rangle = \delta_{ij}$.
      <br><b>Why this is special:</b> In an arbitrary basis, calculating lengths requires a metric tensor ($\mathbf{x}^\top G \mathbf{x}$). In an orthonormal basis, geometry simplifies to the Pythagorean sum of squares.</p>

      <h4>The Expansion Formula (No Solving Required)</h4>
      <p>If $\{q_1, \dots, q_n\}$ is an orthonormal basis for $V$, then for every $\mathbf{v} \in V$:
      $$ \boxed{ \mathbf{v} = \sum_{i=1}^n \langle q_i, \mathbf{v} \rangle q_i } $$
      <b>Proof:</b> Let $\mathbf{v} = \sum c_j q_j$. Take the inner product with $q_k$:
      $$ \langle q_k, \mathbf{v} \rangle = \sum c_j \langle q_k, q_j \rangle = \sum c_j \delta_{kj} = c_k $$
      Thus, coordinates are simply inner products. No linear system needs to be solved.</p>

      <h4>Parseval's Identity</h4>
      <p>$$ \|\mathbf{v}\|^2 = \sum_{i=1}^n |\langle q_i, \mathbf{v} \rangle|^2 $$
      The norm of a vector equals the Euclidean norm of its coordinate vector. This is why orthonormal coordinate changes are <b>isometries</b>.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/orthonormal_coordinates.gif"
             alt="In an orthonormal basis, coordinates are inner products"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> $\mathbf{v} = (q_1^\top \mathbf{v}) q_1 + (q_2^\top \mathbf{v}) q_2$. When the basis is orthonormal, the expansion coefficients are simply the inner products—no matrix inversion required.</figcaption>
      </figure>

      <h3>5.4 The Gram-Schmidt Process</h3>
      <p>Gram-Schmidt is not just an algorithm; it is the <b>constructive proof</b> that every finite-dimensional subspace admits an orthonormal basis. It systematically manufactures geometry from algebra.</p>
      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: Gram-Schmidt Process</h3>
        <p>Watch step-by-step how an orthonormal basis is built. At each step, we subtract the projection onto the previously constructed subspace to get a new orthogonal direction.</p>
        <iframe src="widgets/la_batch2.html?mode=gs" width="100%" height="550" style="border:none; border-radius: 8px; background:#0b0f14; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);"></iframe>
      </div>

      <div class="insight">
        <h4>Gram-Schmidt is "Projection Theorem in Disguise"</h4>
        <p>At step $j$, we want a vector in $\operatorname{span}(a_1, \dots, a_j)$ that is orthogonal to $\operatorname{span}(a_1, \dots, a_{j-1})$.
        <br>There is exactly <b>one</b> way to do this:
        <ul>
            <li>Take $a_j$.</li>
            <li>Subtract its <b>orthogonal projection</b> onto the subspace spanned by the previous vectors.</li>
        </ul>
        This residual $u_j = a_j - \operatorname{proj}_{W_{j-1}}(a_j)$ captures exactly the "new information" in $a_j$ orthogonal to what we already know. Normalizing it gives the basis vector.</p>
      </div>

      <p><b>The Idea:</b> Given linearly independent vectors $a_1, \dots, a_k$, we construct orthonormal vectors $q_1, \dots, q_k$ such that for each $j$, $\operatorname{span}(q_1, \dots, q_j) = \operatorname{span}(a_1, \dots, a_j)$.
      <br><b>Step-by-step Construction:</b>
      1. <b>First Vector:</b> $q_1 = a_1 / \|a_1\|$.
      2. <b>Subsequent Vectors:</b> To find $q_j$, take $a_j$ and subtract its projection onto the subspace spanned by the previous vectors $W_{j-1} = \operatorname{span}(q_1, \dots, q_{j-1})$.
      $$ u_j = a_j - \operatorname{proj}_{W_{j-1}}(a_j) = a_j - \sum_{i=1}^{j-1} \langle q_i, a_j \rangle q_i $$
      3. <b>Normalize:</b> $q_j = u_j / \|u_j\|$.
      </p>
      <p>The vector $u_j$ is guaranteed to be non-zero because $a_j$ is linearly independent of the previous vectors. It is orthogonal to all previous $q_i$ by construction.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/gram-schmidt-orthogonalization.png"
             alt="Step-by-step geometric illustration of the Gram-Schmidt orthogonalization process"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 8:</i> Gram-Schmidt orthogonalization: subtract projections to remove components in previously constructed directions.</figcaption>
      </figure>

      <h3>5.5 Orthogonal Matrices and Isometries</h3>
      <p>A square matrix $Q$ is <b>orthogonal</b> if its columns form an orthonormal basis. This is algebraically equivalent to $Q^\top Q = I$.
      <br><b>Properties of Orthogonal Matrices:</b>
      <ul>
        <li><b>Isometry:</b> $\|Q\mathbf{x}\| = \|\mathbf{x}\|$ and $\langle Q\mathbf{x}, Q\mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{y} \rangle$. They represent rigid motions (rotations and reflections).</li>
        <li><b>Inverse:</b> $Q^{-1} = Q^\top$. This makes solving linear systems trivial: $Q\mathbf{x}=\mathbf{b} \implies \mathbf{x} = Q^\top \mathbf{b}$.</li>
        <li><b>Stability:</b> Multiplication by orthogonal matrices does not amplify numerical errors ($\kappa(Q)=1$).</li>
      </ul>
      </p>

      <h4>The QR Decomposition (Preview)</h4>
      <p>The Gram-Schmidt process can be written in matrix form as $A = QR$.
      <ul>
        <li>$Q$ has orthonormal columns (the geometry).</li>
        <li>$R$ is upper triangular (the coordinates).</li>
      </ul>
      This factorization is the foundation of stable least squares algorithms, covered in depth in <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a>.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Orthogonality & Projections</h3>
        <p><b>Visualize Vector Relationships:</b> Observe how geometric relationships change. The tool displays the orthogonal projection—the shadow of one vector onto another. This is the foundation of least squares.</p>
        <iframe src="widgets/la_batch2.html?mode=kernel" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>
    </section>

    <!-- SECTION 6: MATRIX CALCULUS -->
    <section class="section-card" id="section-matrix-calculus">
      <h2>6. Matrix Calculus Basics</h2>
      <p>Gradient-based optimization relies heavily on the computation of gradients and Hessians. Computing derivatives with respect to vectors and matrices is therefore a core skill. Unlike scalar calculus, where order doesn't matter, in matrix calculus we must carefully preserve non-commutativity and track dimensions. The "master key" to matrix calculus is the <b>differential</b>.</p>

      <p>For a function $f: \mathbb{R}^{n \times m} \to \mathbb{R}$, the gradient $\nabla f(X)$ is defined as the unique matrix satisfying the first-order approximation:
      $$ f(X + \Delta) = f(X) + \langle \nabla f(X), \Delta \rangle + o(\|\Delta\|) $$
      where $\langle A, B \rangle = \mathrm{tr}(A^\top B)$ is the standard trace inner product. This definition provides a systematic algorithm for finding gradients:</p>
      <ol>
        <li>Perturb $X$ by a small direction $\Delta$.</li>
        <li>Expand $f(X + \Delta)$ using Taylor series or algebraic properties.</li>
        <li>Isolate the term that is <i>linear</i> in $\Delta$.</li>
        <li>Rewrite that linear term in the form $\langle G, \Delta \rangle = \mathrm{tr}(G^\top \Delta)$. Then $G$ is the gradient.</li>
      </ol>

      <h4>Gradients of Linear and Quadratic Forms</h4>
      <p>For $\mathbf{x} \in \mathbb{R}^n$, $A \in \mathbb{R}^{n \times n}$, and $\mathbf{b} \in \mathbb{R}^n$:</p>
      <table class="data-table">
        <thead>
          <tr>
            <th>Function $f(\mathbf{x})$</th>
            <th>Gradient $\nabla f(\mathbf{x})$</th>
            <th>Hessian $\nabla^2 f(\mathbf{x})$</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>$\mathbf{b}^\top \mathbf{x}$</td>
            <td>$\mathbf{b}$</td>
            <td>$0$</td>
            <td>Linear function</td>
          </tr>
          <tr>
            <td>$\mathbf{x}^\top A \mathbf{x}$</td>
            <td>$(A + A^\top)\mathbf{x}$</td>
            <td>$A + A^\top$</td>
            <td>General quadratic</td>
          </tr>
          <tr>
            <td>$\mathbf{x}^\top A \mathbf{x}$</td>
            <td>$2A\mathbf{x}$</td>
            <td>$2A$</td>
            <td>If $A$ is symmetric ($A=A^\top$)</td>
          </tr>
          <tr>
            <td>$\|Ax - b\|_2^2$</td>
            <td>$2A^\top(A\mathbf{x} - \mathbf{b})$</td>
            <td>$2A^\top A$</td>
            <td>Least Squares objective</td>
          </tr>
        </tbody>
      </table>

      <div class="proof-box">
        <h4>Derivation: Gradient of Quadratic Form (Detailed)</h4>
        <p>Let $f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$. We want to find the vector $\mathbf{g}$ such that $f(\mathbf{x}+h) \approx f(\mathbf{x}) + \mathbf{g}^\top h$.</p>
        <div class="proof-step">
          <strong>Step 1: Perturb the input.</strong>
          Substitute $\mathbf{x} + h$ into the function:
          $$ f(\mathbf{x}+h) = (\mathbf{x}+h)^\top A (\mathbf{x}+h) $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Expand the terms.</strong>
          Using the distributivity of matrix multiplication:
          $$ = (\mathbf{x}^\top + h^\top) (A\mathbf{x} + Ah) $$
          $$ = \mathbf{x}^\top A \mathbf{x} + \mathbf{x}^\top A h + h^\top A \mathbf{x} + h^\top A h $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Identify the linear part (The Differential).</strong>
          We separate terms by order of $h$:
          <ul>
            <li><b>Constant term:</b> $\mathbf{x}^\top A \mathbf{x} = f(\mathbf{x})$.</li>
            <li><b>Linear term:</b> $\mathbf{x}^\top A h + h^\top A \mathbf{x}$.</li>
            <li><b>Higher-order term:</b> $h^\top A h$ (this is $O(\|h\|^2)$, so we ignore it for the gradient).</li>
          </ul>
        </div>
        <div class="proof-step">
          <strong>Step 4: Combine the linear terms.</strong>
          Notice that $h^\top A \mathbf{x}$ is a scalar. A scalar is equal to its own transpose:
          $$ h^\top A \mathbf{x} = (h^\top A \mathbf{x})^\top = \mathbf{x}^\top A^\top h $$
          Now the linear part becomes:
          $$ \mathbf{x}^\top A h + \mathbf{x}^\top A^\top h = \mathbf{x}^\top (A + A^\top) h $$
        </div>
        <div class="proof-step">
          <strong>Step 5: Extract the Gradient.</strong>
          The definition of the gradient $\nabla f(\mathbf{x})$ is the vector $\mathbf{g}$ satisfying $\text{Linear Part} = \mathbf{g}^\top h$.
          Comparing $\mathbf{x}^\top (A + A^\top) h$ with $\mathbf{g}^\top h$, we see:
          $$ \mathbf{g}^\top = \mathbf{x}^\top (A + A^\top) \implies \mathbf{g} = (A + A^\top)\mathbf{x} $$
          <b>Special Case:</b> If $A$ is symmetric ($A=A^\top$), then $A+A^\top = 2A$, so $\nabla f(\mathbf{x}) = 2A\mathbf{x}$.
        </div>
      </div>

      <h4>Gradients with Respect to Matrices</h4>
      <p>For a matrix variable $X \in \mathbb{R}^{m \times n}$, the gradient is an $m \times n$ matrix. The definition is consistent with the scalar directional derivative.</p>
      <ul>
        <li><b>Rule 1: Linear Trace:</b> $\nabla_X \mathrm{tr}(A^\top X) = A$.
        <br><i>Proof:</i> $\mathrm{tr}(A^\top X) = \langle A, X \rangle$. The function is linear: $f(X+\Delta) = \langle A, X+\Delta \rangle = f(X) + \langle A, \Delta \rangle$. The gradient is the coefficient of $\Delta$, which is $A$.</li>
        <li><b>Rule 2: Quadratic Trace:</b> $\nabla_X \mathrm{tr}(X^\top A X) = (A + A^\top)X$. If $A$ is symmetric, this is $2AX$.
        <br><i>Note:</i> This corresponds to $\frac{d}{dx}(ax^2) = 2ax$. For vector $\mathbf{x}$, $\nabla (\mathbf{x}^\top A\mathbf{x}) = (A+A^\top)\mathbf{x}$. For matrix $X$, the columns transform independently if $A$ is scalar, but coupled if $A$ is a matrix.
        <br><i>Derivation:</i>
        $$ \begin{aligned}
        f(X+\Delta) &= \mathrm{tr}((X+\Delta)^\top A (X+\Delta)) \\
        &= \mathrm{tr}(X^\top AX + \Delta^\top AX + X^\top A\Delta + \Delta^\top A\Delta) \\
        &\approx f(X) + \mathrm{tr}(\Delta^\top AX) + \mathrm{tr}(X^\top A \Delta) \\
        &= f(X) + \mathrm{tr}(\Delta^\top AX) + \mathrm{tr}(\Delta^\top A^\top X) \quad \text{(using } \mathrm{tr}(M)=\mathrm{tr}(M^\top)) \\
        &= f(X) + \mathrm{tr}(\Delta^\top (A+A^\top)X) \\
        &= f(X) + \langle (A+A^\top)X, \Delta \rangle
        \end{aligned} $$
        Thus $\nabla f(X) = (A+A^\top)X$.
        </li>
        <li><b>Rule 3: Log Determinant:</b> $\nabla_X \log \det X = X^{-\top}$. If $X \in \mathbb{S}^n_{++}$ (symmetric positive definite), then $\nabla_X \log \det X = X^{-1}$.
        <br>
        <div class="proof-box">
          <h4>Derivation: Gradient of Log Determinant</h4>
          <p>We derive the gradient of $f(X) = \log \det X$ (defined for $\det X > 0$) by identifying the linear term in the Taylor expansion of $f(X+\Delta)$ around $X$. This step-by-step derivation uses the perturbation method, which is robust and general.</p>
          <div class="proof-step">
            <strong>Step 1: Factor out X.</strong>
            We want to isolate the perturbation $\Delta$. Since $X$ is invertible, we can write $X+\Delta = X(I + X^{-1}\Delta)$.
            $$ \log \det (X + \Delta) = \log \det (X(I + X^{-1}\Delta)) $$
            Using the multiplicative property of the determinant ($\det(AB) = \det(A)\det(B)$) and the additive property of the logarithm ($\log(ab) = \log a + \log b$):
            $$ = \log (\det X \cdot \det(I + X^{-1}\Delta)) = \log \det X + \log \det(I + X^{-1}\Delta) $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Relate Determinant to Trace via Eigenvalues.</strong>
            Let $E = X^{-1}\Delta$. We need to approximate $\log \det(I + E)$ for small $E$.
            Let $\lambda_i$ be the eigenvalues of $E$. The eigenvalues of $I+E$ are then $1+\lambda_i$.
            $$ \det(I + E) = \prod_{i=1}^n (1 + \lambda_i) $$
            Taking the natural logarithm turns the product into a sum:
            $$ \log \det(I + E) = \sum_{i=1}^n \log(1 + \lambda_i) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Linearize via Taylor Series.</strong>
            Recall the scalar Taylor series expansion for the natural logarithm near 1:
            $$ \log(1+z) \approx z \quad \text{for } z \approx 0 $$
            For small perturbations $E$, the eigenvalues $\lambda_i$ are small.
            Applying this approximation to each term:
            $$ \sum_{i=1}^n \log(1 + \lambda_i) \approx \sum_{i=1}^n \lambda_i $$
            We recognize the sum of eigenvalues $\sum \lambda_i$ as the <b>trace</b> of the matrix $E$.
            $$ \sum_{i=1}^n \lambda_i = \mathrm{tr}(E) = \mathrm{tr}(X^{-1}\Delta) $$
            Thus, to first order, the change in the log-determinant is exactly the trace of the relative change matrix $E$.
          </div>
          <div class="proof-step">
            <strong>Step 4: Extract the Gradient.</strong>
            Substituting back, the first-order approximation of the function is:
            $$ f(X+\Delta) \approx f(X) + \mathrm{tr}(X^{-1}\Delta) $$
            The gradient $\nabla f(X)$ is defined as the matrix $G$ such that the linear term is $\langle G, \Delta \rangle = \mathrm{tr}(G^\top \Delta)$.
            We rewrite our linear term to match this form:
            $$ \mathrm{tr}(X^{-1}\Delta) = \mathrm{tr}((X^{-\top})^\top \Delta) = \langle X^{-\top}, \Delta \rangle $$
            Comparing terms, we identify $G = X^{-\top}$.
            <br><b>Result:</b> $\nabla_X \log \det X = X^{-\top}$.
            <br>If $X$ is symmetric ($X=X^\top$), then $\nabla f(X) = X^{-1}$.
          </div>
        </div>
        </li>
      </ul>
      <div class="insight">
        <h4>Chain Rule for Matrix Functions</h4>
        <p>If $f(X) = g(h(X))$, then the differential is $df = \mathbf{g}'(h(X)) \circ dh$. For example, to differentiate $\|Ax - b\|_2^2$:</p>
        <ol>
          <li>Let $\mathbf{r} = A\mathbf{x} - \mathbf{b}$. Then $f = \mathbf{r}^\top \mathbf{r}$.</li>
          <li>$df = 2\mathbf{r}^\top dr$.</li>
          <li>$dr = A dx$.</li>
          <li>Substitute: $df = 2\mathbf{r}^\top A dx = (2A^\top \mathbf{r})^\top dx$.</li>
          <li>The gradient is the transpose of the coefficient of $dx^\top$ (or the vector multiplying $dx$), so $\nabla f = 2A^\top \mathbf{r} = 2A^\top(A\mathbf{x}-\mathbf{b})$.</li>
        </ol>
      </div>

      <div class="proof-box">
        <h4>Derivation: Hessian of Least Squares</h4>
        <p>Let $f(\mathbf{x}) = \|A\mathbf{x} - \mathbf{b}\|_2^2$. We show $\nabla^2 f(\mathbf{x}) = 2A^\top A$ and prove its convexity.</p>

        <div class="proof-step">
          <strong>Step 1: Expand.</strong>
          $$ f(\mathbf{x}) = (A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = \mathbf{x}^\top A^\top A \mathbf{x} - 2\mathbf{b}^\top A \mathbf{x} + \mathbf{b}^\top \mathbf{b} $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Gradient.</strong>
          Using the rules derived above:
          <ul>
            <li>$\nabla (\mathbf{x}^\top (A^\top A) \mathbf{x}) = 2A^\top A \mathbf{x}$ (since $A^\top A$ is symmetric).</li>
            <li>$\nabla (-2(A^\top \mathbf{b})^\top \mathbf{x}) = -2A^\top \mathbf{b}$.</li>
            <li>$\nabla (\mathbf{b}^\top \mathbf{b}) = 0$.</li>
          </ul>
          Summing these: $\nabla f(\mathbf{x}) = 2A^\top A \mathbf{x} - 2A^\top \mathbf{b} = 2A^\top (A\mathbf{x} - \mathbf{b})$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Hessian.</strong>
          Differentiating $\nabla f(\mathbf{x})$ with respect to $\mathbf{x}$:
          $$ \nabla^2 f(\mathbf{x}) = \nabla_x (2A^\top A \mathbf{x}) = 2A^\top A $$
          The Hessian is constant, characteristic of quadratic functions.
        </div>

        <div class="proof-step">
          <strong>Step 4: Convexity (PSD Check).</strong>
          For any vector $\mathbf{v} \in \mathbb{R}^n$:
          $$ \mathbf{v}^\top \nabla^2 f(\mathbf{x}) \mathbf{v} = \mathbf{v}^\top (2A^\top A) \mathbf{v} = 2(A\mathbf{v})^\top (A\mathbf{v}) = 2\|A\mathbf{v}\|_2^2 \ge 0 $$
          Since the quadratic form is always non-negative, the Hessian is Positive Semidefinite (PSD). Thus, the least squares objective is always <b>convex</b>.
        </div>
      </div>

      <div class="theorem-box">
        <h4>Lemma: Quadratic Minimization (Completion of the Square)</h4>
        <p>This result is fundamental to convex optimization derivation. Let $H \in \mathbb{S}_{++}^n$ (symmetric positive definite) and $\mathbf{g} \in \mathbb{R}^n$. Then the unique minimizer of the quadratic form:
        $$ \inf_{\mathbf{x}\in\mathbb R^n}\left\{\frac12 \mathbf{x}^\top H\mathbf{x} + \mathbf{g}^\top \mathbf{x}\right\} $$
        is given by $\boxed{\mathbf{x}^\star = -H^{-1}\mathbf{g}}$, and the optimal value is $\boxed{-\frac12 \mathbf{g}^\top H^{-1}\mathbf{g}}$.</p>
        <p><i>Proof:</i> Complete the square. Let $\mathbf{x} = \mathbf{z} - H^{-1}\mathbf{g}$.
        $$ \frac12 (\mathbf{z} - H^{-1}\mathbf{g})^\top H (\mathbf{z} - H^{-1}\mathbf{g}) + \mathbf{g}^\top (\mathbf{z} - H^{-1}\mathbf{g}) $$
        Expanding this yields $\frac12 \mathbf{z}^\top H \mathbf{z} - \frac12 \mathbf{g}^\top H^{-1}\mathbf{g}$. Since $H \succ 0$, the minimum is at $\mathbf{z}=0$.
        </p>
      </div>
    </section>

    <!-- SECTION 5: PSD MATRICES -->
    <section class="section-card" id="section-psd">
      <h2>5. Positive Semidefinite Matrices</h2>

      <h3>5.1 Symmetric Matrices and the Spectral Theorem</h3>
      <p>In convex optimization, a central object is the Hessian matrix $\nabla^2 f(\mathbf{x})$. By Schwarz's Theorem, if a function is twice continuously differentiable, its Hessian is symmetric. A matrix $A \in \mathbb{R}^{n \times n}$ is symmetric if $A = A^\top$. The set of such matrices is denoted $\mathbb{S}^n$.</p>

      <h4>The Spectral Theorem</h4>
      <p>If $A \in \mathbb{S}^n$, then:</p>
      <ol>
        <li>All <b>eigenvalues</b> ($\lambda_1, \dots, \lambda_n$) are <b>real numbers</b>.</li>
        <li>There exists a set of <b>orthonormal eigenvectors</b> $q_1, \dots, q_n$.</li>
        <li>$A$ can be diagonalized as $A = Q \Lambda Q^\top$, where $Q$ is an orthogonal matrix ($Q^\top Q = I$) containing the eigenvectors, and $\Lambda$ is a diagonal matrix containing the eigenvalues.</li>
      </ol>

      <h4>Why are Eigenvectors of Symmetric Matrices Orthogonal?</h4>
      <p>The Spectral Theorem guarantees orthogonal eigenvectors for symmetric matrices. The proof relies on the adjoint property.</p>
      <div class="proof-box">
        <h4>Proof of Orthogonality</h4>
        <div class="proof-step">
            <strong>Setup:</strong>
            Let $A$ be symmetric ($A^\top = A$). Let $v_1, v_2$ be eigenvectors with distinct eigenvalues $\lambda_1 \neq \lambda_2$:
            $$ A v_1 = \lambda_1 v_1, \quad A v_2 = \lambda_2 v_2 $$
        </div>
        <div class="proof-step">
            <strong>Step 1: Evaluate $\langle A v_1, v_2 \rangle$ directly.</strong>
            $$ \langle A v_1, v_2 \rangle = \langle \lambda_1 v_1, v_2 \rangle = \lambda_1 \langle v_1, v_2 \rangle $$
        </div>
        <div class="proof-step">
            <strong>Step 2: Evaluate using symmetry.</strong>
            $$ \langle A v_1, v_2 \rangle = \langle v_1, A^\top v_2 \rangle = \langle v_1, A v_2 \rangle = \langle v_1, \lambda_2 v_2 \rangle = \lambda_2 \langle v_1, v_2 \rangle $$
        </div>
        <div class="proof-step">
            <strong>Step 3: Conclusion.</strong>
            Subtracting the two results: $(\lambda_1 - \lambda_2) \langle v_1, v_2 \rangle = 0$.
            Since $\lambda_1 \neq \lambda_2$, we must have $\langle v_1, v_2 \rangle = 0$.
        </div>
      </div>

      <p><b>Implication for convexity:</b> The "shape" of a multivariable function (bowl vs. saddle) is entirely determined by the signs of the eigenvalues of its Hessian.</p>

      <h3>5.2 Positive Semidefinite (PSD) Matrices</h3>
      <p>This definition is central to the course. It provides the matrix equivalent of a "non-negative number" and underpins the definition of <b>Convex Functions</b> (<a href="../05-convex-functions-basics/index.html">Lecture 05</a>) and <b>Semidefinite Programming</b> (<a href="../07-convex-problems-standard/index.html">Lecture 08</a>).</p>

      <div class="insight">
        <h4>Forward Connection: The PSD Cone</h4>
        <p>The set of all PSD matrices forms a convex cone, denoted $\mathbb{S}^n_+$. This geometric object is the foundation of <b>Semidefinite Programming (SDP)</b>, a powerful generalization of linear programming that we will study in <a href="../07-convex-problems-standard/index.html">Lecture 08</a>. As LP optimizes over the non-negative orthant, SDP optimizes over the PSD cone.</p>
      </div>

      <h4>The Variational Definition</h4>
      <p>A symmetric matrix $A \in \mathbb{S}^n$ is <a href="javascript:void(0)" class="definition-link" data-term="positive semidefinite">Positive Semidefinite (PSD)</a>, denoted as $A \succeq 0$, if:</p>
      $$ \mathbf{x}^\top A \mathbf{x} \ge 0 \quad \text{for all } \mathbf{x} \in \mathbb{R}^n $$
      <p><b>Geometric Intuition (Generalized Non-Negative Numbers):</b> Just as a non-negative number $a \ge 0$ allows us to define convex quadratic functions like $f(\mathbf{x}) = ax^2$, a PSD matrix allows us to define convex quadratic forms in higher dimensions. If $A \succeq 0$, the function $f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$ looks like a bowl. If $A$ has a negative eigenvalue, the function looks like a saddle (curves down in some directions), breaking convexity.</p>

      <div class="example-box">
        <h4>Example: Checking Energy</h4>
        <p>Let $A = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$. Is it PSD?
        <br>Let's check the "energy" for an arbitrary vector $\mathbf{x} = (x_1, x_2)^\top$.
        $$ \mathbf{x}^\top A \mathbf{x} = 2x_1^2 - 2x_1 x_2 + 2x_2^2 = x_1^2 + (x_1 - x_2)^2 + x_2^2 $$
        Since squares are always non-negative, the sum is $\ge 0$ for any real $x_1, x_2$.
        <br>Thus, $A \succeq 0$. In fact, it is Positive Definite (strictly positive for $\mathbf{x} \neq 0$).
        </p>
      </div>

      <h4>The Spectral Definition</h4>
      <p>A symmetric matrix $A \in \mathbb{S}^n$ is PSD if and only if all its eigenvalues are non-negative:</p>
      $$ \lambda_i(A) \ge 0 \quad \text{for all } i = 1, \dots, n $$

      <h4>Factorization of PSD Matrices (Gram Matrix)</h4>
      <p>A symmetric matrix $A$ is PSD if and only if it can be written as a Gram matrix: $A = B^\top B$ for some matrix $B$ (not necessarily square).
      <br><i>Proof:</i> If $A \succeq 0$, spectral decomposition gives $A = Q \Lambda Q^\top = (Q \Lambda^{1/2}) (Q \Lambda^{1/2})^\top$. Let $B = (Q \Lambda^{1/2})^\top$. This $B$ is a "square root" of $A$.
      Conversely, if $A = B^\top B$, then $\mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top B^\top B \mathbf{x} = (B\mathbf{x})^\top (B\mathbf{x}) = \|B\mathbf{x}\|_2^2 \ge 0$.
      </p>

      <div class="proof-box">
        <h4>Proof: Equivalence of Definitions</h4>
        <p><b>Goal:</b> Prove $(\forall \mathbf{x}, \mathbf{x}^\top A \mathbf{x} \ge 0) \iff (\forall i, \lambda_i(A) \ge 0)$.</p>
        <div class="proof-step">
          <strong>Variational $\implies$ Eigenvalues:</strong>
          Let $\mathbf{v}$ be an eigenvector with eigenvalue $\lambda$.
          $$ \mathbf{v}^\top A \mathbf{v} = \mathbf{v}^\top (\lambda \mathbf{v}) = \lambda \|\mathbf{v}\|_2^2 $$
          Since $\mathbf{v}^\top A \mathbf{v} \ge 0$ and $\|\mathbf{v}\|_2^2 > 0$, we must have $\lambda \ge 0$.
        </div>
        <div class="proof-step">
          <strong>Eigenvalues $\implies$ Variational:</strong>
          Since $A$ is symmetric, $A = Q \Lambda Q^\top$. Let $\mathbf{y} = Q^\top \mathbf{x}$.
          $$ \mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top Q \Lambda Q^\top \mathbf{x} = \mathbf{y}^\top \Lambda \mathbf{y} = \sum_{i=1}^n \lambda_i y_i^2 $$
          Since $\lambda_i \ge 0$ and $y_i^2 \ge 0$, the sum is non-negative.
        </div>
      </div>

      <h3>5.3 Geometry of PSD Matrices: Ellipsoids</h3>
      <p>Positive Definite (PD) matrices define the shape of <b>ellipsoids</b>. For a matrix $P \in \mathbb{S}^n_{++}$, the set:
      $$ \mathcal{E} = \{ \mathbf{x} \in \mathbb{R}^n \mid \mathbf{x}^\top P \mathbf{x} \le 1 \} $$
      is an ellipsoid centered at the origin.
      </p>
      <ul>
        <li><b>Axes alignment:</b> The axes of the ellipsoid are aligned with the eigenvectors of $P$.</li>
        <li><b>Axis lengths:</b> The semi-axis lengths are $1/\sqrt{\lambda_i}$. Note the inverse relationship: large eigenvalue $\implies$ steep curvature $\implies$ short axis.</li>
      </ul>

      <h3>5.4 The Rayleigh Quotient: Bridge to Optimization</h3>
      <p>For a symmetric matrix $A \in \mathbb{S}^n$, the <b>Rayleigh Quotient</b> is defined as:</p>
      $$ R_A(\mathbf{x}) := \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}, \quad \mathbf{x} \neq 0 $$
      
      <h4>The Core Theorem: Min/Max = Extreme Eigenvalues</h4>
      <p>For symmetric $A$ with eigenvalues $\lambda_{\min} \le \dots \le \lambda_{\max}$:</p>
      $$ \lambda_{\min} = \min_{\mathbf{x} \neq 0} R_A(\mathbf{x}) \quad \text{and} \quad \lambda_{\max} = \max_{\mathbf{x} \neq 0} R_A(\mathbf{x}) $$
      <p>This variational characterization is fundamental. It explains why optimization algorithms like Power Iteration converge to the top eigenvector.</p>

      <div class="proof-box">
        <h4>Proof: The "Weighted Average" Intuition</h4>
        <p>Why does the Rayleigh quotient never escape the range $[\lambda_{\min}, \lambda_{\max}]$?</p>
        <div class="proof-step">
          <strong>Step 1: Spectral Decomposition.</strong>
          Let $\mathbf{y} = Q^\top \mathbf{x}$. Then $\|\mathbf{y}\|_2 = \|\mathbf{x}\|_2$.
          $$ R_A(\mathbf{x}) = \frac{\mathbf{y}^\top \Lambda \mathbf{y}}{\mathbf{y}^\top \mathbf{y}} = \frac{\sum_{i=1}^n \lambda_i y_i^2}{\sum_{i=1}^n y_i^2} $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Define Weights.</strong>
          Let $w_i = \frac{y_i^2}{\sum_j y_j^2}$. Notice that $w_i \ge 0$ and $\sum w_i = 1$. Thus:
          $$ R_A(\mathbf{x}) = \sum_{i=1}^n w_i \lambda_i $$
          This is a <b>convex combination</b> of the eigenvalues.
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          The value must lie in $[\min \lambda_i, \max \lambda_i]$.
        </div>
      </div>

      <h4>Convexity of $\lambda_{\max}$</h4>
      <p>This variational characterization proves that $\lambda_{\max}(A)$ is a <b>convex function</b> of $A$. For fixed $\mathbf{x}$, the function $g(A) = \mathbf{x}^\top A \mathbf{x}$ is linear in $A$. Since $\lambda_{\max}(A)$ is the pointwise supremum of linear functions (indexed by $\mathbf{x}$), it is convex.</p>

      <h3>5.5 The Schur Complement Lemma</h3>
      <p>The Schur Complement is essential for handling block matrices in optimization. It allows conversion of nonlinear constraints into Linear Matrix Inequalities (LMIs).</p>

      <div class="insight">
        <h4>Geometric Intuition: Completing the Square</h4>
        <p>Consider a scalar quadratic form $q(\mathbf{x}, \mathbf{y}) = ax^2 + 2bxy + cy^2$. Completing the square ($a>0$):
        $$ q(\mathbf{x}, \mathbf{y}) = a(\mathbf{x} + \frac{b}{a}\mathbf{y})^2 + (c - \frac{b^2}{a})\mathbf{y}^2 $$
        The sign depends on $S = c - b^2/a$. The Schur complement is the matrix generalization of this coefficient.</p>
      </div>

      <figure style="text-align: center;">
        <img src="assets/schur_complement_block.png"
             alt="Schur Complement Block Matrix Transformation Visualization"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 9:</i> Visualization of the Schur complement process.</figcaption>
      </figure>

      <h4>Theorem: Schur Complement Condition for PSD</h4>
      <p>Let $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ with $A \succ 0$. Then:
      $$ M \succeq 0 \iff S = C - B^\top A^{-1} B \succeq 0 $$
      </p>

      <div class="proof-box">
        <h4>Proof via Gaussian Elimination</h4>
        <p>We can diagonalize $M$ using a block elimination matrix:
        $$ \begin{bmatrix} I & 0 \\ -B^\top A^{-1} & I \end{bmatrix} \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} \begin{bmatrix} I & -A^{-1}B \\ 0 & I \end{bmatrix} = \begin{bmatrix} A & 0 \\ 0 & C - B^\top A^{-1} B \end{bmatrix} $$
        Let the transformation matrix be $L^\top$. Then $L^\top M L = \text{diag}(A, S)$.
        By congruence (Sylvester's Law of Inertia), $M$ has the same number of positive, negative, and zero eigenvalues as the block diagonal matrix.
        Since $A \succ 0$, $M \succeq 0$ iff $S \succeq 0$.
        </p>
      </div>

      <div class="insight">
          <h4>The Triangle of Equivalence</h4>
          <p>For a positive definite matrix $Y \succ 0$, the following are equivalent:</p>
          $$
          \boxed{
          \begin{matrix}
          \textbf{Scalar Inequality} & & \textbf{Block Matrix LMI} \\
          t \ge \mathbf{x}^\top Y^{-1} \mathbf{x} & \iff & \begin{bmatrix} Y & \mathbf{x} \\ \mathbf{x}^\top & t \end{bmatrix} \succeq 0 \\
          & \Updownarrow & \\
          & \textbf{Rank-1 Update} & \\
          & tY - \mathbf{x}\mathbf{x}^\top \succeq 0 &
          \end{matrix}
          }
          $$
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Matrix Geometry & Definiteness</h3>
        <p><b>Explore Linear Maps and Quadratic Forms:</b> This interactive tool connects the linear transformation $A\mathbf{x}$ with the quadratic form $\mathbf{x}^\top A\mathbf{x}$.</p>
        <iframe src="widgets/la_batch1.html?mode=eigen" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1); width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Hessian Landscape Visualizer</h3>
        <p><b>Connect Eigenvalues to Function Curvature:</b> This 3D visualizer renders the surface of a quadratic function $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top Q \mathbf{x}$ and displays its Hessian matrix $Q$. The eigenvalues of the Hessian directly control the curvature:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Large positive eigenvalues:</b> Steep curvature (fast convergence in optimization)</li>
          <li><b>Small positive eigenvalues:</b> Flat directions (slow convergence)</li>
          <li><b>Condition number ($\kappa = \lambda_{\max}/\lambda_{\min}$):</b> When this ratio is large, gradient descent converges slowly</li>
        </ul>
        <p><i>Practical insight:</i> This visualization explains why preconditioning (transforming to balance eigenvalues) dramatically speeds up iterative solvers!</p>
        <div id="widget-hessian-landscape" style="width: 100%; max-width: 800px; height: 400px; position: relative; margin: 0 auto;"></div>
      </div>

      <h3>5.6 The Loewner Order</h3>
      <p>For symmetric matrices $X, Y \in \mathbb{S}^n$, we write <b>$X \succeq Y$</b> if and only if $X - Y$ is <b>positive semidefinite (PSD)</b>, meaning:</p>
      $$
      \mathbf{v}^\top (X - Y) \mathbf{v} \ge 0 \quad \text{for all } \mathbf{v} \in \mathbb{R}^n
      $$

      <div class="proof-box">
        <h4>Key Properties of the Loewner Order</h4>
        <ul>
          <li><b>Eigenvalue characterization:</b> $X \succeq 0$ if and only if all eigenvalues of $X$ are nonnegative.</li>
          <li><b>Transitivity:</b> $X \succeq Y$ and $Y \succeq Z \implies X \succeq Z$.</li>
          <li><b>Additivity:</b> $X \succeq Y \implies X+Z \succeq Y+Z$.</li>
          <li><b>Congruence:</b> $X \succeq Y \implies T^\top X T \succeq T^\top Y T$ for any matrix $T$.</li>
        </ul>
      </div>

      <div class="recap-box">
        <h3>Summary: The PSD Checklist</h3>
        <p>To check if a symmetric matrix $A$ is PSD ($A \succeq 0$), use any of these equivalent tests:</p>
        <ol>
          <li><b>Eigenvalues:</b> All $\lambda_i(A) \ge 0$.</li>
          <li><b>Energy:</b> $\mathbf{x}^\top A \mathbf{x} \ge 0$ for all vectors $\mathbf{x}$.</li>
          <li><b>Principal Minors:</b> All principal minors are non-negative.</li>
          <li><b>Factorization:</b> $A = B^\top B$ for some matrix $B$ (Cholesky/Gram).</li>
        </ol>
      </div>
    </section>

    <!-- SECTION 6: PROJECTIONS -->
    <section class="section-card" id="section-projections">
      <h2>6. Projections onto Subspaces and Affine Sets</h2>

      <p>The concept of "finding the closest point" is the geometric heart of optimization. Whether solving least squares, applying constraints, or analyzing duality, we are often just computing an orthogonal projection.</p>

      <h3>6.1 1D Projection: The Shadow</h3>
      <p>Before tackling general subspaces, let's derive the projection of a vector $\mathbf{x}$ onto the line spanned by a single non-zero vector $a \in \mathbb{R}^n$. This simple case contains all the intuition needed for the general theory.</p>
      <p>We seek the scalar $\alpha$ that minimizes the squared distance $\|x - \alpha a\|_2^2$.
      $$ f(\alpha) = \|\mathbf{x} - \alpha a\|_2^2 = (\mathbf{x} - \alpha a)^\top (\mathbf{x} - \alpha a) = \|\mathbf{x}\|_2^2 - 2\alpha (a^\top \mathbf{x}) + \alpha^2 \|a\|_2^2 $$
      This is a simple convex quadratic in $\alpha$. Setting the derivative to zero:
      $$ f'(\alpha) = -2(a^\top \mathbf{x}) + 2\alpha \|a\|_2^2 = 0 \implies \alpha^\star = \frac{a^\top \mathbf{x}}{\|a\|_2^2} $$
      The projection is the vector $\mathbf{p} = \alpha^\star a$:
      $$ \mathbf{p} = \frac{a^\top \mathbf{x}}{a^\top a} a = \left( \frac{a a^\top}{a^\top a} \right) \mathbf{x} $$
      </p>

      <div class="insight">
        <h4>Geometric Insight: Orthogonality of the Residual</h4>
        <p>Let $\mathbf{r} = \mathbf{x} - \mathbf{p}$ be the error (residual) vector. Check its inner product with $a$:
        $$ a^\top \mathbf{r} = a^\top \left( \mathbf{x} - \frac{a^\top \mathbf{x}}{a^\top a} a \right) = a^\top \mathbf{x} - \frac{a^\top \mathbf{x}}{a^\top a} (a^\top a) = a^\top \mathbf{x} - a^\top \mathbf{x} = 0 $$
        The error is orthogonal to the subspace (line). This <b>Orthogonality Principle</b> is universal: the optimal error is always perpendicular to the feasible set (for subspaces and affine sets).</p>
      </div>

      <h3>6.2 Orthogonal Projection onto a General Subspace</h3>
      <p>Let $\mathcal{S} \subseteq \mathbb{R}^m$ be a subspace and $\mathbf{b} \in \mathbb{R}^m$. The <b>orthogonal projection</b> $\mathbf{p} \in \mathcal{S}$ of $\mathbf{b}$ is the unique vector in $\mathcal{S}$ minimizing $\|\mathbf{b} - \mathbf{p}\|_2$.</p>

      <h4>The Orthogonal Decomposition Theorem</h4>
      <p>Every vector $\mathbf{x} \in \mathbb{R}^n$ can be uniquely decomposed as:
      $$ \mathbf{x} = \mathbf{p} + \mathbf{z} $$
      where $\mathbf{p} \in W$ (the projection onto subspace $W$) and $\mathbf{z} \in W^\perp$ (the component orthogonal to $W$). This decomposition is unique and provides the solution to the closest-point problem:
      $$ \operatorname{proj}_W(\mathbf{x}) = \arg\min_{\mathbf{w} \in W} \|\mathbf{x} - \mathbf{w}\|_2 $$
      </p>

      <div class="proof-box">
        <h4>Proof: Optimality via the Pythagorean Theorem</h4>
        <div class="proof-step">
          <strong>Setup:</strong> Assume we have the orthogonal decomposition $\mathbf{x} = \mathbf{p} + \mathbf{z}$ with $\mathbf{p} \in W$ and $\mathbf{z} \in W^\perp$. We must show that $\mathbf{p}$ minimizes the distance $\|\mathbf{x} - \mathbf{w}\|_2$ over all $\mathbf{w} \in W$.
        </div>
        <div class="proof-step">
          <strong>Key Observation:</strong> Let $\mathbf{w} \in W$ be any candidate. The error vector decomposes as:
          $$ \mathbf{x} - \mathbf{w} = (\mathbf{p} + \mathbf{z}) - \mathbf{w} = (\mathbf{p} - \mathbf{w}) + \mathbf{z} $$
          Since $\mathbf{p}, \mathbf{w} \in W$, their difference $(\mathbf{p} - \mathbf{w}) \in W$ (subspaces are closed under vector addition). By definition, $\mathbf{z} \in W^\perp$, so $(\mathbf{p} - \mathbf{w}) \perp \mathbf{z}$.
        </div>
        <div class="proof-step">
          <strong>Apply Pythagorean Theorem:</strong> For orthogonal vectors, the squared norm of the sum equals the sum of squared norms:
          $$ \|\mathbf{x} - \mathbf{w}\|_2^2 = \|(\mathbf{p} - \mathbf{w}) + \mathbf{z}\|_2^2 = \|\mathbf{p} - \mathbf{w}\|_2^2 + \|\mathbf{z}\|_2^2 $$
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> The term $\|\mathbf{z}\|_2^2$ is fixed (depends only on $\mathbf{x}$, not on our choice of $\mathbf{w}$). To minimize the total error, we must minimize $\|\mathbf{p} - \mathbf{w}\|_2^2$. Since norms are non-negative, the minimum value of $\|\mathbf{p} - \mathbf{w}\|_2^2$ is zero, achieved when $\mathbf{w} = \mathbf{p}$.
          <br><b>Result:</b> The unique closest point in $W$ to $\mathbf{x}$ is the orthogonal projection $\mathbf{p} = \operatorname{proj}_W(\mathbf{x})$.
        </div>
      </div>

      <h4>Characterization via Orthogonality</h4>
      <p>The projection is characterized by the condition that the residual is orthogonal to the entire subspace:
      $$ \mathbf{b} - \mathbf{p} \perp \mathcal{S} \quad \iff \quad \mathbf{v}^\top(\mathbf{b} - \mathbf{p}) = 0 \ \ \forall \mathbf{v} \in \mathcal{S} $$
      This is the <b>normal equations</b> condition that appears throughout optimization theory.</p>

      <div style="display: flex; gap: 16px; flex-wrap: wrap; margin: 24px 0;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/layer2_21_dot_product_projection.gif"
               alt="Dot product as projection onto a direction"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> The dot product $\mathbf{x}^\top \mathbf{u} = \|\mathbf{u}\| \times (\text{signed shadow length of } \mathbf{x} \text{ on } \mathbf{u})$. Projection is fundamentally about measuring "how much" of one vector lies along another.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/layer2_23_orthogonality_projection.gif"
               alt="Projection decomposition: v = proj + residual"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> The orthogonal decomposition $\mathbf{v} = \operatorname{proj}_\mathbf{u}(\mathbf{v}) + \text{residual}$. The residual is always perpendicular to $\mathbf{u}$, which is the geometric signature of optimality.</figcaption>
        </figure>
      </div>

      <h3>6.2.1 Projection via a Basis</h3>
      <p>If $Q \in \mathbb{R}^{m \times k}$ has orthonormal columns spanning $\mathcal{S}$, the projector is:
      $$ P = QQ^\top \quad \text{and} \quad \mathbf{p} = P\mathbf{b} = QQ^\top \mathbf{b} $$
      </p>

      <h4>Fundamental Properties of Projection Operators</h4>
      <p>The orthogonal projection map $P_W: \mathbf{x} \mapsto \operatorname{proj}_W(\mathbf{x})$ is a <b>linear operator</b> with special algebraic properties:</p>
      <ul>
        <li><b>Linearity:</b> $P_W(\alpha \mathbf{x} + \beta \mathbf{y}) = \alpha P_W(\mathbf{x}) + \beta P_W(\mathbf{y})$ for all scalars $\alpha, \beta$ and vectors $\mathbf{x}, \mathbf{y}$.</li>
        <li><b>Idempotence:</b> $P_W^2 = P_W$. Projecting twice is the same as projecting once (you're already in the subspace after the first projection).</li>
        <li><b>Self-Adjointness (Symmetry):</b> $P_W^\top = P_W$. The projection operator is symmetric with respect to the standard inner product.</li>
      </ul>
      <p>These properties characterize orthogonal projection matrices. Conversely, any matrix $P$ satisfying $P^2 = P$ and $P^\top = P$ is the orthogonal projector onto its column space $\mathcal{R}(P)$.</p>

      <div class="insight">
        <h4>Geometric Meaning of Idempotence</h4>
        <p>The property $P^2 = P$ has a beautiful geometric interpretation: if you project a vector $\mathbf{x}$ onto a subspace $W$ to get $\mathbf{p}$, then projecting $\mathbf{p}$ again onto $W$ simply returns $\mathbf{p}$ itself. Mathematically:
        $$ P(P\mathbf{x}) = P\mathbf{p} = \mathbf{p} = P\mathbf{x} $$
        This is because $\mathbf{p}$ already lives in $W$, so the closest point in $W$ to $\mathbf{p}$ is just $\mathbf{p}$ itself. Idempotence says that projection is a "stabilizing" operation: once you're in the target subspace, you stay there.</p>
      </div>

      <h3>6.2.2 The Hat Matrix</h3>
      <p>If $A \in \mathbb{R}^{m \times n}$ has full column rank, the projection matrix onto its column space is given by the famous "Hat Matrix":
      $$ P = A(A^\top A)^{-1} A^\top \quad \text{and} \quad \mathbf{p} = P\mathbf{b} $$
      <b>Derivation:</b> This formula is not magic. It arises from solving the Least Squares problem $\min \|A\mathbf{x} - \mathbf{b}\|^2$.
      The optimal coefficients $\mathbf{x}$ solve the <b>Normal Equations</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$ (derived in Section 9).
      Solving for $\mathbf{x}$ gives $\mathbf{x} = (A^\top A)^{-1} A^\top \mathbf{b}$.
      The projection is the corresponding point in the column space: $\mathbf{p} = A\mathbf{x} = A(A^\top A)^{-1} A^\top \mathbf{b}$.
      </p>

      <h3>6.3 Projection onto Closed Convex Sets</h3>
      <p>The concept of projection extends beyond subspaces. For any nonempty, closed convex set $C$, and any point $y$, the projection $\Pi_C(y)$ is the unique point in $C$ closest to $y$.
      $$ \Pi_C(y) = \operatorname{argmin}_{x \in C} \|x - y\|_2 $$
      <b>Existence & Uniqueness:</b> Existence follows from $C$ being closed (and the norm being coercive). Uniqueness follows from the strict convexity of the Euclidean norm and the convexity of $C$.
      <br><b>The Projection Inequality:</b> The defining characteristic of the projection $x^* = \Pi_C(y)$ is the obtuse angle condition:
      $$ \langle y - x^*, z - x^* \rangle \le 0 \quad \forall z \in C $$
      Geometrically, the vector from the projection $x^*$ to the target $y$ makes an obtuse angle with any vector pointing from $x^*$ into the set. This condition is the ancestor of KKT stationarity and separating hyperplanes.</p>

      <h3>6.4 Projection onto an Affine Set</h3>
      <p>Consider an affine set defined by $\mathcal{A} = \{ \mathbf{x} \in \mathbb{R}^n \mid F\mathbf{x} = \mathbf{g} \}$, where $F \in \mathbb{R}^{\mathbf{p} \times n}$ has full row rank and $\mathbf{g} \in \mathbb{R}^p$. Projecting onto this set reduces to subspace projection by translating.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/projection-onto-affine-set.png"
             alt="Projection onto an affine set visualized as translation to a subspace, projection, then translation back"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 12:</i> Projection onto an affine set: translate to a parallel subspace, project orthogonally, then shift back by a particular solution.</figcaption>
      </figure>

      <h4>Explicit Formula via Least Norm</h4>
      <p>We solve the problem $\min \|\mathbf{x} - \mathbf{y}\|_2^2$ s.t. $F\mathbf{x} = \mathbf{g}$. The KKT conditions yield the linear system:
      $$ \begin{bmatrix} I & F^\top \\ F & 0 \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ \nu \end{bmatrix} = \begin{bmatrix} \mathbf{y} \\ \mathbf{g} \end{bmatrix} $$
      Solving for $\mathbf{x}$ gives the explicit projection formula:
      $$ \Pi_{\mathcal{A}}(\mathbf{y}) = \mathbf{y} - F^\top (F F^\top)^{-1} (F \mathbf{y} - \mathbf{g}) $$
      This is often more practical than constructing a nullspace basis $Z$.</p>

      <h4>Parametric Representation</h4>
      <p>Every point in $\mathcal{A}$ can be written as $\mathbf{x} = x_0 + Zt$, where:</p>
      <ul>
        <li>$x_0$ is any particular solution satisfying $Fx_0 = \mathbf{g}$.</li>
        <li>$Z$ is a matrix whose columns form a basis for $\mathcal{N}(F)$ (the nullspace of $F$).</li>
        <li>$t \in \mathbb{R}^k$ where $k = n - \mathbf{p}$ (dimension of the nullspace).</li>
      </ul>

      <h4>Euclidean Projection Formula (Explicit via KKT)</h4>
      <p>Alternatively, using Lagrange multipliers (a technique we will explore in Lecture 09), we can derive the projection without forming a nullspace basis.</p>
      $$
      \Pi_{\mathcal{A}}(\mathbf{y}) = \mathbf{y} - F^\top (F F^\top)^{-1} (F \mathbf{y} - \mathbf{g})
      $$

      <div class="proof-box">
        <h4>Derivation via Lagrange Multipliers</h4>
        <div class="proof-step">
          <strong>Step 1: Formulation.</strong> Minimize $\frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2$ subject to $F\mathbf{x} = \mathbf{g}$.
          The Lagrangian is $L(\mathbf{x}, \nu) = \frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2 + \nu^\top (F\mathbf{x} - \mathbf{g})$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Optimality Conditions.</strong>
          Gradient w.r.t $\mathbf{x}$: $\mathbf{x} - \mathbf{y} + F^\top \nu = 0 \implies \mathbf{x} = \mathbf{y} - F^\top \nu$.
          Primal feasibility: $F\mathbf{x} = \mathbf{g}$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Solve for Dual Variable $\nu$.</strong>
          Substitute $\mathbf{x}$ into the constraint:
          $$ F(\mathbf{y} - F^\top \nu) = \mathbf{g} \implies F\mathbf{y} - F F^\top \nu = \mathbf{g} \implies F F^\top \nu = F\mathbf{y} - \mathbf{g} $$
          Assuming $F$ has full row rank, $F F^\top$ is invertible:
          $$ \nu = (F F^\top)^{-1} (F\mathbf{y} - \mathbf{g}) $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Substitute back.</strong>
          $$ \mathbf{x} = \mathbf{y} - F^\top (F F^\top)^{-1} (F\mathbf{y} - \mathbf{g}) $$
        </div>
      </div>

      <h4>Euclidean Projection Formula (Nullspace)</h4>
      <p>The <b>Euclidean projection</b> of any point $\mathbf{y} \in \mathbb{R}^n$ onto $\mathcal{A}$ can also be computed via the nullspace basis:</p>
      $$
      \Pi_{\mathcal{A}}(\mathbf{y}) = \mathbf{x}_0 + Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - \mathbf{x}_0)
      $$

      <div class="proof-box">
        <h4>Geometric Interpretation and Derivation</h4>
        <p>This formula says: "Project $(\mathbf{y} - x_0)$ onto the nullspace $\mathcal{N}(F)$, then shift back by $x_0$."</p>

        <div class="proof-step">
          <strong>Step 1: Translate to subspace problem.</strong>
          We want to find $\mathbf{x} \in \mathcal{A}$ minimizing $\|x - y\|_2$. Let $\mathbf{x} = x_0 + \mathbf{w}$, where $\mathbf{w} \in \mathcal{N}(F)$.
          The problem becomes minimizing $\|(x_0 + w) - y\|_2 = \|w - (y - x_0)\|_2$ subject to $\mathbf{w} \in \mathcal{N}(F)$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Project onto nullspace.</strong>
          This is now a projection of the vector $(\mathbf{y} - x_0)$ onto the subspace $\mathcal{N}(F)$.
          Since the columns of $Z$ form a basis for $\mathcal{N}(F)$, the projection matrix onto this subspace is $P_{\mathcal{N}(F)} = Z(Z^\top Z)^{-1} Z^\top$.
          Thus, the optimal $\mathbf{w}^*$ is:
          $$ \mathbf{w}^* = \Pi_{\mathcal{N}(F)}(\mathbf{y} - x_0) = Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - x_0) $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Shift back.</strong>
          The projection onto the affine set is $\mathbf{x}^* = x_0 + \mathbf{w}^*$.
          $$ \Pi_{\mathcal{A}}(\mathbf{y}) = x_0 + Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - x_0) $$
        </div>
      </div>

      <div class="example-box">
        <h4>Example 1: Projection onto a Line</h4>
        <p>We compute the projection of the vector $\mathbf{b} = (2, 3)^\top$ onto the line spanned by the vector $\mathbf{u} = (1, 1)^\top$. The projection $\mathbf{p}$ is given by the formula:
        $$ \mathbf{p} = \frac{\mathbf{u}^\top \mathbf{b}}{\mathbf{u}^\top \mathbf{u}} \mathbf{u} = \frac{(1)(2) + (1)(3)}{(1)(1) + (1)(1)} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{5}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2.5 \\ 2.5 \end{pmatrix} $$
        The residual vector is $\mathbf{r} = \mathbf{b} - \mathbf{p} = (2 - 2.5, 3 - 2.5)^\top = (-0.5, 0.5)^\top$. We verify that the residual is orthogonal to the line: $\mathbf{u}^\top \mathbf{r} = (1)(-0.5) + (1)(0.5) = 0$.
        </p>
      </div>

    </section>

    <!-- SECTION 7: LEAST SQUARES -->
    <section class="section-card" id="section-least-squares">
      <h2>7. The Method of Least Squares</h2>

      <h3>The Problem: Overdetermined Systems</h3>
      <p>In practice, we often encounter linear systems $A\mathbf{x}=\mathbf{b}$ with no exact solution. This typically occurs when $m > n$ (more equations than unknowns) and $\mathbf{b} \notin \mathcal{R}(A)$. Since we cannot reach $\mathbf{b}$ exactly, we settle for the "closest possible" point in the range of $A$.
      <br>The "least squares" approach finds this best approximate solution by minimizing the sum of squared errors—the squared Euclidean norm of the residual $r = A\mathbf{x}-\mathbf{b}$. This formulates the canonical <b>unconstrained optimization problem</b>:</p>
      $$ \min_{\mathbf{x} \in \mathbb{R}^n} \|A\mathbf{x} - \mathbf{b}\|_2^2 $$

      <div class="insight">
        <h4>Forward Connection: Optimization Formulation</h4>
        <p>This problem is an instance of <b>Unconstrained Convex Optimization</b>. It minimizes a convex quadratic loss function. The solution methods we derive here (analytical gradients) generalize to the iterative descent methods (Gradient Descent, Newton's Method) covered in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>. Furthermore, it is a specific case of <b>Quadratic Programming (QP)</b> (<a href="../07-convex-problems-standard/index.html">Lecture 07</a>), which adds linear constraints to the problem.</p>
      </div>

      <h3>Geometric Interpretation: Projection</h3>
      <p>The solution to the least squares problem has a clean geometric interpretation. The vector $A\mathbf{x}$ is constrained to lie in the <b>column space</b> of $A$, denoted $\mathcal{R}(A)$. The optimization problem $\min_x \|A\mathbf{x} - \mathbf{b}\|_2$ is therefore equivalent to finding the point $\mathbf{p} \in \mathcal{R}(A)$ that is closest to $\mathbf{b}$.</p>
      <p><b>The Shadow Analogy:</b> Imagine shining a light directly from above (perpendicular to the subspace). The "shadow" cast by $\mathbf{b}$ onto the subspace $\mathcal{R}(A)$ is the projection $\mathbf{p}$. The vector connecting the tip of the shadow to the tip of $\mathbf{b}$ is the error (or residual), and it stands perfectly upright (orthogonal) to the floor.</p>

      <h4>The 1D Case: Projection onto a Vector</h4>
      <p>Before solving the general case, consider projecting $\mathbf{x}$ onto the line spanned by a vector $\mathbf{y} \neq 0$. We want to find $t$ to minimize $\|x - ty\|_2^2$.
      $$ \phi(t) = \|\mathbf{x} - ty\|_2^2 = \|\mathbf{x}\|_2^2 - 2t(\mathbf{x}^\top \mathbf{y}) + t^2 \|\mathbf{y}\|_2^2 $$
      Taking the derivative with respect to $t$:
      $$ \phi'(t) = -2(\mathbf{x}^\top \mathbf{y}) + 2t \|\mathbf{y}\|_2^2 $$
      Setting $\phi'(t) = 0$ gives the optimal scaling:
      $$ t^\star = \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{y}\|_2^2} $$
      The projection is $\mathbf{p} = t^\star \mathbf{y}$.
      <br><i>Orthogonality Check:</i> Let the residual be $\mathbf{r} = \mathbf{x} - \mathbf{p}$. Then $\mathbf{y}^\top \mathbf{r} = \mathbf{y}^\top \mathbf{x} - t^\star \mathbf{y}^\top \mathbf{y} = \mathbf{y}^\top \mathbf{x} - \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{y}\|^2} \|\mathbf{y}\|^2 = 0$. The residual is orthogonal to $\mathbf{y}$. This intuition generalizes to subspaces.</p>

      <p>By the Pythagorean theorem, the closest point $\mathbf{p}$ must satisfy a specific geometric condition: the error vector $\mathbf{b} - \mathbf{p}$ must be orthogonal to the subspace $\mathcal{R}(A)$. <b>Think of dropping a perpendicular:</b> The shortest path from a point to a plane (subspace) is the straight line perpendicular to that plane.</p>

      <div class="proof-box">
        <h4>Proof: Optimality requires Orthogonality</h4>
        <p>Why must the optimal residual be orthogonal? Let $\mathbf{x}$ be our current guess, producing a point $\mathbf{p} = A\mathbf{x}$ in the column space. Let $\mathbf{r} = \mathbf{b} - \mathbf{p}$ be the residual.</p>
        <div class="proof-step">
            <strong>Hypothesis:</strong> Suppose $\mathbf{r}$ is <b>not</b> orthogonal to the column space.
            Then there exists some direction $\mathbf{v} \in \mathcal{R}(A)$ such that $\langle \mathbf{r}, \mathbf{v} \rangle \neq 0$.
        </div>
        <div class="proof-step">
            <strong>Correction Step:</strong> We can improve our solution by moving a small amount $\epsilon$ in the direction $\mathbf{v}$. Consider the new point $\mathbf{p}' = \mathbf{p} + \epsilon \mathbf{v}$.
            The new squared error is:
            $$ \|\mathbf{b} - \mathbf{p}'\|^2 = \|\mathbf{b} - \mathbf{p} - \epsilon \mathbf{v}\|^2 = \|\mathbf{r} - \epsilon \mathbf{v}\|^2 $$
            Expanding this:
            $$ \|\mathbf{r}\|^2 - 2\epsilon \langle \mathbf{r}, \mathbf{v} \rangle + \epsilon^2 \|\mathbf{v}\|^2 $$
        </div>
        <div class="proof-step">
            <strong>Descent Argument:</strong> For very small $\epsilon$, the linear term $-2\epsilon \langle \mathbf{r}, \mathbf{v} \rangle$ dominates the quadratic term.
            If we choose the sign of $\epsilon$ to match the sign of the inner product, the term $-2\epsilon \langle \mathbf{r}, \mathbf{v} \rangle$ will be negative.
            Thus, for small enough $\epsilon$, the total error decreases: $\|\mathbf{b} - \mathbf{p}'\|^2 < \|\mathbf{r}\|^2$.
        </div>
        <div class="proof-step">
            <strong>Conclusion:</strong> If the residual is not orthogonal, we can always find a better point. Therefore, the optimal point <b>must</b> have a residual orthogonal to every direction in the subspace.
            $$ \mathbf{b} - A\mathbf{x}^\star \perp \mathcal{R}(A) $$
        </div>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/least-squares-orthogonal-projection.png"
             alt="Least squares as orthogonal projection of b onto the column space of A with residual orthogonal to the subspace"
             style="max-width: 900px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 13:</i> Least squares geometry: the fitted vector $\mathbf{p}=A\mathbf{x}^\star$ is the projection of $\mathbf{b}$ onto $\mathcal{R}(A)$, and the residual $\mathbf{r}=\mathbf{b}-\mathbf{p}$ is orthogonal to the subspace.</figcaption>
      </figure>

      <div style="display: flex; gap: 16px; flex-wrap: wrap; margin: 24px 0;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/least_squares_projection.gif"
               alt="Least squares as projection onto column space"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> As $\mathbf{b}$ moves, its projection $\mathbf{p} = A\mathbf{x}^\star$ onto $\mathrm{Col}(A)$ updates, with residual length $\|\mathbf{b} - A\mathbf{x}^\star\|$ shown.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/least_squares_via_qr.gif"
               alt="Least squares via QR decomposition"
               style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
          <figcaption><i>Animation:</i> Stable computation via QR: compute $\mathbf{y} = Q^\top \mathbf{b}$, then $\mathbf{x}^\star = R^{-1}\mathbf{y}$. The result matches the projection.</figcaption>
        </figure>
      </div>

      <h3>The Normal Equations</h3>
      <p>The orthogonality condition, $(\mathbf{b} - A\mathbf{x}^\star) \perp \mathcal{R}(A)$, implies that the residual vector must be orthogonal to every basis vector of the subspace. Since the columns of $A$ span $\mathcal{R}(A)$, the residual must be orthogonal to each column of $A$. This can be expressed compactly as:
      $$ A^\top (\mathbf{b} - A\mathbf{x}^\star) = 0 $$
      Expanding this yields the <b>normal equations</b>:
      $$ A^\top A \mathbf{x}^\star = A^\top \mathbf{b} $$
      If the matrix $A$ has linearly independent columns (full column rank), then $A^\top A$ is invertible, and we can write the unique solution as $\mathbf{x}^\star = (A^\top A)^{-1} A^\top \mathbf{b}$.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Least Squares Projection</h3>
        <p><b>See the Geometry:</b> The solution $A\mathbf{x}^\star$ is the orthogonal projection of the target vector $\mathbf{b}$ onto the subspace spanned by the columns of $A$. The residual vector $\mathbf{r} = \mathbf{b} - A\mathbf{x}^\star$ is perpendicular (orthogonal) to the subspace.</p>
        <iframe src="widgets/la_batch2.html?mode=kernel" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>

      <h3>Uniqueness of the Solution</h3>
      <ul>
        <li>If $A$ has full column rank ($\mathrm{rank}(A) = n$), the solution $\mathbf{x}^\star$ is unique.</li>
        <li>If $A$ is rank-deficient ($\mathrm{rank}(A) < n$), there are infinitely many solutions. In this case, the pseudoinverse is used to find the solution with the minimum norm.</li>
      </ul>

      <div class="example-box">
        <h4>Example 2: Solving Least Squares with Normal Equations</h4>
        <p>Consider the least squares problem for the system $A\mathbf{x}=\mathbf{b}$ with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} $$
        First, we form the normal equations $A^\top A \mathbf{x} = A^\top \mathbf{b}$:
        $$ A^\top A = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} $$
        $$ A^\top \mathbf{b} = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 3 \end{pmatrix} $$
        We solve the system $\begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \mathbf{x} = \begin{pmatrix} 3 \\ 3 \end{pmatrix}$, which yields the solution $\mathbf{x}^\star = (3/4, 3/4)^\top$.
        The projection is $\mathbf{p} = A\mathbf{x}^\star = (3/2, 0, 3/2)^\top$, and the residual is $\mathbf{r} = \mathbf{b} - \mathbf{p} = (1/2, 0, -1/2)^\top$. We can verify the orthogonality condition: $A^\top \mathbf{r} = (0, 0)^\top$.
        </p>
      </div>

      <div class="example-box">
        <h4>Example 3: Rank-Deficient Case</h4>
        <p>Consider the system with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 3 \\ 6 \end{pmatrix} $$
        The columns of $A$ are linearly dependent, so the system is rank-deficient. The vector $\mathbf{b}$ is in the column space of $A$, so there are infinitely many solutions. The normal equations are $A^\top A \mathbf{x} = \begin{pmatrix} 5 & 5 \\ 5 & 5 \end{pmatrix} \mathbf{x} = \begin{pmatrix} 15 \\ 15 \end{pmatrix}$. This system has infinitely many solutions of the form $x_1 + x_2 = 3$. The pseudoinverse would provide the minimum-norm solution, $x_1 = x_2 = 1.5$.
        </p>
      </div>

      <h3>Variants: Weighted and Constrained Least Squares</h3>


      <h3>(a) Weighted Least Squares (WLS)</h3>
      <p>Given SPD weight $W \in \mathbb{R}^{m \times m}$:
      $$ \min_x \|A\mathbf{x} - \mathbf{b}\|_W^2 := (A\mathbf{x} - \mathbf{b})^\top W(A\mathbf{x} - \mathbf{b}) $$
      Let $C$ be a matrix such that $W = C^\top C$ (e.g., via Cholesky decomposition). Then the problem is equivalent to ordinary least squares in the <b>whitened</b> system, minimizing $\|C(Ax - b)\|_2^2 = \|CAx - Cb\|_2^2$. The normal equations are $A^\top W A \mathbf{x} = A^\top W \mathbf{b}$.</p>

      <h3>(b) Constrained to an Affine Set</h3>
      <p>Solve:
      $$ \min_x \|A\mathbf{x} - \mathbf{b}\|_2^2 \quad \text{s.t.} \quad F\mathbf{x} = \mathbf{g} $$
      One method: parametrize $\mathbf{x} = x_0 + Z\mathbf{y}$, where $Fx_0 = \mathbf{g}$ and columns of $Z$ form a basis for $\mathcal{N}(F)$. Then minimize $\|A(x_0 + Zy) - b\|_2^2$ over $\mathbf{y}$ (an unconstrained LS). QR on $AZ$ is typically best.</p>

    </section>

    <!-- SECTION 8: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>8. Review & Cheat Sheet</h2>
      <div class="lecture-summary" style="margin-bottom: 20px;">
        <p>This section condenses the lecture into a quick-reference format for definitions, properties, and standard results.</p>
      </div>

      <h3>Definitions</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>Definition</th>
            <th>Key Properties</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>Subspace</b></td>
            <td>Set $S$ closed under addition and scalar multiplication</td>
            <td>Contains $\mathbf{0}$. Intersection of subspaces is a subspace.</td>
          </tr>
          <tr>
            <td><b>Inner Product</b></td>
            <td>$\langle \mathbf{x}, \mathbf{y} \rangle$</td>
            <td>Bilinear, Symmetric, Positive Definite. Enables angles and projection.</td>
          </tr>
          <tr>
            <td><b>Norm</b></td>
            <td>$\|x\|$</td>
            <td>Positivity, Homogeneity, Triangle Inequality.</td>
          </tr>
          <tr>
            <td><b>PSD Matrix</b></td>
            <td>$A \succeq 0$</td>
            <td>$\mathbf{x}^\top A \mathbf{x} \ge 0 \ \forall \mathbf{x}$. Eigenvalues $\lambda_i \ge 0$.</td>
          </tr>
          <tr>
            <td><b>Orthogonal Matrix</b></td>
            <td>$Q^\top Q = I$</td>
            <td>Preserves norms/angles. Columns are orthonormal.</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Theorems</h3>
      <ul>
        <li><b>Rank-Nullity:</b> $\dim \mathcal{R}(A) + \dim \mathcal{N}(A) = n$ (for $A \in \mathbb{R}^{m \times n}$).</li>
        <li><b>Spectral Theorem:</b> Real symmetric matrices have real eigenvalues and orthogonal eigenvectors.</li>
        <li><b>Schur Complement:</b> $M \succeq 0 \iff D \succ 0$ and $A - BD^{-1}B^\top \succeq 0$.</li>
        <li><b>Fundamental Theorem of Linear Algebra:</b> $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ and $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.</li>
      </ul>

      <h3>Standard Formulas</h3>
      <ul>
        <li><b>Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$</li>
        <li><b>Projection onto Subspace:</b> $P = A(A^\top A)^{-1}A^\top$ (if $A$ full rank)</li>
        <li><b>Cauchy-Schwarz:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$</li>
        <li><b>Gradient of Quadratic:</b> $\nabla (\mathbf{x}^\top A \mathbf{x}) = (A+A^\top)\mathbf{x}$</li>
      </ul>

      <h3>Roadmap: Where This Material Appears</h3>
      <p>This lecture is not just background; it is the vocabulary for the entire course. Here is where these concepts return:</p>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>Used In</th>
            <th>Why it matters</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>PSD Matrices</b></td>
            <td>L03 (Ellipsoids), L05 (Hessian), L08 (SDP)</td>
            <td>The condition for convexity ($f'' \ge 0$) and modern conic solvers.</td>
          </tr>
          <tr>
            <td><b>Projections</b></td>
            <td>L03 (Separating Hyperplanes), L05 (Proximity)</td>
            <td>The geometric interpretation of optimality conditions and duality.</td>
          </tr>
          <tr>
            <td><b>SVD / Eigenvalues</b></td>
            <td>L01 (Condition Number), L08 (Matrix Norms)</td>
            <td>Numerical stability analysis and robust optimization.</td>
          </tr>
          <tr>
            <td><b>Least Squares</b></td>
            <td>L07 (QP examples), L09 (Dual derivation)</td>
            <td>The fundamental convex optimization problem; used to construct dual bounds.</td>
          </tr>
        </tbody>
      </table>
    </section>

        <!-- SECTION 9: EXERCISES -->
    <section class="section-card" id="section-exercises">
      <h2><i data-feather="edit-3"></i> 9. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises reinforce the foundational tools of linear algebra used in optimization. Focus on the geometry of subspaces, the calculus of gradients (crucial for finding optimality conditions), and the properties of PSD matrices (essential for convexity).</p>
      </div>
<h3>P0.1 — Linear Independence</h3>
      <p>Determine whether the following sets of vectors are linearly independent. If dependent, exhibit a linear combination summing to zero.</p>
      <ol type="a">
        <li>$v_1 = (1, 2, 3)^\top, v_2 = (4, 5, 6)^\top, v_3 = (7, 8, 9)^\top$.</li>
        <li>$v_1 = (1, 0, 0)^\top, v_2 = (1, 1, 0)^\top, v_3 = (1, 1, 1)^\top$.</li>
        <li>The columns of an upper triangular matrix with non-zero diagonal entries.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution (Step-by-Step)</h4>
        <ol type="a">
          <li><b>Status: Linearly Dependent.</b>
          <br><i>Walkthrough:</i>
          <br>1. <b>Observation:</b> Look at the vectors $(1,2,3), (4,5,6), (7,8,9)$. The components increase by constant steps. This suggests an arithmetic progression.
          <br>2. <b>Check differences:</b>
             $$ v_2 - v_1 = (3, 3, 3)^\top $$
             $$ v_3 - v_2 = (3, 3, 3)^\top $$
          <br>3. <b>Formulate relation:</b> Since the differences are equal, $v_2 - v_1 = v_3 - v_2$.
          <br>4. <b>Rearrange to standard form:</b> Move everything to one side to find the linear combination summing to zero.
             $$ v_1 - 2v_2 + v_3 = 0 $$
          <br>5. <b>Conclusion:</b> We found non-zero coefficients $(1, -2, 1)$ that annihilate the vectors. Thus, they are dependent.
          </li>
          <li><b>Status: Linearly Independent.</b>
          <br><i>Walkthrough:</i>
          <br>1. <b>Matrix Method:</b> Stack the vectors as columns of a matrix $A$:
             $$ A = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix} $$
          <br>2. <b>Analyze Structure:</b> The matrix is <b>upper triangular</b> (zeros below the diagonal).
          <br>3. <b>Determinant Test:</b> For triangular matrices, the determinant is the product of diagonal entries.
             $$ \det(A) = 1 \cdot 1 \cdot 1 = 1 $$
          <br>4. <b>Conclusion:</b> Since $\det(A) \neq 0$, the matrix is invertible, and its columns are linearly independent.
          </li>
          <li><b>Status: Linearly Independent.</b>
          <br><i>Reasoning:</i> This generalizes part (b). Any upper triangular matrix $U$ with non-zero diagonal entries $u_{ii} \neq 0$ has determinant $\det(U) = \prod u_{ii} \neq 0$. Therefore, its columns form a basis for $\mathbb{R}^n$.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Definition:</b> A set of vectors $\{v_1, \dots, v_k\}$ is <b>linearly independent</b> if the only linear combination summing to zero is the trivial one ($\sum c_i v_i = 0 \implies c_i = 0 \ \forall i$).
        <br><b>Matrix View:</b> The matrix $A = [v_1 \dots v_k]$ formed by these vectors has full column rank ($\mathrm{rank}(A) = k$) if and only if $\mathcal{N}(A) = \{0\}$.
        <br><b>Determinant Test (Square Case):</b> For $n$ vectors in $\mathbb{R}^n$, they form a <b>basis</b> (independent and spanning) if and only if $\det([v_1 \dots v_n]) \neq 0$.
        <br><b>Geometric Intuition:</b> Linearly dependent vectors are "redundant"; one can be written as a combination of the others, collapsing the span into a lower-dimensional subspace.</p>
      </div>




<h3>P0.2 — The Rank-Nullity Theorem</h3>
      <p>Let $A$ be a $10 \times 15$ matrix.</p>
      <ol type="a">
        <li>What is the maximum possible rank of $A$?</li>
        <li>If the rank of $A$ is 8, what is the dimension of the nullspace $\mathcal{N}(A)$?</li>
        <li>If $A \mathbf{x} = 0$ has only the solution $\mathbf{x}=0$, is this possible? Explain.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>The rank is bounded by the dimensions: $\mathrm{rank}(A) \le \min(m, n) = \min(10, 15) = 10$.</li>
          <li>By the Rank-Nullity Theorem, $\dim(\mathcal{N}(A)) + \mathrm{rank}(A) = n$. Here $n=15$ (number of columns). So $\dim(\mathcal{N}(A)) = 15 - 8 = 7$.</li>
          <li>The condition "only the solution $\mathbf{x}=0$" means $\mathcal{N}(A) = \{0\}$, so $\dim(\mathcal{N}(A)) = 0$. By Rank-Nullity, this would imply $\mathrm{rank}(A) = 15 - 0 = 15$. However, we established in (a) that the maximum rank is 10. Thus, this is <b>impossible</b>. An underdetermined system ($m < n$) always has a non-zero nullspace.</li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-Nullity Theorem:</b> For any matrix $A \in \mathbb{R}^{m \times n}$, the dimension of the domain splits into the nullspace and the row space:
        $$ \dim \mathcal{R}(A) + \dim \mathcal{N}(A) = n $$
        <br><b>Rank Inequalities:</b> $\mathrm{rank}(A) \le \min(m, n)$. Also, $\mathrm{rank}(AB) \le \min(\mathrm{rank}(A), \mathrm{rank}(B))$ and $\mathrm{rank}(A+B) \le \mathrm{rank}(A) + \mathrm{rank}(B)$.
        <br><b>Full Rank Conditions:</b>
        <ul>
            <li><b>Full Column Rank ($m \ge n$):</b> $\mathrm{rank}(A) = n \iff \mathcal{N}(A) = \{0\} \iff A^\top A$ is invertible.</li>
            <li><b>Full Row Rank ($m \le n$):</b> $\mathrm{rank}(A) = m \iff \mathcal{R}(A) = \mathbb{R}^m \iff AA^\top$ is invertible.</li>
        </ul></p>
      </div>




<h3>P0.3 — Trace and Determinant</h3>
      <ol type="a">
        <li>Show that $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$ but generally $\mathrm{tr}(ABC) \neq \mathrm{tr}(BAC)$. Construct a $2 \times 2$ counterexample.</li>
        <li>Let $A \in \mathbb{R}^{n \times n}$ be skew-symmetric ($A^\top = -A$). Show that $\mathbf{x}^\top A \mathbf{x} = 0$ for all $\mathbf{x}$.</li>
        <li>Use the result from (b) to prove that if $n$ is odd, $\det(A) = 0$. (Hint: $\det(A^\top) = \det(-A)$).</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Cyclic Property:</b> Let $X = AB$. Then $\mathrm{tr}(XC) = \mathrm{tr}(CX)$ (basic cyclic property). Substituting $X=AB$, we get $\mathrm{tr}((AB)C) = \mathrm{tr}(C(AB)) = \mathrm{tr}(CAB)$. Applying it again gives $\mathrm{tr}(BCA)$.
          <br><b>Counterexample for Non-Cyclic:</b> Let $A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$, $B = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$, $C = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$.
          <br>$ABC = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \implies \mathrm{tr}=1$.
          <br>$BAC = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \mathbf{0} \implies \mathrm{tr}=0$.
          </li>
          <li><b>Skew-Symmetric Form:</b> The scalar $\mathbf{x}^\top A \mathbf{x}$ is its own transpose.
          $$ \mathbf{x}^\top A \mathbf{x} = (\mathbf{x}^\top A \mathbf{x})^\top = \mathbf{x}^\top A^\top \mathbf{x} $$
          Since $A^\top = -A$, we have $\mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top (-A) \mathbf{x} = -\mathbf{x}^\top A \mathbf{x}$.
          The only number equal to its negative is 0. Thus $\mathbf{x}^\top A \mathbf{x} = 0$.
          </li>
          <li><b>Determinant of Skew-Symmetric:</b>
          $$ \det(A) = \det(A^\top) = \det(-A) = (-1)^n \det(A) $$
          If $n$ is odd, $(-1)^n = -1$. So $\det(A) = -\det(A)$, which implies $2\det(A) = 0 \implies \det(A) = 0$.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Trace Properties:</b>
        <ul>
            <li><b>Linearity:</b> $\mathrm{tr}(\alpha A + \beta B) = \alpha \mathrm{tr}(A) + \beta \mathrm{tr}(B)$.</li>
            <li><b>Cyclic Invariance:</b> $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) = \mathrm{tr}(CAB)$. Note: $\mathrm{tr}(ABC) \neq \mathrm{tr}(BAC)$ in general.</li>
        </ul>
        <br><b>Determinant Properties:</b>
        <ul>
            <li><b>Multiplicativity:</b> $\det(AB) = \det(A)\det(B)$.</li>
            <li><b>Transpose:</b> $\det(A^\top) = \det(A)$.</li>
        </ul>
        <br><b>Skew-Symmetric Matrices ($A^\top = -A$):</b>
        <ul>
            <li><b>Quadratic Form:</b> $\mathbf{x}^\top A \mathbf{x} = 0$ for all $\mathbf{x} \in \mathbb{R}^n$.</li>
            <li><b>Eigenvalues:</b> Purely imaginary or zero. If $n$ is odd, at least one eigenvalue is 0 ($\det(A)=0$).</li>
        </ul></p>
      </div>




<h3>P0.4 — Norm Equivalence</h3>
      <p>In finite dimensions, all norms are equivalent. For $\mathbf{x} \in \mathbb{R}^n$, prove the following inequalities:</p>
      <ol type="a">
        <li>$\|x\|_\infty \le \|\mathbf{x}\|_2 \le \sqrt{n} \|\mathbf{x}\|_\infty$</li>
        <li>$\|x\|_2 \le \|\mathbf{x}\|_1 \le \sqrt{n} \|\mathbf{x}\|_2$</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Left inequality:</b> $\|x\|_\infty^2 = \max_i |x_i|^2 \le \sum_i x_i^2 = \|\mathbf{x}\|_2^2$. Taking square roots gives $\|x\|_\infty \le \|\mathbf{x}\|_2$.
          <br><b>Right inequality:</b> $\|x\|_2^2 = \sum_i x_i^2 \le \sum_i (\max_j |x_j|)^2 = \sum_i \|\mathbf{x}\|_\infty^2 = n \|\mathbf{x}\|_\infty^2$. Taking square roots gives $\|x\|_2 \le \sqrt{n}\|\mathbf{x}\|_\infty$.
          </li>
          <li><b>Left inequality:</b> Square $\|x\|_1$: $\|x\|_1^2 = (\sum |x_i|)^2 = \sum x_i^2 + \sum_{i \ne j} |x_i||x_j| = \|\mathbf{x}\|_2^2 + \text{non-negative terms} \ge \|\mathbf{x}\|_2^2$. Thus $\|x\|_1 \ge \|\mathbf{x}\|_2$.
          <br><b>Right inequality:</b> Use Cauchy-Schwarz with the vector of ones $\mathbf{1}$ and the vector $|\mathbf{x}| = (|x_1|, \dots, |x_n|)$.
          $$ \|\mathbf{x}\|_1 = \sum |x_i| \cdot 1 = |\mathbf{x}|^\top \mathbf{1} \le \||\mathbf{x}|\|_2 \|\mathbf{1}\|_2 = \|\mathbf{x}\|_2 \sqrt{n} $$
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Norm Equivalence Theorem:</b> In finite-dimensional vector spaces, all norms are equivalent. For any two norms $\|\cdot\|_a, \|\cdot\|_b$, there exist constants $c, C > 0$ such that $c \|\mathbf{x}\|_a \le \|\mathbf{x}\|_b \le C \|\mathbf{x}\|_a$. This implies they define the same topology (convergence sequences are the same).
        <br><b>Standard Inequalities:</b>
        <ul>
            <li>$\|x\|_\infty \le \|\mathbf{x}\|_2 \le \|\mathbf{x}\|_1$ (Hierarchy of norms)</li>
            <li>$\|x\|_2 \le \sqrt{n} \|\mathbf{x}\|_\infty$</li>
            <li>$\|x\|_1 \le \sqrt{n} \|\mathbf{x}\|_2$ (Cauchy-Schwarz with $\mathbf{1}$)</li>
        </ul>
        <br><b>Geometry:</b>
        <ul>
            <li>$\ell_1$ ball: Cross-polytope (Diamond).</li>
            <li>$\ell_2$ ball: Sphere.</li>
            <li>$\ell_\infty$ ball: Hypercube.</li>
        </ul></p>
      </div>




<h3>P0.5 — Least Squares from Scratch</h3>
      <p>Consider the function $f(\mathbf{x}) = \frac{1}{2} \|A\mathbf{x} - \mathbf{b}\|_2^2$.</p>
      <ol type="a">
        <li>Expand the squared norm into terms involving $\mathbf{x}^\top A^\top A \mathbf{x}$, etc.</li>
        <li>Compute the gradient $\nabla f(\mathbf{x})$ step-by-step.</li>
        <li>Set the gradient to zero to derive the Normal Equations.</li>
        <li>Show that if $\mathcal{N}(A) = \{0\}$, the Hessian is positive definite, ensuring a unique global minimum.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>$f(\mathbf{x}) = \frac{1}{2}(A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = \frac{1}{2}(\mathbf{x}^\top A^\top - \mathbf{b}^\top)(A\mathbf{x} - \mathbf{b}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \frac{1}{2} \mathbf{x}^\top A^\top \mathbf{b} - \frac{1}{2} \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b}$.
          Since scalar transpose is identity ($\mathbf{x}^\top A^\top \mathbf{b} = \mathbf{b}^\top A \mathbf{x}$), we simplify to:
          $$ f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b} $$
          </li>
          <li><b>Gradient Derivation (Term-by-Term):</b>
            <br>We compute the gradient of each term in the expanded expression $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b}$.
            <br>
            <br><b>Term 1: Quadratic Term $\frac{1}{2} \mathbf{x}^\top (A^\top A) \mathbf{x}$</b>
            <br>Let $Q = A^\top A$. Note that $Q$ is symmetric ($Q^\top = (A^\top A)^\top = A^\top A = Q$).
            <br>The gradient of a quadratic form $\mathbf{x}^\top Q \mathbf{x}$ is given by $(Q + Q^\top)\mathbf{x}$.
            <br>Since $Q$ is symmetric, $Q + Q^\top = 2Q$.
            <br>Therefore, $\nabla (\frac{1}{2} \mathbf{x}^\top Q \mathbf{x}) = \frac{1}{2} (2Q\mathbf{x}) = Q\mathbf{x} = A^\top A \mathbf{x}$.
            <br>
            <br><b>Term 2: Linear Term $-\mathbf{b}^\top A \mathbf{x}$</b>
            <br>This term represents a dot product. We can use the property of the transpose to group the constant vectors:
            $$ -\mathbf{b}^\top (A \mathbf{x}) = -(\mathbf{b}^\top A) \mathbf{x} = -(A^\top \mathbf{b})^\top \mathbf{x} $$
            This is now in the form $\mathbf{c}^\top \mathbf{x}$ where $\mathbf{c} = -A^\top \mathbf{b}$.
            <br>The gradient of a linear function $\mathbf{c}^\top \mathbf{x}$ is simply the vector $\mathbf{c}$.
            <br>Thus, $\nabla (-\mathbf{b}^\top A \mathbf{x}) = -A^\top \mathbf{b}$.
            <br>
            <br><b>Term 3: Constant Term $\frac{1}{2} \mathbf{b}^\top \mathbf{b}$</b>
            <br>This term does not depend on $\mathbf{x}$, so its gradient is $\mathbf{0}$.
            <br>
            <br><b>Result:</b> Summing the gradients gives:
            $$ \nabla f(\mathbf{x}) = A^\top A \mathbf{x} - A^\top \mathbf{b} = A^\top (A\mathbf{x} - \mathbf{b}) $$
          </li>
          <li>$\nabla f(\mathbf{x}) = 0 \implies A^\top A \mathbf{x} - A^\top \mathbf{b} = 0 \implies A^\top A \mathbf{x} = A^\top \mathbf{b}$. These are the Normal Equations.
          </li>
          <li>$\nabla^2 f(\mathbf{x}) = A^\top A$.
          If $\mathcal{N}(A) = \{0\}$, then for any $\mathbf{v} \neq 0$, $A\mathbf{v} \neq 0$.
          $$ \mathbf{v}^\top A^\top A \mathbf{v} = (A\mathbf{v})^\top (A\mathbf{v}) = \|A\mathbf{v}\|_2^2 > 0 $$
          Thus the Hessian is positive definite, which guarantees strict convexity and a unique global minimum.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Least Squares Objective:</b> $f(\mathbf{x}) = \frac{1}{2}\|A\mathbf{x} - \mathbf{b}\|_2^2$ is a convex quadratic function.
        <br><b>Gradients:</b>
        <ul>
            <li>$\nabla (\frac{1}{2} \mathbf{x}^\top Q \mathbf{x}) = Q\mathbf{x}$ (for symmetric $Q$).</li>
            <li>$\nabla (c^\top \mathbf{x}) = c$.</li>
        </ul>
        <br><b>Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$. This system represents the condition $A^\top (A\mathbf{x} - \mathbf{b}) = 0$, meaning the residual is orthogonal to the columns of $A$.
        <br><b>Convexity & Uniqueness:</b>
        <ul>
            <li>The Hessian $\nabla^2 f(\mathbf{x}) = A^\top A$ is always PSD ($\succeq 0$), ensuring convexity.</li>
            <li>If $\mathcal{N}(A) = \{0\}$ (Full Column Rank), $A^\top A$ is PD ($\succ 0$), ensuring <b>strict convexity</b> and a <b>unique global minimum</b>.</li>
        </ul></p>
      </div>




<h3>P0.6 — Matrix Calculus Practice</h3>
      <p>Compute the gradient with respect to $X \in \mathbb{R}^{n \times n}$ for the following functions:</p>
      <ol type="a">
        <li>$f(X) = \mathrm{tr}(A X B)$, where $A, B$ are constant matrices.</li>
        <li>$f(X) = \mathrm{tr}(X^\top X)$.</li>
        <li>$f(X) = a^\top X \mathbf{b}$, where $a, \mathbf{b}$ are constant vectors.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>Use the cyclic property: $\mathrm{tr}(AXB) = \mathrm{tr}(BA X)$.
          The gradient of $\mathrm{tr}(M X)$ is $M^\top$. Here $M = BA$.
          Thus $\nabla_X f(X) = (BA)^\top = A^\top B^\top$.
          </li>
          <li>$f(X) = \|X\|_F^2 = \sum_{ij} X_{ij}^2$.
          $\frac{\partial f}{\partial X_{ij}} = 2 X_{ij}$.
          Thus $\nabla_X f(X) = 2X$.
          </li>
          <li>$a^\top X \mathbf{b} = \mathrm{tr}(a^\top X \mathbf{b}) = \mathrm{tr}(\mathbf{b} a^\top X)$.
          Using the rule from (a) with $M = \mathbf{b} a^\top$:
          $\nabla_X f(X) = (\mathbf{b} a^\top)^\top = (a^\top)^\top \mathbf{b}^\top = a \mathbf{b}^\top$.
          (This is an outer product matrix).
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Matrix Calculus Toolkit:</b>
        <br><b>Definition:</b> The gradient $\nabla_X f(X)$ is the matrix $G$ such that the first-order approximation is $f(X+H) \approx f(X) + \langle G, H \rangle = f(X) + \mathrm{tr}(G^\top H)$.
        <br><b>Key Formulas:</b>
        <ul>
            <li>$\nabla_X \mathrm{tr}(A X) = A^\top$.</li>
            <li>$\nabla_X \mathrm{tr}(X^\top A X) = (A + A^\top)X$. (For symmetric $A$, $2AX$).</li>
            <li>$\nabla_X \|X\|_F^2 = 2X$.</li>
        </ul>
        <br><b>Strategy:</b> Perturb $X \to X + H$, expand terms linear in $H$, and rearrange trace terms to match the form $\mathrm{tr}(G^\top H)$.</p>
      </div>




<h3>P0.7 — Hessian of a Cubic</h3>
      <p>Let $f: \mathbb{R}^2 \to \mathbb{R}$ be defined by $f(\mathbf{x}) = x_1^3 + x_2^3 + 2x_1 x_2$.</p>
      <ol type="a">
        <li>Compute the gradient $\nabla f(\mathbf{x})$.</li>
        <li>Compute the Hessian matrix $\nabla^2 f(\mathbf{x})$.</li>
        <li>For which $\mathbf{x}$ is the Hessian Positive Semidefinite? (This identifies the region where the function is locally convex).</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>Gradient:
          $$ \frac{\partial f}{\partial x_1} = 3x_1^2 + 2x_2, \quad \frac{\partial f}{\partial x_2} = 3x_2^2 + 2x_1 $$
          $\nabla f(\mathbf{x}) = \begin{bmatrix} 3x_1^2 + 2x_2 \\ 3x_2^2 + 2x_1 \end{bmatrix}$.
          </li>
          <li>Hessian:
          $$ \nabla^2 f(\mathbf{x}) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} \end{bmatrix} = \begin{bmatrix} 6x_1 & 2 \\ 2 & 6x_2 \end{bmatrix} $$
          </li>
          <li>PSD Condition:
          A $2 \times 2$ matrix is PSD iff trace $\ge 0$ and determinant $\ge 0$.
          Trace: $6x_1 + 6x_2 \ge 0 \implies x_1 + x_2 \ge 0$.
          Determinant: $36x_1 x_2 - 4 \ge 0 \implies 9x_1 x_2 \ge 1$.
          The condition $x_1 x_2 \ge 1/9$ implies $x_1, x_2$ have the same sign.
          If both negative, $x_1 + x_2 < 0$, violating the trace condition.
          Thus, we need $x_1 > 0, x_2 > 0$ and $x_1 x_2 \ge 1/9$. This region (hyperbola in the first quadrant) is where the function is locally convex.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Convexity Condition:</b> A twice-differentiable function is convex on a domain iff its Hessian matrix is Positive Semidefinite ($\nabla^2 f(\mathbf{x}) \succeq 0$) everywhere in that domain.
        <br><b>Sylvester's Criterion (for $A \succ 0$):</b> A symmetric matrix is Positive Definite iff all <b>leading principal minors</b> (determinants of top-left $k \times k$ submatrices) are positive.
        <br><b>2x2 PSD Test:</b> $\begin{bmatrix} a & \mathbf{b} \\ \mathbf{b} & c \end{bmatrix} \succeq 0 \iff a \ge 0, c \ge 0, ac - \mathbf{b}^2 \ge 0$. (Trace $\ge 0$ and Det $\ge 0$).</p>
      </div>




<h3>P0.8 — Testing Positive Semidefiniteness</h3>
      <p>Determine whether the following matrices are PSD, PD, Indefinite, or Negative Definite/Semidefinite.</p>
      <ol type="a">
        <li>$A = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$</li>
        <li>$B = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$</li>
        <li>$C = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 3 \end{bmatrix}$</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Positive Definite.</b> Using Sylvester's Criterion (Leading Principal Minors): $D_1 = 2 > 0$, $D_2 = \det(A) = 4 - 1 = 3 > 0$. Since all leading principal minors are positive, $A \succ 0$. Alternatively, eigenvalues are $\lambda = 1, 3$.</li>
          <li><b>Indefinite.</b> The determinant is $\det(B) = 1 - 4 = -3 < 0$. Since the product of eigenvalues is negative, they must have opposite signs ($\lambda = 3, -1$). Thus, $B$ is indefinite.</li>
          <li><b>Positive Semidefinite.</b> This is a diagonal matrix with entries $1, 0, 3$, which are the eigenvalues. Since all $\lambda_i \ge 0$ and one is zero, $C \succeq 0$ but is not positive definite.</li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Characterizing Definiteness:</b>
        <ul>
            <li><b>Positive Definite (PD):</b> All $\lambda_i > 0$. Strictly convex bowl.</li>
            <li><b>Positive Semidefinite (PSD):</b> All $\lambda_i \ge 0$. Convex bowl (possibly flat).</li>
            <li><b>Indefinite:</b> Some $\lambda_i > 0$, some $\lambda_i < 0$. Saddle point geometry.</li>
            <li><b>Negative Definite (ND):</b> All $\lambda_i < 0$. Concave cap.</li>
        </ul>
        <br><b>Practical Checks:</b>
        <ul>
            <li>Check determinant (product of eigenvalues) and trace (sum of eigenvalues).</li>
            <li>If $\det(A) < 0$, it must have at least one negative eigenvalue (assuming $n$ even? No, if $\det < 0$ product is negative, so odd number of negative eigenvals. If $n=2$, exactly one neg).</li>
        </ul></p>
      </div>




<h3>P0.9 — Schur Complement Application</h3>
      <p>Use the Schur Complement condition (assuming the top-left pivot is positive) to determine the range of $\mathbf{x}$ for which the matrix $M(\mathbf{x})$ is Positive Semidefinite:</p>
      $$ M(\mathbf{x}) = \begin{bmatrix} \mathbf{x} & 1 \\ 1 & \mathbf{x} \end{bmatrix} $$
      <p>Verify your answer by computing the eigenvalues directly.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Schur Complement Method:</b>
        For $M \succeq 0$, we need the top-left block $A=\mathbf{x} > 0$ and the Schur complement $S = D - C A^{-1} B \ge 0$.
        Here $A=\mathbf{x}, B=1, C=1, D=\mathbf{x}$.
        $S = \mathbf{x} - 1(1/\mathbf{x})(1) = \mathbf{x} - 1/\mathbf{x}$.
        We need $\mathbf{x} > 0$ and $\mathbf{x} - 1/\mathbf{x} \ge 0$.
        $\mathbf{x} - 1/\mathbf{x} \ge 0 \implies \mathbf{x}^2 \ge 1$ (since $\mathbf{x}>0$). Thus $\mathbf{x} \ge 1$.
        So the range is $\mathbf{x} \ge 1$.
        </p>
        <p><b>Eigenvalue Verification:</b>
        Characteristic eq: $(\mathbf{x}-\lambda)^2 - 1 = 0 \implies \mathbf{x}-\lambda = \pm 1 \implies \lambda = \mathbf{x} \pm 1$.
        For PSD, we need $\lambda_{\min} = \mathbf{x}-1 \ge 0$ and $\lambda_{\max} = \mathbf{x}+1 \ge 0$.
        $\mathbf{x}-1 \ge 0 \implies \mathbf{x} \ge 1$. This automatically satisfies $\mathbf{x}+1 \ge 2 \ge 0$.
        Matches.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Schur Complement Lemma:</b> A block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ (with $A \succ 0$) is PSD if and only if the Schur complement $S = C - B^\top A^{-1} B$ is PSD.
        <br><b>LMI (Linear Matrix Inequality):</b> A constraint of the form $F(\mathbf{x}) \succeq 0$ where $F$ is affine in $\mathbf{x}$.
        <br><b>Trick:</b> Use Schur complements to convert nonlinear constraints (like $\mathbf{x}^2 \le \mathbf{y}$ or quadratic-over-linear forms) into equivalent Linear Matrix Inequalities. This allows them to be solved using Semidefinite Programming (SDP).</p>
      </div>




<h3>P0.10 — Projection onto a Line</h3>
      <p>Let $a = (1, 1, 1)^\top$. Find the projection of $\mathbf{b} = (1, 0, 0)^\top$ onto the line spanned by $a$. Verify that the residual $\mathbf{b} - \mathbf{p}$ is orthogonal to $a$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Projection formula: $\mathbf{p} = \frac{a^\top \mathbf{b}}{a^\top a} a$.
        $a^\top \mathbf{b} = 1 \cdot 1 + 1 \cdot 0 + 1 \cdot 0 = 1$.
        $a^\top a = 1^2 + 1^2 + 1^2 = 3$.
        $\mathbf{p} = \frac{1}{3} (1, 1, 1)^\top = (1/3, 1/3, 1/3)^\top$.
        <br>Residual $\mathbf{r} = \mathbf{b} - \mathbf{p} = (1-1/3, 0-1/3, 0-1/3)^\top = (2/3, -1/3, -1/3)^\top$.
        <br>Check orthogonality: $a^\top \mathbf{r} = 1(2/3) + 1(-1/3) + 1(-1/3) = 0$. Verified.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Vector Projection:</b> The projection of a vector $\mathbf{b}$ onto the line spanned by a non-zero vector $a$ is given by:
        $$ \Pi_a(\mathbf{b}) = \frac{\langle a, \mathbf{b} \rangle}{\|a\|^2} a $$
        <br><b>Geometric Intuition:</b> This scales $a$ by the "shadow" of $\mathbf{b}$ onto $a$.
        <br><b>Projection Matrix:</b> $P = \frac{a a^\top}{a^\top a}$ is a rank-1 orthogonal projector.</p>
      </div>




<h3>P0.11 — Projection onto a Hyperplane</h3>
      <p>Find the projection of the point $\mathbf{y} = (3, 3)^\top$ onto the line (hyperplane in 2D) defined by $x_1 + x_2 = 2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The line is $a^\top \mathbf{x} = \mathbf{b}$ with $a=(1, 1)^\top, \mathbf{b}=2$.
        The formula for projection onto a hyperplane is $\mathbf{p} = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a$.
        $a^\top \mathbf{y} = 3+3=6$.
        $\|a\|^2 = 2$.
        $\mathbf{p} = (3, 3)^\top - \frac{6 - 2}{2} (1, 1)^\top = (3, 3)^\top - 2(1, 1)^\top = (3, 3)^\top - (2, 2)^\top = (1, 1)^\top$.
        <br>Check: $1+1=2$. Point is on the line. Residual $(2, 2)$ is parallel to normal $(1, 1)$. Correct.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hyperplane Definition:</b> A hyperplane is an affine set defined by a single linear equality constraint: $\mathcal{H} = \{\mathbf{x} \mid a^\top \mathbf{x} = \mathbf{b}\}$.
        <br><b>Projection Formula:</b> To project a point $\mathbf{y}$ onto $\mathcal{H}$, you move in the direction of the normal vector $a$ (the most direct path) by exactly the amount needed to satisfy the equation:
        $$ \Pi_{\mathcal{H}}(\mathbf{y}) = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a $$
        <br><b>Signed Distance:</b> The quantity $\frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|}$ is the signed distance from $\mathbf{y}$ to the hyperplane.</p>
      </div>




<h3>P0.12 — Orthogonal Complements</h3>
      <p>Let $S$ be a subspace of $\mathbb{R}^n$. The orthogonal complement is $S^\perp = \{\mathbf{y} \mid \mathbf{y}^\top \mathbf{x} = 0 \ \forall \mathbf{x} \in S\}$.</p>
      <ol type="a">
        <li>Prove that $S^\perp$ is a subspace.</li>
        <li>Prove that $S \cap S^\perp = \{0\}$.</li>
        <li>If $S = \text{span}((1, 0, 0)^\top, (0, 1, 0)^\top)$, calculate $S^\perp$.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Subspace:</b> Let $y_1, y_2 \in S^\perp$. Then $y_1^\top \mathbf{x} = 0$ and $y_2^\top \mathbf{x} = 0$.
          For any linear combination $\alpha y_1 + \beta y_2$, we have $(\alpha y_1 + \beta y_2)^\top \mathbf{x} = \alpha(y_1^\top \mathbf{x}) + \beta(y_2^\top \mathbf{x}) = 0$.
          Thus closed under linear combinations. Contains 0 since $0^\top \mathbf{x} = 0$.
          </li>
          <li><b>Intersection:</b> Let $\mathbf{x} \in S \cap S^\perp$. Since $\mathbf{x} \in S$ and $\mathbf{x} \in S^\perp$, it must be orthogonal to itself: $\mathbf{x}^\top \mathbf{x} = 0$.
          $\|x\|^2 = 0 \implies \mathbf{x} = 0$. Thus the intersection contains only the zero vector.
          </li>
          <li><b>Calculation:</b> $S$ is the $xy$-plane ($\mathbf{z}=0$). $S^\perp$ is the set of vectors orthogonal to $(1,0,0)$ and $(0,1,0)$.
          $\mathbf{y} \cdot e_1 = y_1 = 0$. $\mathbf{y} \cdot e_2 = y_2 = 0$.
          Thus $\mathbf{y} = (0, 0, y_3)^\top$. $S^\perp$ is the $\mathbf{z}$-axis (span of $e_3$).
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Definition:</b> The orthogonal complement $S^\perp$ contains all vectors orthogonal to every vector in the subspace $S$.
        <br><b>Fundamental Properties:</b>
        <ul>
            <li><b>Decomposition:</b> $\mathbb{R}^n = S \oplus S^\perp$. Any vector $\mathbf{x}$ can be uniquely written as $\mathbf{x} = x_S + x_{\perp}$, with $x_S \in S$ and $x_{\perp} \in S^\perp$.</li>
            <li><b>Dimension:</b> $\dim(S) + \dim(S^\perp) = n$.</li>
            <li><b>Duality:</b> $(S^\perp)^\perp = S$.</li>
        </ul>
        <br><b>Connection to Matrices:</b> $\mathcal{R}(A)^\perp = \mathcal{N}(A^\top)$. (Fundamental Theorem of Linear Algebra).</p>
      </div>




<h3>P0.13 — Frobenius Norm Submultiplicativity</h3>
      <p>We proved $\|AB\|_F \le \|A\|_F \|B\|_F$ in the lecture using Cauchy-Schwarz.
      <br>Re-derive this result by writing $\|X\|_F^2 = \mathrm{tr}(X^\top X)$ and using the property $\mathrm{tr}(M) = \sum \lambda_i(M)$ along with the fact that $\lambda_{\max}(A^\top A) \le \mathrm{tr}(A^\top A)$.</p>
      <div class="solution-box">
        <h4>Solution (Detailed Walkthrough)</h4>
        <div class="proof-step">
          <strong>Step 1: Convert Norm to Trace.</strong>
          The squared Frobenius norm is defined as $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
          We apply this to the product $AB$:
          $$ \|AB\|_F^2 = \mathrm{tr}((AB)^\top (AB)) = \mathrm{tr}(B^\top A^\top A B) $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Use Cyclic Property.</strong>
          Recall that $\mathrm{tr}(XYZ) = \mathrm{tr}(ZXY)$. Let $X = B^\top$, $Y = A^\top A$, $Z = B$.
          We can cycle $B$ to the left (or $B^\top$ to the right). Let's cycle $B$ to the left of $B^\top$:
          $$ \mathrm{tr}(B^\top (A^\top A) B) = \mathrm{tr}((A^\top A) (B B^\top)) $$
          Let $P = A^\top A$ and $Q = B B^\top$. Note that both $P$ and $Q$ are symmetric positive semidefinite (PSD) matrices.
          Our goal is to bound $\mathrm{tr}(PQ)$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Establish Trace Inequality.</strong>
          We claim that for PSD matrices $P, Q$: $\mathrm{tr}(PQ) \le \lambda_{\max}(P) \mathrm{tr}(Q)$.
          <br><i>Proof:</i>
          <br>1. Diagonalize $Q$ as $Q = U \Lambda U^\top$, where $U$ is orthogonal and $\Lambda = \mathrm{diag}(\lambda_i(Q))$. Note $\lambda_i(Q) \ge 0$.
          <br>2. Substitute into the trace:
          $$ \mathrm{tr}(PQ) = \mathrm{tr}(P U \Lambda U^\top) $$
          <br>3. Use the cyclic property again to move $U^\top$:
          $$ = \mathrm{tr}(U^\top P U \Lambda) $$
          <br>4. Let $M = U^\top P U$. The diagonal elements are $M_{ii} = \mathbf{u}_i^\top P \mathbf{u}_i$.
          Since $\mathbf{u}_i$ is a unit vector, the Rayleigh quotient tells us $M_{ii} \le \lambda_{\max}(P)$.
          <br>5. Compute the trace of the product $M \Lambda$ (which is diagonal $\times$ diagonal effectively for the trace sum):
          $$ \mathrm{tr}(M \Lambda) = \sum_{i} M_{ii} \lambda_i(Q) $$
          <br>6. Apply the inequality $M_{ii} \le \lambda_{\max}(P)$:
          $$ \sum_{i} M_{ii} \lambda_i(Q) \le \sum_{i} \lambda_{\max}(P) \lambda_i(Q) = \lambda_{\max}(P) \sum_{i} \lambda_i(Q) $$
          <br>7. Recognize that $\sum \lambda_i(Q) = \mathrm{tr}(Q)$.
          $$ \le \lambda_{\max}(P) \mathrm{tr}(Q) $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Substitute Back.</strong>
          We have:
          $$ \|AB\|_F^2 \le \lambda_{\max}(A^\top A) \mathrm{tr}(B B^\top) $$
          Identify the terms:
          <ul>
            <li>$\lambda_{\max}(A^\top A) = \|A\|_2^2$ (Squared Spectral Norm).</li>
            <li>$\mathrm{tr}(B B^\top) = \mathrm{tr}(B^\top B) = \|B\|_F^2$ (Squared Frobenius Norm).</li>
          </ul>
          So, $\|AB\|_F^2 \le \|A\|_2^2 \|B\|_F^2$.
        </div>
        <div class="proof-step">
          <strong>Step 5: Compare Norms.</strong>
          We know that the spectral norm is always less than or equal to the Frobenius norm: $\|A\|_2 \le \|A\|_F$.
          Therefore, $\|A\|_2^2 \le \|A\|_F^2$.
          $$ \|AB\|_F^2 \le \|A\|_F^2 \|B\|_F^2 $$
          Taking square roots completes the proof.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Frobenius Norm:</b> $\|A\|_F = \sqrt{\sum_{i,j} A_{ij}^2} = \sqrt{\mathrm{tr}(A^\top A)}$. It measures the total "energy" of the matrix entries.
        <br><b>Algebraic Property:</b> The Frobenius norm is <b>submultiplicative</b> (or consistent): $\|AB\|_F \le \|A\|_F \|B\|_F$.
        <br><b>Comparison:</b> $\|A\|_2 \le \|A\|_F \le \sqrt{rank(A)} \|A\|_2$. The spectral norm is "tighter".</p>
      </div>




<h3>P0.14 — Spectral Norm Properties</h3>
      <p>Let $\|A\|_2$ denote the spectral norm (max singular value).</p>
      <ol type="a">
        <li>Show that $\|A\|_2 = 0$ if and only if $A=0$.</li>
        <li>Show that $\|Q A\|_2 = \|A\|_2$ for any orthogonal matrix $Q$. (Orthogonal invariance).</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>$\|A\|_2 = \sigma_{\max}(A)$. Singular values are non-negative. The max is 0 iff all singular values are 0. If $\Sigma=0$ in SVD $A=U\Sigma V^\top$, then $A=0$. Conversely if $A=0$, the maximum stretch is 0.</li>
          <li>$\|QA\|_2 = \sup_{\|\mathbf{x}\|=1} \|QA\mathbf{x}\|_2$. Since $Q$ is orthogonal, it preserves Euclidean norms: $\|QAx\|_2 = \|Ax\|_2$.
          Thus $\sup_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|_2 = \|A\|_2$. The spectral norm is invariant under left (and right) orthogonal multiplication.</li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Definition:</b> The spectral norm (or operator norm) is the maximum amplification of a vector's length: $\|A\|_2 = \sup_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2$.
        <br><b>Calculation:</b> $\|A\|_2 = \sigma_{\max}(A) = \sqrt{\lambda_{\max}(A^\top A)}$.
        <br><b>Invariance:</b> The spectral norm is <b>orthogonally invariant</b>. $\|Q A Z\|_2 = \|A\|_2$ for any orthogonal matrices $Q, Z$. It depends only on the singular values (intrinsic geometry).</p>
      </div>




<h3>P0.15 — Isometries</h3>
      <p>An isometry is a linear map $f(\mathbf{x}) = Q\mathbf{x}$ that preserves distances: $\|Qx - Qy\|_2 = \|x - y\|_2$ for all $\mathbf{x}, \mathbf{y}$.
      <br>Show that this condition implies $Q^\top Q = I$. Thus, isometries are represented by orthogonal matrices.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Step 1: Norm Preservation.</strong>
          Setting $\mathbf{y}=0$, linearity implies $Q(0)=0$, so $\|Qx\|_2 = \|x\|_2$ for all $\mathbf{x}$.
          Squaring both sides:
          $$ \mathbf{x}^\top Q^\top Q \mathbf{x} = \mathbf{x}^\top I \mathbf{x} \implies \mathbf{x}^\top (Q^\top Q - I) \mathbf{x} = 0 $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Polarization Identity.</strong>
          Let $M = Q^\top Q - I$. We have $\mathbf{x}^\top M \mathbf{x} = 0$ for all $\mathbf{x}$. Since $M$ is symmetric, we can recover the values of the bilinear form $\mathbf{x}^\top M \mathbf{y}$ using the polarization identity:
          $$ \mathbf{x}^\top M \mathbf{y} = \frac{1}{4} \left( (\mathbf{x}+\mathbf{y})^\top M (\mathbf{x}+\mathbf{y}) - (\mathbf{x}-\mathbf{y})^\top M (\mathbf{x}-\mathbf{y}) \right) $$
          Since the quadratic form is zero for any vector (including $\mathbf{x}+\mathbf{y}$ and $\mathbf{x}-\mathbf{y}$), the right-hand side is zero.
          Thus, $\mathbf{x}^\top M \mathbf{y} = 0$ for all $\mathbf{x}, \mathbf{y}$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          Choosing $\mathbf{x} = e_i$ and $\mathbf{y} = e_j$ yields $e_i^\top M e_j = M_{ij} = 0$.
          Since all entries are zero, $M = 0$, so $Q^\top Q = I$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Isometry Definition:</b> A linear map $Q$ is an isometry if $\|Qx\|_2 = \|x\|_2$ for all $\mathbf{x}$.
        <br><b>Polarization Identity:</b> Inner products can be recovered solely from norm evaluations:
        $$ \langle \mathbf{x}, \mathbf{y} \rangle = \frac{1}{4} (\|\mathbf{x}+\mathbf{y}\|^2 - \|\mathbf{x}-\mathbf{y}\|^2) $$
        <br><b>Conclusion:</b> Preserving lengths is equivalent to preserving angles (inner products). Thus, isometries in Euclidean space are exactly the orthogonal matrices ($Q^\top Q = I$).</p>
      </div>




<h3>P0.16 — Loewner Order Transitivity</h3>
      <p>The Loewner order is defined as $X \succeq Y \iff X - Y \succeq 0$. Prove that this order is transitive: if $X \succeq Y$ and $Y \succeq Z$, then $X \succeq Z$. Use the variational definition of PSD matrices.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $\mathbf{v} \in \mathbb{R}^n$ be any vector.
        $X \succeq Y \implies \mathbf{v}^\top (X-Y) \mathbf{v} \ge 0 \implies \mathbf{v}^\top X \mathbf{v} \ge \mathbf{v}^\top Y \mathbf{v}$.
        $Y \succeq Z \implies \mathbf{v}^\top (Y-Z) \mathbf{v} \ge 0 \implies \mathbf{v}^\top Y \mathbf{v} \ge \mathbf{v}^\top Z \mathbf{v}$.
        combining inequalities: $\mathbf{v}^\top X \mathbf{v} \ge \mathbf{v}^\top Y \mathbf{v} \ge \mathbf{v}^\top Z \mathbf{v}$.
        Thus $\mathbf{v}^\top (X-Z) \mathbf{v} \ge 0$.
        Since this holds for all $\mathbf{v}$, $X - Z \succeq 0$, so $X \succeq Z$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Loewner Order ($\succeq$):</b> A partial ordering on the set of symmetric matrices defined by $X \succeq Y \iff X - Y \succeq 0$ (i.e., $X-Y$ is PSD).
        <br><b>Key Properties:</b>
        <ul>
            <li><b>Transitivity:</b> $X \succeq Y$ and $Y \succeq Z \implies X \succeq Z$.</li>
            <li><b>Additivity:</b> $X \succeq Y \implies X+Z \succeq Y+Z$.</li>
            <li><b>Congruence:</b> $X \succeq Y \implies T^\top X T \succeq T^\top Y T$ for any $T$.</li>
            <li><b>Inversion:</b> If $X, Y \succ 0$, then $X \succeq Y \implies Y^{-1} \succeq X^{-1}$ (order reversal).</li>
        </ul></p>
      </div>




<h3>P0.17 — Weighted Inner Product</h3>
      <p>Let $A \in \mathbb{S}^n_{++}$ be a symmetric positive definite matrix.
      <br>(a) Prove that $\langle \mathbf{x}, \mathbf{y} \rangle_A = \mathbf{x}^\top A \mathbf{y}$ satisfies all axioms of an inner product.
      <br>(b) Write down the Cauchy-Schwarz inequality for this inner product.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Symmetry:</b> $\langle \mathbf{x}, \mathbf{y} \rangle_A = \mathbf{x}^\top A \mathbf{y} = (\mathbf{x}^\top A \mathbf{y})^\top = \mathbf{y}^\top A^\top \mathbf{x} = \mathbf{y}^\top A \mathbf{x} = \langle \mathbf{y}, \mathbf{x} \rangle_A$ (since $A=A^\top$).
          <br><b>Linearity:</b> Linear in first arg (matrix multiplication distributes).
          <br><b>Positive Definiteness:</b> $\langle \mathbf{x}, \mathbf{x} \rangle_A = \mathbf{x}^\top A \mathbf{x}$. Since $A \succ 0$, this is $>0$ for all $\mathbf{x} \neq 0$.
          </li>
          <li><b>Cauchy-Schwarz:</b> $|\langle \mathbf{x}, \mathbf{y} \rangle_A| \le \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle_A} \sqrt{\langle \mathbf{y}, \mathbf{y} \rangle_A}$.
          Explicitly: $|\mathbf{x}^\top A \mathbf{y}| \le \sqrt{\mathbf{x}^\top A \mathbf{x}} \sqrt{\mathbf{y}^\top A \mathbf{y}}$.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Weighted Inner Product:</b> Given a Positive Definite matrix $P \succ 0$, the function $\langle \mathbf{x}, \mathbf{y} \rangle_P = \mathbf{x}^\top P \mathbf{y}$ defines a valid inner product.
        <br><b>Mahalanobis Distance:</b> The induced norm $\|x\|_P = \sqrt{\mathbf{x}^\top P \mathbf{x}}$ measures distance accounting for correlations/scales defined by $P$.
        <br><b>Geometric Meaning:</b> The unit ball $\{x \mid \mathbf{x}^\top P \mathbf{x} \le 1\}$ is an ellipsoid centered at the origin.</p>
      </div>




<h3>P0.18 — Characterization of Projectors</h3>
      <p>A matrix $P$ is an orthogonal projector if and only if it is idempotent ($P^2=P$) and symmetric ($P^\top=P$).
      <br>(a) Prove that if $P$ is an orthogonal projector onto a subspace $S$, it satisfies these conditions.
      <br>(b) Prove the converse.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>Let $P$ be the orthogonal projector onto $S$. Let $\mathbf{x} = \mathbf{u} + \mathbf{v}$ where $\mathbf{u} \in S, \mathbf{v} \in S^\perp$. Then $P\mathbf{x} = \mathbf{u}$.
          <br><b>Idempotent:</b> $P^2 \mathbf{x} = P(P\mathbf{x}) = P\mathbf{u}$. Since $\mathbf{u} \in S$, $P\mathbf{u} = \mathbf{u}$. Thus $P^2 \mathbf{x} = \mathbf{u} = P\mathbf{x}$. So $P^2 = P$.
          <br><b>Symmetric:</b> Check $\langle P\mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{x}, P\mathbf{y} \rangle$.
          Decompose $\mathbf{x}=x_S+x_\perp, \mathbf{y}=y_S+y_\perp$.
          $\langle P\mathbf{x}, \mathbf{y} \rangle = \langle x_S, y_S+y_\perp \rangle = \langle x_S, y_S \rangle$ (since $S \perp S^\perp$).
          $\langle \mathbf{x}, P\mathbf{y} \rangle = \langle x_S+x_\perp, y_S \rangle = \langle x_S, y_S \rangle$.
          They are equal, so $P$ is symmetric.
          </li>
          <li>Let $P=P^\top=P^2$. Define $S = \mathcal{R}(P)$.
          For any $\mathbf{x}$, $\mathbf{x} = P\mathbf{x} + (I-P)\mathbf{x}$.
          $P\mathbf{x} \in S$. $(I-P)\mathbf{x}$ is in $S^\perp$? Check orthogonality:
          $(P\mathbf{x})^\top (I-P)\mathbf{x} = \mathbf{x}^\top P^\top (I-P) \mathbf{x} = \mathbf{x}^\top (P - P^2) \mathbf{x}$.
          Since $P^2=P$, this is 0.
          Thus $\mathbf{x}$ is decomposed into a component in $S$ and a component orthogonal to $S$. $P$ maps $\mathbf{x}$ to the component in $S$. This is the definition of an orthogonal projector.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Projector Theorem:</b> A matrix $P$ represents an orthogonal projection onto some subspace if and only if:
        <ul>
            <li><b>$P^2 = P$:</b> It is a projection (idempotent). Applying it twice is the same as applying it once.</li>
            <li><b>$P^\top = P$:</b> It is orthogonal (symmetric). The residual is orthogonal to the range.</li>
        </ul>
        <br><b>Oblique Projectors:</b> If $P^2=P$ but $P \neq P^\top$, it is an <i>oblique</i> projection (projects along a non-orthogonal angle).</p>
      </div>




<h3>P0.19 — PSD Cone in 2D</h3>
      <p>Consider the space of $2 \times 2$ symmetric matrices, which has dimension 3.
      <br>Write down the explicit inequalities defining the PSD cone $S \succeq 0$ in terms of the matrix entries $\mathbf{x}, \mathbf{y}, \mathbf{z}$ (where $S = \begin{bmatrix} \mathbf{x} & \mathbf{y} \\ \mathbf{y} & \mathbf{z} \end{bmatrix}$). Relate this to the trace and determinant conditions.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>A $2 \times 2$ matrix is PSD if and only if:
        1. Trace $\ge 0$: $\mathbf{x} + \mathbf{z} \ge 0$.
        2. Determinant $\ge 0$: $xz - \mathbf{y}^2 \ge 0$.
        <br>Note that $xz \ge \mathbf{y}^2 \ge 0$ implies $\mathbf{x}$ and $\mathbf{z}$ have the same sign.
        Since their sum is non-negative, both must be non-negative: $\mathbf{x} \ge 0, \mathbf{z} \ge 0$.
        <br>Thus the conditions are:
        $$ \mathbf{x} \ge 0, \quad \mathbf{z} \ge 0, \quad xz \ge \mathbf{y}^2 $$
        Geometrically, this is a cone in $\mathbb{R}^3$. The boundary $xz=\mathbf{y}^2$ is a rotated quadratic cone surface.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The PSD Cone $\mathbb{S}^n_+$:</b> The set of all symmetric positive semidefinite matrices forms a convex cone in $\mathbb{S}^n$.
        <br><b>Geometry (2x2):</b> In the 3D space of entries $(\mathbf{x}, \mathbf{y}, \mathbf{z})$, the PSD cone is defined by $\mathbf{x} \ge 0, \mathbf{z} \ge 0, xz \ge \mathbf{y}^2$. This looks like an "ice cream cone" with a non-circular cross-section.
        <br><b>Boundary:</b> The boundary of the cone consists of singular PSD matrices (rank deficient, $\det(X)=0$).
        <br><b>Interior:</b> The interior consists of Positive Definite (PD) matrices ($\det(X)>0, \mathbf{x}>0$).</p>
      </div>




<h3>P0.20 — General Quadratic Minimization</h3>
      <p>Solve the unconstrained minimization problem for a general quadratic function:
      $$ \min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}) := \frac{1}{2} \mathbf{x}^\top H \mathbf{x} + \mathbf{g}^\top \mathbf{x} + c $$
      where $H \in \mathbb{S}_{++}^n$ (Positive Definite).</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>By Lemma A2 (Quadratic Minimization), the minimizer is obtained by setting the gradient to zero.
        $$ \nabla f(\mathbf{x}) = H\mathbf{x} + \mathbf{g} = 0 \implies \mathbf{x}^\star = -H^{-1}\mathbf{g} $$
        Substituting this back into the objective to find the optimal value:
        $$ f(\mathbf{x}^\star) = \frac{1}{2} (-H^{-1}\mathbf{g})^\top H (-H^{-1}\mathbf{g}) + \mathbf{g}^\top (-H^{-1}\mathbf{g}) + c $$
        $$ = \frac{1}{2} \mathbf{g}^\top H^{-1} H H^{-1} \mathbf{g} - \mathbf{g}^\top H^{-1} \mathbf{g} + c $$
        $$ = \frac{1}{2} \mathbf{g}^\top H^{-1} \mathbf{g} - \mathbf{g}^\top H^{-1} \mathbf{g} + c = c - \frac{1}{2} \mathbf{g}^\top H^{-1} \mathbf{g} $$
        This formula is the foundation for almost all dual derivations (where we minimize the Lagrangian w.r.t $\mathbf{x}$).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Quadratic Forms:</b> Every convex quadratic problem reduces to solving a linear system ($H\mathbf{x} = -\mathbf{g}$).
        <br><b>Completing the Square:</b> The minimum value is always "constant minus quadratic form of the gradient inverse".</p>
      </div>

      <h3>P0.21 — Chebyshev Approximation Bound</h3>
      <p>Let $x_{\text{ls}}$ be the least-squares solution to $A\mathbf{x} \approx \mathbf{b}$, minimizing $\|A\mathbf{x}-\mathbf{b}\|_2$. Let $x_{\text{ch}}$ be the solution to the Chebyshev approximation problem minimizing $\|A\mathbf{x}-\mathbf{b}\|_\infty$.
      <br>Show that the $\ell_\infty$ error of the least-squares solution is at most $\sqrt{m}$ times the optimal $\ell_\infty$ error.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>This problem compares two different ways to measure the residual error.
        <br>Let $\mathbf{r}_{\text{ls}} = A\mathbf{x}_{\text{ls}} - \mathbf{b}$ and $\mathbf{r}_{\text{ch}} = A\mathbf{x}_{\text{ch}} - \mathbf{b}$.
        <br>We use two fundamental norm inequalities for vectors in $\mathbb{R}^m$:
        <ul>
            <li>$\|\mathbf{z}\|_\infty \le \|\mathbf{z}\|_2$ (The max entry is less than the Euclidean length).</li>
            <li>$\|\mathbf{z}\|_2 \le \sqrt{m} \|\mathbf{z}\|_\infty$ (The Euclidean length is at most $\sqrt{m}$ times the max entry).</li>
        </ul>
        </p>
        <div class="proof-step">
            <strong>Step 1: Bound LS error by its 2-norm.</strong>
            $$ \|\mathbf{r}_{\text{ls}}\|_\infty \le \|\mathbf{r}_{\text{ls}}\|_2 $$
        </div>
        <div class="proof-step">
            <strong>Step 2: Use Least-Squares Optimality.</strong>
            By definition, $x_{\text{ls}}$ minimizes the 2-norm of the residual. Thus, for any other vector $x$ (including $x_{\text{ch}}$), we must have:
            $$ \|\mathbf{r}_{\text{ls}}\|_2 \le \|\mathbf{r}_{\text{ch}}\|_2 $$
        </div>
        <div class="proof-step">
            <strong>Step 3: Bound Chebyshev error by its $\infty$-norm.</strong>
            $$ \|\mathbf{r}_{\text{ch}}\|_2 \le \sqrt{m} \|\mathbf{r}_{\text{ch}}\|_\infty $$
        </div>
        <div class="proof-step">
            <strong>Conclusion:</strong> Chaining the inequalities together:
            $$ \|\mathbf{r}_{\text{ls}}\|_\infty \le \|\mathbf{r}_{\text{ls}}\|_2 \le \|\mathbf{r}_{\text{ch}}\|_2 \le \sqrt{m} \|\mathbf{r}_{\text{ch}}\|_\infty $$
            This proves that using the easy-to-compute Least Squares solution gives a worst-case error that is at most $\sqrt{m}$ times worse than the optimal Chebyshev error.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Approximation Ratios:</b> Norm equivalence constants often translate directly into approximation guarantees for optimization problems involving different norms.</p>
      </div>
</section>



<!-- SECTION 10: APPENDIX -->
<section class="section-card" id="section-appendix">
  <h2>Appendix: Detailed Proofs</h2>

  <h3>A.1 Derivation of the Triangle of Equivalence (Schur Complement)</h3>
  <p>In Section 5.5, we stated that for $Y \succ 0$, the condition $tY - \mathbf{x}\mathbf{x}^\top \succeq 0$ is equivalent to the scalar inequality $t \ge \mathbf{x}^\top Y^{-1} \mathbf{x}$. Here is the step-by-step derivation.</p>

  <div class="proof-box">
    <h4>Proof: From Rank-1 Update to Scalar Condition</h4>
    <p>We want to show $tY - \mathbf{x}\mathbf{x}^\top \succeq 0 \iff t \ge \mathbf{x}^\top Y^{-1} \mathbf{x}$.</p>
    <div class="proof-step">
        <strong>Step 1: Variational Definition.</strong>
        $tY - \mathbf{x}\mathbf{x}^\top \succeq 0$ means for all $\mathbf{v} \in \mathbb{R}^n$:
        $$ \mathbf{v}^\top (tY - \mathbf{x}\mathbf{x}^\top) \mathbf{v} \ge 0 \implies t (\mathbf{v}^\top Y \mathbf{v}) - (\mathbf{v}^\top \mathbf{x})(\mathbf{x}^\top \mathbf{v}) \ge 0 \implies (\mathbf{x}^\top \mathbf{v})^2 \le t (\mathbf{v}^\top Y \mathbf{v}) $$
    </div>
    <div class="proof-step">
        <strong>Step 2: Change of Variables.</strong>
        Since $Y \succ 0$, it has a square root $Y^{1/2}$. Let $\mathbf{u} = Y^{1/2} \mathbf{v}$. Then $\mathbf{v} = Y^{-1/2} \mathbf{u}$.
        The term $\mathbf{v}^\top Y \mathbf{v} = \mathbf{u}^\top Y^{-1/2} Y Y^{-1/2} \mathbf{u} = \mathbf{u}^\top \mathbf{u} = \|\mathbf{u}\|_2^2$.
        The term $\mathbf{x}^\top \mathbf{v} = \mathbf{x}^\top Y^{-1/2} \mathbf{u} = (Y^{-1/2} \mathbf{x})^\top \mathbf{u}$. Let $\mathbf{w} = Y^{-1/2} \mathbf{x}$. Then $\mathbf{x}^\top \mathbf{v} = \mathbf{w}^\top \mathbf{u}$.
    </div>
    <div class="proof-step">
        <strong>Step 3: Optimization View.</strong>
        The inequality becomes $(\mathbf{w}^\top \mathbf{u})^2 \le t \|\mathbf{u}\|_2^2$ for all $\mathbf{u}$.
        We want to find the minimum $t$ that satisfies this. This is equivalent to:
        $$ t \ge \sup_{\mathbf{u} \neq 0} \frac{(\mathbf{w}^\top \mathbf{u})^2}{\|\mathbf{u}\|_2^2} $$
        By the Cauchy-Schwarz inequality, $(\mathbf{w}^\top \mathbf{u})^2 \le \|\mathbf{w}\|_2^2 \|\mathbf{u}\|_2^2$, with equality when $\mathbf{u}$ is parallel to $\mathbf{w}$.
        Thus, the supremum is $\|\mathbf{w}\|_2^2$.
    </div>
    <div class="proof-step">
        <strong>Step 4: Conclusion.</strong>
        $$ t \ge \|\mathbf{w}\|_2^2 = \mathbf{w}^\top \mathbf{w} = (Y^{-1/2} \mathbf{x})^\top (Y^{-1/2} \mathbf{x}) = \mathbf{x}^\top Y^{-1} \mathbf{x} $$
    </div>
  </div>

  <h3>A.2 The Steinitz Exchange Lemma</h3>
  <div class="proof-box">
    <h4>Lemma: Steinitz Exchange Lemma (Proof)</h4>
    <p>Let $V$ be a vector space. Suppose:
    <ul>
        <li>$S = \{s_1,\dots,s_m\}$ spans $V$.</li>
        <li>$L = \{\ell_1,\dots,\ell_k\}$ is linearly independent in $V$.</li>
    </ul>
    Then <b>$k \le m$</b>, and we can replace $k$ vectors in $S$ with $L$ to form a new spanning set.</p>

    <h4>Proof by Step-by-Step Replacement</h4>
    <p>We proceed by induction to inject elements of $L$ into $S$ one by one.</p>
    <div class="proof-step">
        <strong>Step 0:</strong> Start with $S^{(0)} = S$, which spans $V$.
    </div>
    <div class="proof-step">
        <strong>Inductive Step:</strong> Assume $S^{(j-1)}$ spans $V$ and contains $\{\ell_1, \dots, \ell_{j-1}\}$. We want to insert $\ell_j$.
        <br>Since $S^{(j-1)}$ spans, write $\ell_j$ as a linear combination of vectors in $S^{(j-1)}$.
        $$ \ell_j = \sum_{u \in S^{(j-1)}} c_u u $$
        Rearranging: $0 = \ell_j - \sum c_u u$.
        <br>Crucially, at least one vector $u^* \in S^{(j-1)}$ <b>not</b> in $\{\ell_1, \dots, \ell_{j-1}\}$ must have a non-zero coefficient $c_{u^*} \neq 0$.
        <br><i>Why?</i> If only $\ell$'s had non-zero coefficients, then $\ell_j$ would be in the span of previous $\ell$'s, contradicting independence of $L$.
    </div>
    <div class="proof-step">
        <strong>Replacement:</strong> Solve for $u^*$:
        $$ u^* = \frac{1}{c_{u^*}} \ell_j - \sum \dots $$
        This shows $u^*$ is in the span of the new set $S^{(j)} = (S^{(j-1)} \setminus \{u^*\}) \cup \{\ell_j\}$.
        Thus, the new set still spans $V$.
    </div>
    <div class="proof-step">
        <strong>Conclusion:</strong> We can continue this for all $k$ elements of $L$. Since each step consumes a distinct vector from the original $S$ (and never one of the inserted $\ell$'s), we must have enough capacity in $S$. Thus $k \le m$.
    </div>
  </div>
</section>

<!-- SECTION 11: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>Readings & Resources</h2>
      <ul>
        <li>
          <b>Boyd & Vandenberghe, Convex Optimization:</b>
          <ul>
            <li>Appendix A: Mathematical Background (Norms, Analysis, Functions)</li>
            <li>Appendix C: Numerical Linear Algebra (Operations, Factorizations)</li>
          </ul>
        </li>
        <li>
          <b>Gilbert Strang, Introduction to Linear Algebra:</b>
          <ul>
            <li>Chapter 2: Vector Spaces</li>
            <li>Chapter 3: Orthogonality</li>
            <li>Chapter 6: Eigenvalues and Eigenvectors</li>
          </ul>
        </li>
        <li>
          <b>Golub & Van Loan, Matrix Computations:</b>
          <ul>
            <li>Chapter 2: Matrix Multiplication (for numerical aspects)</li>
            <li>Chapter 5: Orthogonalization and Least Squares</li>
          </ul>
        </li>
      </ul>
    </section>


    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="../../static/lib/pyodide/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/resizable.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
