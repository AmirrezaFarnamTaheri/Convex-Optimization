<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "../../static/lib/three/three.module.js"
      }
    }
  </script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture bridges the gap between basic linear algebra and the robust numerical tools used in modern optimization. We move beyond the Normal Equations to explore numerically stable factorizations like QR and the Singular Value Decomposition (SVD), which reveal the geometry of linear maps in their purest form. We also introduce the Moore-Penrose pseudoinverse for solving ill-posed or rank-deficient problems and define the condition number—the fundamental metric for analyzing the stability of algorithms and the convergence speed of gradient descent.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm and matrix completion in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The condition number is the key parameter determining the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a> and motivates the use of Newton's Method.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD) as fundamental tools for numerical linear algebra.</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming the potentially ill-conditioned matrix $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions for rank-deficient or underdetermined systems.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations in data.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA) using truncated SVD.</li>
      </ul>
    </section>

    <!-- SECTION 1: SCALAR INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>1. Scalar Invariants: Determinant and Trace</h2>
      <p>A scalar invariant is a function of a matrix that does not change under similarity transformations ($A \mapsto P^{-1}AP$). Geometrically, these are properties of the linear operator itself, not its specific matrix representation.</p>

      <h3>1.1 Determinant: The Unique Volume Form</h3>
      <p>The determinant is the <b>unique</b> function $\det: \mathbb{F}^{n \times n} \to \mathbb{F}$ satisfying three axioms:</p>
      <ol>
        <li><b>Multilinearity:</b> Linear in each column separately.</li>
        <li><b>Alternating:</b> If two columns are equal, the value is zero (implies sign flip on swap).</li>
        <li><b>Normalization:</b> $\det(I) = 1$.</li>
      </ol>
      <p>These geometric axioms force the algebraic Leibniz formula: $\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{i=1}^n A_{i, \sigma(i)}$.</p>

      <div class="insight">
        <h4>Geometric Interpretation: Volume and Orientation</h4>
        <p>For $A \in \mathbb{R}^{n \times n}$, $|\det(A)|$ represents the volume of the parallelepiped spanned by the columns of $A$. The sign indicates orientation (parity of reflections).
      <br><b>Properties forced by geometry:</b></p>
        <ul>
          <li><b>Invertibility:</b> $\det(A) \neq 0 \iff$ volume is non-zero $\iff$ columns are linearly independent.</li>
          <li><b>Multiplicativity:</b> $\det(AB) = \det(A)\det(B)$. The volume scaling of a composite map is the product of individual scalings.</li>
        </ul>
      </div>

      <h3>1.2 Trace: The Unique Linear Invariant</h3>
      <p>The trace is the unique linear functional $\mathrm{tr}: \mathbb{F}^{n \times n} \to \mathbb{F}$ (up to scaling) satisfying the <b>cyclic property</b>:
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA) $$
      This property implies similarity invariance: $\mathrm{tr}(P^{-1}AP) = \mathrm{tr}(APP^{-1}) = \mathrm{tr}(A)$. Geometrically, the trace represents the <b>derivative</b> of the determinant at the identity:
      $$ \det(I + \epsilon A) = 1 + \epsilon \mathrm{tr}(A) + O(\epsilon^2) $$
      This connects linear algebra to calculus (e.g., divergence of a vector field is the trace of the Jacobian).</p>
    </section>

    <!-- SECTION 2: EIGENVALUES -->
    <section class="section-card" id="section-eigenvalues">
      <h2>2. Eigenvalues and Dynamics</h2>
      <p>We now ask: <b>Are there directions that a linear map preserves, changing only their length?</b> This is the spectral perspective. It collapses the complexity of matrix multiplication into simple scalar multiplication along specific axes.</p>

      <h3>2.1 Invariant Subspaces and Eigenvalues</h3>
      <p>A subspace $U \subseteq V$ is $T$-invariant if $T(u) \in U$ for all $u \in U$.
      <br>An <b>eigenvector</b> $\mathbf{v} \neq 0$ spans a 1-dimensional invariant subspace: $T(\mathbf{v}) = \lambda \mathbf{v}$.
      <br>The scalar $\lambda$ is the <b>eigenvalue</b>. It is a root of the characteristic polynomial $p_T(\lambda) = \det(T - \lambda I)$.</p>

      <div class="insight">
        <h4>Dynamics Intuition: Why Eigenvalues Govern Stability</h4>
        <p>Eigenvectors are the "pure modes" of a linear system.
        <br><b>Discrete Time ($x_{k+1} = Ax_k$):</b> Solutions are $\lambda^k x_0$. Stability requires $|\lambda| < 1$.
        <br><b>Continuous Time ($\dot{x} = Ax$):</b> Solutions are $e^{\lambda t}x_0$. Stability requires $\mathrm{Re}(\lambda) < 0$.
        </p>
      </div>

      <h3>2.2 Diagonalization: When do Eigenvectors Form a Basis?</h3>
      <p>An operator $T$ is <b>diagonalizable</b> if there exists a basis of $V$ consisting of eigenvectors.
      <br><b>The Criterion:</b> Diagonalization is possible if and only if for every eigenvalue $\lambda$, the <b>geometric multiplicity</b> (dimension of eigenspace $\ker(T-\lambda I)$) equals the <b>algebraic multiplicity</b> (root multiplicity of characteristic polynomial).</p>

      <div class="proof-box">
        <h4>Why Columns of $V$ are Eigenvectors</h4>
        <p>If $A = P\Lambda P^{-1}$, then $AP = P\Lambda$.
        <br>Let $P = [v_1 \dots v_n]$ and $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$.
        <br>Matching columns gives $Av_i = \lambda_i v_i$. Thus, the columns of the change-of-basis matrix are the eigenvectors.</p>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/diagonalization_eigenbasis.gif"
             alt="Diagonalization transforms the map into pure scaling along axes"
             style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> Diagonalization allows us to view the linear map as pure scaling along the eigenvector axes.</figcaption>
      </figure>

      <h3>2.3 Diagonalization and its Failure</h3>
      <p><b>Diagonalization ($A = PDP^{-1}$) is the ideal case.</b>
      <br><b>Defective Matrices:</b> If geometric multiplicity &lt; algebraic multiplicity, the matrix cannot be diagonalized. This corresponds to the presence of <b>Jordan Blocks</b>.</p>

      <h4>Jordan Canonical Form</h4>
      <p>Every matrix $A$ is similar to a block diagonal matrix $J$ with Jordan blocks:
      $$ J_k(\lambda) = \begin{bmatrix} \lambda & 1 & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & \lambda \end{bmatrix} $$
      The "1"s on the superdiagonal represent the "defect"—the coupling between modes that prevents pure diagonalization.
      <br><i>Practical Note:</i> Jordan form is numerically unstable. In practice (e.g., NumPy), we use the Schur Decomposition ($A = QUQ^\top$, with Q orthogonal and U quasi-triangular) or SVD.</p>

      <h3>2.4 The Spectral Theorem: Perfect Geometry</h3>
      <p><b>Theorem:</b> If $A$ is <b>symmetric</b> ($A = A^\top$), then:
      <ol>
        <li>All eigenvalues are <b>real</b>.</li>
        <li>Eigenvectors corresponding to distinct eigenvalues are <b>orthogonal</b>.</li>
        <li>$A$ admits an <b>orthonormal basis of eigenvectors</b> ($A = Q \Lambda Q^\top$).</li>
      </ol>
      This is the "perfect" case: no complex numbers, no defects, and the coordinate system is orthogonal. This theorem underpins the geometry of ellipsoids and Hessian analysis.</p>

      <h3>2.5 The Rayleigh Quotient</h3>
      <p>For a symmetric matrix $A$, the <b>Rayleigh Quotient</b> is $R_A(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$.
      <br><b>Variational Characterization:</b>
      $$ \lambda_{\min}(A) = \min_{\mathbf{x} \ne 0} R_A(\mathbf{x}) \quad \text{and} \quad \lambda_{\max}(A) = \max_{\mathbf{x} \ne 0} R_A(\mathbf{x}) $$
      This connects linear algebra to optimization: finding the largest eigenvalue is equivalent to maximizing a quadratic form over the unit sphere.</p>

      <h3>2.6 Generalized Eigenvalues and Whitening</h3>
      <p>The generalized eigenvalue problem $A\mathbf{x} = \lambda B \mathbf{x}$ (with $B \succ 0$) arises when optimizing $\mathbf{x}^\top A \mathbf{x}$ subject to $\mathbf{x}^\top B \mathbf{x} = 1$.
      <br><b>Solution via Whitening:</b>
      1. Factor $B = LL^\top$.
      2. Let $\mathbf{y} = L^\top \mathbf{x}$. The constraint becomes $\|\mathbf{y}\|^2 = 1$.
      3. The problem becomes $C\mathbf{y} = \lambda \mathbf{y}$ where $C = L^{-1} A L^{-\top}$.
      <br>The generalized eigenvalues are the eigenvalues of the "whitened" matrix.</p>
    </section>

    <!-- SECTION 3: INDUCED NORMS -->
    <section class="section-card" id="section-induced-norms">
      <h2>3. Induced Matrix Norms</h2>
      <p>We defined vector norms in Lecture 00. Now we ask: how "big" is a matrix $A$? The <b>induced norm</b> measures the maximum factor by which $A$ can stretch a vector. It is effectively the <b>Lipschitz constant</b> of the linear map.</p>
      $$ \|A\|_p = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|_p}{\|\mathbf{x}\|_p} = \max_{\|\mathbf{x}\|_p = 1} \|A\mathbf{x}\|_p $$

      <h3>3.1 The $\ell_1$ Operator Norm (Max Column Sum)</h3>
      <p>$$ \boxed{\|A\|_1 = \max_{j} \sum_{i=1}^m |a_{ij}|} $$</p>
      <div class="proof-box">
        <h4>Derivation: Budget Allocation Intuition</h4>
        <div class="proof-step">
          <strong>Step 1: Upper Bound.</strong>
          Think of $\|\mathbf{x}\|_1 = 1$ as a "mass budget" of 1.
          $$ \|A\mathbf{x}\|_1 = \left\| \sum_j x_j \mathbf{a}_j \right\|_1 \le \sum_j |x_j| \|\mathbf{a}_j\|_1 $$
          To maximize the output mass, you should put all your budget on the column $\mathbf{a}_k$ that has the largest norm:
          $$ \sum_j |x_j| \|\mathbf{a}_j\|_1 \le (\max_k \|\mathbf{a}_k\|_1) \sum_j |x_j| = \max_k \|\mathbf{a}_k\|_1 $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Achievability.</strong>
          Choose $\mathbf{x} = \mathbf{e}_k$, where $k$ is the index of the largest column. Then $\|A\mathbf{e}_k\|_1 = \|\mathbf{a}_k\|_1$, hitting the bound.
        </div>
      </div>

      <h3>3.2 The $\ell_\infty$ Operator Norm (Max Row Sum)</h3>
      <p>$$ \boxed{\|A\|_\infty = \max_{i} \sum_{j=1}^n |a_{ij}|} $$</p>
      <div class="proof-box">
        <h4>Derivation: Sign Alignment Intuition</h4>
        <div class="proof-step">
          <strong>Step 1: Upper Bound.</strong>
          Here $|x_j| \le 1$. We want to maximize $|(A\mathbf{x})_i| = |\sum_j a_{ij} x_j|$.
          $$ |(A\mathbf{x})_i| \le \sum_j |a_{ij}| |x_j| \le \sum_j |a_{ij}| $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Achievability.</strong>
          Let $k$ be the index of the row with the maximum absolute sum.
          Choose $x_j = \text{sign}(a_{kj})$. Then $(A\mathbf{x})_k = \sum_j |a_{kj}|$, hitting the bound.
        </div>
      </div>

      <h3>3.3 The $\ell_2$ Operator Norm (Spectral Norm)</h3>
      <p>$$ \|A\|_2 = \max_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2 = \sqrt{\lambda_{\max}(A^\top A)} = \sigma_{\max}(A) $$</p>
      <p><b>Geometry:</b> A linear map $A$ transforms the unit sphere into an ellipsoid. The spectral norm $\|A\|_2$ is the length of the <b>longest semi-axis</b> of that ellipsoid (the maximum stretch).</p>

      <h3>3.4 Convexity of Induced Norms</h3>
      <p>Why are induced norms convex functions of the matrix $A$?
      <br><b>Proof:</b>
      $$ \|X\|_{p,q} = \sup_{\|\mathbf{v}\|_p = 1} \|X\mathbf{v}\|_q = \sup_{\|\mathbf{v}\|_p = 1} \sup_{\|\mathbf{u}\|_{q^*} \le 1} \mathbf{u}^\top X \mathbf{v} $$
      For fixed $\mathbf{u}, \mathbf{v}$, the function $f(X) = \mathbf{u}^\top X \mathbf{v} = \langle \mathbf{u}\mathbf{v}^\top, X \rangle$ is linear in $X$.
      <br>Thus, the induced norm is the <b>pointwise supremum</b> of a family of linear functions, which implies it is convex.</p>
    </section>

    <!-- SECTION 4: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>4. The QR Decomposition: Geometry First</h2>

      <p>QR is not just an algorithm. It is the <b>algebraic shadow</b> of the Gram-Schmidt process. It formalizes the fact that every matrix can be split into "perfect geometry" ($Q$) and "coordinates" ($R$).</p>

      <h3>4.1 Definition and Existence</h3>
      <p>Let $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ (Full column rank). There exists a unique factorization:
      $$ \boxed{ A = Q R } $$
      where:
      <ul>
        <li>$Q \in \mathbb{R}^{m \times n}$ has <b>orthonormal columns</b> ($Q^\top Q = I_n$). This represents an isometric embedding.</li>
        <li>$R \in \mathbb{R}^{n \times n}$ is <b>upper triangular</b> with positive diagonal entries. This represents coordinates in the orthonormal basis.</li>
      </ul>
      <p><b>Geometry:</b> The column $a_k$ depends only on $q_1, \dots, q_k$.
      <br><b>Existence:</b> Use Gram-Schmidt. $q_1 = a_1/\|a_1\|$. $q_2 \propto a_2 - \langle q_1, a_2 \rangle q_1$.
      <br><b>Uniqueness:</b> $R$ must account for the scaling. If diagonal entries of $R$ are positive, the decomposition is unique.</p>

      <h3>4.2 Solving Least Squares with QR</h3>
      <p>Consider $\min \|A\mathbf{x} - \mathbf{b}\|_2$.
      $$ \|A\mathbf{x} - \mathbf{b}\|_2 = \|QR\mathbf{x} - \mathbf{b}\|_2 = \|Q(R\mathbf{x}) - \mathbf{b}\|_2 $$
      Multiply by $Q^\top$ (which doesn't change norms for vectors in the range):
      $$ \|Q^\top Q R \mathbf{x} - Q^\top \mathbf{b}\|_2 = \|R\mathbf{x} - Q^\top \mathbf{b}\|_2 $$
      The system $R\mathbf{x} = Q^\top \mathbf{b}$ is triangular and easy to solve by back-substitution.
      <br><b>Stability:</b> $\kappa(R) = \kappa(A)$. We avoid squaring the condition number (unlike Normal Equations).</p>

      <h3>4.3 Householder Reflections</h3>
      <p>While Gram-Schmidt is intuitive, <b>Householder reflections</b> are the gold standard for numerical stability.
      <br>A reflection matrix $H = I - 2\mathbf{u}\mathbf{u}^\top$ ($\|\mathbf{u}\|=1$) is orthogonal and symmetric.
      <br>We can choose $\mathbf{u}$ to map a vector $\mathbf{x}$ to $\|\mathbf{x}\|e_1$ (zeroing out all entries below the first). Applying this sequentially to columns of $A$ yields $R$.
      $$ H_n \dots H_1 A = R \implies A = (H_1 \dots H_n) R = Q R $$</p>
    </section>

    <!-- SECTION 5: SVD -->
    <section class="section-card" id="section-svd">
      <h2>5. Singular Value Decomposition (SVD): The True Geometry</h2>
      <p><b>SVD is the fundamental theorem of linear algebra.</b> It applies to every matrix, square or rectangular, symmetric or not.</p>

      <h3>5.1 The Geometry: Rotate → Scale → Rotate</h3>
      <p>Any matrix $A \in \mathbb{R}^{m \times n}$ can be factored as:
      $$ \boxed{ A = U \Sigma V^\top } $$
      <ul>
        <li>$U \in \mathbb{R}^{m \times m}$: Orthogonal (Left singular vectors). Basis for codomain.</li>
        <li>$V \in \mathbb{R}^{n \times n}$: Orthogonal (Right singular vectors). Basis for domain.</li>
        <li>$\Sigma \in \mathbb{R}^{m \times n}$: Diagonal with entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
      </ul>
      <b>Data Processor Model:</b>
      1. $V^\top$: Rotate input to align with principal axes.
      2. $\Sigma$: Scale along axes (gain $\sigma_i$).
      3. $U$: Rotate output to final orientation.</p>

      <div class="insight">
        <h4>Connection to Eigenvalues</h4>
        <ul>
            <li>Right singular vectors $V$ are eigenvectors of $A^\top A$.</li>
            <li>Left singular vectors $U$ are eigenvectors of $A A^\top$.</li>
            <li>Singular values $\sigma_i = \sqrt{\lambda_i(A^\top A)}$.</li>
        </ul>
      </div>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-wiki.png" alt="SVD Decomposition Wiki" style="width: 100%; height: auto; border-radius: 8px; border: 1px solid var(--border); padding: 8px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
          <figcaption><i>Figure 5a:</i> Standard SVD visualization (Wikimedia).</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-ellipse.png" alt="SVD Ellipse" style="width: 100%; height: auto; border-radius: 8px; border: 1px solid var(--border); padding: 8px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
          <figcaption><i>Figure 5b:</i> Unit circle to ellipse transformation.</figcaption>
        </figure>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: 3D SVD Geometry</h3>
        <p>Visualize the SVD decomposition $A = U \Sigma V^\top$ in 3D as a sequence of three geometric transformations: rotation ($V^\top$), axis-aligned scaling ($\Sigma$), and rotation ($U$).</p>
        <iframe src="widgets/la_batch3.html?mode=svd3" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>

      <h3>5.2 The Four Fundamental Subspaces via SVD</h3>
      <p>Let $r$ be the rank (number of $\sigma_i > 0$).
      <ul>
        <li><b>Column Space:</b> First $r$ columns of $U$.</li>
        <li><b>Left Nullspace:</b> Last $m-r$ columns of $U$.</li>
        <li><b>Row Space:</b> First $r$ columns of $V$.</li>
        <li><b>Nullspace:</b> Last $n-r$ columns of $V$.</li>
      </ul>
      This basis explicitly diagonalizes the operator.</p>

      <h3>5.3 Low-Rank Approximation</h3>
      <p><b>Eckart-Young-Mirsky Theorem:</b> The best rank-$k$ approximation to $A$ (in spectral or Frobenius norm) is the truncated SVD:
      $$ A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top $$
      This is the engine behind Principal Component Analysis (PCA).</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: Rank-1 SVD Approximation</h3>
        <p><b>The Power of Low-Rank Approximation:</b> See how a rank-1 matrix $A_1 = \sigma_1 u_1 v_1^\top$ approximates a general matrix.</p>
        <iframe src="widgets/la_batch2.html?mode=rank1" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>

      <h3>5.4 The Schur Complement</h3>
      <p>For a block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ with $A \succ 0$:
      $$ M \succeq 0 \iff S = C - B^\top A^{-1} B \succeq 0 $$
      This "completing the square" for matrices is essential for converting nonlinear constraints to Linear Matrix Inequalities (SDP).</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: Schur Complement Feasibility</h3>
        <p>Explore the feasible region of the parameter $t$ that makes the block matrix PSD.</p>
        <iframe src="widgets/la_batch3.html?mode=schur" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>

      <h3>5.5 PCA as SVD</h3>
      <p>Principal Component Analysis (PCA) finds the directions of maximum variance.
      <br>Let $X$ be the centered data matrix.
      <br>The covariance is $C \propto X^\top X$.
      <br>SVD of $X = U \Sigma V^\top$ implies $X^\top X = V \Sigma^2 V^\top$.
      <br>Thus, <b>Principal Components</b> are the right singular vectors of $X$.</p>
    </section>

    <!-- SECTION 6: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>6. The Pseudoinverse and Condition Number</h2>

      <h3>6.1 The Moore-Penrose Pseudoinverse ($A^+$)</h3>
      <p>When $A$ is not square or not invertible, we cannot define $A^{-1}$. However, we can define a generalized inverse $A^+$ that "inverts" the action of $A$ on its range and sends the orthogonal complement to zero.</p>

      <h4>The Moore-Penrose Axioms</h4>
      <p>The pseudoinverse $A^+$ is the <b>unique</b> matrix satisfying these four algebraic conditions:
      <ol>
        <li>$A A^+ A = A$ ( $A A^+$ is a projection on the range).</li>
        <li>$A^+ A A^+ = A^+$ ( $A^+$ acts like an inverse on the range of $A$).</li>
        <li>$(A A^+)^\top = A A^+$ (Symmetry of projector).</li>
        <li>$(A^+ A)^\top = A^+ A$ (Symmetry of projector).</li>
      </ol>
      </p>

      <div class="proof-box">
        <h4>Proof: Uniqueness of the Pseudoinverse</h4>
        <p>Suppose $X$ and $Y$ both satisfy the four Moore-Penrose axioms. We show $X=Y$.</p>
        <div class="proof-step">
          <strong>Step 1: Expand using AAX and YAA.</strong>
          Start with $X = XAX$. Substitute $A = AYA$: $X = X(AYA)X = XAYAX$.
          <br>This approach is circular. The clean algebraic proof uses the symmetry properties to swap transposes.
        </div>
        <div class="proof-step">
          <strong>Step 2: Key Algebraic Moves.</strong>
          Use $XA = (XA)^\top = A^\top X^\top$ and $AY = (AY)^\top = Y^\top A^\top$.
          $$ X = XAX = (XA)X = (XA)^\top X = A^\top X^\top X $$
          Substitute $A^\top = (AYA)^\top = A^\top Y^\top A^\top$:
          $$ X = (A^\top Y^\top A^\top) X^\top X = A^\top Y^\top (A^\top X^\top X) $$
          Collapse the terms back: $A^\top X^\top X = (XA)^\top X = XAX = X$.
          $$ X = A^\top Y^\top X = (YA)^\top X = YAX $$
          So $X = YAX$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Symmetry Argument.</strong>
          By a symmetric argument starting with $Y = YAY$, we can derive $Y = YAX$.
          Since $X = YAX$ and $Y = YAX$, we conclude $X = Y$. The pseudoinverse is unique.
        </div>
      </div>

      <p><b>Definition via SVD:</b> If $A = U \Sigma V^\top$, then:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+$ is the $n \times m$ matrix obtained by transposing $\Sigma$ and inverting the non-zero singular values:
      $$
      \Sigma^+ = \begin{bmatrix}
      1/\sigma_1 & & & 0 & \dots \\
      & \ddots & & \vdots & \\
      & & 1/\sigma_r & 0 & \dots \\
      0 & \dots & 0 & 0 & \dots \\
      \vdots & & \vdots & \vdots & \ddots
      \end{bmatrix}
      $$
      This definition explicitly constructs the matrix satisfying the axioms.</p>

      <h4>Properties and Applications</h4>
      <ul>
        <li><b>Minimum Norm Solution:</b> For any system $A\mathbf{x}=\mathbf{b}$ (or the least squares problem), the vector $\mathbf{x}^\star = A^+ \mathbf{b}$ is the solution that has the <b>minimum Euclidean norm</b>.
        <br>
        <div class="proof-box">
          <h4>Proof: $x_{LS} = A^+\mathbf{b}$ minimizes norm</h4>
        <p>The set of all least-squares solutions is the affine set $S_{LS} = x_p + \mathcal{N}(A)$, where $x_p$ is any particular solution. We show $\mathbf{x}^+ = A^+\mathbf{b}$ is the element in $S_{LS}$ with minimum $\ell_2$ norm.</p>

          <div class="proof-step">
          <strong>Step 1: Verification of Solution.</strong>
          First, we check that $\mathbf{x}^+$ actually solves the least squares problem (satisfies Normal Equations).
          Recall $A = U \Sigma V^\top$ and $A^+ = V \Sigma^+ U^\top$.
          $$ A \mathbf{x}^+ = U \Sigma V^\top V \Sigma^+ U^\top \mathbf{b} = U (\Sigma \Sigma^+) U^\top \mathbf{b} $$
          The product $\Sigma \Sigma^+$ is a diagonal projection matrix (1s for indices $1 \dots r$, 0s otherwise). It projects onto the range of $A$.
          Thus $A\mathbf{x}^+$ is the orthogonal projection of $\mathbf{b}$ onto $\mathcal{R}(A)$, which implies $\mathbf{x}^+$ is a least squares solution.
          </div>

          <div class="proof-step">
          <strong>Step 2: Orthogonality of $\mathbf{x}^+$.</strong>
          We check where $\mathbf{x}^+$ lies in the domain.
          $\mathbf{x}^+ = V (\Sigma^+ U^\top \mathbf{b})$. This vector is a linear combination of the first $r$ columns of $V$.
          These columns span the <b>row space</b> $\mathcal{R}(A^\top)$.
          Thus, $\mathbf{x}^+ \in \mathcal{R}(A^\top)$.
          Recall the fundamental orthogonality: $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.
          </div>

          <div class="proof-step">
          <strong>Step 3: Pythagorean Minimization.</strong>
          Let $\mathbf{x}$ be any other solution. It can be written as $\mathbf{x} = \mathbf{x}^+ + \mathbf{z}$, where $\mathbf{z} \in \mathcal{N}(A)$.
          <br>Since $\mathbf{x}^+ \in \mathcal{R}(A^\top)$ and $\mathbf{z} \in \mathcal{N}(A)$, they are orthogonal: $\langle \mathbf{x}^+, \mathbf{z} \rangle = 0$.
          <br>By the Pythagorean Theorem:
          $$ \|\mathbf{x}\|_2^2 = \|\mathbf{x}^+ + \mathbf{z}\|_2^2 = \|\mathbf{x}^+\|_2^2 + \|\mathbf{z}\|_2^2 $$
          Since $\|\mathbf{z}\|_2^2 \ge 0$, the minimum is uniquely achieved when $\mathbf{z}=0$.
          Thus, $\mathbf{x} = \mathbf{x}^+$ is the unique minimum norm solution.
          </div>
        </div>
        </li>
        <li><b>Projectors:</b> $AA^+$ is the orthogonal projector onto the column space $\mathcal{R}(A)$. $A^+A$ is the orthogonal projector onto the row space $\mathcal{R}(A^\top)$.</li>
      </ul>

      <h3>6.2 Condition Number</h3>
      <p>The condition number $\kappa(A)$ measures the sensitivity of the solution of a linear system to perturbations in the data. Formally, it is defined as:</p>
      $$ \kappa(A) = \|A\|_2 \|A^+\|_2 = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} $$
      <p>where $\sigma_{\min}$ is the smallest <i>non-zero</i> singular value.</p>

      <ul>
        <li><b>$\kappa \approx 1$:</b> Well-conditioned. Errors are not amplified. Orthogonal matrices have $\kappa = 1$.</li>
        <li><b>$\kappa \gg 1$:</b> Ill-conditioned. Small errors in $\mathbf{b}$ can lead to massive errors in $\mathbf{x}$.</li>
      </ul>

      <div class="intuition-box">
        <h4>Geometric Intuition: The Skewed Ellipse</h4>
        <p>Recall that a matrix $A$ maps the unit sphere to an ellipsoid.
        <ul>
            <li>$\sigma_{\max}$ is the length of the <b>longest</b> axis (maximum stretch).</li>
            <li>$\sigma_{\min}$ is the length of the <b>shortest</b> axis (minimum stretch).</li>
        </ul>
        The condition number $\kappa = \sigma_{\max}/\sigma_{\min}$ is the <b>aspect ratio</b> of this ellipsoid.
        <br><b>Solving $Ax=b$:</b> We are mapping backwards from the ellipsoid to the sphere.
        <br>If the ellipsoid is very "pancake-like" (high $\kappa$), a small vertical wiggle in $b$ (along the short axis) corresponds to a huge horizontal movement in $x$ (along the direction that was crushed). This is why precision is lost.</p>
      </div>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/conditioning-geometry.png"
               alt="Well-conditioned versus ill-conditioned error amplification geometry"
               style="width: 100%; height: auto; border-radius: 8px; border: 1px solid var(--border); padding: 8px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 6:</i> Condition number as error magnification: the same small uncertainty in $\mathbf{b}$ can map to a small or huge uncertainty in $\mathbf{x}$ depending on $\kappa(A)$.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/zig-zag-ill-conditioning.png"
               alt="Gradient descent zig-zagging on elongated contours in an ill-conditioned problem"
               style="width: 100%; height: auto; border-radius: 8px; border: 1px solid var(--border); padding: 8px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
          <figcaption><i>Figure 7:</i> Ill-conditioning creates elongated level sets; first-order methods like gradient descent can zig-zag across the valley, slowing convergence.</figcaption>
        </figure>
      </div>

      <div class="insight">
        <h4>Rule of Thumb: Digits of Precision</h4>
        <p>If you solve $A\mathbf{x} = \mathbf{b}$ with a condition number $\kappa(A) = 10^k$, you can expect to lose roughly <b>$k$ digits of precision</b>.
        <br><i>Example:</i> Using standard 64-bit floats (approx. 16 decimal digits):
        <ul>
            <li>If $\kappa(A) = 10^4$, you have $16 - 4 = 12$ reliable digits. (Safe).</li>
            <li>If $\kappa(A) = 10^{16}$, you have $16 - 16 = 0$ reliable digits. (Garbage output).</li>
        </ul>
        This is why forming $A^\top A$ (squaring the condition number) can be disastrous. If $\kappa(A) = 10^9$ (solvable), then $\kappa(A^\top A) = 10^{18}$ (unsolvable in double precision).
        </p>
      </div>

      <div class="insight">
        <h4>Impact on Optimization Algorithms</h4>
        <p>The condition number is not just a numerical nuisance; it fundamentally limits the speed of optimization algorithms.
        For a quadratic function $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A \mathbf{x}$, Gradient Descent converges at a rate determined by $\frac{\kappa-1}{\kappa+1}$.
        <ul>
            <li>If $\kappa=1$ (spherical), convergence is instant.</li>
            <li>If $\kappa \gg 1$ (valley), convergence is arbitrarily slow.</li>
        </ul>
        This motivates <b>Newton's Method</b> and preconditioning, which effectively transform the problem to make $\kappa \approx 1$. We will study this in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>.</p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Stability</h3>
        <p><b>Visualize Ill-Conditioning:</b> Compare solving $A\mathbf{x}=\mathbf{b}$ for a well-conditioned versus ill-conditioned matrix. See how small noise in $\mathbf{b}$ amplifies into large errors in $\mathbf{x}$.</p>
        <iframe src="widgets/la_batch3.html?mode=cond" width="100%" height="500" style="border:none; border-radius: 8px; background:#0b0f14; width: 100%; max-width: 900px; display: block; margin: 0 auto;"></iframe>
      </div>

      <h3>6.3 Regularization: Controlled Bias for Stability</h3>
      <p>When a matrix is ill-conditioned, the pseudoinverse solution $x_{LS} = \sum \frac{\mathbf{u}_i^\top \mathbf{b}}{\sigma_i} \mathbf{v}_i$ becomes unstable because division by small $\sigma_i$ amplifies noise.
      <br><b>Regularization</b> stabilizes the inversion by filtering out these small singular values. The classic method is <b>Tikhonov Regularization</b> (Ridge Regression):</p>
      $$ \min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2 $$
      <p>The solution is $\mathbf{x}_{\text{ridge}} = (A^\top A + \lambda I)^{-1} A^\top \mathbf{b}$.</p>

      <h4>SVD Interpretation: Spectral Filtering</h4>
      <p>In terms of the SVD, Tikhonov regularization replaces the inversion $1/\sigma_i$ with a <b>filter factor</b> $f_i(\lambda)$:
      $$ \mathbf{x}_{\text{ridge}} = \sum_{i=1}^r \underbrace{\left( \frac{\sigma_i^2}{\sigma_i^2 + \lambda} \right)}_{\text{Filter } f_i} \frac{1}{\sigma_i} (\mathbf{u}_i^\top \mathbf{b}) \mathbf{v}_i $$
      Let's analyze the filter factor $f_i(\lambda) = \frac{\sigma_i^2}{\sigma_i^2+\lambda}$:
      <ul>
        <li><b>Signal Regime ($\sigma_i \gg \sqrt{\lambda}$):</b> $f_i \approx 1$. We keep the component (pseudoinverse behavior).</li>
        <li><b>Noise Regime ($\sigma_i \ll \sqrt{\lambda}$):</b> $f_i \approx \sigma_i^2/\lambda \to 0$. We suppress the component.</li>
      </ul>
      This acts as a "smooth low-pass filter" on the spectrum of the matrix, damping out high-frequency noise (small singular values) while preserving the dominant structure.</p>

      <h4>Truncated SVD (Hard Thresholding)</h4>
      <p>Another approach is to simply discard singular values below a threshold. This is the optimal low-rank approximation applied to the inversion process.
      $$ \mathbf{x}_{\text{trunc}} = \sum_{\sigma_i > \epsilon} \frac{1}{\sigma_i} (\mathbf{u}_i^\top \mathbf{b}) \mathbf{v}_i $$
      Both methods trade a small amount of bias (exactness) for a large reduction in variance (stability).</p>

      <div class="example-box">
        <h4>Example 1.4: Numerical Comparison: Inversion vs. Regularization</h4>
        <p>Let $A = \begin{bmatrix} 1 & 0 \\ 0 & 10^{-5} \end{bmatrix}$ and $\mathbf{b} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
        <br><b>Exact Inverse:</b> $\mathbf{x} = A^{-1}\mathbf{b} = (1, 10^5)^\top$.
        <br>Suppose $\mathbf{b}$ has noise $\mathbf{\delta} = (0, 10^{-5})^\top$.
        <br><b>Perturbed Inverse:</b> $A^{-1}(\mathbf{b}+\mathbf{\delta}) = A^{-1} (1, 1+10^{-5})^\top = (1, 10^5 + 1)^\top$.
        <br>The noise is amplified by $1/\sigma_{\min} = 10^5$.
        <br><br>
        <b>Tikhonov ($\lambda = 10^{-4}$):</b>
        Filter for $\sigma_2$: $\frac{(10^{-5})^2}{(10^{-5})^2 + 10^{-4}} \approx \frac{10^{-10}}{10^{-4}} = 10^{-6}$.
        The second component is suppressed to $\approx 10^{-6} \cdot 10^5 \cdot 1 \approx 0.1$.
        The solution is roughly $(1, 0.1)^\top$.
        We lost the "true" large signal in the second component (bias), but we are now immune to noise amplification (variance).
        </p>
      </div>

      <div class="insight">
        <h4>The Irreducible Truth</h4>
        <p>You should now be able to see the single thread connecting these topics:
        <br><b>PCA, Least Squares, Pseudoinverses, and Regularization are all consequences of the SVD.</b>
        <br>The SVD exposes the intrinsic geometry of a linear map: which directions survive, which collapse, and which amplify noise. Every stable algorithm in optimization is, explicitly or implicitly, an SVD-aware algorithm.</p>
      </div>
    </section>

    <!-- SECTION 7: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>7. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>

      <h3>The Unified Picture</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>SVD Interpretation ($A=U\Sigma V^\top$)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>PCA</td>
            <td>SVD of centered data matrix</td>
          </tr>
          <tr>
            <td>Variance</td>
            <td>Squared singular values ($\sigma_i^2/N$)</td>
          </tr>
          <tr>
            <td>Least Squares</td>
            <td>Invert $\Sigma$ on range, zero on nullspace</td>
          </tr>
          <tr>
            <td>Ill-conditioning</td>
            <td>Decay of $\sigma_i$ to zero</td>
          </tr>
          <tr>
            <td>Regularization</td>
            <td>Smooth filtering of small $\sigma_i$</td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- SECTION 8: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 8. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.12 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Upper bound ($\le$):</strong> By Hölder's inequality, $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$.
          The definition of the dual norm is $\|y\|_* = \sup_{\|\mathbf{x}\|_p \le 1} \mathbf{x}^\top \mathbf{y}$.
          From Hölder, if $\|x\|_p \le 1$, then $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{y}\|_q$. Thus $\|y\|_* \le \|\mathbf{y}\|_q$.
        </div>
        <div class="proof-step">
          <strong>Lower bound ($\ge$):</strong> The dual norm is defined as the supremum of $\mathbf{x}^\top \mathbf{y}$ over the unit ball $\|\mathbf{x}\|_p \le 1$. To prove $\|y\|_* \ge \|y\|_q$, we need to find a specific unit vector $\mathbf{x}$ such that $\mathbf{x}^\top \mathbf{y} = \|y\|_q$.
          <br><br>
          <b>Step 1: Alignment Construction.</b>
          We construct a vector $\mathbf{x}$ that aligns with $\mathbf{y}$.
          To maximize the sum $\sum x_i y_i$, each term $x_i y_i$ should be positive. Thus, we choose the sign of $x_i$ to match $y_i$.
          To satisfy the equality condition in Hölder's inequality (which occurs when $|x_i|^p \propto |y_i|^q$), we set the magnitude $|x_i| = |y_i|^{q-1}$.
          <br>Thus, let $x_i = \mathrm{sign}(y_i) |y_i|^{q-1}$.
          <br><br>
          <b>Step 2: Evaluate Inner Product.</b>
          $$ \mathbf{x}^\top \mathbf{y} = \sum_{i=1}^n x_i y_i = \sum_{i=1}^n \mathrm{sign}(y_i) |y_i|^{q-1} y_i $$
          Since $\mathrm{sign}(y_i) y_i = |y_i|$, this simplifies to:
          $$ \sum_{i=1}^n |y_i|^{q-1} |y_i| = \sum_{i=1}^n |y_i|^q = \|\mathbf{y}\|_q^q $$
          <br>
          <b>Step 3: Evaluate Norm of $\mathbf{x}$.</b>
          $$ \|\mathbf{x}\|_p^p = \sum_{i=1}^n |x_i|^p = \sum_{i=1}^n (|y_i|^{q-1})^p = \sum_{i=1}^n |y_i|^{p(q-1)} $$
          Recall the conjugate relation $\frac{1}{p} + \frac{1}{q} = 1 \implies p+q = pq \implies p(q-1) = q$.
          $$ \|\mathbf{x}\|_p^p = \sum_{i=1}^n |y_i|^q = \|\mathbf{y}\|_q^q $$
          Taking the $p$-th root: $\|\mathbf{x}\|_p = (\|\mathbf{y}\|_q^q)^{1/p} = \|\mathbf{y}\|_q^{q/p}$.
          <br><br>
          <b>Step 4: Normalization.</b>
          Let $\tilde{\mathbf{x}} = \frac{\mathbf{x}}{\|\mathbf{x}\|_p}$. This vector has unit norm $\|\tilde{\mathbf{x}}\|_p = 1$.
          The inner product is:
          $$ \tilde{\mathbf{x}}^\top \mathbf{y} = \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{x}\|_p} = \frac{\|\mathbf{y}\|_q^q}{\|\mathbf{y}\|_q^{q/p}} = \|\mathbf{y}\|_q^{q - q/p} $$
          Since $q - q/p = q(1 - 1/p) = q(1/q) = 1$, we get:
          $$ \tilde{\mathbf{x}}^\top \mathbf{y} = \|\mathbf{y}\|_q $$
          Since we found a feasible vector achieving this value, the supremum must be at least $\|\mathbf{y}\|_q$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Dual Norm:</b> Defined by $\|y\|_* = \sup \{ \mathbf{y}^\top \mathbf{x} \mid \|\mathbf{x}\| \le 1 \}$.
        <br><b>$\ell_p$ Dual:</b> The dual of the $\ell_p$ norm ($1 < p < \infty$) is the $\ell_q$ norm, where $1/p + 1/q = 1$.
        <br><b>Hölder's Inequality:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$. This is the generalized Cauchy-Schwarz inequality.
        <br><b>Alignment Condition:</b> Equality holds if $|x_i|^p$ is proportional to $|y_i|^q$ (and signs match). This is crucial for constructing worst-case examples in robustness analysis.</p>
      </div>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The Frobenius norm corresponds to the standard Euclidean norm of the vectorized matrix: $\|A\|_F = \|\mathrm{vec}(A)\|_2$.
        The inner product of matrices is $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.
        This is exactly the dot product of the vectorized forms: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        By the standard Cauchy-Schwarz inequality for vectors:
        $$ |\mathrm{vec}(A)^\top \mathrm{vec}(B)| \le \|\mathrm{vec}(A)\|_2 \|\mathrm{vec}(B)\|_2 $$
        Substituting back the matrix notation yields $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The $\mathrm{vec}$ Operator:</b> Stacks columns of a matrix into a single vector in $\mathbb{R}^{mn}$.
        <br><b>Isometry:</b> The mapping $A \mapsto \mathrm{vec}(A)$ is an isometry between $(\mathbb{R}^{m \times n}, \|\cdot\|_F)$ and $(\mathbb{R}^{mn}, \|\cdot\|_2)$.
        <br><b>Inner Product Consistency:</b> $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        <br><b>Result:</b> Cauchy-Schwarz for vectors immediately implies $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.</p>
      </div>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>
      <div class="solution-box">
        <h4>Solution (Detailed Walkthrough)</h4>
        <div class="proof-step">
          <strong>Step 1: Compute $A^\top A$ and its eigenvalues.</strong>
          First, calculate the Gram matrix $G = A^\top A$:
          $$ A^\top A = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} $$
          Now, find the characteristic polynomial $p(\lambda) = \det(G - \lambda I)$:
          $$ \det \begin{bmatrix} 1-\lambda & 1 \\ 1 & 1-\lambda \end{bmatrix} = (1-\lambda)^2 - 1 = \lambda^2 - 2\lambda = \lambda(\lambda-2) $$
          The roots (eigenvalues) are $\lambda_1 = 2$ and $\lambda_2 = 0$.
          <br>The singular values are $\sigma_1 = \sqrt{2}$ and $\sigma_2 = 0$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Compute Right Singular Vectors ($V$).</strong>
          We solve $(G - \lambda I)\mathbf{v} = 0$ for each eigenvalue.
          <br><b>For $\lambda_1 = 2$:</b>
          $$ \begin{bmatrix} 1-2 & 1 \\ 1 & 1-2 \end{bmatrix} \mathbf{v} = \begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \implies -x+y=0 \implies y=x $$
          Normalized eigenvector: $v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
          <br><b>For $\lambda_2 = 0$:</b>
          $$ \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \mathbf{v} = 0 \implies x+y=0 \implies y=-x $$
          Normalized eigenvector: $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
          <br>Matrix $V = [v_1 \ v_2] = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Compute Left Singular Vectors ($U$).</strong>
          Use the formula $u_i = \frac{1}{\sigma_i} A v_i$ for non-zero singular values.
          <br><b>For $\sigma_1 = \sqrt{2}$:</b>
          $$ u_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix} = \frac{1}{\sqrt{2}} \begin{bmatrix} 2/\sqrt{2} \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} $$
          <br><b>For $\sigma_2 = 0$:</b> We cannot use the formula. We must complete the basis for $\mathbb{R}^m$ ($\mathbb{R}^2$).
          We need a unit vector $u_2$ orthogonal to $u_1 = (1, 0)^\top$. Clearly, $u_2 = (0, 1)^\top$ works.
          <br>Matrix $U = [u_1 \ u_2] = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I_2$.
        </div>
        <div class="proof-step">
          <strong>Step 4: Final Assembly.</strong>
          $$ A = U \Sigma V^\top = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \left(\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\right)^\top $$
          <b>Verification:</b>
          $U \Sigma = \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix}$.
          $(U \Sigma) V^\top = \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = A$. Correct.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hand-Calculation Strategy for SVD:</b>
        <ol>
            <li>Compute the Gram matrix $G = A^\top A$.</li>
            <li>Find eigenvalues $\lambda_i$ of $G$. Singular values are $\sigma_i = \sqrt{\lambda_i}$.</li>
            <li>Find normalized eigenvectors $v_i$ of $G$ to form $V$.</li>
            <li>Compute $u_i = \frac{1}{\sigma_i} A v_i$ for non-zero $\sigma_i$.</li>
            <li>If $m > r$, complete $U$ using Gram-Schmidt or inspection (orthogonality).</li>
        </ol>
        <br><b>Check:</b> Verify $AV = U\Sigma$.</p>
      </div>

      <h3>P1.4 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, \mathbf{b} = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>(a) Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        $A^\top A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.
        $A^\top \mathbf{b} = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Solve $\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \mathbf{x} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Multiply by inverse $\frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$:
        $\mathbf{x} = \frac{1}{3} \begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 \\ 5 \end{bmatrix}$.</p>
        <p><b>(b) QR Factorization:</b>
        Gram-Schmidt on columns $a_1, a_2$.
        $u_1 = a_1 = (1, 0, 1)^\top$. $q_1 = (1/\sqrt{2}, 0, 1/\sqrt{2})^\top$.
        $v_2 = a_2 - \langle a_2, q_1 \rangle q_1 = (1, 1, 0)^\top - (1/\sqrt{2}) (1/\sqrt{2}, 0, 1/\sqrt{2})^\top = (1, 1, 0)^\top - (1/2, 0, 1/2)^\top = (1/2, 1, -1/2)^\top$.
        Normalize $v_2$: $\|v_2\|^2 = 1/4 + 1 + 1/4 = 1.5 = 3/2$. $\|v_2\| = \sqrt{3/2}$.
        $q_2 = \sqrt{2/3} (1/2, 1, -1/2)^\top$.
        Solve $R\mathbf{x} = Q^\top \mathbf{b}$.
        $R = \begin{bmatrix} \sqrt{2} & 1/\sqrt{2} \\ 0 & \sqrt{3/2} \end{bmatrix}$. $Q^\top \mathbf{b} = \begin{bmatrix} 3/\sqrt{2} \\ 4\sqrt{2/3}-1/\sqrt{6} \end{bmatrix}$.
        Back substitution yields same $\mathbf{x}$. QR avoids forming $A^\top A$ (better condition number).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Comparing Least Squares Algorithms:</b>
        <ul>
            <li><b>Normal Equations:</b>
                <ul><li>Cost: $O(n^2 m)$ (forming $A^\top A$) + $O(n^3)$ (Cholesky).</li>
                <li>Stability: Unstable. Dependence on $\kappa(A)^2$.</li></ul>
            </li>
            <li><b>QR Factorization:</b>
                <ul><li>Cost: $2 \times$ Normal Equations (roughly).</li>
                <li>Stability: Stable. Dependence on $\kappa(A)$. Standard choice.</li></ul>
            </li>
            <li><b>SVD:</b> Most expensive, but most robust (handles rank deficiency gracefully).</li>
        </ul></p>
      </div>

      <h3>P1.5 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $A = U \Sigma V^\top$. Then $A^+ = V \Sigma^+ U^\top$.
        $P = AA^+ = (U \Sigma V^\top)(V \Sigma^+ U^\top) = U (\Sigma \Sigma^+) U^\top$.
        $\Sigma \Sigma^+$ is a diagonal matrix with 1s for non-zero singular values and 0s otherwise.
        Let $U_r$ be the first $r$ columns of $U$. Then $P = U_r U_r^\top$.
        <br><b>Idempotent:</b> $P^2 = (U_r U_r^\top)(U_r U_r^\top) = U_r (U_r^\top U_r) U_r^\top = U_r I U_r^\top = P$.
        <br><b>Symmetric:</b> $P^\top = (U_r U_r^\top)^\top = (U_r^\top)^\top U_r^\top = U_r U_r^\top = P$.
        <br>Since the columns of $U_r$ form a basis for $\mathcal{R}(A)$, $P$ is the orthogonal projector onto $\mathcal{R}(A)$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Fundamental Projectors via SVD:</b>
        <ul>
            <li><b>Column Space Projector:</b> $P_{\mathcal{R}(A)} = A A^+ = U_r U_r^\top$.</li>
            <li><b>Row Space Projector:</b> $P_{\mathcal{R}(A^\top)} = A^+ A = V_r V_r^\top$.</li>
        </ul>
        <br><b>Intuition:</b> $A^+$ inverts the action on the range and kills the orthogonal complement. $AA^+$ maps range $\to$ range (identity) and orthogonal complement $\to$ 0.</p>
      </div>

      <h3>P1.6 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>We want to find the point $\mathbf{p} = A\mathbf{x}$ in the column space closest to $\mathbf{b}$.
        Problem: $\min_\mathbf{x} \|A\mathbf{x} - \mathbf{b}\|_2^2$.
        Gradient: $2A^\top (A\mathbf{x} - \mathbf{b}) = 0$.
        Normal eqs: $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        Since cols are independent, $A^\top A$ is invertible.
        Solution: $\mathbf{x}^* = (A^\top A)^{-1} A^\top \mathbf{b}$.
        Projection: $\mathbf{p} = A \mathbf{x}^* = A(A^\top A)^{-1} A^\top \mathbf{b}$.
        The matrix mapping $\mathbf{b}$ to $\mathbf{p}$ is $P = A(A^\top A)^{-1} A^\top$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The "Hat" Matrix:</b> $P = A(A^\top A)^{-1} A^\top$.
        <br><b>Requirements:</b> Valid only when $A$ has full column rank (so $A^\top A$ is invertible).
        <br><b>General Case:</b> If $A$ is rank-deficient, use the pseudoinverse form $P = AA^+$.
        <br><b>Properties:</b> $P^2=P$ and $P^\top=P$. The residual is $(I-P)\mathbf{b}$, which projects onto $\mathcal{N}(A^\top)$.</p>
      </div>

      <h3>P1.7 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Normal $a = (1, 1, 1)^\top$. Point $\mathbf{y} = (1, 2, 3)^\top$. Target $\mathbf{b}=3$.
        $\mathbf{p} = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a$.
        $a^\top \mathbf{y} = 1+2+3 = 6$. $\|a\|^2 = 3$.
        $\mathbf{p} = (1, 2, 3)^\top - \frac{6-3}{3} (1, 1, 1)^\top = (1, 2, 3)^\top - (1, 1, 1)^\top = (0, 1, 2)^\top$.
        Check: $0+1+2=3$. Correct.
        Distance: $\|y-p\| = \|(1, 1, 1)\| = \sqrt{3}$.
        Formula: $\frac{|a^\top \mathbf{y} - \mathbf{b}|}{\|a\|} = \frac{|6-3|}{\sqrt{3}} = \frac{3}{\sqrt{3}} = \sqrt{3}$. Matches.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Constraint Projection:</b> The projection of $\mathbf{y}$ onto the affine set $\{x \mid a^\top \mathbf{x} = \mathbf{b}\}$ is:
        $$ \Pi(\mathbf{y}) = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a $$
        <b>Optimization View:</b> This is the solution to $\min_\mathbf{x} \frac{1}{2}\|\mathbf{x}-\mathbf{y}\|^2$ s.t. $a^\top \mathbf{x} = \mathbf{b}$.
        <br><b>Lagrange Multiplier:</b> The term $\frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2}$ corresponds to the Lagrange multiplier $\nu$ associated with the constraint.</p>
      </div>

      <h3>P1.8 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The matrix is diagonal, so its singular values are the absolute values of diagonal entries: $1$ and $\epsilon$.
        Assuming $\epsilon < 1$: $\sigma_{\max} = 1, \sigma_{\min} = \epsilon$.
        $\kappa(A) = 1/\epsilon$.
        As $\epsilon \to 0$, $\kappa(A) \to \infty$. The matrix becomes ill-conditioned (nearly singular).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Condition Number $\kappa(A)$:</b> The ratio $\sigma_{\max}/\sigma_{\min}$ (for square invertible matrices).
        <br><b>Interpretation:</b> The aspect ratio of the hyperellipse image of the unit sphere.
        <br><b>Impact:</b>
        <ul>
            <li><b>Sensitivity:</b> Relative error in $\mathbf{x}$ $\le \kappa(A) \times$ Relative error in $\mathbf{b}$.</li>
            <li><b>Optimization:</b> Controls convergence rate of gradient descent. High $\kappa$ $\implies$ "Zig-zagging".</li>
        </ul></p>
      </div>

      <h3>P1.9 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = \mathbf{x}\mathbf{y}^\top$ where $\mathbf{x} \in \mathbb{R}^m, \mathbf{y} \in \mathbb{R}^n$ are non-zero vectors.
      <br>(a) Construct the Singular Value Decomposition (SVD) of $A$.
      <br>(b) Use the SVD to show that $A^+ = \frac{A^\top}{\|\mathbf{x}\|_2^2 \|\mathbf{y}\|_2^2}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Part (a): SVD Construction.</strong>
          We need to write $A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^\top$.
          <br>1. <b>Singular Value $\sigma_1$:</b> This is the operator norm $\|A\|_2$.
          $$ \|A\|_2 = \sup_{\|v\|=1} \|\mathbf{x}\mathbf{y}^\top v\|_2 = \sup_{\|v\|=1} |\mathbf{y}^\top v| \|\mathbf{x}\|_2 = \|\mathbf{y}\|_2 \|\mathbf{x}\|_2 $$
          Thus $\sigma_1 = \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$.
          <br>2. <b>Left Singular Vector $\mathbf{u}_1$:</b> The output direction is clearly along $\mathbf{x}$. Normalized: $\mathbf{u}_1 = \frac{\mathbf{x}}{\|\mathbf{x}\|_2}$.
          <br>3. <b>Right Singular Vector $\mathbf{v}_1$:</b> The input direction maximizing the gain is along $\mathbf{y}$. Normalized: $\mathbf{v}_1 = \frac{\mathbf{y}}{\|\mathbf{y}\|_2}$.
          <br><b>Check:</b> $\sigma_1 \mathbf{u}_1 \mathbf{v}_1^\top = (\|\mathbf{x}\|\|\mathbf{y}\|) \frac{\mathbf{x}}{\|\mathbf{x}\|} \frac{\mathbf{y}^\top}{\|\mathbf{y}\|} = \mathbf{x}\mathbf{y}^\top = A$.
        </div>
        <div class="proof-step">
          <strong>Part (b): Pseudoinverse Calculation.</strong>
          The pseudoinverse formula is $A^+ = \frac{1}{\sigma_1} \mathbf{v}_1 \mathbf{u}_1^\top$.
          $$ A^+ = \frac{1}{\|\mathbf{x}\|\|\mathbf{y}\|} \left( \frac{\mathbf{y}}{\|\mathbf{y}\|} \right) \left( \frac{\mathbf{x}}{\|\mathbf{x}\|} \right)^\top $$
          $$ = \frac{1}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2} \mathbf{y} \mathbf{x}^\top $$
          Recognizing that $\mathbf{y} \mathbf{x}^\top = (\mathbf{x} \mathbf{y}^\top)^\top = A^\top$:
          $$ A^+ = \frac{A^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2} $$
        </div>
        <div class="proof-step">
          <strong>Alternative Check (Moore-Penrose Axioms):</strong>
          Let $B = \frac{A^\top}{\alpha}$ where $\alpha = \|x\|^2 \|y\|^2$.
          $ABA = A \frac{A^\top}{\alpha} A = \frac{xy^\top yx^\top xy^\top}{\alpha} = \frac{x (y^\top y) (x^\top x) y^\top}{\|x\|^2 \|y\|^2} = \frac{x \|y\|^2 \|x\|^2 y^\top}{\|x\|^2 \|y\|^2} = xy^\top = A$.
          Symmetry holds similarly.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-1 SVD:</b> A matrix $A = xy^\top$ has exactly one non-zero singular value $\sigma_1 = \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$.
        <br><b>Pseudoinverse Formula:</b> For rank-1 matrices, $A^+ = \frac{A^\top}{\|A\|_F^2}$. This formula is extremely useful for updating inverse approximations (like in Quasi-Newton methods).</p>
      </div>

      <h3>P1.10 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$\|A\|_2^2 = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|^2}{\|\mathbf{x}\|^2} = \sup_{\mathbf{x} \ne 0} \frac{\mathbf{x}^\top A^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$.
        The term on the right is the Rayleigh quotient for the symmetric matrix $M = A^\top A$.
        By the variational characterization of eigenvalues, the maximum value of the Rayleigh quotient is $\lambda_{\max}(M)$.
        Thus $\|A\|_2^2 = \lambda_{\max}(A^\top A)$, so $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Variational Definition of Eigenvalues:</b> For symmetric $M$, $\lambda_{\max}(M) = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top M \mathbf{x}$.
        <br><b>Spectral Norm Connection:</b> $\|A\|_2^2 = \max_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|^2 = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top (A^\top A) \mathbf{x} = \lambda_{\max}(A^\top A)$.
        <br><b>Implication:</b> The spectral norm is the square root of the largest eigenvalue of the Gram matrix.</p>
      </div>

      <h3>P1.11 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Frobenius:</b> $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
        $\|UAV^\top\|_F^2 = \mathrm{tr}(V A^\top U^\top U A V^\top) = \mathrm{tr}(V A^\top A V^\top) = \mathrm{tr}(V^\top V A^\top A) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.
        <br><b>Spectral:</b> $\|UAV^\top\|_2 = \sup_{\|\mathbf{x}\|=1} \|UAV^\top \mathbf{x}\|$.
        Since $U$ is orthogonal, $\|Uy\| = \|y\|$. So we maximize $\|A (V^\top \mathbf{x})\|$.
        Since $V$ is orthogonal, as $\mathbf{x}$ ranges over the unit sphere, $\mathbf{y} = V^\top \mathbf{x}$ also ranges over the unit sphere.
        Thus $\sup_{\|\mathbf{x}\|=1} \|A V^\top \mathbf{x}\| = \sup_{\|\mathbf{y}\|=1} \|A\mathbf{y}\| = \|A\|_2$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Invariance:</b> A matrix norm is orthogonally invariant if $\|UAV^\top\| = \|A\|$ for all orthogonal $U, V$.
        <br><b>Schatten p-norms:</b> Defined as the $\ell_p$ norm of the singular values vector $\sigma(A)$.
        <ul>
            <li>$p=\infty$: Spectral Norm.</li>
            <li>$p=2$: Frobenius Norm.</li>
            <li>$p=1$: Nuclear Norm (Trace Norm).</li>
        </ul>
        <br>These are the "natural" norms for matrix analysis.</p>
      </div>

      <h3>P1.12 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Group:</b>
        1. Closure: $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.
        2. Inverse: $Q^{-1} = Q^\top$, which is orthogonal since $(Q^\top)^\top Q^\top = Q Q^\top = I$.
        3. Identity: $I \in O(n)$.
        <br><b>Compactness:</b>
        1. Bounded: The columns are unit vectors, so $\|Q\|_F = \sqrt{n}$. The set is bounded.
        2. Closed: The condition $Q^\top Q = I$ is a continuous equality constraint (level set of a continuous function). Thus the set is closed.
        By Heine-Borel, closed and bounded in finite dimensions implies compact.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The Orthogonal Group $O(n)$:</b> The set of all $n \times n$ orthogonal matrices.
        <br><b>Compactness Proof:</b>
        <ul>
            <li><b>Boundedness:</b> $\|Q\|_F = \sqrt{n}$, so $O(n)$ is contained in a ball of radius $\sqrt{n}$.</li>
            <li><b>Closedness:</b> It is the solution set of continuous polynomial equations $Q^\top Q - I = 0$.</li>
        </ul>
        <br><b>Optimization Consequence:</b> Any continuous function (like a norm or trace) maximized over orthogonal matrices <i>attains</i> its maximum. This is often used to prove existence of SVD.</p>
      </div>
</section>

<!-- SECTION 9: APPENDIX -->
<section class="section-card" id="section-appendix">
  <h2>Appendix: Detailed Derivations</h2>

  <h3>A.1 Variance and the Rayleigh Quotient</h3>
  <p>In Section 5.5, we connected PCA to the SVD. Here we derive this connection from first principles by maximizing variance.</p>

  <h4>Variance as a Quadratic Form</h4>
  <p>Let $X$ be the $N \times n$ centered data matrix (rows are samples). We want to find a unit direction $\mathbf{v}$ that maximizes the variance of the projected data.
  <br>The projection of sample $\mathbf{x}_i$ onto $\mathbf{v}$ is scalar $\mathbf{x}_i^\top \mathbf{v}$.
  <br>The sample variance is the average squared projection:
  $$ \text{Var}(\mathbf{v}) = \frac{1}{N} \sum_{i=1}^N (\mathbf{x}_i^\top \mathbf{v})^2 = \frac{1}{N} \|X\mathbf{v}\|_2^2 $$
  Expanding the norm:
  $$ \text{Var}(\mathbf{v}) = \frac{1}{N} (X\mathbf{v})^\top (X\mathbf{v}) = \mathbf{v}^\top \left( \frac{1}{N} X^\top X \right) \mathbf{v} $$
  Let $C = \frac{1}{N} X^\top X$ be the empirical covariance matrix. The problem is:
  $$ \max_{\mathbf{v}} \mathbf{v}^\top C \mathbf{v} \quad \text{subject to} \quad \|\mathbf{v}\|_2 = 1 $$
  This is exactly the maximization of the Rayleigh Quotient for the matrix $C$.
  </p>

  <div class="proof-box">
    <h4>Proof: The Maximizer is the Top Eigenvector</h4>
    <p>Since $C$ is symmetric positive semidefinite, it has an eigendecomposition $C = Q \Lambda Q^\top$ with eigenvalues $\lambda_1 \ge \dots \ge \lambda_n \ge 0$.
    <br>Let $\mathbf{y} = Q^\top \mathbf{v}$. Then $\|\mathbf{y}\| = \|Q^\top \mathbf{v}\| = \|\mathbf{v}\| = 1$.
    The objective becomes:
    $$ \mathbf{v}^\top C \mathbf{v} = \mathbf{v}^\top Q \Lambda Q^\top \mathbf{v} = \mathbf{y}^\top \Lambda \mathbf{y} = \sum_{i=1}^n \lambda_i y_i^2 $$
    We want to maximize $\sum \lambda_i y_i^2$ subject to $\sum y_i^2 = 1$.
    <br>Since $\lambda_1$ is the largest coefficient, we should put all "mass" on $y_1$.
    Setting $y_1=1$ (and others 0) gives the value $\lambda_1$.
    <br>This corresponds to $\mathbf{y} = e_1$, which means $\mathbf{v} = Q e_1 = q_1$ (the first eigenvector).
    </p>
  </div>

  <h4>Connection to SVD</h4>
  <p>The eigenvectors of the covariance matrix $C = \frac{1}{N} X^\top X$ are the right singular vectors of $X$.
  <br>Recall $X = U \Sigma V^\top$. Then:
  $$ X^\top X = (V \Sigma^\top U^\top) (U \Sigma V^\top) = V (\Sigma^\top \Sigma) V^\top = V \Sigma^2 V^\top $$
  The eigenvectors of $X^\top X$ are the columns of $V$.
  <br>Thus, the principal component directions (eigenvectors of covariance) are exactly the right singular vectors of the data matrix.</p>

  <h3>A.2 Rigorous Derivation of SVD via $A^\top A$</h3>
  <div class="proof-box">
    <h4>Proof: Existence of SVD</h4>
    <p>The existence of the SVD is a direct consequence of the Spectral Theorem applied to the symmetric positive semidefinite matrix $G = A^\top A$.</p>
    <div class="proof-step">
        <strong>Step 1: Spectral Decomposition of $G$.</strong>
          The matrix $G = A^\top A \in \mathbb{R}^{n \times n}$ is symmetric ($G^\top = G$). By the Spectral Theorem, there exists an orthonormal basis $\{v_1, \dots, v_n\}$ of eigenvectors with real eigenvalues $\lambda_1 \ge \dots \ge \lambda_n$.
        $$ A^\top A v_i = \lambda_i v_i $$
          We can verify non-negativity: $\lambda_i = \lambda_i \|v_i\|^2 = v_i^\top (\lambda_i v_i) = v_i^\top (A^\top A v_i) = (Av_i)^\top (Av_i) = \|A v_i\|^2 \ge 0$.
          Thus $\lambda_i \ge 0$.
    </div>
    <div class="proof-step">
        <strong>Step 2: Define Singular Values and Right Singular Vectors.</strong>
          Define the singular values $\sigma_i = \sqrt{\lambda_i}$. Let $r$ be the number of non-zero singular values.
          Define the matrix of right singular vectors $V = [v_1 \dots v_n]$. Since the $v_i$ are orthonormal, $V$ is an orthogonal matrix.
    </div>
    <div class="proof-step">
          <strong>Step 3: Construct Left Singular Vectors ($U$).</strong>
          We need to construct an orthonormal basis $\{u_1, \dots, u_m\}$ for the codomain $\mathbb{R}^m$.
          <br><b>For $i \le r$ (The Range):</b> Define $u_i = \frac{1}{\sigma_i} A v_i$.
          We must verify these are orthonormal:
          $$ u_i^\top u_j = \left( \frac{1}{\sigma_i} A v_i \right)^\top \left( \frac{1}{\sigma_j} A v_j \right) = \frac{1}{\sigma_i \sigma_j} v_i^\top (A^\top A v_j) = \frac{\lambda_j}{\sigma_i \sigma_j} (v_i^\top v_j) $$
          Since $v_i \perp v_j$, this is 0 for $i \ne j$. If $i=j$, it is $\sigma_i^2/\sigma_i^2 = 1$. Thus, $\{u_1, \dots, u_r\}$ are orthonormal.
          <br><b>For $i > r$ (The Null Complement):</b> Pick any orthonormal vectors $\{u_{r+1}, \dots, u_m\}$ orthogonal to the first $r$ vectors.
          <br>Construct $U = [u_1 \dots u_m]$. This is an orthogonal matrix.
    </div>
    <div class="proof-step">
          <strong>Step 4: Matrix Factorization ($AV = U\Sigma$).</strong>
          We compare the action of $A$ on the basis $V$ with the action of $U\Sigma$.
          <br><b>Case $i \le r$:</b> $A v_i = \sigma_i u_i$.
          <br><b>Case $i > r$:</b> Since $\sigma_i = 0$, we have $\lambda_i = 0$, so $\|A v_i\|^2 = 0 \implies A v_i = 0$. Also $\sigma_i u_i = 0$.
          <br>Thus $A v_i = \sigma_i u_i$ for all $i$. In matrix form: $A V = U \Sigma \implies A = U \Sigma V^\top$.
    </div>
  </div>

  <h3>A.3 Low-Rank Approximation (Eckart-Young-Mirsky)</h3>
  <div class="proof-box">
    <h4>Why Truncation Works (The Energy Argument)</h4>
    <p>We want to minimize $\|A - B\|_F$ subject to $\operatorname{rank}(B) \le k$.
    <br><b>Frobenius Norm:</b>
    The Frobenius norm is invariant under orthogonal transformations: $\|U M V^\top\|_F = \|M\|_F$.
    <br>Let $B$ be our rank-$k$ candidate. Work in the SVD coordinates of $A = U \Sigma V^\top$.
    $$ \|A - B\|_F^2 = \|U \Sigma V^\top - B\|_F^2 = \|U (\Sigma - U^\top B V) V^\top\|_F^2 $$
    Let $\tilde{B} = U^\top B V$. Since $U, V$ are invertible, $\operatorname{rank}(\tilde{B}) = \operatorname{rank}(B) \le k$.
    $$ \|A - B\|_F^2 = \|\Sigma - \tilde{B}\|_F^2 = \sum_{i=1}^n (\sigma_i - \tilde{B}_{ii})^2 + \sum_{i \neq j} \tilde{B}_{ij}^2 $$
    To minimize this sum, the off-diagonal terms $\tilde{B}_{ij}$ should be zero. Thus $\tilde{B}$ should be diagonal.
    <br>This reduces to choosing $k$ non-zero diagonal entries $d_1, \dots, d_k$ to approximate $\sigma_1, \dots, \sigma_n$. The optimal choice is to keep the $k$ largest $\sigma_i$.
    <br>The residual error is the tail energy $\sum_{i=k+1}^r \sigma_i^2$.</p>
  </div>

  <h3>A.4 The Leibniz Formula for Determinant</h3>
  <div class="proof-box">
    <h4>Derivation: Why Axioms Force the Formula</h4>
    <p>Starting from the three axioms (Multilinearity, Alternating, Normalization):</p>
    <div class="proof-step">
      <strong>Step 1: Expand columns.</strong> $a_j = \sum_{k=1}^n A_{kj} e_k$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Apply Multilinearity.</strong>
      $$ \det(A) = \sum_{k_1=1}^n \dots \sum_{k_n=1}^n A_{k_1, 1} \dots A_{k_n, n} \det(e_{k_1}, \dots, e_{k_n}) $$
    </div>
    <div class="proof-step">
      <strong>Step 3: Apply Alternation.</strong>
      If any indices match ($k_i = k_j$), the term vanishes. Only permutations survive.
    </div>
    <div class="proof-step">
      <strong>Step 4: Apply Normalization.</strong>
      $\det(e_{\sigma(1)}, \dots, e_{\sigma(n)}) = \mathrm{sgn}(\sigma) \det(I) = \mathrm{sgn}(\sigma)$.
    </div>
    <div class="proof-step">
      <strong>Result:</strong> $\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{j=1}^n A_{\sigma(j), j}$.
    </div>
  </div>

  <h3>A.5 Householder QR Example</h3>
  <div class="example-box">
    <h4>Example: QR via Householder for a 3×2 Matrix</h4>
    <p>Consider $A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}$. We want to zero out the entries below $A_{11}$.</p>
    <div class="proof-step">
      <strong>Step 1: The Target Vector.</strong>
      First column $\mathbf{a}_1 = (1, 1, 0)^\top$. Target length $\sqrt{2}$. Target vector $\mathbf{t} = (-\sqrt{2}, 0, 0)^\top$.
    </div>
    <div class="proof-step">
      <strong>Step 2: The Reflection Normal.</strong>
      $\mathbf{v} = \mathbf{a}_1 - \mathbf{t} = (1+\sqrt{2}, 1, 0)^\top$.
      $\|\mathbf{v}\|^2 = 4 + 2\sqrt{2}$. Unit normal $\mathbf{u} = \mathbf{v}/\|\mathbf{v}\|$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Apply $H_1$.</strong>
      $H_1 \mathbf{a}_1 = \mathbf{t}$.
      $H_1 \mathbf{a}_2 = \mathbf{a}_2 - 2\mathbf{u}(\mathbf{u}^\top \mathbf{a}_2) = \begin{bmatrix} -1/\sqrt{2} \\ -1/\sqrt{2} \\ 1 \end{bmatrix}$.
    </div>
    <p><b>Result:</b> $A$ is transformed into an upper triangular form (for the first column).</p>
  </div>
</section>

<!-- SECTION 10: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="../../static/lib/pyodide/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/resizable.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
