<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "../../static/lib/three/three.module.js"
      }
    }
  </script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>While basic linear algebra focuses on solving equations, advanced linear algebra is about uncovering structure. This lecture bridges the gap between the algebraic manipulation of matrices and the geometric intuition required for modern optimization. We move beyond the unstable Normal Equations to explore robust factorizations like the QR decomposition and the Singular Value Decomposition (SVD), which expose the true geometry of linear maps. We examine the spectral properties of operators, introducing the Moore-Penrose pseudoinverse as the definitive tool for ill-posed problems. Finally, we analyze numerical stability through the lens of the condition number, a concept that will later determine the convergence speed of gradient descent and Newton's method.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm and matrix completion in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The condition number is the key parameter determining the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a> and motivates the use of Newton's Method.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD) as fundamental tools for numerical linear algebra.</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming the potentially ill-conditioned matrix $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions for rank-deficient or underdetermined systems.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations in data.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA) using truncated SVD.</li>
      </ul>
    </section>

    <!-- SECTION 1: SCALAR INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>1. Scalar Invariants: Determinant and Trace</h2>
      <p>A scalar invariant is a function of a matrix that remains unchanged under similarity transformations ($A \mapsto P^{-1}AP$). These invariants capture intrinsic properties of the linear operator itself, independent of the basis used to represent it. The two fundamental polynomial invariants are the Determinant and the Trace.</p>

      <h3>1.1 Determinant: The Unique Volume Form</h3>
      <p>The <b>Determinant</b> is often introduced via recursive formulas, but its true nature is geometric: it is the unique function that measures the signed volume of the parallelepiped spanned by the columns of a matrix. It is characterized by three axioms: multilinearity (it is linear in each column), alternation (swapping two columns flips the sign, implying that a matrix with dependent columns has zero volume), and normalization ($\det(I)=1$). These geometric constraints force the algebraic Leibniz formula: $\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{i=1}^n A_{i, \sigma(i)}$.</p>

      <p>Geometrically, $|\det(A)|$ represents the factor by which the linear map $A$ scales volumes. The sign of the determinant indicates whether the map preserves or reverses orientation. Crucially, $\det(A) = 0$ if and only if the columns are linearly dependent, meaning the map "squashes" space into a lower dimension, reducing volume to zero. This makes the determinant the ultimate test for invertibility.</p>

      <h3>1.2 Trace: The Unique Linear Invariant</h3>
      <p>The <b>Trace</b> of a matrix, $\mathrm{tr}(A) = \sum A_{ii}$, is the unique linear functional that satisfies the cyclic property $\mathrm{tr}(AB) = \mathrm{tr}(BA)$. This property ensures that the trace is invariant under similarity transformations ($\mathrm{tr}(P^{-1}AP) = \mathrm{tr}(A)$). While the determinant measures the product of eigenvalues (volume), the trace measures their sum. Geometrically, the trace represents the infinitesimal change in volume; for a flow described by $\dot{x} = Ax$, the divergence is $\mathrm{tr}(A)$.</p>
    </section>

    <!-- SECTION 2: EIGENVALUES -->
    <section class="section-card" id="section-eigenvalues">
      <h2>2. The Spectral Layer: Eigenvalues and Diagonalization</h2>
      <p>The spectral perspective asks a fundamental question: are there directions that a linear map simply scales, without rotation? These invariant directions are the "pure modes" of the system, simplifying the analysis of complex matrix operations.</p>

      <h3>2.1 Invariant Subspaces and Eigenvalues</h3>
      <p>An <b>eigenvector</b> $\mathbf{v}$ is a non-zero vector that is mapped to a scalar multiple of itself: $A\mathbf{v} = \lambda \mathbf{v}$. The scalar $\lambda$ is the <b>eigenvalue</b>. This equation $A\mathbf{v} - \lambda \mathbf{v} = 0$ implies that the operator $(A - \lambda I)$ has a non-trivial kernel, which occurs if and only if $\det(A - \lambda I) = 0$. The roots of this characteristic polynomial are the eigenvalues. In the basis of eigenvectors, the matrix operation becomes diagonal, decoupling the dimensions into independent scalar scaling operations.</p>

      <h3>2.2 Diagonalization and the Spectral Theorem</h3>
      <p>A matrix is <b>diagonalizable</b> if there exists a basis of eigenvectors. In this case, we can write $A = P \Lambda P^{-1}$, where $\Lambda$ is a diagonal matrix of eigenvalues and $P$ contains the eigenvectors. Not all matrices are diagonalizable (e.g., shear matrices with Jordan blocks), but <b>symmetric matrices</b> ($A = A^\top$) always are. The <b>Spectral Theorem</b> guarantees that for any real symmetric matrix, the eigenvalues are real, and the eigenvectors can be chosen to be orthonormal. This means symmetric matrices can be diagonalized by an orthogonal matrix: $A = Q \Lambda Q^\top$. This result underpins the geometry of ellipsoids and is the reason why Hessian matrices in optimization always have real eigenvalues and orthogonal principal directions.</p>

      <h3>2.3 The Rayleigh Quotient</h3>
      <p>For a symmetric matrix $A$, the <b>Rayleigh Quotient</b> is defined as $R_A(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$. This scalar function measures the "gain" of the matrix in the direction $\mathbf{x}$. The eigenvalues of $A$ can be characterized variationally as the critical values of this quotient. Specifically, the minimum eigenvalue is the global minimum of $R_A(\mathbf{x})$, and the maximum eigenvalue is the global maximum. This connects linear algebra directly to optimization: finding eigenvalues is equivalent to optimizing a quadratic form over the unit sphere.</p>
    </section>

    <!-- SECTION 3: INDUCED NORMS -->
    <section class="section-card" id="section-induced-norms">
      <h2>3. Induced Matrix Norms</h2>
      <p>Vector norms measure the size of vectors; induced matrix norms measure the "size" or "gain" of the linear map itself. The induced norm is defined as the maximum factor by which the matrix stretches a vector: $\|A\|_p = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|_p}{\|\mathbf{x}\|_p}$.</p>

      <h3>3.1 The $\ell_1$ and $\ell_\infty$ Operator Norms</h3>
      <p>The $\ell_1$ operator norm corresponds to the maximum absolute column sum: $\|A\|_1 = \max_{j} \sum_{i} |A_{ij}|$. This is intuitive because the unit ball in $\ell_1$ is a polytope with vertices at the standard basis vectors $e_j$; the maximum stretch must occur at one of these vertices, which maps to the $j$-th column of $A$. Similarly, the $\ell_\infty$ operator norm is the maximum absolute row sum: $\|A\|_\infty = \max_{i} \sum_{j} |A_{ij}|$.</p>

      <h3>3.2 The Spectral Norm ($\ell_2$)</h3>
      <p>The $\ell_2$ operator norm, often called the <b>spectral norm</b>, is denoted $\|A\|_2$. It measures the maximum Euclidean stretch of the matrix. Geometrically, it is the length of the longest semi-axis of the hyperellipsoid image of the unit sphere. It is equal to the largest singular value of $A$: $\|A\|_2 = \sigma_{\max}(A) = \sqrt{\lambda_{\max}(A^\top A)}$. Unlike the Frobenius norm (which measures the element-wise energy), the spectral norm measures the worst-case amplification power of the operator.</p>

      <div class="proof-box">
        <h4>Convexity of Induced Norms</h4>
        <p>All induced norms are convex functions of the matrix $A$. This follows from the fact that $\|A\|$ can be written as the supremum of a family of linear functions: $\|A\| = \sup_{\|\mathbf{x}\|=1, \|\mathbf{y}\|_*=1} \mathbf{y}^\top A \mathbf{x}$. Since the supremum of any collection of convex (here, linear) functions is convex, the induced norm is convex.</p>
      </div>
    </section>

    <!-- SECTION 4: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>4. The QR Decomposition: Geometry First</h2>

      <p>The QR decomposition is the algebraic realization of the Gram-Schmidt process. It formalizes the idea that any matrix can be factored into a "perfect geometry" component ($Q$) and a "coordinate" component ($R$).</p>

      <h3>4.1 Definition and Geometry</h3>
      <p>Any matrix $A \in \mathbb{R}^{m \times n}$ with linearly independent columns can be factored as $A = QR$, where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is upper triangular. This factorization is equivalent to applying the Gram-Schmidt process to the columns of $A$. The columns of $Q$ form an orthonormal basis for the column space of $A$, and the matrix $R$ expresses the original columns as linear combinations of this orthogonal basis. Because $R$ is upper triangular, the $k$-th column of $A$ lies in the span of the first $k$ columns of $Q$.</p>

      <h3>4.2 Householder Reflections</h3>
      <p>While Gram-Schmidt is conceptually simple, it is numerically unstable. In practice, QR is computed using <b>Householder reflections</b>. A Householder reflection is an orthogonal transformation that reflects vectors across a hyperplane. By carefully choosing a sequence of hyperplanes, we can reflect the columns of $A$ one by one to introduce zeros below the diagonal, gradually transforming $A$ into the upper triangular matrix $R$. Since reflections are orthogonal (isometric), this process is numerically stable and does not amplify errors.</p>
    </section>

    <!-- SECTION 5: SVD -->
    <section class="section-card" id="section-svd">
      <h2>5. Singular Value Decomposition (SVD): The True Geometry</h2>
      <p>The Spectral Theorem applies only to symmetric matrices. The <b>Singular Value Decomposition (SVD)</b> is the fundamental theorem of linear algebra because it applies to <i>every</i> matrix, square or rectangular. It reveals the intrinsic geometry of linear maps: every linear transformation is a composition of a rotation, a scaling, and another rotation.</p>

      <h3>5.1 The Geometry: Rotate-Scale-Rotate</h3>
      <p>The SVD states that any matrix $A$ can be factored as $A = U \Sigma V^\top$, where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal matrix of singular values $\sigma_i \ge 0$. This decomposition tells a complete geometric story:
      <br>1. <b>Rotate ($V^\top$):</b> The input vector is rotated to align with a new basis (the right singular vectors).
      <br>2. <b>Scale ($\Sigma$):</b> The vector is stretched or shrunk along the coordinate axes by the singular values $\sigma_i$.
      <br>3. <b>Rotate ($U$):</b> The result is rotated again to the output basis (the left singular vectors).
      <br>This means that any linear map sends the unit sphere to a hyperellipsoid. The singular values are the lengths of the semi-axes of this ellipsoid, and the columns of $U$ are their directions.</p>

      <div class="proof-box">
        <h4>Rigorous Derivation via $A^\top A$</h4>
        <p>The existence of the SVD follows from the spectral decomposition of the symmetric matrix $A^\top A$. Since $A^\top A$ is symmetric and positive semidefinite, it has real, non-negative eigenvalues $\sigma_i^2$ and orthonormal eigenvectors $V$.
        <br>We define the singular values $\sigma_i = \sqrt{\lambda_i(A^\top A)}$ and the right singular vectors $V$.
        <br>For non-zero $\sigma_i$, we define the left singular vectors as $u_i = \frac{1}{\sigma_i} A v_i$. It is easy to verify that these $u_i$ are orthonormal: $\langle u_i, u_j \rangle = \frac{1}{\sigma_i \sigma_j} v_i^\top A^\top A v_j = \frac{\sigma_j^2}{\sigma_i \sigma_j} \delta_{ij} = \delta_{ij}$.
        <br>This construction ensures that $A v_i = \sigma_i u_i$, which in matrix form is $AV = U\Sigma$, or $A = U\Sigma V^\top$.</p>
      </div>

      <h3>5.2 Rank and Fundamental Subspaces</h3>
      <p>The SVD provides optimal bases for the four fundamental subspaces. If $A$ has rank $r$ (meaning $r$ non-zero singular values), then:
      <br>The first $r$ columns of $U$ span the <b>Column Space</b> $\mathcal{R}(A)$.
      <br>The last $m-r$ columns of $U$ span the <b>Left Nullspace</b> $\mathcal{N}(A^\top)$.
      <br>The first $r$ columns of $V$ span the <b>Row Space</b> $\mathcal{R}(A^\top)$.
      <br>The last $n-r$ columns of $V$ span the <b>Nullspace</b> $\mathcal{N}(A)$.
      <br>The SVD explicitly diagonalizes the map between the row space and column space, showing exactly which input dimensions map to which output dimensions.</p>

      <h3>5.3 Low-Rank Approximation</h3>
      <p>The <b>Eckart-Young-Mirsky Theorem</b> states that the truncated SVD provides the optimal low-rank approximation of a matrix. Specifically, to approximate $A$ by a matrix of rank $k$, the best choice (in both spectral and Frobenius norms) is to keep the largest $k$ singular values and set the rest to zero: $A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top$. This result is the theoretical foundation for Principal Component Analysis (PCA) and image compression, as it guarantees that we capture the maximum possible "energy" of the matrix with a limited number of components.</p>
    </section>

    <!-- SECTION 6: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>6. The Pseudoinverse and Condition Number</h2>

      <h3>6.1 The Moore-Penrose Pseudoinverse</h3>
      <p>When a matrix $A$ is not invertible (either not square or singular), we can't solve $Ax=b$ uniquely. The <b>Moore-Penrose pseudoinverse</b> $A^+$ generalizes the concept of an inverse. It is defined via the SVD: if $A = U \Sigma V^\top$, then $A^+ = V \Sigma^+ U^\top$, where $\Sigma^+$ is obtained by inverting the non-zero singular values and transposing.
      <br>Algebraically, $A^+$ is the unique matrix satisfying the Moore-Penrose conditions ($AA^+A=A$, etc.).
      <br>Practically, $\mathbf{x} = A^+ \mathbf{b}$ gives the <b>minimum-norm least-squares solution</b>. If the system is overdetermined, it minimizes the error $\|Ax-b\|$. If it is underdetermined, it finds the solution with the smallest $\|x\|$. It "does the best it can" with the range and "does the least harm" in the domain.</p>

      <h3>6.2 The Condition Number</h3>
      <p>The <b>condition number</b> $\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}$ measures the numerical stability of a linear system. Geometrically, it is the aspect ratio of the ellipsoid image of the unit sphere. If $\kappa(A)$ is large ("ill-conditioned"), the ellipsoid is very long and thin. In this case, solving $Ax=b$ (mapping back from the ellipsoid to the sphere) is unstable: a small error in $\mathbf{b}$ along the short axis can be amplified into a huge error in $\mathbf{x}$ along the long axis.
      <br>Ideally, $\kappa(A) \approx 1$ (like an orthogonal matrix). When $\kappa(A)$ is large, we lose digits of precision. Tikhonov regularization (adding $\lambda I$ to $A^\top A$) works by effectively increasing all singular values, putting a floor on the denominator and reducing the condition number, thus stabilizing the inversion at the cost of introducing a small bias.</p>
    </section>

    <!-- SECTION 7: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>7. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>
    </section>

    <!-- SECTION 8: EXERCISES -->
    <section class="section-card" id="section-exercises">
      <h2><i data-feather="edit-3"></i> 8. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore matrix norms, SVD properties, and the geometry of projections.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $1/p + 1/q = 1$. Use Hölder's inequality.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Upper bound by Hölder: $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$, so $\|\mathbf{y}\|_* \le \|\mathbf{y}\|_q$.
        Lower bound: Construct $\mathbf{x}$ with $x_i = \text{sgn}(y_i)|y_i|^{q-1}$ normalized. Then $\mathbf{x}^\top \mathbf{y} = \|\mathbf{y}\|_q$. Thus $\|\mathbf{y}\|_* = \|\mathbf{y}\|_q$.</p>
      </div>

      <h3>P1.2 — SVD Computation</h3>
      <p>Compute the SVD of $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$ manually.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$A^\top A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$. Eigenvalues: 2, 0. Singular values: $\sqrt{2}, 0$.
        $V$: eigenvectors of $A^\top A$ are $\frac{1}{\sqrt{2}}(1, 1)$ and $\frac{1}{\sqrt{2}}(1, -1)$.
        $U$: $u_1 = \frac{1}{\sigma_1} A v_1 = (1, 0)$. Complete basis with $(0, 1)$.
        $A = I \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} V^\top$.</p>
      </div>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="../../static/lib/pyodide/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/resizable.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
