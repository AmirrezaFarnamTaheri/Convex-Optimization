<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>13. Algorithms I: Unconstrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <link rel="stylesheet" href="../../static/lib/prism/themes/prism.min.css" />
  <script src="../../static/lib/prism/prism.min.js"></script>
  <script src="../../static/lib/prism/components/prism-python.min.js"></script>
  <script src="../../static/lib/feather/feather.min.js"></script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../12-geometric-problems/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../14-equality-constrained-minimization/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>13. Algorithms I: Unconstrained Minimization</h1>
      <div class="lecture-meta">
        <span>Date: 2026-01-27</span>
        <span>Duration: 90 min</span>
        <span>Tags: algorithms, unconstrained, strong-convexity, newton-method</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> A rigorous zero‑to‑hero analysis of descent methods for unconstrained convex optimization. Covers gradient descent, steepest descent, and Newton's method with full convergence proofs, including strong convexity implications and self‑concordance.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05</a></p>
      </div>
    </header>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will have a deep, step-by-step understanding of the mechanics of unconstrained optimization. We will break down complex theorems into simple building blocks.</p>
      <ul style="line-height: 1.8;">
        <li><strong>Rigorous Convergence Analysis:</strong> You will learn not just <em>that</em> gradient descent converges, but exactly <em>how fast</em> and <em>why</em>. We will use Taylor's theorem to derive precise error bounds, showing mathematically that the error $f(x_k) - p^\star$ shrinks by a predictable fraction at every step.</li>
        <li><strong>Geometry of Optimization:</strong> We will move beyond algebraic formulas to visual intuition. You will learn how the "shape" of the function—specifically the <strong>condition number</strong> $\kappa$ (how stretched out the elliptical level sets are)—dictates the speed of convergence. We will see why "skinny" valleys cause gradient descent to zig-zag painfully.</li>
        <li><strong>The Role of Norms in Steepest Descent:</strong> We will discover that "steepest" is a relative term. Depending on whether you measure distance with a ruler (Euclidean norm), a Manhattan grid ($\ell_1$ norm), or a custom gauge (Quadratic norm), the "steepest" direction changes completely. This unifies Gradient Descent, Coordinate Descent, and Newton's Method under one umbrella.</li>
        <li><strong>Newton's Method Demystified:</strong> We will provide three distinct interpretations of Newton's method: as minimizing a quadratic approximation, as steepest descent in a curvature-adjusted norm, and as an affine-invariant operation. You will understand the "two-phase" convergence: a linear "damped" phase followed by an explosive "quadratic" phase where the number of correct digits doubles at every step.</li>
        <li><strong>Practical Implementation:</strong> We will bridge the gap between theory and code. You will see how to implement these algorithms in Python, handle practical issues like line search (step size selection), and benchmark them on real problems like Analytic Centering.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>0. Notation and the "Model"</h3>
      <p>
        We study the <strong>unconstrained optimization problem</strong>:
        $$ p^\star = \inf_{x\in \mathrm{dom}\,f} f(x) \tag{9.1} $$
        Here, we are looking for a vector $x$ that minimizes the scalar function $f(x)$. "Unconstrained" means we are not solving $Ax=b$ or $f_i(x) \le 0$ explicitly. However, implicit constraints exist because the function is only defined on its domain, $\mathrm{dom}\,f$.
        <br><br>
        To analyze algorithms rigorously, we make specific assumptions about the function $f$. These are not just bureaucratic details; they are the mathematical foundation that allows us to prove convergence. If any of these assumptions fail, the algorithms might oscillate, get stuck, or diverge.
      </p>
      <ul>
        <li><strong>Assumption 1: Convexity.</strong>
            <br>$f$ is convex on $\mathrm{dom}\,f$.
            <br><em>Meaning:</em> The domain is a convex set (no holes, straight line between any two points stays inside). The function curves "upward" like a bowl.
            <br><em>Why it's critical:</em> Convexity guarantees that <strong>any local minimum is a global minimum</strong>. We don't have to worry about getting stuck in a shallow "pothole" while a deeper valley exists elsewhere.
        </li>
        <li><strong>Assumption 2: Twice Continuously Differentiable ($C^2$).</strong>
            <br>We assume $f \in C^2(\mathrm{dom}\,f)$.
            <ul>
                <li>The <strong>Gradient</strong> $\nabla f(x) \in \mathbb{R}^n$ exists and is continuous. This vector points in the direction of steepest ascent. We use it to find the direction to move.</li>
                <li>The <strong>Hessian</strong> $\nabla^2 f(x) \in \mathbb{R}^{n \times n}$ exists and is continuous. This matrix describes the local curvature. We use it to measure how "curved" the bowl is, which determines the step size.</li>
            </ul>
        </li>
        <li><strong>Assumption 3: Open Domain.</strong>
            <br>$\mathrm{dom}\,f$ is an <strong>open set</strong>.
            <br><em>Meaning:</em> The set does not include its boundary. For example, $(0, \infty)$ is open; $[0, \infty)$ is not.
            <br><em>Why it's critical:</em> For any point $x$ in an open domain, there is a tiny ball around $x$ that is entirely contained within the domain. This ensures that we can always take a step $\epsilon$ in <em>any</em> direction without "falling off the edge" of the feasible set. Without this, standard derivatives (which rely on limits approaching from all directions) would not be well-defined at the boundary.
        </li>
        <li><strong>Assumption 4: Solvability.</strong>
            <br>The problem is <strong>solvable</strong>.
            <br><em>Meaning:</em> The optimal value $p^\star = \min_x f(x)$ is finite (not $-\infty$) and is actually achieved at some point $x^\star$.
            <br><em>Counter-example:</em> $f(x) = e^{-x}$ is bounded below by 0, but never reaches it ($x \to \infty$). Our theory assumes an optimal point $x^\star$ explicitly exists so we can measure the distance $\|x - x^\star\|$.
        </li>
      </ul>
      <p>
        <strong>Important Subtlety on Domains & Extended Values:</strong>
        <br>Strictly speaking, if $f$ is a map $\mathbb{R}^n \to \mathbb{R}$, its domain is the entire space. In convex analysis, we essentially work with <strong>extended-valued</strong> functions $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$, where the effective domain is $\mathrm{dom}\,f = \{x \mid f(x) < +\infty\}$.
        <br>For example, the logarithmic barrier $f(x) = -\sum \log(x_i)$ is formally defined as $+\infty$ if any $x_i \le 0$. Its effective domain is the open positive orthant $\mathbb{R}^n_{++}$. Twice differentiability forces us to restrict our attention to this open set where derivatives are well-defined. Our algorithms must stay strictly inside this open set; stepping onto the boundary (where $f=+\infty$) is forbidden.
      </p>
      <p>
        <strong>The "Container" Set $S$:</strong>
        One of the most important moves in the analysis is "localizing" all claims to the initial sublevel set:
        $$ S=\{x\in\mathrm{dom}\,f \mid f(x)\le f(x^{(0)})\} \tag{9.3} $$
        <strong>Why is this set so critical?</strong>
        <ol>
            <li><strong>The Trap (Descent Property):</strong> Most methods in Chapter 9 are <strong>descent methods</strong>, enforcing $f(x^{(k+1)}) \le f(x^{(k)})$. Inductively, $f(x^{(k)}) \le f(x^{(0)})$, so every iterate $x^{(k)}$ lives inside $S$. Any property we assume "for all $x \in S$" automatically applies to every point the algorithm visits.</li>
            <li><strong>Closedness & Compactness:</strong> We typically assume $f$ is a <strong>closed function</strong> (meaning its epigraph is closed, or equivalently, its sublevel sets are closed).
                <ul>
                    <li>If we assume Strong Convexity (Section 3), $S$ becomes bounded.</li>
                    <li>A closed and bounded set in $\mathbb{R}^n$ is <strong>compact</strong>.</li>
                    <li>Compactness is the magic property that guarantees continuous functions (like $\lambda_{\max}(\nabla^2 f(x))$) attain their maximum and minimum. This allows us to rigorously define constants $m$ and $M$ such that $mI \preceq \nabla^2 f(x) \preceq MI$ everywhere in $S$.</li>
                </ul>
            </li>
        </ol>
      </p>

      <h3>1. The Core Equivalence (Optimality Conditions)</h3>
      <p>
        The unconstrained problem is simply:
        $$ \min_x f(x) \tag{9.1} $$
      </p>
      <h4>1.1 Why Convexity + Differentiability Makes Optimality Global</h4>
      <p>
        One of the most powerful theorems in optimization states that for convex functions, the condition $\nabla f(x) = 0$ is <strong>necessary and sufficient</strong> for a point to be a global minimizer.
        $$ \nabla f(x^\star)=0 \iff x^\star \text{ is a global minimizer} \tag{9.2} $$
        <br>
        <strong>Contrast with Non-Convex Calculus:</strong> In standard calculus, $\nabla f(x)=0$ only tells you that you are at a <em>stationary point</em>. You could be at:
        <ul>
            <li>A local minimum (bottom of a small valley).</li>
            <li>A local maximum (top of a hill).</li>
            <li>A saddle point (like the center of a Pringles chip).</li>
        </ul>
        However, <strong>convexity upgrades "stationary point" to "global minimizer."</strong> There are no local traps. If the slope is zero, you are at the absolute bottom.
        <br><br>
        This relies on the <strong>first-order characterization of convexity</strong>:
        $$ f(y) \ge f(x) + \nabla f(x)^T (y-x) \qquad (\star) $$
        Geometrically, this inequality says that the first-order Taylor approximation (the tangent hyperplane) is a <strong>global underestimator</strong> of the function. The graph of the function lies entirely <em>above</em> its tangent plane at any point $x$.
      </p>

      <div style="background: var(--code-bg); padding: 16px; border-radius: 8px; border-left: 4px solid var(--accent); margin-bottom: 24px;">
          <h4 style="margin-top: 0;">Detailed Proof of the Global Underestimator Property $(\star)$</h4>
          <p>We start from the definition of convexity and derive the gradient inequality.</p>
          <ol>
              <li><strong>Definition of Convexity:</strong> By definition, for any two points $x, y$ and any $t \in [0,1]$, the function segment lies below the chord connecting them:
                  $$ f(x+t(y-x)) \le (1-t)f(x)+t f(y) $$
              </li>
              <li><strong>Isolate $f(y)$:</strong> We want to bound $f(y)$. Let's rearrange the inequality.
                  $$ f(x+t(y-x)) \le f(x) - tf(x) + tf(y) $$
                  $$ f(x+t(y-x)) - f(x) \le t(f(y)-f(x)) $$
              </li>
              <li><strong>Form Difference Quotient:</strong> Divide both sides by $t$ (assuming $t > 0$):
                  $$ \frac{f(x+t(y-x))-f(x)}{t} \le f(y)-f(x) $$
              </li>
              <li><strong>Take the Limit ($t \to 0$):</strong> Now, let $t$ approach 0 from the positive side ($t \downarrow 0$).
                  <br>The Left Hand Side is exactly the definition of the <strong>directional derivative</strong> of $f$ at $x$ in the direction $(y-x)$.
                  $$ \lim_{t \to 0} \frac{f(x+t(y-x))-f(x)}{t} = \nabla f(x)^T(y-x) $$
              </li>
              <li><strong>Conclusion:</strong> Substituting the limit back into the inequality:
                  $$ \nabla f(x)^T(y-x) \le f(y) - f(x) $$
                  Rearranging gives the result:
                  $$ f(y) \ge f(x) + \nabla f(x)^T (y-x) $$
              </li>
          </ol>
      </div>

      <p><strong>Proof of Equivalence ($\nabla f(x^\star)=0 \iff \text{Global Min}$):</strong></p>
      <ul>
          <li><strong>Sufficiency ($\nabla f(x^\star)=0 \Rightarrow$ Min):</strong>
              <br>Suppose we found a point $x^\star$ where the gradient is zero. Does this guarantee it's the best point?
              <br>Yes. We simply plug $\nabla f(x^\star) = 0$ into the convexity inequality $(\star)$. For <em>any</em> other point $y$ in the domain:
              $$ f(y) \ge f(x^\star) + \underbrace{\nabla f(x^\star)^T}_{0} (y-x^\star) $$
              $$ f(y) \ge f(x^\star) $$
              Since $f(y)$ is always greater than or equal to $f(x^\star)$, $x^\star$ must be a global minimizer.
          </li>
          <li style="margin-top: 16px;"><strong>Necessity (Min $\Rightarrow \nabla f(x^\star)=0$):</strong>
              <br>Conversely, if $x^\star$ is a minimizer, <em>must</em> the gradient be zero?
              <br>We prove this by contradiction. Assume $x^\star$ is a local minimizer but $\nabla f(x^\star) \neq 0$.
              <ol>
                  <li><strong>Open Domain Assumption:</strong> Since $\mathrm{dom}\,f$ is open, we can move a small distance in <em>any</em> direction without hitting a wall.</li>
                  <li><strong>Choose Descent Direction:</strong> Let $v = -\nabla f(x^\star)$. This is the direction of steepest descent.</li>
                  <li><strong>Single-Variable Function:</strong> Let's restrict the function to the line passing through $x^\star$ in direction $v$. Define $\phi(t) = f(x^\star + tv)$.
                      <br>Since $x^\star$ (where $t=0$) is a minimizer, $\phi(t)$ must have a local minimum at $t=0$.</li>
                  <li><strong>First Derivative Test:</strong> Calculus tells us that for $t=0$ to be a minimum, the derivative $\phi'(0)$ must be 0.
                      <br>Let's compute $\phi'(0)$ using the chain rule:
                      $$ \phi'(0) = \nabla f(x^\star)^T v = \nabla f(x^\star)^T (-\nabla f(x^\star)) = -\|\nabla f(x^\star)\|_2^2 $$
                  </li>
                  <li><strong>Contradiction:</strong> We assumed $\nabla f(x^\star) \neq 0$, so the squared norm $\|\nabla f(x^\star)\|_2^2$ is strictly positive.
                      <br>Thus, $\phi'(0) < 0$.
                      <br>Since the derivative is negative, the function is <em>decreasing</em> at $t=0$. This means that for a tiny step $t > 0$, $\phi(t) < \phi(0)$, or equivalently $f(x^\star + tv) < f(x^\star)$.
                  </li>
                  <li><strong>Conclusion:</strong> We found a point with a lower function value. This contradicts the assumption that $x^\star$ is a minimizer. Therefore, the gradient $\nabla f(x^\star)$ <em>must</em> be 0.</li>
              </ol>
          </li>
      </ul>

      <h3>2. Motivating Examples</h3>
      <p>Before diving into the proofs, let's look at the canonical problems we are trying to solve. These examples will serve as our testbed for algorithms.</p>
      <h4>2.1 Quadratic Minimization</h4>
      <p>
        The most basic convex optimization problem is the unconstrained quadratic program:
        $$ \min_x \ f(x) = \frac12 x^T P x + q^T x + r,\qquad P\in S^n_+ \tag{9.4} $$
        where $P$ is symmetric positive semidefinite ($P \succeq 0$). This is the "Hello World" of optimization.
      </p>
      <div style="background: var(--code-bg); padding: 12px; border-left: 4px solid var(--accent); margin-bottom: 16px;">
        <strong>Step-by-Step Derivation of the Gradient $\nabla f(x)$:</strong>
        <br>
        1. <strong>Linear Term:</strong> The gradient of $q^T x = \sum q_i x_i$ is simply the vector $q$.
        <br>
        2. <strong>Quadratic Term:</strong> Let's compute $\nabla (\frac{1}{2} x^T P x)$. It helps to write it in scalar sum notation:
        $$ \phi(x) = \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n P_{ij} x_i x_j $$
        Now take the partial derivative with respect to a specific variable $x_k$. We need to find all terms in the sum that contain $x_k$.
        <br>
        - Terms where $i=k$ and $j \neq k$: $\frac{1}{2} \sum_{j \neq k} P_{kj} x_k x_j$. Derivative is $\frac{1}{2} \sum_{j \neq k} P_{kj} x_j$.
        <br>
        - Terms where $j=k$ and $i \neq k$: $\frac{1}{2} \sum_{i \neq k} P_{ik} x_i x_k$. Derivative is $\frac{1}{2} \sum_{i \neq k} P_{ik} x_i$.
        <br>
        - Term where $i=k$ and $j=k$: $\frac{1}{2} P_{kk} x_k^2$. Derivative is $P_{kk} x_k$.
        <br>
        Adding them up:
        $$ \frac{\partial \phi}{\partial x_k} = \frac{1}{2} \sum_{j} P_{kj} x_j + \frac{1}{2} \sum_{i} P_{ik} x_i $$
        Since $P$ is symmetric ($P_{ij} = P_{ji}$), the indices $i$ and $j$ play the same role. Both sums are effectively the same: the dot product of the $k$-th row of $P$ with $x$.
        $$ \frac{\partial \phi}{\partial x_k} = \sum_{j} P_{kj} x_j = (Px)_k $$
        Thus, the vector gradient is $\nabla f(x) = Px + q$.
      </div>
      <p>
        Setting the gradient to zero gives the linear system:
        $$ Px^\star = -q $$
      </p>
      <ul>
          <li><strong>Case 1: $P \succ 0$ (Positive Definite).</strong> The matrix $P$ is invertible (all eigenvalues $> 0$). There is a <strong>unique global minimizer</strong>: $x^\star = -P^{-1}q$. The level sets are ellipsoids, and the function looks like a perfect bowl.</li>
          <li><strong>Case 2: $P \succeq 0$ but Singular.</strong> The matrix $P$ has a zero eigenvalue, meaning the bowl has a "flat" valley floor (like a trough).
            <ul>
                <li>If $-q \in \mathrm{range}(P)$, there are infinitely many solutions (a linear subspace of solutions lying in the valley).</li>
                <li>If $-q \notin \mathrm{range}(P)$, the system is inconsistent. The function is <strong>unbounded below</strong>. It slopes downward along the flat valley forever (like a tilted trough), so $p^\star = -\infty$.</li>
            </ul>
          </li>
      </ul>

      <h4>2.2 Least Squares</h4>
      <p>
        $$ \min_x \|Ax-b\|_2^2 $$
        This is the foundation of regression and data fitting. We can expand the squared norm to reveal it is just a specific type of quadratic function:
        $$ f(x) = (Ax-b)^T (Ax-b) = x^T A^T A x - 2b^T A x + b^T b $$
        Comparing this to the standard form in 2.1:
        <ul>
            <li>$P = 2A^T A$ (Note: $A^T A$ is always symmetric positive semidefinite).</li>
            <li>$q = -2A^T b$.</li>
            <li>$r = b^T b$.</li>
        </ul>
        The optimality condition $\nabla f(x) = 0$ becomes the famous <strong>Normal Equations</strong>:
        $$ 2A^T A x - 2A^T b = 0 \implies A^T A x = A^T b $$
      </p>

      <h4>2.3 Log-Sum-Exp (Softmax)</h4>
      <p>
        This function approximates the "max" function ($\max_i (a_i^T x + b_i)$) but is smooth and convex:
        $$ f(x)=\log\Big(\sum_{i=1}^m e^{a_i^T x+b_i}\Big) $$
        This is crucial in Machine Learning (e.g., Logistic Regression, Neural Networks) where it acts as a loss function.
        <br><br>
        <strong>Gradient Interpretation:</strong>
        Let $z_i = a_i^T x + b_i$. By the chain rule:
        $$ \nabla f(x) = \frac{1}{\sum_j e^{z_j}} \sum_{i=1}^m e^{z_i} a_i = \sum_{i=1}^m \left( \frac{e^{z_i}}{\sum_j e^{z_j}} \right) a_i $$
        We define the weights $w_i(x)$ as the <strong>Softmax</strong> function probabilities:
        $$ w_i(x) = \frac{e^{z_i}}{\sum_j e^{z_j}} $$
        So, $\nabla f(x) = \sum_{i=1}^m w_i(x) a_i$.
        <br>
        <strong>Intuition:</strong> The gradient is a <strong>convex combination</strong> (weighted average) of the direction vectors $a_i$.
        <ul>
            <li>If one term $k$ is much larger than the others, $e^{z_k}$ dominates the sum. Then $w_k \approx 1$ and other weights $\approx 0$.</li>
            <li>In this case, $\nabla f(x) \approx a_k$. This matches the gradient of the hard $\max$ function $\max_i(z_i)$, which would simply be the gradient of the "active" term.</li>
        </ul>
      </p>

      <h4>2.4 Analytic Centers (Logarithmic Barriers)</h4>
      <p>
        $$ f(x) = -\sum_{i=1}^m \log(b_i - a_i^T x) $$
        defined on the polyhedron $\mathcal{P} = \{x \mid a_i^T x < b_i \forall i\}$.
        <br>
        <strong>Intuition:</strong>
        Each term $-\log(b_i - a_i^T x)$ acts as a <strong>repulsive force field</strong>.
        <ul>
            <li>If $x$ is far from the wall $a_i^T x = b_i$, the term is small.</li>
            <li>As $x$ approaches the wall ($a_i^T x \to b_i$), the value explodes to $+\infty$.</li>
        </ul>
        Minimizing $f(x)$ finds a point $x^\star$ that is "maximally inside" the set, balancing the repulsion from all walls simultaneously. This point is called the <strong>Analytic Center</strong>. This concept is the engine behind <strong>Interior Point Methods</strong>, which solve constrained problems by following a path of analytic centers.
      </p>

      <h3>3. Strong Convexity: The Engine of Convergence</h3>
      <p>
        To prove that algorithms converge <em>fast</em>, mere convexity is not enough. We need <strong>Strong Convexity</strong>. This assumption prevents the function from being too "flat."
        <br><br>
        Formally, there exists a scalar $m>0$ such that:
        $$ \nabla^2 f(x)\succeq mI,\qquad \forall x\in S. \tag{9.7} $$
        <strong>Geometric Meaning:</strong>
        <ul>
            <li>$\nabla^2 f(x) \succeq mI$ means that all eigenvalues of the Hessian are $\ge m$.</li>
            <li>The function is "at least as curved as a quadratic bowl with curvature $m$."</li>
            <li>If you slice the graph in any direction, the resulting 1D curve is bounded below by a parabola $y = \frac{m}{2}x^2$.</li>
        </ul>
        This strictly forbids flat regions (where curvature is 0). It ensures that as we move away from the minimum, the function value rises sharply.
      </p>

      <h4>3.1 From Taylor’s Theorem to the Quadratic Lower Bound</h4>
      <p>
        Let's derive the most important consequence of strong convexity. It states that the function lies <em>above</em> a paraboloid.
        <br>
        Consider two points $x$ (our current location) and $y$ (any other point). Taylor’s theorem states:
        $$ f(y) = f(x) + \nabla f(x)^T(y-x) + \underbrace{\frac{1}{2} (y-x)^T \nabla^2 f(z) (y-x)}_{\text{Second Order Term}} $$
        where $z$ is some point on the line segment between $x$ and $y$.
        <br><br>
        <strong>Applying the Assumption:</strong>
        Since we assume $\nabla^2 f(z) \succeq mI$, we know that for any vector $v$, $v^T \nabla^2 f(z) v \ge m \|v\|_2^2$.
        Setting $v = y-x$:
        $$ (y-x)^T \nabla^2 f(z) (y-x) \ge m \|y-x\|_2^2 $$
        Substituting this back into the Taylor expansion gives the <strong>Quadratic Lower Bound</strong>:
        $$ f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{m}{2} \|y-x\|_2^2 \tag{9.8} $$
        <strong>Comparison:</strong>
        <ul>
            <li><strong>Convexity ($m=0$):</strong> $f(y) \ge f(x) + \nabla f(x)^T(y-x)$. The graph is above the flat tangent plane.</li>
            <li><strong>Strong Convexity ($m>0$):</strong> The graph is above a specific upward-curving paraboloid touching at $x$.</li>
        </ul>
      </p>

      <h4>3.2 Bounding Suboptimality and Distance to Optimizer</h4>
      <p>
        The quadratic lower bound is a powerful tool. It allows us to relate the size of the gradient $\|\nabla f(x)\|$ to how close we are to the solution.
        <br><br>
        <strong>1. Suboptimality Bound ($f(x) - p^\star$):</strong>
        <br>
        Fix $x$. The inequality (9.8) holds for <em>all</em> $y$.
        $$ f(y) \ge \underbrace{f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|^2}_{\text{Let's call this RHS } g(y)} $$
        Since $f(y) \ge g(y)$, the minimum of $f$ must be greater than the minimum of $g$:
        $$ p^\star = \min_y f(y) \ge \min_y g(y) $$
        $g(y)$ is a simple quadratic function of $y$. We can find its minimum easily by setting its gradient to zero:
        $$ \nabla g(y) = \nabla f(x) + m(y-x) = 0 \implies \tilde{y} = x - \frac{1}{m}\nabla f(x) $$
        Plugging this optimal $\tilde{y}$ back into $g(y)$:
        $$ \min_y g(y) = g(\tilde{y}) = f(x) + \nabla f(x)^T \left(-\frac{1}{m}\nabla f(x)\right) + \frac{m}{2} \left\|-\frac{1}{m}\nabla f(x)\right\|^2 $$
        $$ = f(x) - \frac{1}{m}\|\nabla f(x)\|^2 + \frac{1}{2m}\|\nabla f(x)\|^2 $$
        $$ = f(x) - \frac{1}{2m}\|\nabla f(x)\|^2 $$
        So we have:
        $$ p^\star \ge f(x) - \frac{1}{2m}\|\nabla f(x)\|_2^2 $$
        Rearranging this gives us a bound on how suboptimal our current point $x$ is:
        $$ f(x)-p^\star \le \frac{1}{2m}\|\nabla f(x)\|_2^2 \tag{9.9} $$
        <strong>Why this is useful:</strong> We can't see $p^\star$, but we can calculate $\|\nabla f(x)\|$. This bound tells us strictly: "If your gradient is small, you are guaranteed to be close to the optimal value."
      </p>
      <p>
        <strong>2. Distance to Optimizer ($\|x - x^\star\|$):</strong>
        <br>
        We can also bound the distance in the domain.
        Apply the master inequality (9.8) specifically at $y=x^\star$:
        $$ p^\star \ge f(x)+\nabla f(x)^T(x^\star-x)+\frac{m}{2}\|x^\star-x\|^2 $$
        Since $p^\star \le f(x)$ (obviously), we have:
        $$ 0 \ge \nabla f(x)^T(x^\star-x)+\frac{m}{2}\|x^\star-x\|^2 $$
        Move the dot product to the other side:
        $$ -\nabla f(x)^T(x^\star-x) \ge \frac{m}{2}\|x^\star-x\|^2 $$
        Use the Cauchy-Schwarz inequality on the left ($|a^Tb| \le \|a\|\|b\|$):
        $$ \|\nabla f(x)\|_2 \|x^\star-x\|_2 \ge \frac{m}{2}\|x^\star-x\|_2^2 $$
        Divide both sides by $\|x^\star - x\|_2$ (assuming we aren't already at the optimum):
        $$ \|\nabla f(x)\|_2 \ge \frac{m}{2}\|x^\star-x\|_2 $$
        Or:
        $$ \|x-x^\star\|_2 \le \frac{2}{m}\|\nabla f(x)\|_2 \tag{9.11} $$
        This implies the optimizer $x^\star$ is <strong>unique</strong>. (If there were two minima, the gradient would be zero at both, implying distance is zero).
      </p>

      <h3>4. Smoothness: The Guarantee of Descent</h3>
      <p>
        Strong Convexity gave us a lower bound (paraboloid below). To ensure our algorithms don't oscillate wildly, we also need an upper bound on curvature.
        <br>
        Since $f$ is twice continuously differentiable and we are working on a closed and bounded set $S$, the maximum eigenvalue of the Hessian must be bounded above by some constant $M$:
        $$ \nabla^2 f(x)\preceq MI,\qquad \forall x\in S \tag{9.12} $$
        This property is called <strong>M-smoothness</strong> (or Lipschitz continuity of the gradient with constant $M$).
        <br>
        <em>Interpretation:</em> The function is "no more curved than a quadratic bowl with curvature $M$." It doesn't have arbitrarily sharp kinks.
      </p>

      <h4>4.1 Quadratic Upper Bound</h4>
      <p>
        Using Taylor's theorem exactly as before, but substituting the upper bound $\nabla^2 f(z) \preceq MI$:
        $$ f(y) \le f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}\|y-x\|_2^2 \tag{9.13} $$

        <div style="border: 2px solid var(--border); padding: 16px; border-radius: 8px; margin: 16px 0;">
        <strong>The Sandwich Principle:</strong>
        <br>Combining (9.8) and (9.13), we see that the function $f(y)$ is <strong>sandwiched</strong> between two quadratic bowls anchored at $x$:
        $$ \underbrace{f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|^2}_{\text{Lower Paraboloid (Blue)}} \le f(y) \le \underbrace{f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}\|y-x\|^2}_{\text{Upper Paraboloid (Red)}} $$
        The function lives in the "Goldilocks zone" between these two quadratics.
        </div>
      </p>
      <p>
        <strong>Implication for Descent:</strong> The upper bound guarantees that if we take a small enough step opposite to the gradient, the function value <em>must</em> decrease.
        <br>
        Let's try to minimize the Upper Bound (the Red Paraboloid). The minimum occurs at $y = x - \frac{1}{M}\nabla f(x)$.
        <br>
        Plugging this specific step into the inequality (9.13):
        $$ f\left(x - \frac{1}{M}\nabla f(x)\right) \le f(x) + \nabla f(x)^T\left(-\frac{1}{M}\nabla f(x)\right) + \frac{M}{2}\left\|-\frac{1}{M}\nabla f(x)\right\|^2 $$
        $$ = f(x) - \frac{1}{M}\|\nabla f(x)\|^2 + \frac{M}{2}\frac{1}{M^2}\|\nabla f(x)\|^2 $$
        $$ = f(x) - \frac{1}{M}\|\nabla f(x)\|^2 + \frac{1}{2M}\|\nabla f(x)\|^2 $$
        $$ f(x_{\text{next}}) \le f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 \tag{9.14} $$
        <strong>Conclusion:</strong> A single gradient step with size $1/M$ is guaranteed to reduce the function value by at least $\frac{1}{2M}\|\nabla f(x)\|^2$.
      </p>
      <p>
        <strong>Summary of Bounds:</strong>
        Combining the Upper and Lower bounds allows us to bracket the distance to the optimum:
        $$ \frac{1}{2M}\|\nabla f(x)\|^2 \le f(x)-p^\star \le \frac{1}{2m}\|\nabla f(x)\|^2 $$
      </p>

      <h3>5. Condition Number of Sublevel Sets</h3>
      <p>
        We established that $mI \preceq \nabla^2 f(x) \preceq MI$.
        The ratio $\kappa = M/m$ is the condition number bound.
        $$ \kappa(\nabla^2 f(x))=\frac{\lambda_{\max}(\nabla^2 f(x))}{\lambda_{\min}(\nabla^2 f(x))}\le \frac{M}{m} $$
        This number $\kappa$ is the villain of first-order optimization.
      </p>
      <h4>5.1 Geometric Meaning: Width and Eccentricity</h4>
      <p>
        The condition number $\kappa$ measures the <strong>anisotropy</strong> (direction-dependence) of the function's curvature.
        <br>
        Intuitively, the sublevel sets $S_\alpha = \{x \mid f(x) \le \alpha\}$ look like ellipsoids.
        <ul>
            <li>If $\kappa \approx 1$, the sublevel sets are spherical. Gradient descent points directly to the center.</li>
            <li>If $\kappa$ is large (e.g., 1000), the sublevel sets are long, skinny ellipses (like a cigar).</li>
        </ul>
        <br>
        <strong>Formal Definition via Width:</strong>
        For a convex set $C$, the width in direction $q$ ($W(C,q)$) is the distance between parallel supporting hyperplanes perpendicular to $q$.
        The <strong>condition number of a set</strong> is the ratio of its maximum width squared to its minimum width squared:
        $$ \mathrm{cond}(C) = \frac{W_{\max}^2}{W_{\min}^2} $$
      </p>
      <div style="background: var(--code-bg); padding: 16px; border-radius: 8px; border-left: 4px solid var(--accent); margin-bottom: 16px;">
        <strong>Example 9.1: Width of an Ellipsoid.</strong>
        <br>Consider the ellipsoid $\mathcal{E}=\{x\mid (x-x_0)^T A^{-1}(x-x_0)\le 1\}$ with positive definite matrix $A$.
        <br><strong>1. Width Calculation:</strong> The width in direction $q$ (unit vector) is given by $W(\mathcal{E}, q) = 2\sqrt{q^T A q}$.
        <br><strong>2. Extremes:</strong>
        <br>- The maximum width corresponds to the largest eigenvalue of $A$: $W_{\max} = 2\sqrt{\lambda_{\max}(A)}$.
        <br>- The minimum width corresponds to the smallest eigenvalue: $W_{\min} = 2\sqrt{\lambda_{\min}(A)}$.
        <br><strong>3. Condition Number:</strong>
        $$ \mathrm{cond}(\mathcal{E}) = \frac{(2\sqrt{\lambda_{\max}})^2}{(2\sqrt{\lambda_{\min}})^2} = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)} = \kappa(A) $$
        Geometrically, if the sublevel sets of our function look like this ellipsoid, then their condition number is bounded by $M/m$.
      </div>

      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/kappa_effect_paths.gif" alt="Gradient descent zig-zag worsening with condition number" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;"><strong>The Condition Number Tax:</strong> As $\kappa = M/m$ grows (ellipses become thinner), gradient descent zig-zags significantly, slowing convergence linearly with $\kappa$.</figcaption>
      </figure>

      <h3>6. Descent Methods</h3>
      <p>
        We are now ready to state the general algorithm. We produce a sequence of points $x^{(k)}$ that moves towards the minimum.
        <br>
        <strong>General Descent Algorithm:</strong>
        <pre><code class="language-python">
def descent_method(f, gradient, x0, tol=1e-6):
    x = x0
    while True:
        # 1. Compute Descent Direction (delta_x)
        # For standard gradient descent, this is simply -gradient(x).
        # For Newton's method, it is -inv(Hessian) * gradient(x).
        delta_x = compute_direction(x)

        # 2. Check Stopping Criterion
        # A robust check uses the Newton Decrement or the Gradient norm.
        # Based on (9.9), if ||grad|| is small, the gap f(x) - p* is small.
        if -gradient(x).dot(delta_x) / 2 <= tol:
            break

        # 3. Line Search (Step Size Selection)
        # We need to find a scalar t > 0 such that the function actually decreases.
        # f(x + t*delta_x) < f(x)
        t = line_search(f, x, delta_x)

        # 4. Update
        x = x + t * delta_x
    return x
        </code></pre>
        The search direction $\Delta x$ must be a <strong>descent direction</strong>: $\nabla f(x)^T \Delta x < 0$. This ensures that the angle between the step and the gradient is greater than 90 degrees (going "downhill"), so for small enough steps, the function value decreases.
      </p>

      <h4>6.1 Exact Line Search: A Rigorous Proof of Linear Convergence</h4>
      <p>
        Let's prove mathematically why Gradient Descent works.
        Suppose we use <strong>Exact Line Search</strong>, where we choose the step size $t$ to minimize $f$ exactly along the ray: $t = \text{argmin}_{s \ge 0} f(x + s \Delta x)$.
        <br><br>
        <strong>Proof Goal:</strong> Show that the error $\epsilon_k = f(x^{(k)}) - p^\star$ decreases by a constant factor $c < 1$ at every step ($\epsilon_{k+1} \le c \cdot \epsilon_k$).
        <br><br>
        <strong>Step 1: Guaranteed Decrease (using Smoothness).</strong>
        We don't know the <em>exact</em> optimal $t$, but we know it must be <em>at least as good</em> as any arbitrary guess. Let's test the specific guess $t = 1/M$.
        <br>
        From the smoothness upper bound (9.13), taking a step of size $1/M$ along the direction $d = -\nabla f(x)$ gives:
        $$ f(x - \frac{1}{M}\nabla f(x)) \le f(x) + \nabla f(x)^T(-\frac{1}{M}\nabla f(x)) + \frac{M}{2}\left\|-\frac{1}{M}\nabla f(x)\right\|^2 $$
        $$ = f(x) - \frac{1}{M}\|\nabla f(x)\|^2 + \frac{1}{2M}\|\nabla f(x)\|^2 $$
        $$ = f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 $$
        Since the Exact Line Search finds the <em>best</em> $t$, the resulting function value $f(x_{\text{next}})$ must be even lower than this specific guess:
        $$ f(x_{\text{next}}) \le f(x - \frac{1}{M}\nabla f(x)) \le f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 $$
        <br>
        <strong>Step 2: Relating Gradient to Error (using Strong Convexity).</strong>
        We need to replace the gradient term $\|\nabla f(x)\|^2$ with the error term $(f(x) - p^\star)$.
        Recall the Strong Convexity bound (9.9):
        $$ f(x) - p^\star \le \frac{1}{2m}\|\nabla f(x)\|^2 \implies \|\nabla f(x)\|^2 \ge 2m(f(x) - p^\star) $$
        Substituting this into our inequality from Step 1:
        $$ f(x_{\text{next}}) \le f(x) - \frac{1}{2M} \left( 2m(f(x) - p^\star) \right) $$
        $$ f(x_{\text{next}}) \le f(x) - \frac{m}{M}(f(x) - p^\star) $$
        <br>
        <strong>Step 3: Conclusion.</strong>
        Subtract $p^\star$ from both sides to see the evolution of the error:
        $$ f(x_{\text{next}}) - p^\star \le (f(x) - p^\star) - \frac{m}{M}(f(x) - p^\star) $$
        $$ \epsilon_{k+1} \le \left(1 - \frac{m}{M}\right) \epsilon_k $$
        This is <strong>Linear Convergence</strong>. The error contracts by a fixed factor $c = 1 - m/M < 1$ at every step.
        <br>
        <em>Note:</em> The contraction factor $1 - 1/\kappa$ depends directly on the condition number $\kappa = M/m$.
        <ul>
            <li>If $\kappa=1$ (spherical), $c=0$. Error becomes 0 in one step!</li>
            <li>If $\kappa=1000$ (ill-conditioned), $c = 0.999$. We only remove 0.1% of the error per step. This explains the "zig-zag" behavior.</li>
        </ul>
      </p>

      <h4>6.2 Backtracking (Armijo) Line Search: Proof of Termination and Convergence</h4>
      <p>
        Exact minimization is usually too expensive. Instead, we use <strong>Backtracking Line Search</strong>. It shrinks the step size $t$ until the "Armijo sufficient decrease condition" is met:
        $$ f(x+t\Delta x) \le f(x) + \alpha t \nabla f(x)^T \Delta x, \qquad 0 < \alpha < 0.5, \ \ 0 < \beta < 1 $$
        We must prove two things: (1) the loop eventually terminates, and (2) the accepted step is "large enough" to guarantee linear convergence.
      </p>
      <p>
        <strong>1. Termination Proof (Why Armijo holds for small $t$):</strong>
        Let $g = \nabla f(x)$ and $\Delta x = -g$.
        From the smoothness bound (9.13) along the ray:
        $$ f(x-tg) \le f(x) - t\|g\|^2 + \frac{M t^2}{2}\|g\|^2 $$
        We want this to be $\le f(x) - \alpha t \|g\|^2$.
        $$ f(x) - t\|g\|^2 + \frac{M t^2}{2}\|g\|^2 \le f(x) - \alpha t \|g\|^2 $$
        Subtracting $f(x)$ and dividing by $t\|g\|^2$ (assuming $g \neq 0$):
        $$ -1 + \frac{M t}{2} \le -\alpha \implies \frac{M t}{2} \le 1 - \alpha \implies t \le \frac{2(1-\alpha)}{M} $$
        Since $\alpha < 0.5$, the RHS is positive. Thus, for any $t \le \frac{2(1-\alpha)}{M}$, the Armijo condition <strong>must</strong> hold. The backtracking loop (which shrinks $t$ by $\beta$) will eventually produce a $t$ below this threshold.
      </p>
      <p>
        <strong>2. Lower Bound on Accepted Step Size:</strong>
        Let $t_0 = \frac{2(1-\alpha)}{M}$.
        <ul>
            <li>If the initial $t=1$ works, $t \ge 1$.</li>
            <li>If we backtrack, the loop stops at the <em>first</em> $t$ such that $t \le t_0$ (approximately). Since the previous step $t/\beta$ was rejected (meaning $t/\beta > t_0$), the accepted $t$ must satisfy $t > \beta t_0$.</li>
        </ul>
        Thus, the accepted step is bounded below:
        $$ t \ge \min\left(1, \frac{2\beta(1-\alpha)}{M}\right) $$
      </p>
      <p>
        <strong>3. Convergence Rate:</strong>
        The Armijo condition guarantees a decrease of at least $-\alpha t \|g\|^2$. Using the lower bound on $t$:
        $$ f(x^+) - f(x) \le -\alpha \min\left(1, \frac{2\beta(1-\alpha)}{M}\right) \|g\|^2 $$
        Combining this with the Strong Convexity property $\|\nabla f(x)\|^2 \ge 2m(f(x)-p^\star)$, we get:
        $$ f(x^+) - p^\star \le f(x) - p^\star - C \cdot 2m (f(x) - p^\star) = (1 - 2mC) (f(x) - p^\star) $$
        where $C$ is the constant derived above. This proves <strong>Linear Convergence</strong> for backtracking line search.
      </p>
      <pre><code class="language-python">
def backtracking_line_search(f, x, delta_x, gradient, alpha=0.3, beta=0.8):
    t = 1.0
    current_f = f(x)

    # Calculate the slope of the linear requirement line
    # gradient.dot(delta_x) is negative (descent).
    # We want a drop of at least alpha * t * slope.
    linear_term = alpha * gradient.dot(delta_x)

    while f(x + t * delta_x) > current_f + t * linear_term:
        t = t * beta # Shrink step
    return t
      </code></pre>
      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/backtracking_armijo.gif" alt="Backtracking line search visualization" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Backtracking reduces step size $t$ until the function value drops below the linear extrapolation (Armijo line).</figcaption>
      </figure>

      <h3>7. Steepest Descent</h3>
      <p>
        "Steepness" is not absolute; it depends on how you measure distance.
        Standard gradient descent moves in the direction $-\nabla f(x)$, which is the steepest direction <strong>only if distance is measured in the Euclidean norm ($\ell_2$)</strong>.
        <br><br>
        <strong>Definition:</strong> The normalized steepest descent direction $\Delta x_{\mathrm{nsd}}$ solves the problem:
        $$ \Delta x_{\mathrm{nsd}} = \text{argmin}_{v} \{ \nabla f(x)^T v \mid \|v\| \le 1 \} $$
        In plain English: "Among all steps of length 1, which one reduces the function value the most?"
      </p>

      <h4>7.1 The Role of Dual Norms</h4>
      <p>
        To solve the minimization problem above, we need the concept of <strong>Dual Norms</strong>.
        <br>
        Recall that for any norm $\|\cdot\|$, its dual norm $\|\cdot\|_*$ is defined as:
        $$ \|z\|_* = \sup \{z^T x \mid \|x\| \le 1\} $$
        This definition implies the generalized Cauchy-Schwarz inequality: $|z^T x| \le \|z\|_* \|x\|$.
        <br><br>
        Since we want to minimize $\nabla f(x)^T v$, we are looking for the vector $v$ that aligns most negatively with the gradient. The minimum value attained is exactly $-\|\nabla f(x)\|_*$.
      </p>

      <h4>7.2 Unnormalized Step and Cases</h4>
      <p>
        For convenience, we define the <strong>unnormalized</strong> steepest descent step $\Delta x_{\mathrm{sd}}$ by scaling the normalized direction by the magnitude of the gradient (measured in the dual norm):
        $$ \Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_* \Delta x_{\mathrm{nsd}} $$
        This ensures that the predicted drop in function value is:
        $$ \nabla f(x)^T \Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_* (\nabla f(x)^T \Delta x_{\mathrm{nsd}}) = \|\nabla f(x)\|_* (-\|\nabla f(x)\|_* ) = -\|\nabla f(x)\|_*^2 $$
      </p>

      <div class="card-grid" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 20px;">

        <!-- Case 1: L2 Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">1. Euclidean Norm ($\ell_2$)</h4>
            <p><strong>Norm:</strong> $\|v\|_2 = \sqrt{v^T v}$. <strong>Dual:</strong> $\|g\|_2$.</p>
            <p><strong>Geometry:</strong> The unit ball is a perfect sphere.</p>
            <p><strong>Direction:</strong> The vector that minimizes $g^T v$ over the sphere is simply opposite to $g$.
            $$ \Delta x_{\mathrm{sd}} = -\nabla f(x) $$</p>
            <p><strong>Result:</strong> Standard Gradient Descent. The gradient is perpendicular to the tangent plane of the level set.</p>
        </div>

        <!-- Case 2: Quadratic Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">2. Quadratic Norm (Preconditioning)</h4>
            <p><strong>Norm:</strong> $\|v\|_P = \sqrt{v^T P v}$ ($P \in S^n_{++}$). <strong>Dual:</strong> $\|g\|_{P^{-1}} = \sqrt{g^T P^{-1} g}$.</p>
            <p><strong>Geometry:</strong> The unit ball is an ellipsoid $\{v \mid v^T P v \le 1\}$.</p>
            <p><strong>Direction:</strong> The direction is skewed by the inverse of $P$.
            $$ \Delta x_{\mathrm{sd}} = -P^{-1}\nabla f(x) $$</p>
            <p><strong>Result:</strong> Preconditioned Gradient Descent. By choosing $P \approx \nabla^2 f(x)$, we align the "metric" with the function's curvature, effectively turning the elongated level sets into circles.</p>
            <figure style="margin: 12px 0; text-align: center;">
                <img src="assets/preconditioning_comparison.gif" alt="Preconditioned GD vs GD" style="width: 100%; border-radius: 4px;">
            </figure>
        </div>

        <!-- Case 3: L1 Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">3. $\ell_1$ Norm (Manhattan)</h4>
            <p><strong>Norm:</strong> $\|v\|_1 = \sum |v_i|$. <strong>Dual:</strong> $\|g\|_\infty = \max_i |g_i|$.</p>
            <p><strong>Geometry:</strong> The unit ball is a diamond (polytope) with corners on the axes.</p>
            <p><strong>Direction:</strong> The minimum of a linear function on a diamond occurs at a vertex. Let $i$ be the index where $|(\nabla f)_i|$ is maximal.
            $$ \Delta x_{\mathrm{sd}} = -\text{sign}\left( \frac{\partial f}{\partial x_i} \right) e_i $$
            </p>
            <p><strong>Result:</strong> Coordinate Descent (Greedy). We only update the <em>single</em> variable with the largest partial derivative.</p>
            <figure style="margin: 12px 0; text-align: center;">
                <img src="assets/l1_coordinate_descent_vs_gd.gif" alt="L1 Coordinate Descent vs GD" style="width: 100%; border-radius: 4px;">
            </figure>
        </div>
      </div>

      <h4>7.2 Convergence Analysis for Steepest Descent under General Norms</h4>
      <p>
        Does changing the norm change the convergence rate? Yes, but it's subtle. The rate is still linear, but the constant depends on how well the norm matches the geometry of the function.
        <br>
        <strong>1. Norm Equivalence:</strong> Since all norms on $\mathbb{R}^n$ are equivalent, there exist constants $\gamma, \tilde{\gamma} \in (0,1]$ such that:
        $$ \|u\| \ge \gamma \|u\|_2 \quad \text{and} \quad \|z\|_* \ge \tilde{\gamma} \|z\|_2 $$
        <strong>2. Smoothness Bound in General Norm:</strong>
        The standard smoothness bound uses Euclidean norm. We convert it:
        $$ f(x+v) \le f(x) + \nabla f(x)^T v + \frac{M}{2} \|v\|_2^2 \le f(x) + \nabla f(x)^T v + \frac{M}{2\gamma^2} \|v\|^2 $$
        <strong>3. Decrease Step:</strong>
        Taking the unnormalized steepest descent step $\Delta x_{\mathrm{sd}}$ with step size $t$:
        <ul>
            <li>$\nabla f^T \Delta x_{\mathrm{sd}} = -\|\nabla f\|_*^2$</li>
            <li>$\|\Delta x_{\mathrm{sd}}\| = \|\nabla f\|_*$</li>
        </ul>
        Plugging this into the bound:
        $$ f(x+t\Delta x_{\mathrm{sd}}) \le f(x) - t\|\nabla f\|_*^2 + \frac{M t^2}{2\gamma^2} \|\nabla f\|_*^2 = f(x) - \|\nabla f\|_*^2 \left( t - \frac{M t^2}{2\gamma^2} \right) $$
        Optimizing $t$ yields a guaranteed decrease proportional to $\|\nabla f\|_*^2$.
        <br>
        <strong>Conclusion:</strong> We still get linear convergence $f(x^{(k)}) - p^\star \le c^k (f(x^{(0)}) - p^\star)$. The constant $c$ depends on the condition number $M/m$ and the "norm distortion" constants $\gamma, \tilde{\gamma}$.
        <br><em>Key Takeaway:</em> If we choose a norm (like the quadratic norm defined by the Hessian) where $\gamma \approx 1$ and the condition number is small, convergence is lightning fast. This is the justification for Newton's method.
      </p>

      <h3>8. Newton's Method</h3>
      <p>
        Newton's method is the gold standard for unconstrained minimization. While Gradient Descent uses only slope (gradient) information, Newton's method uses <strong>curvature</strong> (Hessian) information to take much smarter, more accurate steps.
        $$ \Delta x_{\mathrm{nt}} = -\nabla^2 f(x)^{-1} \nabla f(x) $$
      </p>
      <h4>8.1 Three Interpretations</h4>
      <p>Why is this specific formula the "right" one? There are three powerful ways to view it.</p>
      <ol>
          <li><strong>Minimizing the Quadratic Model (Local Approximation):</strong>
          <br>We approximate $f$ near $x$ by its second-order Taylor expansion (a parabola):
          $$ \hat{f}(x+v) = f(x) + \nabla f(x)^T v + \frac{1}{2}v^T \nabla^2 f(x) v $$
          Instead of just moving "down", we jump directly to the bottom of this bowl.
          <br>To find the minimum, set the derivative with respect to $v$ to zero:
          $$ \nabla \hat{f}(v) = \nabla f(x) + \nabla^2 f(x) v = 0 \implies v = -\nabla^2 f(x)^{-1} \nabla f(x) $$
          <figure style="margin: 12px 0; text-align: center;">
            <img src="assets/newton_quadratic_model.gif" alt="Newton step minimizes local quadratic model" style="width: 100%; max-width: 500px; border-radius: 8px; border: 1px solid #ddd;">
            <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Newton's method approximates the function with a quadratic bowl and jumps to its minimum.</figcaption>
          </figure>
          </li>

          <li style="margin-top: 24px;"><strong>Steepest Descent in Hessian Norm (Geometry-Adaptive):</strong>
          <br>Recall that "steepest descent" depends on the norm. If we choose the norm based on the local curvature:
          $$ \|v\|_H = \sqrt{v^T \nabla^2 f(x) v} $$
          Then the steepest descent direction is exactly the Newton step.
          <br><strong>Interpretation:</strong> Newton's method resizes the coordinate system at every step to make the function look spherical. It turns long narrow valleys into nice round bowls locally.
          </li>

          <li style="margin-top: 24px;"><strong>Affine Invariance (Coordinate Independence):</strong>
          <br>This is a crucial property for robust software. If we simply rename our variables (e.g., measuring length in meters vs millimeters), Gradient Descent will change its path completely. Newton's Method will not.
          <br>Let $x = Ty$ be an affine change of coordinates.
          <br>
          <strong>Proof of Affine Invariance:</strong>
          Let $g(y) = f(Ty)$ be the function in the new coordinates.
          <br>
          1. <strong>Gradients:</strong> By chain rule, $\nabla g(y) = T^T \nabla f(x)$.
          <br>
          2. <strong>Hessians:</strong> $\nabla^2 g(y) = T^T \nabla^2 f(x) T$.
          <br>
          3. <strong>Newton Step for $g$:</strong>
          $$ \Delta y_{\mathrm{nt}} = -(\nabla^2 g(y))^{-1} \nabla g(y) = -(T^T \nabla^2 f(x) T)^{-1} (T^T \nabla f(x)) $$
          Using the matrix inverse property $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$:
          $$ = -T^{-1} (\nabla^2 f(x))^{-1} (T^{-T} T^T) \nabla f(x) $$
          $$ = -T^{-1} (\nabla^2 f(x))^{-1} \nabla f(x) = T^{-1} \Delta x_{\mathrm{nt}} $$
          Since $x = Ty$, a displacement of $\Delta y$ corresponds to a physical displacement of $T \Delta y = \Delta x$. The steps correspond perfectly!
          <br>
          <em>Contrast with Gradient Descent:</em> The gradient descent step is $\Delta y = -T^T \nabla f(x)$. This involves $T^T$, not $T^{-1}$. It does <strong>not</strong> transform correctly.
          </li>
      </ol>

      <h4>8.2 The Newton Decrement $\lambda(x)$</h4>
      <p>
        We define the <strong>Newton Decrement</strong> $\lambda(x)$ as:
        $$ \lambda(x) = \sqrt{\nabla f(x)^T (\nabla^2 f(x))^{-1} \nabla f(x)} $$
        This single number plays a dual role:
        <ul>
            <li><strong>Stopping Criterion:</strong> It is an affine-invariant measure of how close we are to the solution. Specifically, $\frac{1}{2}\lambda(x)^2$ is exactly the vertical distance between $f(x)$ and the minimum of the quadratic model. If $\lambda(x)^2/2 < 10^{-6}$, we are effectively done.</li>
            <li><strong>Directional Derivative:</strong> The slope of the function in the Newton direction is $-\lambda(x)^2$.
            $$ \nabla f(x)^T \Delta x_{\mathrm{nt}} = -\nabla f(x)^T (\nabla^2 f)^{-1} \nabla f(x) = -\lambda(x)^2 $$
            This guarantees it is always a descent direction (unless $\lambda=0$, meaning we are at the optimum).
            </li>
        </ul>
      </p>

      <h4>8.3 Implementation</h4>
      <pre><code class="language-python">
def newton_step(f, grad, hess, x):
    g = grad(x)
    H = hess(x)

    # Solve linear system H * dx = -g
    # Note: Use a solver (like Cholesky), never invert a matrix explicitly!
    delta_x = np.linalg.solve(H, -g)

    # Compute Newton Decrement for stopping
    # lambda^2 = g^T * H^-1 * g = g^T * (-dx)
    lambda_sq = -g.dot(delta_x)

    return delta_x, lambda_sq
      </code></pre>

      <h3>9. Convergence Analysis & Self-Concordance</h3>
      <p>
        Newton's method is famous for its speed, but analyzing it reveals a fascinating "two-phase" structure.
      </p>
      <h4>9.1 The Two Phases of Newton's Method</h4>
      <ul>
          <li><strong>Phase 1: Damped Newton (Far from $x^\star$):</strong>
          <br>When we are far from the solution, the quadratic approximation might be poor. The "full" Newton step ($t=1$) might overshoot or even increase the function value.
          <br>During this phase, <strong>backtracking line search</strong> is essential. It chooses a step size $t < 1$ to ensure stability.
          <br>The algorithm behaves like a solid descent method with a guaranteed constant decrease in function value at every step.
          </li>
          <li><strong>Phase 2: Quadratic Convergence (Close to $x^\star$):</strong>
          <br>Once we enter the "basin of attraction" (specifically, when the Newton decrement $\lambda(x) < 0.68$ for self-concordant functions), magic happens.
          <br>The backtracking line search <strong>always</strong> selects $t=1$.
          <br>The algorithm converges <strong>quadratically</strong>. This means the number of correct digits <strong>doubles</strong> at every iteration. (e.g., error goes $10^{-2} \to 10^{-4} \to 10^{-8} \to 10^{-16}$).
          </li>
      </ul>

      <div style="background: var(--code-bg); padding: 12px; border-left: 4px solid var(--accent); margin-bottom: 24px;">
        <strong>Derivation of Quadratic Convergence (Proof Sketch):</strong>
        <br>
        Assume the Hessian is Lipschitz continuous ($\|H(y)-H(x)\| \le L\|y-x\|$).
        <br>1. <strong>Residual Bound:</strong> By analyzing the error in the gradient update $\nabla f(x_{k+1})$, one can show:
        $$ \|\nabla f(x_{k+1})\|_2 \le \frac{L}{2m^2} \|\nabla f(x_k)\|_2^2 $$
        <br>2. <strong>Implication:</strong> This inequality has the form $u_{k+1} \le C u_k^2$. If $u_k$ is small enough (specifically $< 1/C$), squaring it makes it vanish incredibly fast.
      </div>

      <h4>9.2 Self-Concordance: Removing the Unknown Constants</h4>
      <p>
        Classical convergence proofs rely on constants like $m$ (strong convexity) and $L$ (Lipschitz Hessian). These constants are often unknown, hard to estimate, and depend on the units of measurement (not affine invariant).
        <br><br>
        <strong>The Solution:</strong> Nesterov and Nemirovski introduced <strong>Self-Concordance</strong>. This is a property that limits how fast the curvature changes <em>relative to the curvature itself</em>.
      </p>
      <div style="background: var(--code-bg); padding: 12px; border-left: 4px solid var(--accent); margin-bottom: 16px;">
        <strong>Definition (1D):</strong> A convex function $\phi: \mathbb{R} \to \mathbb{R}$ is self-concordant if:
        $$ |\phi'''(x)| \le 2 (\phi''(x))^{3/2} $$
        <strong>Affine Invariance:</strong> If $f$ is self-concordant, then any affine transformation $g(y) = f(Ay+b)$ is also self-concordant.
      </div>
      <p>
        <strong>Why it matters:</strong>
        <ol>
            <li><strong>Universal Constants:</strong> For <em>any</em> self-concordant function, Newton's method is guaranteed to enter the quadratic convergence phase once $\lambda(x) < 0.68$. We don't need to know $m$ or $L$!</li>
            <li><strong>Complexity Theory:</strong> It allows us to prove that the number of Newton steps required to solve a problem depends only on the <em>structure</em> of the problem (e.g., number of log-barrier terms), not on the data values. This is the foundation of <strong>Interior Point Methods</strong>.</li>
            <li><strong>Examples:</strong> Linear and quadratic functions (3rd derivative is 0) are self-concordant. The logarithmic barrier $-\sum \log x_i$ is self-concordant.</li>
        </ol>
      </p>

      <h4>9.3 Quasi-Newton Methods (BFGS)</h4>
      <p>
        For very large problems ($n > 1000$), storing the Hessian matrix ($n^2$ floats) and solving the linear system ($n^3$ flops) is too expensive.
        <strong>Quasi-Newton methods</strong> aim to replicate the superlinear speed of Newton's method without computing the Hessian explicitly.
        <br>
        <strong>The Secant Equation:</strong>
        We want to build an approximation $B_{k+1} \approx \nabla^2 f(x_{k+1})$ based on the information gained from the last step.
        Let $s_k = x_{k+1} - x_k$ (change in position) and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$ (change in gradient).
        By Taylor's theorem, $\nabla f(x_{k+1}) \approx \nabla f(x_k) + \nabla^2 f(x_{k+1}) (x_{k+1} - x_k)$.
        Thus, we require our approximation $B_{k+1}$ to satisfy the <strong>Secant Equation</strong>:
        $$ B_{k+1} s_k = y_k $$
        <br>
        <strong>BFGS (Broyden–Fletcher–Goldfarb–Shanno):</strong>
        This is the most popular update rule. It updates the <em>inverse</em> Hessian approximation $H_k \approx (\nabla^2 f)^{-1}$ directly, which allows us to find the search direction $\Delta x = -H_k \nabla f(x)$ in $O(n^2)$ time.
        $$ H_{k+1} = (I - \rho_k s_k y_k^T) H_k (I - \rho_k y_k s_k^T) + \rho_k s_k s_k^T, \quad \text{where } \rho_k = \frac{1}{y_k^T s_k} $$
        <ul>
            <li><strong>Cost:</strong> $O(n^2)$ per iteration (matrix-vector multiplication).</li>
            <li><strong>Convergence:</strong> Superlinear (faster than linear, slower than quadratic). It typically finds the solution in very few steps compared to GD.</li>
            <li><strong>Memory:</strong> L-BFGS (Limited-memory BFGS) stores only the last $m$ pairs of $(s_k, y_k)$ vectors, reducing memory to $O(mn)$, enabling optimization with millions of variables (e.g., Deep Learning).</li>
        </ul>
      </p>

    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Gradient Descent Visualizer</h3>
        <p>Visualize the trajectory of gradient descent for a 2D quadratic function.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- Widget 4: Norm Steepest Descent -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Steepest Descent Geometry</h3>
        <p>Visualizing how the norm determines the "steepest" direction. L2 steepest opposes the gradient. L1 steepest snaps to an axis (coordinate descent).</p>
        <div id="widget-norm-steepest" style="width: 100%; height: auto; position: relative; text-align: center;"></div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">GD vs. Newton Race</h3>
        <p>Compare the convergence of gradient descent and Newton's method.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- Widget 3 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Convergence Rate Comparison</h3>
        <p>Plots the convergence rates of different first-order methods.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Solved Exercises & Example Problems</h2>

      <div class="example-problem">
        <h3>Exercise 9.1: Minimizing a Quadratic Function (Detailed)</h3>
        <p><strong>Problem:</strong> Consider minimizing $f(x)=\frac{1}{2}x^T P x+q^T x+r$ where $P \in S^n$.
        <br>(a) Show that if $P \not\succeq 0$, the problem is unbounded below.
        <br>(b) Suppose $P \succeq 0$ but $Px = -q$ has no solution. Show it is unbounded below.</p>

        <p><strong>Step-by-Step Solution:</strong></p>
        <p><strong>Part (a): $P \not\succeq 0$ (Negative Eigenvalue).</strong>
        <br>1. <strong>Eigen Decomposition:</strong> If $P$ is not positive semidefinite, there exists at least one eigenvector $v$ with a negative eigenvalue $\lambda < 0$. Thus, $v^T P v = \lambda \|v\|^2 < 0$.
        <br>2. <strong>Construct Ray:</strong> Consider the path $x(t) = tv$ for $t > 0$.
        <br>3. <strong>Evaluate Function:</strong>
        $$ f(tv) = \frac{1}{2}t^2 (v^T P v) + t(q^T v) + r $$
        <br>4. <strong>Limit:</strong> As $t \to \infty$, the term $t^2 (v^T P v)$ is negative and grows quadratically. The linear term $t(q^T v)$ grows slower.
        <br>Thus, $f(tv) \to -\infty$. The function has no minimum.
        </p>

        <p><strong>Part (b): $P \succeq 0$ but $q \notin \mathcal{R}(P)$.</strong>
        <br>1. <strong>Linear Algebra Fact:</strong> Since $P$ is symmetric, the nullspace $\mathcal{N}(P)$ is orthogonal to the range $\mathcal{R}(P)$. Any vector $q$ can be decomposed into $q_{\text{range}} + q_{\text{null}}$. If $q \notin \mathcal{R}(P)$, then $q_{\text{null}} \neq 0$.
        <br>2. <strong>Pick Direction:</strong> Let $z = -q_{\text{null}}$. Note that $Pz = 0$ (by definition of nullspace).
        <br>3. <strong>Evaluate Function along $x(t) = tz$:</strong>
        $$ f(tz) = \frac{1}{2}t^2 (z^T P z) + t(q^T z) + r $$
        Since $Pz = 0$, the quadratic term vanishes ($z^T P z = z^T 0 = 0$).
        $$ f(tz) = 0 + t(q^T z) + r $$
        <br>4. <strong>Analyze Slope:</strong> $q^T z = (q_{\text{range}} + q_{\text{null}})^T (-q_{\text{null}}) = -q_{\text{range}}^T q_{\text{null}} - \|q_{\text{null}}\|^2$.
        Since range and nullspace are orthogonal, $q_{\text{range}}^T q_{\text{null}} = 0$.
        Thus, $q^T z = -\|q_{\text{null}}\|^2 < 0$.
        <br>5. <strong>Limit:</strong> As $t \to \infty$, $f(tz) \to -\infty$ linearly.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.2: Quadratic-over-Linear Fraction</h3>
        <p><strong>Problem:</strong> Minimize $f(x) = \frac{\|Ax-b\|_2^2}{c^T x + d}$ subject to $c^T x + d > 0$.
        <br>Assumptions: $\text{rank}(A)=n$ and $b \notin \mathcal{R}(A)$.
        <br><strong>(a) Show $f$ is closed.</strong>
        <br><strong>(b) Show the minimizer lies on a specific line.</strong>
        </p>
        <p><strong>Solution:</strong>
        <br><strong>Part (a):</strong>
        Since $b \notin \mathcal{R}(A)$, the numerator $N(x) = \|Ax-b\|^2$ is always strictly positive (bounded below by $\text{dist}(b, \mathcal{R}(A))^2 > 0$).
        Consider the sublevel set $S_\alpha = \{x \mid f(x) \le \alpha\}$.
        As $x$ approaches the boundary where $c^T x + d \to 0$, the denominator vanishes while the numerator stays positive. Thus $f(x) \to +\infty$.
        This means no sequence in $S_\alpha$ can approach the boundary. The sublevel sets are closed.
        <br><br>
        <strong>Part (b):</strong>
        Let $x_1 = (A^T A)^{-1} A^T b$. This is the unconstrained minimizer of the numerator (Least Squares solution).
        Let $x_2 = (A^T A)^{-1} c$. This vector is related to the gradient of the denominator transformed by the Hessian of the numerator.
        <br>
        Let $P = A^T A$. The function is $f(x) = \frac{\delta^2 + (x-x_1)^T P (x-x_1)}{c^T x + d}$.
        Using a change of variables $y = x - x_1$, we essentially minimize $\frac{\delta^2 + y^T P y}{\alpha + c^T y}$.
        The Cauchy-Schwarz inequality (applied in the $P$-norm) implies that for any fixed value of the denominator, the numerator is minimized when $y$ is parallel to $P^{-1} c$.
        Thus, the optimal $x^\star$ must lie on the line passing through $x_1$ in the direction of $x_2$.
        The problem reduces to minimizing a scalar function $\phi(t)$ along this line.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.5: Backtracking Line Search Proof (Rigorous)</h3>
        <p><strong>Problem:</strong> Prove that if $0 < t \le -\frac{\nabla f(x)^T \Delta x}{M\|\Delta x\|_2^2}$, the backtracking condition holds for $\alpha \le 0.5$.</p>
        <p><strong>Proof:</strong></p>
        <ol>
            <li><strong>Smoothness Upper Bound:</strong> Start with the M-smoothness bound (9.13) along direction $\Delta x$:
            $$ f(x+t\Delta x) \le f(x) + t\nabla f^T \Delta x + \frac{M}{2}t^2\|\Delta x\|^2 $$
            </li>
            <li><strong>Substitute $t$ bound:</strong> We are given $t \le \frac{-\nabla f^T \Delta x}{M\|\Delta x\|^2}$. Multiply by $\frac{M}{2}t\|\Delta x\|^2$:
            $$ \frac{M}{2}t^2\|\Delta x\|^2 \le \frac{1}{2}t (-\nabla f^T \Delta x) $$
            </li>
            <li><strong>Plug into Upper Bound:</strong> Substitute this inequality back into Step 1:
            $$ f(x+t\Delta x) \le f(x) + t\nabla f^T \Delta x - \frac{1}{2}t \nabla f^T \Delta x $$
            $$ f(x+t\Delta x) \le f(x) + \frac{1}{2}t\nabla f^T \Delta x $$
            </li>
            <li><strong>Compare to Armijo:</strong> The Armijo condition is $f(x+t\Delta x) \le f(x) + \alpha t \nabla f^T \Delta x$.
            We just proved it holds for $\alpha = 0.5$.
            Since $\nabla f^T \Delta x < 0$ (descent), adding $\alpha t \nabla f^T \Delta x$ with a <em>smaller</em> $\alpha$ (e.g., 0.1) results in a larger (easier to satisfy) RHS.
            $$ f(x) + 0.5 t \nabla f^T \Delta x \le f(x) + \alpha t \nabla f^T \Delta x \quad (\text{for } \alpha \le 0.5) $$
            Thus, the condition holds.
            </li>
        </ol>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.7: Steepest Descent Identities</h3>
        <p><strong>Problem:</strong> Prove (a) $\nabla f^T \Delta x_{\mathrm{nsd}} = -\|\nabla f\|_*$ and (b) $\nabla f^T \Delta x_{\mathrm{sd}} = -\|\nabla f\|_*^2$.</p>
        <p><strong>Proof:</strong>
        <br><strong>(a):</strong> By definition, $\Delta x_{\mathrm{nsd}} = \text{argmin}_{\|v\| \le 1} g^T v$.
        Recall $\|g\|_* = \sup_{\|v\| \le 1} g^T v$.
        Since the unit ball is symmetric ($v \in B \iff -v \in B$), $\inf g^T v = -\sup g^T (-v) = -\sup g^T v = -\|g\|_*$.
        <br><strong>(b):</strong> Since $\Delta x_{\mathrm{sd}} = \|\nabla f\|_* \Delta x_{\mathrm{nsd}}$, we have:
        $$ g^T \Delta x_{\mathrm{sd}} = \|\nabla f\|_* (g^T \Delta x_{\mathrm{nsd}}) = \|\nabla f\|_* (-\|\nabla f\|_*) = -\|\nabla f\|_*^2 $$
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.8: Steepest Descent in $\ell_\infty$ Norm</h3>
        <p><strong>Problem:</strong> Find the normalized steepest descent direction for the $\ell_\infty$ norm.</p>
        <p><strong>Solution:</strong>
        <br>We solve $\min \sum g_i v_i$ subject to $\|v\|_\infty \le 1$ (i.e., $-1 \le v_i \le 1$).
        <br>The sum is minimized when each term $g_i v_i$ is minimized independently.
        <ul>
            <li>If $g_i > 0$, choose $v_i = -1$.</li>
            <li>If $g_i < 0$, choose $v_i = +1$.</li>
            <li>If $g_i = 0$, $v_i$ can be anything (say 0).</li>
        </ul>
        Thus, $(\Delta x_{\mathrm{nsd}})_i = -\text{sign}(g_i)$.
        <br>The unnormalized direction is $\Delta x_{\mathrm{sd}} = -\|g\|_1 \text{sign}(g)$, since the dual norm of $\ell_\infty$ is $\ell_1$.
        <br><strong>Interpretation:</strong> This is a "coordinate-saturated" step. We push every variable as far as possible against the gradient sign.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.15: Proving Self-Concordance</h3>
        <p><strong>Problem:</strong> Prove the following functions are self-concordant by restricting to a line.
        <br>(a) $f(x,y)=-\log(y^2-x^T x)$ on $\{(x,y)\mid \|x\|_2<y\}$.
        <br>(b) $f(x,y)=-2\log y-\log(y^{2/p}-x^2)$, $p\ge 1$.
        <br>(c) $f(x,y)=-\log y-\log(\log y-x)$.
        </p>
        <p><strong>Solution Strategy (Composition with Logarithm Rule):</strong>
        <br>A function $\phi(t) = -\log(-g(t)) - \log t$ is self-concordant if $|g'''(t)| \le 3g''(t)/t$.
        <br><strong>(a):</strong> $f(x,y) = -\log y - \log(y - x^T x/y)$. Restricting to a line $x(t) = \hat{x} + tv, y(t) = \hat{y} + tw$ yields a form $-\log(\text{quadratic}) - \log(\text{linear})$.
        <br>Specifically, if we reparameterize by $y$, the inner term becomes $g(y) = -\alpha - \beta y + \gamma/y$ with $\gamma \ge 0$.
        Checking derivatives: $g''(y) = 2\gamma/y^3, g'''(y) = -6\gamma/y^4$.
        The condition $|g'''| \le 3g''/y$ becomes $6\gamma/y^4 \le 3(2\gamma/y^3)/y$, which holds with equality.
        <br><strong>(b):</strong> Factor $y^{2/p}-x^2 = (y^{1/p}-x)(y^{1/p}+x)$.
        $f = (-\log y - \log(y^{1/p}-x)) + (-\log y - \log(y^{1/p}+x))$.
        Restricting to a line and reparameterizing by $y$ gives terms like $g(y) = -Ay - B - y^{1/p}$.
        Derivatives check out for $p \ge 1$: condition is $(2p-1)/p \le 3 \iff 2p-1 \le 3p \iff p \ge -1$.
        <br><strong>(c):</strong> Restricting to a line yields $-\log y - \log(-g(y))$ where $g(y) = a+by-\log y$.
        $g'' = 1/y^2, g''' = -2/y^3$.
        Condition $|-2/y^3| \le 3(1/y^2)/y$ holds ($2 \le 3$).
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.2: Newton's Method for Logistic Regression</h3>
        <p><strong>Problem:</strong> Derive the Newton step for Logistic Regression explicitly.</p>
        <p><strong>Solution:</strong></p>
        <p><strong>Objective:</strong> $f(w) = \sum_{i=1}^m \log(1 + \exp(w^T x_i)) - y_i w^T x_i$.
        <br>Let $z_i = w^T x_i$. The sigmoid function is $p_i(w) = \sigma(z_i) = \frac{1}{1+e^{-z_i}}$.
        <br><strong>Gradient:</strong>
        $$ \frac{\partial f}{\partial w} = \sum_{i=1}^m (p_i - y_i) x_i = X^T (p - y) $$
        <br><strong>Hessian:</strong>
        $$ \frac{\partial^2 f}{\partial w^2} = \sum_{i=1}^m \frac{\partial p_i}{\partial z_i} x_i x_i^T = \sum_{i=1}^m p_i(1-p_i) x_i x_i^T = X^T D X $$
        where $D$ is diagonal with $D_{ii} = p_i(1-p_i)$.
        <br><strong>Newton Step:</strong>
        $$ \Delta w = -(X^T D X)^{-1} X^T (p - y) $$
        This formula is the basis for the <strong>Iteratively Reweighted Least Squares (IRLS)</strong> algorithm used by standard statistical packages.
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.3: Steepest Descent in $\ell_\infty$ Norm</h3>
        <p><strong>Problem:</strong> Explicitly derive the steepest descent direction for the $\ell_\infty$ norm.</p>
        <p><strong>Solution:</strong></p>
        <p>We solve $\min \nabla f^T v$ s.t. $\|v\|_\infty \le 1$.
        The objective is $\sum_{i=1}^n (\nabla f)_i v_i$.
        Since the variables $v_i$ are constrained independently ($|v_i| \le 1$), we can minimize the sum by minimizing each term separately.
        To minimize $c_i v_i$ subject to $v_i \in [-1, 1]$:
        <ul>
            <li>If $c_i > 0$, set $v_i = -1$.</li>
            <li>If $c_i < 0$, set $v_i = +1$.</li>
            <li>If $c_i = 0$, $v_i$ is arbitrary (usually 0).</li>
        </ul>
        Thus, $v_i = -\text{sign}((\nabla f)_i)$.
        <strong>Geometric Insight:</strong> In the $\ell_\infty$ "box" world, the fastest way down is to go to a corner of the hypercube.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.15: Proving Self-Concordance</h3>
        <p><strong>Problem:</strong> Prove $f(x,y) = -\log(y^2 - x^T x)$ is self-concordant.</p>
        <p><strong>Solution Strategy:</strong></p>
        <p>We use the <strong>barrier method</strong> logic.
        1. <strong>Restrict to a line:</strong> $x(t) = \hat{x} + tv$, $y(t) = \hat{y} + tw$.
        2. <strong>Composition with Log:</strong> Use the property that $-\log(-g(t)) - \log t$ is self-concordant if $|g'''| \le 3g''/t$.
        3. <strong>Algebra:</strong> Factor $y^2 - x^T x$ into geometric components.
        For $f(x,y)$, restricting to a line yields a function of the form $-\log(\text{quadratic})$.
        Since $-\log(y)$ is SC and affine transformations preserve SC, the result holds.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.30: Analytic Center (Gradient vs Newton)</h3>
        <p><strong>Problem:</strong> Compute the analytic center of $a_i^T x \le 1, |x_i| \le 1$ using Gradient Descent and Newton's Method. Compare convergence.</p>
        <p><strong>Objective:</strong> $f(x) = -\sum_{i=1}^m \log(1-a_i^T x) - \sum_{j=1}^n \log(1-x_j^2)$.</p>
        <p><strong>Gradient Derivation:</strong>
        <br>Term 1: $-\log(1-a_i^T x)$. Gradient is $\frac{a_i}{1-a_i^T x}$. Sum is $\sum \frac{a_i}{1-a_i^T x}$.
        <br>Term 2: $-\log(1-x_j^2)$. Partial derivative is $\frac{2x_j}{1-x_j^2}$. Vector is $\frac{2x}{1-x^2}$ (elementwise).
        <br>$\nabla f(x) = A^T \text{diag}(1/s) \mathbf{1} + \frac{2x}{1-x^2}$ where $s = 1 - Ax$.
        </p>
        <p><strong>Hessian Derivation:</strong>
        <br>Term 1: $\nabla (\frac{a_i}{1-a_i^T x}) = \frac{a_i a_i^T}{(1-a_i^T x)^2}$. Sum is $A^T \text{diag}(1/s^2) A$.
        <br>Term 2: Diagonal with entries $\frac{2(1+x_j^2)}{(1-x_j^2)^2}$.
        </p>

        <p><strong>Python Implementation:</strong></p>
<pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

def solve_analytic_center(m=300, n=50):
    np.random.seed(1)
    A = np.random.randn(m, n) * 0.6  # Scale for feasibility at 0

    def f_val(x):
        s = 1.0 - A @ x
        t = 1.0 - x**2
        if np.any(s <= 0) or np.any(t <= 0): return np.inf
        return -np.sum(np.log(s)) - np.sum(np.log(t))

    def grad(x):
        s = 1.0 - A @ x
        return A.T @ (1.0/s) + 2*x/(1.0 - x**2)

    def hess(x):
        s = 1.0 - A @ x
        w = 1.0/(s**2)
        H = A.T @ (A * w[:, None])
        diag2 = 2.0 * (1.0 + x**2) / ((1.0 - x**2)**2)
        return H + np.diag(diag2)

    def backtracking(x, dx, g, alpha=0.01, beta=0.5):
        t = 1.0
        fx = f_val(x)
        g_dot_dx = g @ dx
        while True:
            xn = x + t * dx
            if f_val(xn) <= fx + alpha * t * g_dot_dx: return t
            t *= beta

    # Newton Loop
    x = np.zeros(n)
    newton_hist = []
    for _ in range(60):
        g = grad(x)
        H = hess(x)
        dx = np.linalg.solve(H, -g)
        lambda_sq = -g @ dx
        newton_hist.append(lambda_sq)
        if lambda_sq/2 < 1e-8: break
        t = backtracking(x, dx, g, alpha=0.25)
        x += t * dx

    return newton_hist
</code></pre>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.31: Approximate Newton Methods</h3>
        <p><strong>Problem:</strong> Compare full Newton with (a) Re-using Hessian every $N$ steps, and (b) Diagonal approximation.</p>
        <p><strong>Code & Analysis:</strong></p>
<pre><code class="language-python">
# Diagonal Newton: Use only diagonal of Hessian
# H_diag = np.sum((A**2).T * (1/s**2), axis=1) + 2*(1+x**2)/(t**2)
# dx = -g / H_diag

# Reuse Hessian: Compute H only if k % N == 0
# L = cholesky(H)
# dx = solve(L.T, solve(L, -g))
</code></pre>
        <p><strong>Results:</strong>
        <br><strong>Newton:</strong> Converges in ~15 iterations. Quadratic phase is visible.
        <br><strong>Reuse Hessian ($N=5$):</strong> Converges but takes more iterations. Directions become "stale," leading to smaller steps in line search.
        <br><strong>Diagonal:</strong> Much faster per iteration ($O(n)$ vs $O(n^3)$), but convergence is linear and depends heavily on coordinate coupling.
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.5: BFGS Update Rank</h3>
        <p><strong>Problem:</strong> Analyze the rank of the BFGS update matrix and explain why it preserves symmetry.</p>
        <p><strong>Step-by-Step Solution:</strong></p>
        <p>The standard BFGS update for the Hessian approximation $B_{k+1}$ (not inverse) is:
        $$ B_{k+1} = B_k + \frac{y y^T}{y^T s} - \frac{B_k s s^T B_k}{s^T B_k s} $$
        where $s = x_{k+1} - x_k$ and $y = \nabla f(x_{k+1}) - \nabla f(x_k)$.
        <ol>
            <li><strong>Structure of Term 1:</strong> The term $\frac{y y^T}{y^T s}$ is an <strong>outer product</strong> of the vector $y$ with itself.
                <br>For any vector $u$, $u u^T$ is a matrix of Rank 1. (Every column is a multiple of $u$).
            </li>
            <li><strong>Structure of Term 2:</strong> The term $\frac{(B_k s) (B_k s)^T}{s^T B_k s}$ involves the vector $v = B_k s$.
                <br>It can be written as $\frac{v v^T}{c}$. This is also a Rank 1 matrix.
            </li>
            <li><strong>Total Rank:</strong> The update adds two rank-1 matrices to $B_k$.
                $$ \text{rank}(B_{k+1} - B_k) \le \text{rank}(U) + \text{rank}(V) = 2 $$
                Therefore, BFGS is a <strong>Rank-2 Update</strong>.
            </li>
            <li><strong>Symmetry:</strong> Since $B_k$ is symmetric, and both $y y^T$ and $v v^T$ are symmetric (outer products are always symmetric), the sum $B_{k+1}$ remains symmetric.
            </li>
            <li><strong>Positive Definiteness:</strong> It can be proven that if $B_k \succ 0$ and $y^T s > 0$ (curvature condition), then $B_{k+1} \succ 0$.
            </li>
        </ol>
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.6: Convergence Rate Comparison ($f(x)=x^4$)</h3>
        <p><strong>Problem:</strong> Compare the convergence of Gradient Descent and Newton's method on the strictly convex function $f(x) = x^4$. Note that $f''(0) = 0$, so it is not strongly convex at the optimum.</p>
        <p><strong>Analysis:</strong></p>
        <p><strong>1. Gradient Descent:</strong>
        <br>Gradient $f'(x) = 4x^3$. Update rule with step size $t$:
        $$ x_{k+1} = x_k - t(4x_k^3) = x_k(1 - 4t x_k^2) $$
        For small $x_k$, the term $4tx_k^2$ is tiny.
        The ratio $|x_{k+1} / x_k| \approx 1$. The contraction is extremely slow (sublinear). It takes a huge number of steps to converge.
        </p>
        <p><strong>2. Newton's Method:</strong>
        <br>Hessian $f''(x) = 12x^2$. Newton step $\Delta x_{nt} = -f'(x)/f''(x)$:
        $$ \Delta x_{nt} = -\frac{4x^3}{12x^2} = -\frac{1}{3}x $$
        Update rule (with step $t=1$):
        $$ x_{k+1} = x_k - \frac{1}{3}x_k = \frac{2}{3}x_k $$
        <br><strong>Convergence Rate:</strong>
        $$ x_k = \left(\frac{2}{3}\right)^k x_0 $$
        This is <strong>Linear Convergence</strong> with a constant factor $2/3$.
        <br><strong>Key Insight:</strong> Even though $f$ is not strongly convex (so standard theorems don't guarantee quadratic convergence), Newton's method is still affine invariant and handles the curvature much better than GD. However, the "singularity" at $x=0$ prevents the quadratic "doubling of digits" phase.
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.7: Nesterov Acceleration</h3>
        <p><strong>Problem:</strong> Explain the difference between "Heavy Ball" momentum and Nesterov momentum mathematically and intuitively.</p>
        <p><strong>Solution:</strong></p>
        <p>Let $\beta \in [0,1)$ be the momentum parameter (friction).
        <br>
        <strong>1. Heavy Ball (Polyak) Momentum:</strong>
        $$ x_{k+1} = \underbrace{x_k - \alpha \nabla f(x_k)}_{\text{GD Step}} + \underbrace{\beta(x_k - x_{k-1})}_{\text{Momentum}} $$
        We take a step based on the <em>current</em> gradient, then add the old velocity.
        <br>
        <strong>2. Nesterov Accelerated Gradient (NAG):</strong>
        Nesterov's insight: If we know we are going to move in the direction of momentum, why compute the gradient at the old spot?
        Let's "look ahead" to where the momentum will take us.
        <ol>
            <li><strong>Lookahead:</strong> $y_k = x_k + \beta(x_k - x_{k-1})$</li>
            <li><strong>Correct:</strong> $x_{k+1} = y_k - \alpha \nabla f(y_k)$</li>
        </ol>
        <strong>Intuition:</strong> Imagine rolling a ball down a hill.
        <ul>
            <li><strong>Heavy Ball:</strong> You look at the slope where you are standing ($x_k$), calculate the vector, and add it to your current velocity. If the valley curves sharply ahead, you might overshoot because your velocity vector is "stale."</li>
            <li><strong>Nesterov:</strong> You first project yourself to where your momentum will carry you ($y_k$). You check the slope <em>there</em>. If the slope at $y_k$ points back uphill, you assume a "correction" immediately, preventing the overshoot.</li>
        </ul>
        This subtle change allows Nesterov to achieve the optimal $O(1/k^2)$ convergence rate for convex functions, beating the $O(1/k)$ of GD.
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.8: The Newton Decrement</h3>
        <p><strong>Problem:</strong> Prove that $\frac{1}{2}\lambda(x)^2 = f(x) - \inf_v \hat{f}(x+v)$, where $\hat{f}$ is the quadratic approximation.</p>
        <p><strong>Proof:</strong></p>
        <ol>
            <li><strong>Quadratic Model:</strong> The second-order Taylor expansion at $x$ is:
            $$ \hat{f}(x+v) = f(x) + \nabla f(x)^T v + \frac{1}{2} v^T \nabla^2 f(x) v $$
            </li>
            <li><strong>Minimizing the Model:</strong> To find the minimum of this quadratic bowl, we set the gradient w.r.t $v$ to zero:
            $$ \nabla \hat{f} = \nabla f(x) + \nabla^2 f(x) v = 0 \implies v^\star = -\nabla^2 f(x)^{-1} \nabla f(x) = \Delta x_{nt} $$
            So the Newton step <em>is</em> the minimizer of the model.
            </li>
            <li><strong>Minimum Value:</strong> Substitute $v^\star$ back into the model:
            $$ \inf_v \hat{f} = f(x) + \nabla f^T v^\star + \frac{1}{2} (v^\star)^T \nabla^2 f v^\star $$
            Substitute $v^\star = -H^{-1}g$:
            $$ = f(x) + g^T (-H^{-1}g) + \frac{1}{2} (-H^{-1}g)^T H (-H^{-1}g) $$
            $$ = f(x) - g^T H^{-1} g + \frac{1}{2} g^T H^{-1} H H^{-1} g $$
            $$ = f(x) - \lambda(x)^2 + \frac{1}{2} g^T H^{-1} g $$
            $$ = f(x) - \lambda(x)^2 + \frac{1}{2} \lambda(x)^2 $$
            $$ = f(x) - \frac{1}{2}\lambda(x)^2 $$
            </li>
            <li><strong>Conclusion:</strong> Rearranging gives:
            $$ f(x) - \inf_v \hat{f}(x+v) = \frac{1}{2}\lambda(x)^2 $$
            The quantity $\lambda^2/2$ is exactly the <strong>predicted reduction</strong> in the function value based on the quadratic model. This makes it an excellent stopping criterion.
            </li>
        </ol>
      </div>

    </section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 9 — Unconstrained Minimization</li>
        <li><strong>Detailed Lecture Notes:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">PDF (Chapter 9)</a></li>
      </ul>
    </section>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="../../static/lib/pyodide/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initGradientDescentVisualizer } from './widgets/js/gradient-descent-visualizer.js';
    initGradientDescentVisualizer('widget-1');
  </script>
  <script type="module">
    import { initGDvsNewton } from './widgets/js/gd-vs-newton.js';
    initGDvsNewton('widget-2');
  </script>
  <script type="module">
    import { initConvergenceRate } from './widgets/js/convergence-rate.js';
    initConvergenceRate('widget-3');
  </script>
  <script type="module">
    import { initNormSteepest } from './widgets/js/norm-steepest.js';
    initNormSteepest('widget-norm-steepest');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/resizable.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
