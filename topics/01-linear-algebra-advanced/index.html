<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture bridges the gap between basic linear algebra and the robust numerical tools used in modern optimization. We move beyond the Normal Equations to explore numerically stable factorizations like QR and the Singular Value Decomposition (SVD), which reveal the geometry of linear maps in their purest form. We also introduce the Moore-Penrose pseudoinverse for solving ill-posed or rank-deficient problems and define the condition number—the fundamental metric for analyzing the stability of algorithms and the convergence speed of gradient descent.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm and matrix completion in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The condition number is the key parameter determining the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a> and motivates the use of Newton's Method.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD) as fundamental tools for numerical linear algebra.</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming the potentially ill-conditioned matrix $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions for rank-deficient or underdetermined systems.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations in data.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA) using truncated SVD.</li>
      </ul>
    </section>

    <!-- SECTION 1: ABSTRACT VECTOR SPACES -->
    <section class="section-card" id="section-vector-spaces">
      <h2>1. Abstract Vector Spaces</h2>
      <p>In Lecture 00, we worked with $\mathbb{R}^n$, where vectors were tuples. Here we abstract that away. A <b>Vector Space</b> is defined not by what vectors <i>are</i>, but by what you can <i>do</i> with them. This "conceptual compression" reveals that matrices, functions, and polynomials are all the same kind of object.</p>

      <h3>1.1 The Axiomatic Definition</h3>
      <p>A vector space over a field $\mathbb{F}$ is a set $V$ equipped with two operations—addition ($+$) and scalar multiplication ($\cdot$)—that satisfy the following 8 axioms for all $u,v,w \in V$ and $\alpha, \beta \in \mathbb{F}$. These axioms are exactly what is needed to make linear combinations behave predictably.</p>

      <ul>
        <li><b>Additivity ($V$ is an Abelian Group):</b>
          <ul>
            <li><b>Closure:</b> $u+v \in V$.</li>
            <li><b>Associativity & Commutativity:</b> $(u+v)+w = u+(v+w)$ and $u+v = v+u$.</li>
            <li><b>Identity:</b> There exists a unique $\mathbf{0} \in V$ such that $v+\mathbf{0}=v$.</li>
            <li><b>Inverses:</b> For every $v$, there exists a unique $-v$ such that $v+(-v)=\mathbf{0}$.</li>
          </ul>
        </li>
        <li><b>Scalar Structure:</b>
          <ul>
            <li><b>Compatibility:</b> $\alpha(\beta v) = (\alpha\beta)v$. (Scaling twice matches multiplying scalars).</li>
            <li><b>Identity:</b> $1 \cdot v = v$. (Scaling by 1 does nothing).</li>
          </ul>
        </li>
        <li><b>Distributivity (Linearity):</b>
          <ul>
            <li>$\alpha(u+v) = \alpha u + \alpha v$ (Distributes over vector sum).</li>
            <li>$(\alpha+\beta)u = \alpha u + \beta u$ (Distributes over scalar sum).</li>
          </ul>
        </li>
      </ul>

      <p><b>Immediate Theorems:</b> From these axioms, we rigorously prove derived facts like:
      <br>1. $\mathbf{0}$ is unique.
      <br>2. $0 \cdot v = \mathbf{0}$. (Proof: $0v = (0+0)v = 0v + 0v \implies \mathbf{0} = 0v$).
      <br>3. $(-1)v = -v$. (Proof: $v + (-1)v = 1v + (-1)v = (1-1)v = 0v = \mathbf{0}$).</p>

      <h3>1.2 Subspaces: "Closed Worlds"</h3>
      <p>The most important subsets in linear algebra are those that inherit the linear structure. A subset $W \subseteq V$ is a <b>subspace</b> if it is a self-contained linear universe.</p>

      <h4>The One-Line Subspace Test</h4>
      <p>A nonempty subset $W \subseteq V$ is a subspace if and only if it is closed under all linear combinations:
      $$ \forall u,v \in W, \alpha, \beta \in \mathbb{F} \implies \alpha u + \beta v \in W $$
      This implies $W$ contains $\mathbf{0}$, is closed under addition, and is closed under scaling.
      <br><b>Fast Check:</b> If a set does not contain $\mathbf{0}$, it is <b>not</b> a subspace.</p>

      <h4>Fundamental Subspaces</h4>
      <ul>
        <li><b>Intersection:</b> $\bigcap W_i$ is always a subspace. (This is the "AND" of linear constraints).</li>
        <li><b>Sum:</b> $U + W = \{u+w \mid u \in U, w \in W\}$ is a subspace. It is the "smallest closed world" containing both.</li>
        <li><b>Direct Sum:</b> If $U \cap W = \{0\}$, then every element in $U+W$ has a <i>unique</i> representation. We write $U \oplus W$.</li>
      </ul>

      <h3>1.3 Span: The Closure Operator</h3>
      <p>The <b>span</b> of a set $S$, denoted $\mathrm{span}(S)$, is the set of all <i>finite</i> linear combinations of vectors in $S$.
      $$ \mathrm{span}(S) = \left\{ \sum_{i=1}^k \alpha_i v_i \;\middle|\; v_i \in S, \alpha_i \in \mathbb{F}, k \in \mathbb{N} \right\} $$
      <b>Theorem:</b> $\mathrm{span}(S)$ is the smallest subspace containing $S$.
      <br><i>Proof (via Intersection):</i> Define $\mathrm{span}(S) = \bigcap \{W \le V : S \subseteq W\}$. Since the intersection of subspaces is a subspace, and every subspace containing $S$ must contain all linear combinations of $S$, the definitions match.</p>

      <p><b>Solvability:</b> Asking "Does $Ax=b$ have a solution?" is equivalent to "Is $b \in \mathrm{span}(\text{columns of } A)$?". Span translates existence problems into geometric containment problems.</p>

      <h3>1.4 Linear Maps (Structure-Preserving Functions)</h3>
      <p>A function $T: V \to W$ is a <b>Linear Map</b> if it preserves the vector space operations:
      $$ T(\alpha \mathbf{u} + \beta \mathbf{v}) = \alpha T(\mathbf{u}) + \beta T(\mathbf{v}) $$
      This is the definition of a homomorphism in algebra. It implies $T(0)=0$ and $T(-u) = -T(u)$.
      <br><b>Kernel and Image:</b> The kernel $\ker(T) = \{\mathbf{v} \mid T(\mathbf{v}) = 0\}$ is a subspace of the domain. The image $\mathrm{im}(T) = \{T(\mathbf{v}) \mid \mathbf{v} \in V\}$ is a subspace of the codomain.</p>
    </section>

    <!-- SECTION 2: BASES AND COORDINATES -->
    <section class="section-card" id="section-bases">
      <h2>2. Bases, Coordinates, and Dimension</h2>

      <p>Up to now, we have "generated" subspaces via Span. However, spans are wasteful: they can contain redundant vectors. This section is about <b>minimal</b> representations.
      <br>The key questions are:
      1. When does a set have <i>no redundancy</i>? (Independence)
      2. When does a set provide <i>unique coordinates</i>? (Basis)
      3. Is the "number of degrees of freedom" well-defined? (Dimension)</p>

      <h3>2.1 Linear Independence: The "No Redundancy" Condition</h3>
      <p><b>Definition:</b> A finite set of vectors $\{v_1, \dots, v_k\}$ is <b>linearly independent</b> if the only linear combination that sums to zero is the trivial one:
      $$ \sum_{i=1}^k \alpha_i v_i = 0 \implies \alpha_1 = \dots = \alpha_k = 0 $$

      <h4>Independence as "Uniqueness of Representation"</h4>
      <p>This is the deep meaning of independence.
      <br><b>Proposition:</b> $v_1, \dots, v_k$ are independent if and only if every vector $w \in \mathrm{span}(v_1, \dots, v_k)$ has a <b>unique</b> representation as a linear combination.
      <br><i>Proof:</i> If there were two representations $\sum \alpha_i v_i = \sum \beta_i v_i$, then subtracting them gives $\sum (\alpha_i - \beta_i) v_i = 0$. If independent, $\alpha_i - \beta_i = 0$, so $\alpha_i = \beta_i$. Uniqueness holds.</p>

      <h4>The Redundancy Theorem</h4>
      <p>Linear independence is equivalent to saying "no vector is redundant".
      <br><b>Theorem:</b> A set is linearly dependent if and only if there exists some index $j$ such that $v_j \in \mathrm{span}(\{v_i\}_{i \neq j})$.
      <br><i>Proof:</i>
      <br>($\Rightarrow$) If $\sum \alpha_i v_i = 0$ with $\alpha_j \neq 0$, then $v_j = -\sum_{i \ne j} (\alpha_i/\alpha_j) v_i$.
      <br>($\Leftarrow$) If $v_j = \sum_{i \ne j} c_i v_i$, then $1v_j - \sum c_i v_i = 0$ is a non-trivial relation.</p>

      <h3>2.2 Basis: Existence + Uniqueness = Coordinate System</h3>
      <p>A <b>Basis</b> is a set $\mathcal{B} = \{b_1, \dots, b_n\}$ that is both:
      1. <b>Spanning:</b> $\mathrm{span}(\mathcal{B}) = V$ (Every vector can be represented).
      2. <b>Independent:</b> No redundancy (Representation is unique).
      </p>

      <div class="theorem-box">
        <h4>Theorem: The Basis Theorem</h4>
        <p>If $\mathcal{B}$ is a basis of $V$, then every vector $v \in V$ can be written in <b>exactly one way</b> as:
        $$ v = \sum_{i=1}^n c_i b_i $$
        The scalars $(c_1, \dots, c_n)$ are the <b>coordinates</b> of $v$ in basis $\mathcal{B}$, denoted $[v]_\mathcal{B}$.
        <br><b>Crucial Distinction:</b> Coordinates are not vectors.
        <ul>
            <li>$v$ lives in the abstract vector space $V$.</li>
            <li>$[v]_\mathcal{B}$ lives in the coordinate space $\mathbb{F}^n$.</li>
        </ul>
        This isomorphism $v \leftrightarrow [v]_\mathcal{B}$ justifies treating vectors as column arrays <i>after</i> a basis is fixed.</p>
      </div>

      <h3>2.3 Dimension: Degrees of Freedom</h3>
      <p>We define $\dim(V)$ as the number of vectors in a basis. But how do we know all bases have the same size? This is the central structural theorem of linear algebra, relying on the <b>Exchange Lemma</b>.</p>

      <div class="proof-box">
        <h4>Lemma: Steinitz Exchange Lemma</h4>
        <p>If $S = \{s_1, \dots, s_m\}$ spans $V$ and $L = \{\ell_1, \dots, \ell_k\}$ is linearly independent in $V$, then:
        <br>1. <b>$k \le m$</b> (You cannot have more independent vectors than the size of a spanning set).
        <br>2. We can replace $k$ vectors of $S$ with the vectors in $L$ to form a new spanning set.</p>

        <h4>Proof (Step-by-Step)</h4>
        <p>We proceed by induction.</p>
        <div class="proof-step">
            <strong>Step 1: Express $\ell_1$.</strong> Since $S$ spans, $\ell_1 = \sum a_i s_i$. Since $\ell_1 \neq 0$, some coefficient $a_j \neq 0$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Swap.</strong> We can solve for $s_j$ in terms of $\ell_1$ and the other $s$'s. Thus, we can replace $s_j$ with $\ell_1$ and still span $V$.
        </div>
        <div class="proof-step">
            <strong>Step 3: Iterate.</strong> Suppose we have replaced $r$ vectors. To insert $\ell_{r+1}$, we express it in the current basis. Because $L$ is independent, $\ell_{r+1}$ cannot depend only on the $\ell$'s already inserted; it must use at least one of the remaining $s$'s. This allows us to swap that $s$ out.
        </div>
        <div class="proof-step">
            <strong>Step 4: Count.</strong> Each independent vector consumes one spanning vector. Thus, we must have enough spanning vectors to accommodate all independent ones: $k \le m$.
        </div>
      </div>

      <h4>The Dimension Theorem</h4>
      <p>All bases of a finite-dimensional vector space have the same number of elements.
      <br><i>Proof:</i> Let $B_1$ (size $n$) and $B_2$ (size $m$) be bases.
      <br>Since $B_1$ spans and $B_2$ is independent, $m \le n$ (by Exchange Lemma).
      <br>Since $B_2$ spans and $B_1$ is independent, $n \le m$.
      <br>Thus $n=m$. This invariant number is the <b>Dimension</b>.</p>

      <h3>2.4 Matrices as Representations of Linear Maps</h3>
      <p>Up to now, we’ve treated matrices as arrays. But why do they exist?
      <br><b>A matrix is a linear map written in coordinates after you choose bases.</b></p>

      <h4>Constructing the Matrix (Column by Column)</h4>
      <p>Let $V$ have basis $B=\{b_1, \dots, b_n\}$ and $W$ have basis $C=\{c_1, \dots, c_m\}$.
      <br>For a map $T: V \to W$, the matrix $A = [T]_{C \leftarrow B}$ is constructed by the rule:
      <br><b>The $j$-th column of $A$ is the coordinate vector of $T(b_j)$ in basis $C$.</b>
      $$ T(b_j) = \sum_{i=1}^m A_{ij} c_i \quad \iff \quad \text{Column } j = [T(b_j)]_C $$
      This guarantees that $[T(v)]_C = A [v]_B$. The diagram commutes.</p>

      <h4>Composition = Multiplication</h4>
      <p>Why is matrix multiplication "row-by-column"? Because it simulates function composition.
      <br>If $S: U \to V$ and $T: V \to W$, then the matrix for $T \circ S$ must satisfy:
      $$ [T(S(u))]_C = [T]_{C \leftarrow B} [S(u)]_B = [T]_{C \leftarrow B} ([S]_{B \leftarrow A} [u]_A) $$
      Associativity ($A(BC)=(AB)C$) is forced by associativity of function composition ($f \circ (g \circ h) = (f \circ g) \circ h$). Non-commutativity ($AB \ne BA$) is forced because order of operations matters.</p>

      <h4>Change of Basis and Similarity</h4>
      <p>If we change the basis of $V$ from $B$ to $B'$, the vector $v$ doesn't change, but its coordinates do.
      <br>Let $P = [\mathrm{Id}]_{B \leftarrow B'}$ be the change-of-basis matrix. It maps new coordinates to old: $[v]_B = P [v]_{B'}$.
      <br>Suppose $A$ represents $T$ in basis $B$ (so $[T(v)]_B = A [v]_B$) and $A'$ represents $T$ in basis $B'$.
      <br>We can derive $A'$ by tracking the coordinates:
      $$ [T(v)]_{B'} = P^{-1} [T(v)]_B = P^{-1} (A [v]_B) = P^{-1} A (P [v]_{B'}) = (P^{-1} A P) [v]_{B'} $$
      Thus:
      $$ A' = P^{-1} A P $$
      This relation is called <b>Similarity</b>.
      <br><b>Key Insight:</b> Similar matrices ($A \sim P^{-1}AP$) represent the <b>same linear operator</b> in different coordinate systems. Thus, they share all intrinsic properties (invariants):
      <ul>
        <li><b>Rank:</b> Dimension of the image.</li>
        <li><b>Determinant & Trace:</b> Coefficients of characteristic polynomial.</li>
        <li><b>Eigenvalues:</b> Roots of $\det(A-\lambda I)=0$.</li>
      </ul>
      </p>
    </section>

    <!-- SECTION 3: SCALAR INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>3. Scalar Invariants: Determinant and Trace</h2>
      <p>A scalar invariant is a function of a matrix that does not change under similarity transformations ($A \mapsto P^{-1}AP$). Geometrically, these are properties of the linear operator itself, not its specific matrix representation. There are exactly two fundamental independent polynomial invariants: the Determinant and the Trace.</p>

      <h3>3.1 Determinant: The Unique Multilinear Alternating Form</h3>
      <p>The determinant is not fundamentally a recursive formula or a cofactor expansion. It is a <b>characterized object</b>. It is the unique function $\det: \mathbb{F}^{n \times n} \to \mathbb{F}$ satisfying three unavoidable axioms:</p>
      <ol>
        <li><b>Multilinearity (in columns):</b> The function is linear in each column separately.</li>
        <li><b>Alternating:</b> If two columns are equal, the determinant is zero. (This forces sign flips on column swaps).</li>
        <li><b>Normalization:</b> $\det(I) = 1$.</li>
      </ol>
      <p><b>Why this definition is powerful:</b>
      <ul>
        <li><b>Uniqueness:</b> There is only one such function (the Leibniz formula). We don't assume a formula; we derive it from the requirements of geometry.</li>
        <li><b>Singularity:</b> If columns are linearly dependent, one column is a combination of others. By multilinearity and alternation, $\det(A) = 0$. Thus, $\det(A) \neq 0$ is the exact algebraic test for invertibility.</li>
        <li><b>Volume:</b> Axioms 1-3 are exactly the properties of signed volume for a parallelepiped. $\det(A)$ measures how $A$ scales volume.</li>
      </ul>
      </p>

      <h4>Multiplicativity ($\det(AB) = \det(A)\det(B)$)</h4>
      <p>This is not an accident of algebra. It is a consequence of uniqueness. Define $f(A) = \det(AB)/\det(B)$. One can show $f$ satisfies the three axioms, so $f(A)$ must be $\det(A)$.
      <br><b>Geometric Meaning:</b> The volume scaling of a composition is the product of the individual scalings.</p>

      <h3>3.2 Trace: The Unique Linear Invariant</h3>
      <p>The trace is often defined as the sum of diagonal elements. Its deep characterization is complementary to the determinant.</p>
      <p><b>Theorem:</b> The trace is the unique linear functional $\mathrm{tr}: \mathbb{F}^{n \times n} \to \mathbb{F}$ (up to scaling) that satisfies the <b>cyclic property</b>:
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA) $$
      This property forces similarity invariance: $\mathrm{tr}(P^{-1}AP) = \mathrm{tr}(A)$. Thus, trace is a property of the linear map, not the matrix.</p>

      <h4>Trace as Infinitesimal Determinant</h4>
      <p>While determinant measures global volume scaling, trace measures <b>infinitesimal</b> volume change.
      $$ \det(I + \epsilon A) = 1 + \epsilon \mathrm{tr}(A) + O(\epsilon^2) $$
      This links linear algebra to vector calculus: $\mathrm{div}(Ax) = \mathrm{tr}(A)$. The trace measures the rate of flow expansion/contraction.</p>
    </section>

    <!-- SECTION 4: EIGENVALUES -->
    <section class="section-card" id="section-eigenvalues">
      <h2>4. The Spectral Layer: Eigenvalues and Diagonalization</h2>
      <p>We now ask: <b>Are there directions that a linear map preserves, changing only their length?</b> This is the spectral perspective. It collapses the complexity of matrix multiplication into simple scalar multiplication along specific axes.</p>

      <h3>4.1 Eigenvalues: The Invariant Directions</h3>
      <p>A linear map $T$ is complicated because it mixes directions. An <b>eigenvector</b> is a non-zero vector $\mathbf{v}$ whose direction is preserved by $T$, scaled only by the <b>eigenvalue</b> $\lambda$: $T(\mathbf{v}) = \lambda \mathbf{v}$.
      <br><b>The Logic of Detection:</b>
      $$ T(\mathbf{v}) = \lambda \mathbf{v} \iff (T - \lambda I)\mathbf{v} = 0 \iff \ker(T - \lambda I) \neq \{0\} $$
      Thus, $\lambda$ is an eigenvalue if and only if the operator $T - \lambda I$ loses rank. This is detected by the characteristic polynomial $p_T(\lambda) = \det(T - \lambda I) = 0$.
      <br><b>Basis Invariance:</b> Eigenvalues are intrinsic to the map. Since $\det(P(A-\lambda I)P^{-1}) = \det(A-\lambda I)$, the spectrum is independent of coordinates.</p>

      <h3>4.2 Diagonalization: Basis of Eigenvectors</h3>
      <p>An operator $T$ is <b>diagonalizable</b> if and only if there exists a basis of $V$ consisting entirely of eigenvectors. In this basis, the matrix of $T$ is diagonal, meaning the map acts as independent scaling along $n$ axes.
      <br><b>Failure Mode:</b> Diagonalization fails when there are "missing" eigenvectors. This happens if the <b>geometric multiplicity</b> (dimension of eigenspace $E_\lambda$) is strictly less than the <b>algebraic multiplicity</b> (root multiplicity of $p_T$).
      <br><b>Sufficient Condition:</b> If an $n \times n$ matrix has $n$ <b>distinct</b> eigenvalues, the corresponding eigenvectors are linearly independent, so the matrix is guaranteed to be diagonalizable.</p>

      <h3>4.3 Jordan Canonical Form: The Nilpotent Core</h3>
      <p>When diagonalization fails ($g_\lambda < m_\lambda$), what structure replaces it? We must include <b>generalized eigenvectors</b>—vectors that are not invariant but are "eventually" killed by $(T-\lambda I)^k$.
      <br><b>The Structure:</b> Every linear operator over $\mathbb{C}$ decomposes into a direct sum of <b>Jordan blocks</b>. A Jordan block looks like $J = \lambda I + N$, where $N$ is <b>nilpotent</b> ($N^k = 0$).
      $$ J_k(\lambda) = \begin{bmatrix} \lambda & 1 & 0 & \dots \\ 0 & \lambda & 1 & \dots \\ \vdots & \ddots & \ddots & 1 \\ 0 & \dots & 0 & \lambda \end{bmatrix} $$
      The nilpotent part $N$ represents the "drift" or coupling between dimensions that prevents clean separation. Diagonalization is simply the special case where all Jordan blocks are $1 \times 1$.</p>

      <h3>4.4 The Spectral Theorem: Perfect Geometry</h3>
      <p>The Spectral Theorem is the pinnacle of linear algebra. It states that for the right class of operators, none of the pathologies of Jordan form occur, and geometry aligns perfectly with algebra.</p>
      <p><b>Theorem:</b> If $T$ is <b>self-adjoint</b> ($T = T^*$), then:
      <ol>
        <li>All eigenvalues are <b>real</b>. (Complex rotations are impossible).</li>
        <li>Eigenvectors corresponding to distinct eigenvalues are <b>orthogonal</b>. (Invariant directions are perpendicular).</li>
        <li>$V$ admits an <b>orthonormal basis</b> of eigenvectors.</li>
      </ol>
      </p>
      <p><b>Matrix Form:</b> If $A$ is symmetric, $A = Q \Lambda Q^\top$, where $Q$ is an orthogonal matrix ($Q^\top Q = I$).
      <br><b>Why this is definitive:</b> It allows us to decompose any symmetric matrix into a sum of rank-1 orthogonal projections: $A = \sum \lambda_i q_i q_i^\top$. This explains the geometry of ellipsoids ($\mathbf{x}^\top A \mathbf{x} = 1$) and is the foundation for Principal Component Analysis.</p>

      <h3>4.4 The Rayleigh Quotient</h3>
      <p>For a symmetric matrix $A$, the <b>Rayleigh Quotient</b> is defined as:
      $$ R_A(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}} $$
      This scalar measures the "gain" of the matrix in the direction $\mathbf{x}$. Its critical points are the eigenvectors.</p>
      <div class="theorem-box">
        <h4>Variational Characterization of Eigenvalues</h4>
        <p>The eigenvalues of a symmetric matrix $A$ are the stationary values of the Rayleigh Quotient. Specifically:
        $$ \lambda_{\min}(A) = \min_{\mathbf{x} \ne 0} R_A(\mathbf{x}) \quad \text{and} \quad \lambda_{\max}(A) = \max_{\mathbf{x} \ne 0} R_A(\mathbf{x}) $$
        </p>
      </div>
      <p>This connects linear algebra to optimization: finding the largest eigenvalue is equivalent to maximizing a quadratic form over the unit sphere.</p>
    </section>

    <!-- SECTION 5: INDUCED NORMS -->
    <section class="section-card" id="section-induced-norms">
      <h2>5. Induced Matrix Norms</h2>
      <p>We defined vector norms in Lecture 00. Now we ask: how "big" is a matrix $A$? The <b>induced norm</b> measures the maximum factor by which $A$ can stretch a vector.</p>
      $$ \|A\|_p = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|_p}{\|\mathbf{x}\|_p} = \max_{\|\mathbf{x}\|_p = 1} \|A\mathbf{x}\|_p $$

      <h3>5.1 The $\ell_1$ Operator Norm (Max Column Sum)</h3>
      <p>$$ \|A\|_1 = \max_{j} \sum_{i=1}^m |a_{ij}| $$</p>
      <div class="proof-box">
        <h4>Derivation</h4>
        <p>Let $\mathbf{x}$ be a unit vector in $\ell_1$. Then $A\mathbf{x} = \sum x_j a_j$ (linear combination of columns).
        $$ \|A\mathbf{x}\|_1 = \|\sum_j x_j a_j\|_1 \le \sum_j |x_j| \|a_j\|_1 \le (\max_j \|a_j\|_1) \sum_j |x_j| = \max_j \|a_j\|_1 $$
        Equality is achieved by choosing $\mathbf{x} = e_k$ where $k$ is the index of the column with the largest norm.</p>
      </div>

      <h3>5.2 The $\ell_\infty$ Operator Norm (Max Row Sum)</h3>
      <p>$$ \|A\|_\infty = \max_{i} \sum_{j=1}^n |a_{ij}| $$</p>
      <p><i>Intuition:</i> To maximize the $\infty$-norm of $A\mathbf{x}$, we want to maximize a single component $|(A\mathbf{x})_i| = |\sum_j a_{ij} x_j|$. With constraints $|x_j| \le 1$, we set $x_j = \text{sign}(a_{ij})$ to make all terms positive and maximal.</p>

      <h3>5.3 The $\ell_2$ Operator Norm (Spectral Norm)</h3>
      <p>$$ \|A\|_2 = \max_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2 = \sqrt{\lambda_{\max}(A^\top A)} = \sigma_{\max}(A) $$</p>
      <p>This is the square root of the maximum eigenvalue of $A^\top A$, which is the largest <b>singular value</b> of $A$. It represents the maximum stretch of the matrix in the Euclidean sense. This norm plays a crucial role in convergence analysis, as it dictates the worst-case amplification of errors.</p>

      <h3>5.4 Convexity of Induced Norms (Deep Dive)</h3>
      <p>Why are induced norms convex functions of the matrix $A$? We can prove this using the property that the pointwise supremum of linear functions is convex.</p>
      <div class="proof-box">
        <h4>Proof: Convexity via Dual Norms</h4>
        <div class="proof-step">
          <strong>Step 1: Definition.</strong> The induced norm is defined as:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\mathbf{v} \ne 0} \frac{\|X\mathbf{v}\|_a}{\|\mathbf{v}\|_\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \|X\mathbf{v}\|_a $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Dual Norm Representation.</strong> Recall that any norm $\|z\|_a$ can be expressed as a supremum over its dual norm ball: $\|z\|_a = \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top \mathbf{z}$.
          Substituting this into the definition:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \left( \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top (X\mathbf{v}) \right) $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Linear Structure.</strong>
          $$ \|X\|_{a,\mathbf{b}} = \sup \{ \mathbf{u}^\top X \mathbf{v} \mid \|\mathbf{u}\|_{a^*} \le 1, \|\mathbf{v}\|_\mathbf{b} = 1 \} $$
          Notice that for fixed vectors $\mathbf{u}$ and $\mathbf{v}$, the function $f_{\mathbf{u},\mathbf{v}}(X) = \mathbf{u}^\top X \mathbf{v}$ is <b>linear</b> in the entries of $X$.
          Specifically, $\mathbf{u}^\top X \mathbf{v} = \mathrm{tr}(\mathbf{u}^\top X \mathbf{v}) = \mathrm{tr}(\mathbf{v} \mathbf{u}^\top X) = \langle uv^\top, X \rangle$.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong>
          The induced norm $\|X\|_{a,b}$ is the <b>pointwise supremum</b> of a family of linear functions. Since the supremum of any collection of convex functions is convex, the induced norm is a convex function of $X$.
        </div>
      </div>
    </section>

    <!-- SECTION 6: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>6. The QR Decomposition: Geometry First</h2>

      <p>QR is not just an algorithm. It is the <b>algebraic shadow</b> of the Gram-Schmidt process. It formalizes the fact that every matrix can be split into "perfect geometry" ($Q$) and "coordinates" ($R$).</p>

      <h3>6.1 Definition and Existence</h3>
      <p>Let $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ (Full column rank). There exists a unique factorization:
      $$ \boxed{ A = Q R } $$
      where:
      <ul>
        <li>$Q \in \mathbb{R}^{m \times n}$ has <b>orthonormal columns</b> ($Q^\top Q = I_n$). This represents an isometric embedding.</li>
        <li>$R \in \mathbb{R}^{n \times n}$ is <b>upper triangular</b> with positive diagonal entries. This represents coordinates in the orthonormal basis.</li>
      </ul>
      <p><b>Geometry-First Proof:</b> Consider the nested subspaces $W_j = \operatorname{span}(a_1, \dots, a_j)$. By Gram-Schmidt, there exists a unique orthonormal basis $\{q_1, \dots, q_n\}$ such that $\operatorname{span}(q_1, \dots, q_j) = W_j$.
      Since $a_j \in W_j$, it can be written as a linear combination of only the first $j$ basis vectors: $a_j = \sum_{i=1}^j r_{ij} q_i$.
      This triangular dependency is exactly the matrix equation $A = QR$.
      <br><b>Uniqueness:</b> The positivity of the diagonal entries $r_{jj} = \langle q_j, a_j \rangle$ fixes the sign of the basis vectors, making the decomposition unique.</p>

      <h3>6.2 Why QR is the "Correct" Way to Solve Least Squares</h3>
      <p>Interpret $A=QR$ as composition: $x \xrightarrow{R} y \xrightarrow{Q} Ax$.
      <ul>
        <li>$R$ maps coordinates in $\mathbb{R}^n$ to coordinates in an <b>orthonormal basis</b> of the column space. (Handles conditioning).</li>
        <li>$Q$ embeds those coordinates isometrically into $\mathbb{R}^m$. (Handles geometry).</li>
      </ul>
      </p>
      <p><b>Solving Least Squares:</b> Minimize $\|A\mathbf{x} - \mathbf{b}\|_2$.
      Using Normal Equations ($A^\top A \mathbf{x} = A^\top \mathbf{b}$) squares the condition number ($\kappa(A^TA) = \kappa(A)^2$), doubling precision loss.
      Using QR:
      $$ \|QR\mathbf{x} - \mathbf{b}\|_2 = \|R\mathbf{x} - Q^\top \mathbf{b}\|_2 \quad \text{(since } Q \text{ preserves norms on its range)} $$
      This reduces the problem to solving the triangular system $R\mathbf{x} = Q^\top \mathbf{b}$ via back-substitution. The condition number is $\kappa(R) = \kappa(A)$. QR is stable because it separates geometry from conditioning.</p>

      <h3>6.3 Full vs. Thin QR</h3>
      <p>
      <ul>
        <li><b>Thin QR ($Q \in \mathbb{R}^{m \times n}$):</b> Only computes an orthonormal basis for the column space. Sufficient for least squares.</li>
        <li><b>Full QR ($Q \in \mathbb{R}^{m \times m}$):</b> Extends $Q$ to a basis for the entire space $\mathbb{R}^m$. $A = [Q_1 \ Q_2] \begin{bmatrix} R \\ 0 \end{bmatrix}$. $Q_2$ forms a basis for the left nullspace $\mathcal{N}(A^\top)$.</li>
      </ul>
      </p>
    </section>

    <!-- SECTION 7: SVD -->
    <section class="section-card" id="section-svd">
      <h2>7. Singular Value Decomposition (SVD): The True Geometry</h2>
      <p>If the spectral theorem was "perfect geometry under symmetry," and QR was "geometry separated from coordinates," then <b>SVD is the fundamental theorem of linear algebra</b>. It applies to every matrix, square or rectangular, symmetric or not.</p>

      <h3>7.1 The Geometry of SVD: Rotate → Scale → Rotate</h3>
      <p><b>Theorem (SVD):</b> Any matrix $A \in \mathbb{R}^{m \times n}$ can be factored as:
      $$ \boxed{ A = U \Sigma V^\top } $$
      where $U$ and $V$ are orthogonal matrices and $\Sigma$ is diagonal with non-negative entries.
      <br><b>Interpretation:</b> Every linear map decomposes into:
      $$ x \xrightarrow{V^\top} \text{Rotate (Input Basis)} \xrightarrow{\Sigma} \text{Scale (Stretch/Shrink)} \xrightarrow{U} \text{Rotate (Output Basis)} $$
      </p>

      <div class="insight">
        <h4>Why SVD must exist (The "Spectral" Proof)</h4>
        <p>We do not diagonalize $A$; we diagonalize $A^\top A$.
        <br>1. $A^\top A$ is symmetric positive semidefinite. By the Spectral Theorem, $A^\top A = V \Lambda V^\top$ with $\lambda_i \ge 0$.
        <br>2. Define singular values $\sigma_i = \sqrt{\lambda_i}$.
        <br>3. Define right singular vectors $v_i$ as the columns of $V$. These are directions in the input space.
        <br>4. <b>Crucial Geometry:</b> $\|A v_i\|^2 = v_i^\top A^\top A v_i = \lambda_i \|v_i\|^2 = \sigma_i^2$. So $A$ stretches $v_i$ by exactly $\sigma_i$.
        <br>5. Define left singular vectors $u_i = \frac{1}{\sigma_i} A v_i$ (for $\sigma_i > 0$). These are orthonormal in the output space.
        <br>6. Thus $A v_i = \sigma_i u_i$, which in matrix form is $AV = U\Sigma$, or $A = U\Sigma V^\top$.
        </p>
      </div>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-wiki.png" alt="SVD Decomposition Wiki" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption><i>Figure 5a:</i> Standard SVD visualization (Wikimedia).</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-ellipse.png" alt="SVD Ellipse" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption><i>Figure 5b:</i> Unit circle to ellipse transformation.</figcaption>
        </figure>
      </div>

      <h3>7.2 Rank, Nullspace, and Range (Geometric View)</h3>
      <p>SVD reveals the intrinsic dimensions of the map.</p>
      <ul>
        <li><b>Rank:</b> $r = \text{number of } \sigma_i > 0$.</li>
        <li><b>Range (Column Space):</b> Spanned by the first $r$ columns of $U$.</li>
        <li><b>Nullspace:</b> Spanned by the last $n-r$ columns of $V$ (corresponding to $\sigma_i=0$).</li>
      </ul>
      <p>This provides a numerically stable way to compute rank and bases for fundamental subspaces.</p>

      <h3>7.3 Low-Rank Approximation (Eckart-Young-Mirsky)</h3>
      <p>SVD solves the problem of finding the best approximation of a matrix by one of lower rank.
      $$ A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top $$
      This matrix $A_k$ minimizes $\|A - X\|$ in both Spectral and Frobenius norms among all matrices of rank $k$.
      <br><b>Applications:</b> Image compression, PCA, Denoising.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> An image can be viewed as a matrix of pixel intensities. By computing its SVD and keeping only the largest singular values, we can reconstruct the image with far less data.</p>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>7.4 Application: PCA as SVD of Data</h3>
      <p>Principal Component Analysis (PCA) is often taught as "statistics." It is actually <b>pure geometry</b>—specifically, the SVD of the centered data matrix.
      <br>Let $X \in \mathbb{R}^{N \times d}$ be the data matrix (rows are points), <b>centered</b> so column means are zero.
      <br><b>The Problem:</b> Find the direction $\mathbf{v}$ ($\|\mathbf{v}\|=1$) maximizing variance $\frac{1}{N}\|X\mathbf{v}\|^2$.
      <br><b>The Solution via SVD:</b> Let $X = U \Sigma V^\top$.
      $$ \|X\mathbf{v}\|^2 = \mathbf{v}^\top (V \Sigma^2 V^\top) \mathbf{v} $$
      This quadratic form is maximized when $\mathbf{v}$ is the first column of $V$ (the first right singular vector).
      <br><b>Key Identity:</b>
      $$ \boxed{ \text{PCA} = \text{SVD of centered data} } $$
      <ul>
        <li><b>Principal Directions:</b> Columns of $V$ (axes of the data ellipsoid).</li>
        <li><b>Variances:</b> $\sigma_i^2 / N$.</li>
        <li><b>Principal Scores:</b> $XV = U\Sigma$ (coordinates in the principal basis).</li>
      </ul>
      </p>
    </section>
    </section>

    <!-- SECTION 8: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>8. The Pseudoinverse and Condition Number</h2>

      <h3>8.1 The Unified Picture: Least Squares, Pseudoinverse, and Regularization</h3>
      <p>SVD unifies the three core concepts of solving linear systems: least squares, pseudoinverse, and regularization.</p>

      <h4>1. Least Squares via SVD</h4>
      <p>Solve $\min \|A\mathbf{x} - \mathbf{b}\|_2^2$. Using $A=U\Sigma V^\top$, the problem decouples into independent scalar problems in the rotated coordinates:
      $$ \min_{\mathbf{y}} \|\Sigma \mathbf{y} - U^\top \mathbf{b}\|_2^2 = \sum_{i=1}^r (\sigma_i y_i - c_i)^2 + \sum_{i=r+1}^m c_i^2 $$
      where $\mathbf{y} = V^\top \mathbf{x}$ and $\mathbf{c} = U^\top \mathbf{b}$.
      <br>Solution: $y_i = c_i / \sigma_i$ for $\sigma_i > 0$, and $y_i$ is arbitrary for $\sigma_i = 0$.
      <br><b>Minimum Norm Solution:</b> Setting the arbitrary $y_i=0$ (for $i>r$) minimizes $\|\mathbf{x}\|_2 = \|\mathbf{y}\|_2$. This yields the Pseudoinverse solution.</p>

      <h4>2. The Moore-Penrose Pseudoinverse ($A^+$)</h4>
      <p>The matrix that performs this "invert where possible, zero otherwise" operation is $A^+ = V \Sigma^+ U^\top$, where $\Sigma^+_{ii} = 1/\sigma_i$ if $\sigma_i > 0$ and $0$ otherwise.
      <br><b>Properties:</b>
      <ul>
        <li><b>Algebraic:</b> It is the unique matrix satisfying the four Moore-Penrose axioms ($AA^+A=A$, etc.).</li>
        <li><b>Geometric:</b> $A^+$ inverts the map on $\mathcal{R}(A)$ and annihilates $\mathcal{R}(A)^\perp$.</li>
      </ul>
      </p>

      <h4>3. Regularization as Spectral Filtering</h4>
      <p>When $\sigma_i$ is small, $1/\sigma_i$ is huge, amplifying noise (ill-conditioning). Regularization fixes this by modifying the inversion spectrum.
      <br><b>Tikhonov Regularization (Ridge):</b> $\min \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2$.
      <br>Solution: $\mathbf{x}_\lambda = (A^\top A + \lambda I)^{-1} A^\top \mathbf{b}$.
      <br><b>SVD View:</b> In singular coordinates, the filter factor becomes:
      $$ y_i(\lambda) = \frac{\sigma_i}{\sigma_i^2 + \lambda} c_i $$
      <ul>
        <li>Large $\sigma_i (\gg \lambda)$: $y_i \approx c_i / \sigma_i$ (standard inversion).</li>
        <li>Small $\sigma_i (\ll \lambda)$: $y_i \approx \sigma_i c_i / \lambda \to 0$ (noise suppression).</li>
      </ul>
      This smoothly "turns off" the inversion for unstable directions.</p>

      <h3>8.2 Condition Number</h3>
      <p>The condition number $\kappa(A)$ measures the sensitivity of the solution of a linear system to perturbations in the data. Formally, it is defined as:</p>
      $$ \kappa(A) = \|A\|_2 \|A^+\|_2 = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} $$
      <p>where $\sigma_{\min}$ is the smallest <i>non-zero</i> singular value.</p>

      <ul>
        <li><b>$\kappa \approx 1$:</b> Well-conditioned. Errors are not amplified. Orthogonal matrices have $\kappa = 1$.</li>
        <li><b>$\kappa \gg 1$:</b> Ill-conditioned. Small errors in $\mathbf{b}$ can lead to massive errors in $\mathbf{x}$.</li>
      </ul>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/conditioning-geometry.png"
               alt="Well-conditioned versus ill-conditioned error amplification geometry"
               style="width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 6:</i> Condition number as error magnification: the same small uncertainty in $\mathbf{b}$ can map to a small or huge uncertainty in $\mathbf{x}$ depending on $\kappa(A)$.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/zig-zag-ill-conditioning.png"
               alt="Gradient descent zig-zagging on elongated contours in an ill-conditioned problem"
               style="width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 7:</i> Ill-conditioning creates elongated level sets; first-order methods like gradient descent can zig-zag across the valley, slowing convergence.</figcaption>
        </figure>
      </div>

      <div class="insight">
        <h4>Impact on Optimization Algorithms</h4>
        <p>The condition number is not just a numerical nuisance; it fundamentally limits the speed of optimization algorithms.
        For a quadratic function $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A \mathbf{x}$, Gradient Descent converges at a rate determined by $\frac{\kappa-1}{\kappa+1}$.
        <ul>
            <li>If $\kappa=1$ (spherical), convergence is instant.</li>
            <li>If $\kappa \gg 1$ (valley), convergence is arbitrarily slow.</li>
        </ul>
        This motivates <b>Newton's Method</b> and preconditioning, which effectively transform the problem to make $\kappa \approx 1$. We will study this in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>.</p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Stability</h3>
        <p><b>Visualize Ill-Conditioning:</b> Compare solving $A\mathbf{x}=\mathbf{b}$ for a well-conditioned matrix versus an ill-conditioned one. In the ill-conditioned case, the "ellipse" of the matrix is very skinny. A small change in the target vector $\mathbf{b}$ (moving the target slightly off the major axis) requires a massive change in $\mathbf{x}$ to compensate.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 9: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>9. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>
    </section>

    <!-- SECTION 10: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 10. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.12 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Upper bound ($\le$):</strong> By Hölder's inequality, $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$.
          The definition of the dual norm is $\|y\|_* = \sup_{\|\mathbf{x}\|_p \le 1} \mathbf{x}^\top \mathbf{y}$.
          From Hölder, if $\|x\|_p \le 1$, then $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{y}\|_q$. Thus $\|y\|_* \le \|\mathbf{y}\|_q$.
        </div>
        <div class="proof-step">
          <strong>Lower bound ($\ge$):</strong> We need to construct a vector $\mathbf{x}$ that "aligns" with $\mathbf{y}$ to maximize the inner product.
          We want $x_i$ to have the same sign as $y_i$, and magnitudes such that equality holds in Hölder's.
          Choose $x_i = \mathrm{sign}(y_i) |y_i|^{q-1}$.
          <br>Check inner product: $\mathbf{x}^\top \mathbf{y} = \sum \mathrm{sign}(y_i)|y_i|^{q-1} y_i = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          <br>Check norm of $\mathbf{x}$: $\|x\|_p^p = \sum |x_i|^p = \sum |y_i|^{p(q-1)}$.
          Since $(p-1)q = p \implies p(q-1) = q$: $\|x\|_p^p = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          So $\|x\|_p = \|y\|_q^{q/p}$.
          <br>Now normalize to get a unit vector $\tilde{\mathbf{x}} = \mathbf{x} / \|\mathbf{x}\|_p = \mathbf{x} / \|\mathbf{y}\|_q^{q/p}$.
          $$ \tilde{\mathbf{x}}^\top \mathbf{y} = \frac{\|\mathbf{y}\|_q^q}{\|\mathbf{y}\|_q^{q/p}} = \|\mathbf{y}\|_q^{q - q/p} = \|\mathbf{y}\|_q^1 = \|\mathbf{y}\|_q $$
          Since we found a unit vector achieving this value, $\|y\|_* \ge \|\mathbf{y}\|_q$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Dual Norm:</b> Defined by $\|y\|_* = \sup \{ \mathbf{y}^\top \mathbf{x} \mid \|\mathbf{x}\| \le 1 \}$.
        <br><b>$\ell_p$ Dual:</b> The dual of the $\ell_p$ norm ($1 < p < \infty$) is the $\ell_q$ norm, where $1/p + 1/q = 1$.
        <br><b>Hölder's Inequality:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$. This is the generalized Cauchy-Schwarz inequality.
        <br><b>Alignment Condition:</b> Equality holds if $|x_i|^p$ is proportional to $|y_i|^q$ (and signs match). This is crucial for constructing worst-case examples in robustness analysis.</p>
      </div>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The Frobenius norm corresponds to the standard Euclidean norm of the vectorized matrix: $\|A\|_F = \|\mathrm{vec}(A)\|_2$.
        The inner product of matrices is $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.
        This is exactly the dot product of the vectorized forms: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        By the standard Cauchy-Schwarz inequality for vectors:
        $$ |\mathrm{vec}(A)^\top \mathrm{vec}(B)| \le \|\mathrm{vec}(A)\|_2 \|\mathrm{vec}(B)\|_2 $$
        Substituting back the matrix notation yields $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The $\mathrm{vec}$ Operator:</b> Stacks columns of a matrix into a single vector in $\mathbb{R}^{mn}$.
        <br><b>Isometry:</b> The mapping $A \mapsto \mathrm{vec}(A)$ is an isometry between $(\mathbb{R}^{m \times n}, \|\cdot\|_F)$ and $(\mathbb{R}^{mn}, \|\cdot\|_2)$.
        <br><b>Inner Product Consistency:</b> $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        <br><b>Result:</b> Cauchy-Schwarz for vectors immediately implies $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.</p>
      </div>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>1. Compute $A^\top A$ and its eigenvalues:</strong>
          $A^\top A = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$.
          Characteristic eq: $(1-\lambda)^2 - 1 = 0 \implies \lambda^2 - 2\lambda = 0 \implies \lambda_1=2, \lambda_2=0$.
          Singular values: $\sigma_1 = \sqrt{2}, \sigma_2 = 0$.
        </div>
        <div class="proof-step">
          <strong>2. Compute $V$ (eigenvectors of $A^\top A$):</strong>
          For $\lambda_1=2$: $\begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \mathbf{v} = 0 \implies v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
          For $\lambda_2=0$: $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
          $V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>3. Compute $U$:</strong>
          $u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
          Since $m=2$, we need a second vector $u_2$ orthogonal to $u_1$. $u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
          $U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>Result:</strong> $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \left(\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\right)^\top$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hand-Calculation Strategy for SVD:</b>
        <ol>
            <li>Compute the Gram matrix $G = A^\top A$.</li>
            <li>Find eigenvalues $\lambda_i$ of $G$. Singular values are $\sigma_i = \sqrt{\lambda_i}$.</li>
            <li>Find normalized eigenvectors $v_i$ of $G$ to form $V$.</li>
            <li>Compute $u_i = \frac{1}{\sigma_i} A v_i$ for non-zero $\sigma_i$.</li>
            <li>If $m > r$, complete $U$ using Gram-Schmidt or inspection (orthogonality).</li>
        </ol>
        <br><b>Check:</b> Verify $AV = U\Sigma$.</p>
      </div>

      <h3>P1.4 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, \mathbf{b} = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>(a) Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        $A^\top A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.
        $A^\top \mathbf{b} = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Solve $\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \mathbf{x} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Multiply by inverse $\frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$:
        $\mathbf{x} = \frac{1}{3} \begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 \\ 5 \end{bmatrix}$.</p>
        <p><b>(b) QR Factorization:</b>
        Gram-Schmidt on columns $a_1, a_2$.
        $u_1 = a_1 = (1, 0, 1)^\top$. $q_1 = (1/\sqrt{2}, 0, 1/\sqrt{2})^\top$.
        $v_2 = a_2 - \langle a_2, q_1 \rangle q_1 = (1, 1, 0)^\top - (1/\sqrt{2}) (1/\sqrt{2}, 0, 1/\sqrt{2})^\top = (1, 1, 0)^\top - (1/2, 0, 1/2)^\top = (1/2, 1, -1/2)^\top$.
        Normalize $v_2$: $\|v_2\|^2 = 1/4 + 1 + 1/4 = 1.5 = 3/2$. $\|v_2\| = \sqrt{3/2}$.
        $q_2 = \sqrt{2/3} (1/2, 1, -1/2)^\top$.
        Solve $R\mathbf{x} = Q^\top \mathbf{b}$.
        $R = \begin{bmatrix} \sqrt{2} & 1/\sqrt{2} \\ 0 & \sqrt{3/2} \end{bmatrix}$. $Q^\top \mathbf{b} = \begin{bmatrix} 3/\sqrt{2} \\ 4\sqrt{2/3}-1/\sqrt{6} \end{bmatrix}$.
        Back substitution yields same $\mathbf{x}$. QR avoids forming $A^\top A$ (better condition number).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Comparing Least Squares Algorithms:</b>
        <ul>
            <li><b>Normal Equations:</b>
                <ul><li>Cost: $O(n^2 m)$ (forming $A^\top A$) + $O(n^3)$ (Cholesky).</li>
                <li>Stability: Unstable. Dependence on $\kappa(A)^2$.</li></ul>
            </li>
            <li><b>QR Factorization:</b>
                <ul><li>Cost: $2 \times$ Normal Equations (roughly).</li>
                <li>Stability: Stable. Dependence on $\kappa(A)$. Standard choice.</li></ul>
            </li>
            <li><b>SVD:</b> Most expensive, but most robust (handles rank deficiency gracefully).</li>
        </ul></p>
      </div>

      <h3>P1.5 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $A = U \Sigma V^\top$. Then $A^+ = V \Sigma^+ U^\top$.
        $P = AA^+ = (U \Sigma V^\top)(V \Sigma^+ U^\top) = U (\Sigma \Sigma^+) U^\top$.
        $\Sigma \Sigma^+$ is a diagonal matrix with 1s for non-zero singular values and 0s otherwise.
        Let $U_r$ be the first $r$ columns of $U$. Then $P = U_r U_r^\top$.
        <br><b>Idempotent:</b> $P^2 = (U_r U_r^\top)(U_r U_r^\top) = U_r (U_r^\top U_r) U_r^\top = U_r I U_r^\top = P$.
        <br><b>Symmetric:</b> $P^\top = (U_r U_r^\top)^\top = (U_r^\top)^\top U_r^\top = U_r U_r^\top = P$.
        <br>Since the columns of $U_r$ form a basis for $\mathcal{R}(A)$, $P$ is the orthogonal projector onto $\mathcal{R}(A)$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Fundamental Projectors via SVD:</b>
        <ul>
            <li><b>Column Space Projector:</b> $P_{\mathcal{R}(A)} = A A^+ = U_r U_r^\top$.</li>
            <li><b>Row Space Projector:</b> $P_{\mathcal{R}(A^\top)} = A^+ A = V_r V_r^\top$.</li>
        </ul>
        <br><b>Intuition:</b> $A^+$ inverts the action on the range and kills the orthogonal complement. $AA^+$ maps range $\to$ range (identity) and orthogonal complement $\to$ 0.</p>
      </div>

      <h3>P1.6 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>We want to find the point $\mathbf{p} = A\mathbf{x}$ in the column space closest to $\mathbf{b}$.
        Problem: $\min_\mathbf{x} \|A\mathbf{x} - \mathbf{b}\|_2^2$.
        Gradient: $2A^\top (A\mathbf{x} - \mathbf{b}) = 0$.
        Normal eqs: $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        Since cols are independent, $A^\top A$ is invertible.
        Solution: $\mathbf{x}^* = (A^\top A)^{-1} A^\top \mathbf{b}$.
        Projection: $\mathbf{p} = A \mathbf{x}^* = A(A^\top A)^{-1} A^\top \mathbf{b}$.
        The matrix mapping $\mathbf{b}$ to $\mathbf{p}$ is $P = A(A^\top A)^{-1} A^\top$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The "Hat" Matrix:</b> $P = A(A^\top A)^{-1} A^\top$.
        <br><b>Requirements:</b> Valid only when $A$ has full column rank (so $A^\top A$ is invertible).
        <br><b>General Case:</b> If $A$ is rank-deficient, use the pseudoinverse form $P = AA^+$.
        <br><b>Properties:</b> $P^2=P$ and $P^\top=P$. The residual is $(I-P)\mathbf{b}$, which projects onto $\mathcal{N}(A^\top)$.</p>
      </div>

      <h3>P1.7 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Normal $a = (1, 1, 1)^\top$. Point $\mathbf{y} = (1, 2, 3)^\top$. Target $\mathbf{b}=3$.
        $\mathbf{p} = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a$.
        $a^\top \mathbf{y} = 1+2+3 = 6$. $\|a\|^2 = 3$.
        $\mathbf{p} = (1, 2, 3)^\top - \frac{6-3}{3} (1, 1, 1)^\top = (1, 2, 3)^\top - (1, 1, 1)^\top = (0, 1, 2)^\top$.
        Check: $0+1+2=3$. Correct.
        Distance: $\|y-p\| = \|(1, 1, 1)\| = \sqrt{3}$.
        Formula: $\frac{|a^\top \mathbf{y} - \mathbf{b}|}{\|a\|} = \frac{|6-3|}{\sqrt{3}} = \frac{3}{\sqrt{3}} = \sqrt{3}$. Matches.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Constraint Projection:</b> The projection of $\mathbf{y}$ onto the affine set $\{x \mid a^\top \mathbf{x} = \mathbf{b}\}$ is:
        $$ \Pi(\mathbf{y}) = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a $$
        <b>Optimization View:</b> This is the solution to $\min_\mathbf{x} \frac{1}{2}\|\mathbf{x}-\mathbf{y}\|^2$ s.t. $a^\top \mathbf{x} = \mathbf{b}$.
        <br><b>Lagrange Multiplier:</b> The term $\frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2}$ corresponds to the Lagrange multiplier $\nu$ associated with the constraint.</p>
      </div>

      <h3>P1.8 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The matrix is diagonal, so its singular values are the absolute values of diagonal entries: $1$ and $\epsilon$.
        Assuming $\epsilon < 1$: $\sigma_{\max} = 1, \sigma_{\min} = \epsilon$.
        $\kappa(A) = 1/\epsilon$.
        As $\epsilon \to 0$, $\kappa(A) \to \infty$. The matrix becomes ill-conditioned (nearly singular).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Condition Number $\kappa(A)$:</b> The ratio $\sigma_{\max}/\sigma_{\min}$ (for square invertible matrices).
        <br><b>Interpretation:</b> The aspect ratio of the hyperellipse image of the unit sphere.
        <br><b>Impact:</b>
        <ul>
            <li><b>Sensitivity:</b> Relative error in $\mathbf{x}$ $\le \kappa(A) \times$ Relative error in $\mathbf{b}$.</li>
            <li><b>Optimization:</b> Controls convergence rate of gradient descent. High $\kappa$ $\implies$ "Zig-zagging".</li>
        </ul></p>
      </div>

      <h3>P1.9 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = xy^\top$ where $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ are non-zero vectors. Show that $A^+ = \frac{A^\top}{\|\mathbf{x}\|_2^2 \|\mathbf{y}\|_2^2}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>SVD of rank-1 matrix: $A = \sigma \mathbf{u} \mathbf{v}^\top$.
        $\|A\|_F = \|x\|\|y\| = \sigma$.
        $\mathbf{u} = \mathbf{x}/\|\mathbf{x}\|$, $\mathbf{v} = \mathbf{y}/\|\mathbf{y}\|$.
        Check: $A = (\|\mathbf{x}\|\|\mathbf{y}\|) \frac{\mathbf{x}}{\|\mathbf{x}\|} \frac{\mathbf{y}^\top}{\|\mathbf{y}\|} = xy^\top$. Correct.
        Pseudoinverse: $A^+ = \frac{1}{\sigma} \mathbf{v} \mathbf{u}^\top = \frac{1}{\|\mathbf{x}\|\|\mathbf{y}\|} \frac{\mathbf{y}}{\|\mathbf{y}\|} \frac{\mathbf{x}^\top}{\|\mathbf{x}\|} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.
        Note that $A^\top = (xy^\top)^\top = yx^\top$.
        Thus $A^+ = \frac{A^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-1 SVD:</b> A matrix $A = xy^\top$ has exactly one non-zero singular value $\sigma_1 = \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$.
        <br><b>Singular Vectors:</b> $u_1 = \mathbf{x}/\|\mathbf{x}\|$, $v_1 = \mathbf{y}/\|\mathbf{y}\|$.
        <br><b>Pseudoinverse Formula:</b> For rank-1 matrices, $A^+ = \frac{A^\top}{\|A\|_F^2} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>

      <h3>P1.10 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$\|A\|_2^2 = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|^2}{\|\mathbf{x}\|^2} = \sup_{\mathbf{x} \ne 0} \frac{\mathbf{x}^\top A^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$.
        The term on the right is the Rayleigh quotient for the symmetric matrix $M = A^\top A$.
        By the variational characterization of eigenvalues, the maximum value of the Rayleigh quotient is $\lambda_{\max}(M)$.
        Thus $\|A\|_2^2 = \lambda_{\max}(A^\top A)$, so $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Variational Definition of Eigenvalues:</b> For symmetric $M$, $\lambda_{\max}(M) = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top M \mathbf{x}$.
        <br><b>Spectral Norm Connection:</b> $\|A\|_2^2 = \max_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|^2 = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top (A^\top A) \mathbf{x} = \lambda_{\max}(A^\top A)$.
        <br><b>Implication:</b> The spectral norm is the square root of the largest eigenvalue of the Gram matrix.</p>
      </div>

      <h3>P1.11 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Frobenius:</b> $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
        $\|UAV^\top\|_F^2 = \mathrm{tr}(V A^\top U^\top U A V^\top) = \mathrm{tr}(V A^\top A V^\top) = \mathrm{tr}(V^\top V A^\top A) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.
        <br><b>Spectral:</b> $\|UAV^\top\|_2 = \sup_{\|\mathbf{x}\|=1} \|UAV^\top \mathbf{x}\|$.
        Since $U$ is orthogonal, $\|Uy\| = \|y\|$. So we maximize $\|A (V^\top \mathbf{x})\|$.
        Since $V$ is orthogonal, as $\mathbf{x}$ ranges over the unit sphere, $\mathbf{y} = V^\top \mathbf{x}$ also ranges over the unit sphere.
        Thus $\sup_{\|\mathbf{x}\|=1} \|A V^\top \mathbf{x}\| = \sup_{\|\mathbf{y}\|=1} \|A\mathbf{y}\| = \|A\|_2$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Invariance:</b> A matrix norm is orthogonally invariant if $\|UAV^\top\| = \|A\|$ for all orthogonal $U, V$.
        <br><b>Schatten p-norms:</b> Defined as the $\ell_p$ norm of the singular values vector $\sigma(A)$.
        <ul>
            <li>$p=\infty$: Spectral Norm.</li>
            <li>$p=2$: Frobenius Norm.</li>
            <li>$p=1$: Nuclear Norm (Trace Norm).</li>
        </ul>
        <br>These are the "natural" norms for matrix analysis.</p>
      </div>

      <h3>P1.12 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Group:</b>
        1. Closure: $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.
        2. Inverse: $Q^{-1} = Q^\top$, which is orthogonal since $(Q^\top)^\top Q^\top = Q Q^\top = I$.
        3. Identity: $I \in O(n)$.
        <br><b>Compactness:</b>
        1. Bounded: The columns are unit vectors, so $\|Q\|_F = \sqrt{n}$. The set is bounded.
        2. Closed: The condition $Q^\top Q = I$ is a continuous equality constraint (level set of a continuous function). Thus the set is closed.
        By Heine-Borel, closed and bounded in finite dimensions implies compact.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The Orthogonal Group $O(n)$:</b> The set of all $n \times n$ orthogonal matrices.
        <br><b>Compactness Proof:</b>
        <ul>
            <li><b>Boundedness:</b> $\|Q\|_F = \sqrt{n}$, so $O(n)$ is contained in a ball of radius $\sqrt{n}$.</li>
            <li><b>Closedness:</b> It is the solution set of continuous polynomial equations $Q^\top Q - I = 0$.</li>
        </ul>
        <br><b>Optimization Consequence:</b> Any continuous function (like a norm or trace) maximized over orthogonal matrices <i>attains</i> its maximum. This is often used to prove existence of SVD.</p>
      </div>
</section>

    <!-- SECTION 11: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>11. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
