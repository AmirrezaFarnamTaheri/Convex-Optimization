<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture bridges the gap between basic linear algebra and the robust numerical tools used in modern optimization. We move beyond the Normal Equations to explore numerically stable factorizations like QR and the Singular Value Decomposition (SVD), which reveal the geometry of linear maps in their purest form. We also introduce the Moore-Penrose pseudoinverse for solving ill-posed or rank-deficient problems and define the condition number—the fundamental metric for analyzing the stability of algorithms and the convergence speed of gradient descent.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm and matrix completion in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The condition number is the key parameter determining the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a> and motivates the use of Newton's Method.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD) as fundamental tools for numerical linear algebra.</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming the potentially ill-conditioned matrix $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions for rank-deficient or underdetermined systems.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations in data.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA) using truncated SVD.</li>
      </ul>
    </section>

    <!-- SECTION 1: SCALAR INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>1. Scalar Invariants: Determinant and Trace</h2>
      <p>A scalar invariant is a function of a matrix that does not change under similarity transformations ($A \mapsto P^{-1}AP$). Geometrically, these are properties of the linear operator itself, not its specific matrix representation. There are exactly two fundamental independent polynomial invariants: the Determinant and the Trace.</p>

      <h3>1.1 Determinant: The Unique Volume Form</h3>
      <p>The determinant is often taught as a recursive formula. It is better understood as a characterized object. It is the <b>unique</b> function $\det: \mathbb{F}^{n \times n} \to \mathbb{F}$ satisfying three axioms:</p>
      <ol>
        <li><b>Multilinearity:</b> Linear in each column separately.</li>
        <li><b>Alternating:</b> If two columns are equal, the value is zero. (Implies swapping columns flips the sign).</li>
        <li><b>Normalization:</b> $\det(I) = 1$.</li>
      </ol>
      <p>These axioms force the Leibniz formula: $\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{i=1}^n A_{i, \sigma(i)}$.</p>

      <div class="proof-box">
        <h4>Derivation: Why Axioms Force the Formula</h4>
        <p>This derivation shows that the determinant is not an arbitrary definition, but a necessary consequence of geometry.
        <br>1. <b>Expand columns:</b> Write each column $a_j$ as a linear combination of standard basis vectors $e_i$: $a_j = \sum_i A_{ij} e_i$.
        <br>2. <b>Apply Multilinearity:</b> Expanding $\det(a_1, \dots, a_n)$ involves $n$ sums.
        $$ \det(A) = \sum_{i_1, \dots, i_n} A_{i_1, 1} \dots A_{i_n, n} \det(e_{i_1}, \dots, e_{i_n}) $$
        <br>3. <b>Apply Alternation:</b> If any two indices $i_j, i_k$ are the same, $\det(e_{i_1}, \dots, e_{i_n}) = 0$. Thus, we only sum over <b>permutations</b> $\sigma$ where all indices are distinct.
        <br>4. <b>Apply Normalization:</b> Swapping columns to order the basis vectors $(e_{\sigma(1)}, \dots, e_{\sigma(n)})$ into $(e_1, \dots, e_n)$ requires sign flips.
        $$ \det(e_{\sigma(1)}, \dots, e_{\sigma(n)}) = \mathrm{sgn}(\sigma) \det(I) = \mathrm{sgn}(\sigma) $$
        <br><b>Result:</b> $\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{j=1}^n A_{\sigma(j), j}$.</p>
      </div>

      <h4>Geometric Interpretation: Volume and Orientation</h4>
      <p>For $A \in \mathbb{R}^{n \times n}$, $|\det(A)|$ is the volume of the parallelepiped spanned by the columns of $A$.
      <br><b>Properties forced by geometry:</b>
      <ul>
        <li><b>Invertibility:</b> $\det(A) \neq 0 \iff$ columns are linearly independent $\iff$ volume is non-zero.</li>
        <li><b>Multiplicativity:</b> $\det(AB) = \det(A)\det(B)$. The volume scaling of a composite map is the product of the individual scalings.</li>
        <li><b>Orientation:</b> The sign of $\det(A)$ indicates whether the map preserves or reverses orientation (e.g., reflection has det -1).</li>
      </ul>
      </p>

      <h3>1.2 Trace: The Unique Linear Invariant</h3>
      <p>The trace is the unique linear functional $\mathrm{tr}: \mathbb{F}^{n \times n} \to \mathbb{F}$ (up to scaling) that satisfies the <b>cyclic property</b>:
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA) $$
      This property implies similarity invariance: $\mathrm{tr}(P^{-1}AP) = \mathrm{tr}(APP^{-1}) = \mathrm{tr}(A)$. Thus, the trace is defined for linear operators, not just matrices.</p>

      <div class="proof-box">
        <h4>Proof: Uniqueness of Trace</h4>
        <p>Why is the sum of diagonal elements the <i>only</i> invariant?
        <br>Let $f: \mathbb{F}^{n \times n} \to \mathbb{F}$ be a linear functional satisfying $f(AB) = f(BA)$ and $f(I) = n$.
        <br>1. <b>Similarity Invariance:</b> $f(P A P^{-1}) = f(P^{-1} P A) = f(A)$.
        <br>2. <b>Reduction to Diagonal:</b> Over $\mathbb{C}$, any matrix is similar to an upper triangular matrix (Schur Form) with eigenvalues on the diagonal. By invariance, $f(A)$ depends only on these diagonal entries $\lambda_i$.
        <br>3. <b>Symmetry:</b> Permutation matrices are orthogonal, so $f$ is invariant under reordering $\lambda_i$. Linearity implies $f(\operatorname{diag}(\lambda_1, \dots, \lambda_n)) = c \sum \lambda_i$.
        <br>4. <b>Normalization:</b> $f(I) = c \sum 1 = cn = n \implies c=1$.
        <br>Thus, $f(A) = \sum \lambda_i = \sum A_{ii} = \mathrm{tr}(A)$.</p>
      </div>

      <h4>Geometric Interpretation: Infinitesimal Volume Change</h4>
      <p>While determinant is global volume scaling, trace is the <b>derivative</b> of volume.
      $$ \det(I + \epsilon A) = 1 + \epsilon \mathrm{tr}(A) + O(\epsilon^2) $$
      This connects linear algebra to calculus:
      <ul>
        <li><b>Divergence:</b> For a vector field $f(x) = Ax$, the divergence is $\nabla \cdot f = \mathrm{tr}(A)$. It measures the rate of flow expansion.</li>
        <li><b>Derivative of Det:</b> $\frac{d}{dt} \det(A(t)) = \det(A(t)) \mathrm{tr}(A(t)^{-1} A'(t))$.</li>
      </ul>
      </p>
    </section>

    <!-- SECTION 4: EIGENVALUES -->
    <section class="section-card" id="section-eigenvalues">
      <h2>2. The Spectral Layer: Eigenvalues and Diagonalization</h2>
      <p>We now ask: <b>Are there directions that a linear map preserves, changing only their length?</b> This is the spectral perspective. It collapses the complexity of matrix multiplication into simple scalar multiplication along specific axes.</p>

      <h3>2.1 Invariant Subspaces and Eigenvalues</h3>
      <p>The concept of eigenvalues begins with <b>invariant subspaces</b>. A subspace $U \subseteq V$ is $T$-invariant if $T(u) \in U$ for all $u \in U$. This allows us to restrict the operator to a smaller domain, simplifying the analysis.</p>
      <ul>
        <li><b>Trivial Invariants:</b> $\{0\}$, $V$, $\text{ker}(T)$, and $\text{im}(T)$ are always invariant.</li>
        <li><b>Invariant Directions (Eigenvectors):</b> If a 1-dimensional subspace spanned by $v \neq 0$ is invariant, then $T(v)$ must be a multiple of $v$. This forces the condition $T(v) = \lambda v$. Geometrically, $T$ maps the line spanned by $v$ into itself; it does not rotate $v$ away, it only scales it.</li>
      </ul>

      <div class="insight">
        <h4>Dynamics Intuition: Why Eigenvalues Govern Stability</h4>
        <p>Eigenvectors are the "pure modes" of a linear system.
        <br><b>Discrete Time ($x_{k+1} = Ax_k$):</b> If $x_0$ is an eigenvector with value $\lambda$, then $x_k = A^k x_0 = \lambda^k x_0$. The magnitude $|\lambda|$ determines stability:
        <ul>
            <li>$|\lambda| < 1$: Decay to zero (stable).</li>
            <li>$|\lambda| > 1$: Explosion (unstable).</li>
        </ul>
        <br><b>Continuous Time ($\dot{x} = Ax$):</b> Solutions are $x(t) = e^{At}x_0 = e^{\lambda t}x_0$. The real part $\mathrm{Re}(\lambda)$ matters:
        <ul>
            <li>$\mathrm{Re}(\lambda) < 0$: Decay (stable).</li>
            <li>$\mathrm{Re}(\lambda) > 0$: Growth (unstable).</li>
        </ul>
        </p>
      </div>

      <p>A non-zero vector $\mathbf{v}$ is an <b>eigenvector</b> with <b>eigenvalue</b> $\lambda$ if $T(\mathbf{v}) = \lambda \mathbf{v}$. This is a statement about invariant 1D subspaces.
      <br><b>The Logic of Detection:</b>
      $$ T(\mathbf{v}) = \lambda \mathbf{v} \iff (T - \lambda I)\mathbf{v} = 0 \iff \ker(T - \lambda I) \neq \{0\} $$
      Thus, $\lambda$ is an eigenvalue if and only if the operator $T - \lambda I$ is singular (not invertible). This is detected by the characteristic polynomial $p_T(\lambda) = \det(T - \lambda I) = 0$.
      <br><b>Basis Invariance:</b> Since $P(A-\lambda I)P^{-1} = PAP^{-1} - \lambda I$, similar matrices have the same characteristic polynomial and thus the same eigenvalues.</p>

      <h3>2.2 Diagonalization: When do Eigenvectors Form a Basis?</h3>
      <p>An operator $T$ is <b>diagonalizable</b> if there exists a basis of $V$ consisting of eigenvectors. In this basis, the matrix of $T$ is diagonal.
      <br><b>The Criterion:</b> Diagonalization is possible if and only if for every eigenvalue $\lambda$, the geometric multiplicity equals the algebraic multiplicity ($g_\lambda = m_\lambda$).
      <ul>
        <li><b>Algebraic Multiplicity ($m_\lambda$):</b> Multiplicity of $\lambda$ as a root of $p_T(\lambda)$.</li>
        <li><b>Geometric Multiplicity ($g_\lambda$):</b> Dimension of the eigenspace $E_\lambda = \ker(T - \lambda I)$.</li>
      </ul>
      <p><b>Distinct Eigenvalues implies Diagonalizable:</b> If an $n \times n$ matrix has $n$ distinct eigenvalues, the corresponding eigenvectors are linearly independent, forming a basis.</p>

      <div class="proof-box">
        <h4>Why Columns of $V$ are Eigenvectors</h4>
        <p>If $A = V\Lambda V^{-1}$ where $\Lambda$ is diagonal, then multiplying by $V$ gives $AV = V\Lambda$.
        <br>Let $V = [v_1 \dots v_n]$ and $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$.
        <br>Matching columns on both sides:
        $$ [Av_1 \dots Av_n] = [\lambda_1 v_1 \dots \lambda_n v_n] $$
        Thus, for each column $i$, we have $Av_i = \lambda_i v_i$. This forces the columns of $V$ to be eigenvectors.</p>
      </div>

      <div class="insight">
        <h4>The "Same Vectors, Different Coordinates" Insight</h4>
        <p>We did not change the underlying space $\mathbb{R}^n$. We changed the <b>coordinate system</b> used to describe vectors.
        <br>If $x$ are coordinates in the standard basis and $y$ are coordinates in the eigenbasis ($x = Vy$), then the linear map simplifies:
        $$ x \mapsto Ax \quad \iff \quad Vy \mapsto A(Vy) = V\Lambda y \quad \iff \quad y \mapsto \Lambda y $$
        In the new coordinates $y$, the matrix is diagonal. Diagonalization is the process of finding the coordinate system where the map uncouples.</p>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/diagonalization_eigenbasis.gif"
             alt="Diagonalization transforms the map into pure scaling along axes"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
        <figcaption><i>Animation:</i> Diagonalization allows us to view the linear map as pure scaling along the eigenvector axes. This is the ideal case.</figcaption>
      </figure>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/trace_sum_eigenvalues_constant.gif"
             alt="Trace is the sum of eigenvalues"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 8px; background: white;" />
        <figcaption><i>Animation:</i> The sum of eigenvalues (trace) is conserved even as the matrix deforms, acting as a "total scaling budget".</figcaption>
      </figure>

      <h3>2.3 Diagonalization and its Failure</h3>
      <p><b>Diagonalization ($A = PDP^{-1}$) is the ideal case:</b> the linear map splits into independent scalings along axes.
      <br><b>Defective Matrices present complications:</b> If a matrix lacks a full set of eigenvectors, it cannot be diagonalized. This happens in the presence of <b>Jordan Blocks</b>.</p>

      <h4>1) Why Diagonalization Fails (The Obstruction)</h4>
      <p>Recall the criterion: A matrix $A \in \mathbb{C}^{n \times n}$ is diagonalizable iff the total dimension of its eigenspaces equals $n$. Failure occurs exactly when, for some eigenvalue $\lambda$:</p>
      $$ \underbrace{\dim\ker(A-\lambda I)}_{\text{geometric multiplicity } g_\lambda} < \underbrace{\text{mult}_\lambda}_{\text{algebraic multiplicity } m_\lambda} $$
      <p><b>Interpretation:</b>
      <ul>
        <li>The characteristic polynomial promises $m_\lambda$ "slots" for eigenvalues.</li>
        <li>The eigenspace only supplies $g_\lambda$ genuine invariant directions.</li>
        <li>The missing $m_\lambda - g_\lambda$ directions must exist—but <b>not</b> as eigenvectors. They are hidden in the "generalized" structure.</li>
      </ul>
      </p>

      <h4>2) Generalized Eigenvectors: The Missing Link</h4>
      <p>If $\mathbf{v}$ is an eigenvector, $(A-\lambda I)\mathbf{v} = 0$. But what if there are vectors $\mathbf{w} \neq 0$ such that:
      $$ (A-\lambda I)\mathbf{w} = \mathbf{v} \quad \text{(an eigenvector)} $$
      Then $\mathbf{w}$ is <b>not</b> an eigenvector, but applying $A$ gives:
      $$ A\mathbf{w} = \lambda \mathbf{w} + \mathbf{v} $$
      So $\mathbf{w}$ is <i>almost</i> invariant—it scales by $\lambda$, but suffers a "drift" term aligned with $\mathbf{v}$. This is a <b>generalized eigenvector</b>.</p>

      <h4>3) Generalized Eigenspaces</h4>
      <p>For an eigenvalue $\lambda$, define the <b>generalized eigenspace</b>:
      $$ \mathcal{G}_\lambda := \ker\big((A-\lambda I)^n\big) $$
      <b>Facts:</b>
      <ul>
        <li>$\dim(\mathcal{G}_\lambda) = m_\lambda$. (It recovers the full algebraic multiplicity).</li>
        <li>$\ker(A-\lambda I) \subseteq \mathcal{G}_\lambda$.</li>
        <li>$\mathcal{G}_\lambda$ is invariant under $A$.</li>
      </ul>
      This space contains the missing dimensions.</p>

      <h4>4) Jordan Form: Handling Non-Diagonalizable Matrices</h4>
      <p>When a matrix lacks a full set of eigenvectors, it cannot be diagonalized. The <b>Jordan Canonical Form</b> provides the next-best structure.</p>

      <div class="theorem-box">
        <h4>Jordan Canonical Form Theorem</h4>
        <p>Every matrix $A \in \mathbb{C}^{n \times n}$ can be written as $A = P J P^{-1}$ where $J$ is block diagonal:
        $$ J = \operatorname{diag}(J_{k_1}(\lambda_1), \dots, J_{k_r}(\lambda_r)) $$
        Each <b>Jordan block</b> $J_k(\lambda)$ has $\lambda$ on the diagonal and $1$ on the superdiagonal:
        $$ J_k(\lambda) = \begin{bmatrix} \lambda & 1 & 0 & \dots \\ 0 & \lambda & 1 & \dots \\ \vdots & \ddots & \ddots & 1 \\ 0 & \dots & 0 & \lambda \end{bmatrix} $$
        <b>Key insight:</b> The form is unique up to block ordering. Block sizes measure "defectiveness" (geometric vs. algebraic multiplicity gap).
        </p>
      </div>

      <div class="insight">
        <h4>Key Insights: Jordan Form in Practice</h4>
        <p><b>Semisimple + Nilpotent Decomposition:</b> Jordan form reveals $A = S + N$ where $S$ is diagonalizable (semisimple), $N$ is nilpotent ($N^k = 0$), and they commute. The nilpotent part $N$ measures the "defect" from diagonalizability.</p>
        <ul>
          <li><b>Algebraic classification:</b> Preserves eigenvalues and multiplicities, but not geometry (orthogonality, singular values).</li>
          <li><b>Dynamics insight:</b> Even if $|\lambda| < 1$, Jordan blocks cause transient growth ($\sim \lambda^t t^k$ terms) before decay. Stability requires more than $\rho(A) < 1$.</li>
          <li><b>Numerical instability:</b> Jordan form is discontinuous—infinitesimal perturbations break blocks. For computation, use <b>Schur Decomposition</b> or SVD instead.</li>
          <li><b>Real vs. Complex:</b> Over $\mathbb{R}$, complex eigenvalues yield $2 \times 2$ rotation blocks. This is why symmetric matrices (Spectral Theorem) are powerful—real eigenvalues + orthogonal bases.</li>
        </ul>
        <p><b>The Big Picture:</b> <i>Spectral Theorem</i> = perfect geometry (self-adjoint). <i>Jordan Form</i> = perfect algebra (similarity classification). <i>SVD</i> = perfect geometry for all matrices.</p>
      </div>

      <h3>2.4 The Spectral Theorem: Perfect Geometry</h3>
      <p><b>Theorem:</b> If $T$ is <b>self-adjoint</b> ($T = T^*$), then:
      <ol>
        <li>All eigenvalues are <b>real</b>.</li>
        <li>Eigenvectors corresponding to distinct eigenvalues are <b>orthogonal</b>.</li>
        <li>$V$ admits an <b>orthonormal basis of eigenvectors</b>.</li>
      </ol>
      This is the "perfect" case: no complex numbers, no defects, and the coordinate system is orthogonal.
      <br><b>Matrix Form:</b> If $A$ is symmetric, $A = Q \Lambda Q^\top$ with $Q$ orthogonal.
      <br><b>Why this matters:</b> This theorem underpins the geometry of ellipsoids ($\mathbf{x}^\top A \mathbf{x} = 1$), the analysis of Hessian curvature, and the convergence of optimization algorithms.</p>

      <h3>2.5 The Rayleigh Quotient: Bridge to Optimization</h3>
      <p>For a symmetric matrix $A$, the <b>Rayleigh Quotient</b> is defined as:
      $$ R_A(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}} $$
      This scalar measures the "gain" or "effective eigenvalue" of the matrix in the direction $\mathbf{x}$. Its critical points are the eigenvectors.</p>
      <div class="theorem-box">
        <h4>Variational Characterization of Eigenvalues</h4>
        <p>The eigenvalues of a symmetric matrix $A$ are the stationary values of the Rayleigh Quotient. Specifically:
        $$ \lambda_{\min}(A) = \min_{\mathbf{x} \ne 0} R_A(\mathbf{x}) \quad \text{and} \quad \lambda_{\max}(A) = \max_{\mathbf{x} \ne 0} R_A(\mathbf{x}) $$
        This allows us to treat eigenvalues as solutions to optimization problems.</p>
      </div>
      <div class="proof-box">
        <h4>Proof: Weighted Average Intuition</h4>
        <p>Why do min/max Rayleigh quotients give the extreme eigenvalues? The cleanest intuition comes from the Spectral Theorem.</p>
        <div class="proof-step">
          <strong>Step 1: Diagonalization.</strong>
          Since $A$ is symmetric, $A = Q \Lambda Q^\top$ with orthonormal $Q$.
          Let $\mathbf{y} = Q^\top \mathbf{x}$. Since $Q$ is orthogonal, $\|\mathbf{y}\|_2 = \|\mathbf{x}\|_2$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Convex Combination.</strong>
          $$ R_A(\mathbf{x}) = \frac{\mathbf{x}^\top Q \Lambda Q^\top \mathbf{x}}{\mathbf{x}^\top \mathbf{x}} = \frac{\mathbf{y}^\top \Lambda \mathbf{y}}{\mathbf{y}^\top \mathbf{y}} = \frac{\sum \lambda_i y_i^2}{\sum y_i^2} $$
          Define weights $w_i = y_i^2 / (\sum y_j^2)$. Then $w_i \ge 0$ and $\sum w_i = 1$.
          $$ R_A(\mathbf{x}) = \sum_{i=1}^n w_i \lambda_i $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          The Rayleigh quotient is a <b>weighted average (convex combination) of the eigenvalues</b>. Thus it must lie between $\lambda_{\min}$ and $\lambda_{\max}$.
          We achieve the max by putting all "mass" $w_i$ on the largest eigenvalue (choosing $\mathbf{x}$ as the corresponding eigenvector).
        </div>
      </div>
      <p>This connects linear algebra to optimization: finding the largest eigenvalue is equivalent to maximizing a quadratic form over the unit sphere.</p>

      <h3>2.6 Generalized Eigenvalues and Whitening</h3>
      <p>In many optimization problems, we care about the ratio of two quadratic forms. Let $A, B \in \mathbb{S}^n$ with $B \succ 0$. A <b>generalized eigenpair</b> $(\lambda, \mathbf{x})$ satisfies:
      $$ A\mathbf{x} = \lambda B \mathbf{x}, \quad \mathbf{x} \neq 0 $$
      This arises when optimizing $\mathbf{x}^\top A \mathbf{x}$ subject to an ellipsoidal constraint $\mathbf{x}^\top B \mathbf{x} = 1$.</p>

      <h4>Reduction to Standard Eigenvalue Problem</h4>
      <p>Since $B \succ 0$, it has a Cholesky factorization $B = L L^\top$. Let $\mathbf{x} = L^{-\top} \mathbf{y}$.
      $$ A (L^{-\top} \mathbf{y}) = \lambda B (L^{-\top} \mathbf{y}) = \lambda (L L^\top) L^{-\top} \mathbf{y} = \lambda L \mathbf{y} $$
      Left-multiplying by $L^{-1}$:
      $$ (L^{-1} A L^{-\top}) \mathbf{y} = \lambda \mathbf{y} $$
      Thus, the generalized eigenvalues of $(A, B)$ are the ordinary eigenvalues of the symmetric matrix $C = L^{-1} A L^{-\top}$ (often written $B^{-1/2} A B^{-1/2}$).
      <br><b>Key Insight:</b> This process is called <b>whitening</b>. It transforms the geometry so that the $B$-norm becomes the Euclidean norm.</p>

      <h4>Generalized Rayleigh Quotient</h4>
      <p>The generalized Rayleigh quotient is:
      $$ R_{A,B}(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top B \mathbf{x}} $$
      <b>Variational Theorem:</b>
      $$ \lambda_{\max}(A, B) = \max_{\mathbf{x} \neq 0} \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top B \mathbf{x}} $$
      <b>Proof:</b> Substitute $\mathbf{x} = B^{-1/2}\mathbf{y}$. Then $\mathbf{x}^\top B \mathbf{x} = \mathbf{y}^\top \mathbf{y}$ and $\mathbf{x}^\top A \mathbf{x} = \mathbf{y}^\top (B^{-1/2} A B^{-1/2}) \mathbf{y}$.
      The problem becomes maximizing the standard Rayleigh quotient of $C = B^{-1/2} A B^{-1/2}$.</p>

      <div class="insight">
        <h4>The "Best Constant" Interpretation (LMI Feasibility)</h4>
        <p>The generalized maximum eigenvalue is the smallest scalar $t$ such that $tB - A \succeq 0$.
        <br><i>Proof:</i>
        $$ tB - A \succeq 0 \iff \mathbf{x}^\top (tB - A) \mathbf{x} \ge 0 \ \forall \mathbf{x} \iff t (\mathbf{x}^\top B \mathbf{x}) \ge \mathbf{x}^\top A \mathbf{x} \iff t \ge \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top B \mathbf{x}} $$
        The tightest such bound is the maximum of the quotient.
        <br>This connects spectral theory to <b>Semidefinite Programming</b>: finding generalized eigenvalues is equivalent to determining the feasibility of an LMI.</p>
      </div>
    </section>

    <!-- SECTION 5: INDUCED NORMS -->
    <section class="section-card" id="section-induced-norms">
      <h2>3. Induced Matrix Norms</h2>
      <p>We defined vector norms in Lecture 00. Now we ask: how "big" is a matrix $A$? The <b>induced norm</b> measures the maximum factor by which $A$ can stretch a vector. It is effectively the <b>Lipschitz constant</b> of the linear map.</p>
      $$ \|A\|_p = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|_p}{\|\mathbf{x}\|_p} = \max_{\|\mathbf{x}\|_p = 1} \|A\mathbf{x}\|_p $$

      <h3>3.1 The $\ell_1$ Operator Norm (Max Column Sum)</h3>
      <p>$$ \boxed{\|A\|_1 = \max_{j} \sum_{i=1}^m |a_{ij}|} $$</p>
      <div class="proof-box">
        <h4>Derivation: Budget Allocation Intuition</h4>
        <div class="proof-step">
          <strong>Step 1: Upper Bound.</strong>
          Think of $\|\mathbf{x}\|_1 = 1$ as a "mass budget" of 1 that you can distribute among the coordinates $x_j$. The map $A$ takes mass from coordinate $j$ and distributes it according to column $j$.
          $$ \|A\mathbf{x}\|_1 = \left\| \sum_j x_j \mathbf{a}_j \right\|_1 \le \sum_j |x_j| \|\mathbf{a}_j\|_1 $$
          To maximize the output mass, you should put all your budget on the column $\mathbf{a}_k$ that has the largest norm.
          $$ \sum_j |x_j| \|\mathbf{a}_j\|_1 \le (\max_k \|\mathbf{a}_k\|_1) \sum_j |x_j| = \max_k \|\mathbf{a}_k\|_1 $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Achievability.</strong>
          Simply choose $\mathbf{x} = \mathbf{e}_k$, where $k$ is the index of the largest column. Then $\|A\mathbf{e}_k\|_1 = \|\mathbf{a}_k\|_1$, hitting the bound.
        </div>
      </div>

      <h3>3.2 The $\ell_\infty$ Operator Norm (Max Row Sum)</h3>
      <p>$$ \boxed{\|A\|_\infty = \max_{i} \sum_{j=1}^n |a_{ij}|} $$</p>
      <div class="proof-box">
        <h4>Derivation: Sign Alignment Intuition</h4>
        <div class="proof-step">
          <strong>Step 1: Upper Bound.</strong>
          Here $\|\mathbf{x}\|_\infty = 1$ means $|x_j| \le 1$ for all $j$. We want to maximize the absolute value of some row output $(A\mathbf{x})_i = \sum_j a_{ij} x_j$.
          $$ |(A\mathbf{x})_i| \le \sum_j |a_{ij}| |x_j| \le \sum_j |a_{ij}| $$
          So the output cannot exceed the maximum absolute row sum.
        </div>
        <div class="proof-step">
          <strong>Step 2: Achievability (Constructing the "Bad" Input).</strong>
          Let $k$ be the index of the row with the maximum absolute sum.
          Construct $\mathbf{x}$ such that $x_j = \text{sign}(a_{kj})$. Then $\|\mathbf{x}\|_\infty = 1$.
          The $k$-th entry of the output is:
          $$ (A\mathbf{x})_k = \sum_j a_{kj} \text{sign}(a_{kj}) = \sum_j |a_{kj}| = R $$
          Thus $\|A\mathbf{x}\|_\infty \ge R$, proving equality.
        </div>
      </div>

      <h3>3.3 The $\ell_2$ Operator Norm (Spectral Norm)</h3>
      <p>$$ \|A\|_2 = \max_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2 = \sqrt{\lambda_{\max}(A^\top A)} = \sigma_{\max}(A) $$</p>
      <p><b>Geometry:</b> A linear map $A$ transforms the unit sphere into an ellipsoid. The spectral norm $\|A\|_2$ is the length of the <b>longest semi-axis</b> of that ellipsoid (the maximum stretch).</p>
      <p><b>Lipschitz Interpretation:</b> The induced norm is the smallest number $L$ such that $\|Ax - Ay\| \le L \|x - y\|$. It is the "best Lipschitz constant" for the linear map.</p>

      <h3>3.4 Convexity of Induced Norms</h3>
      <p>Why are induced norms convex functions of the matrix $A$? We can prove this using the property that the pointwise supremum of linear functions is convex.</p>
      <div class="proof-box">
        <h4>Proof: Convexity via Dual Norms</h4>
        <div class="proof-step">
          <strong>Step 1: Definition.</strong> The induced norm is defined as:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\mathbf{v} \ne 0} \frac{\|X\mathbf{v}\|_a}{\|\mathbf{v}\|_\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \|X\mathbf{v}\|_a $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Dual Norm Representation.</strong> Recall that any norm $\|z\|_a$ can be expressed as a supremum over its dual norm ball: $\|z\|_a = \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top \mathbf{z}$.
          Substituting this into the definition:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \left( \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top (X\mathbf{v}) \right) $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Linear Structure.</strong>
          $$ \|X\|_{a,\mathbf{b}} = \sup \{ \mathbf{u}^\top X \mathbf{v} \mid \|\mathbf{u}\|_{a^*} \le 1, \|\mathbf{v}\|_\mathbf{b} = 1 \} $$
          Notice that for fixed vectors $\mathbf{u}$ and $\mathbf{v}$, the function $f_{\mathbf{u},\mathbf{v}}(X) = \mathbf{u}^\top X \mathbf{v}$ is <b>linear</b> in the entries of $X$.
          Specifically, $\mathbf{u}^\top X \mathbf{v} = \mathrm{tr}(\mathbf{u}^\top X \mathbf{v}) = \mathrm{tr}(\mathbf{v} \mathbf{u}^\top X) = \langle uv^\top, X \rangle$.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong>
          The induced norm $\|X\|_{a,b}$ is the <b>pointwise supremum</b> of a family of linear functions. Since the supremum of any collection of convex functions is convex, the induced norm is a convex function of $X$.
        </div>
      </div>
    </section>

    <!-- SECTION 6: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>4. The QR Decomposition: Geometry First</h2>

      <p>QR is not just an algorithm. It is the <b>algebraic shadow</b> of the Gram-Schmidt process. It formalizes the fact that every matrix can be split into "perfect geometry" ($Q$) and "coordinates" ($R$).</p>

      <h3>4.1 Definition and Existence</h3>
      <p>Let $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ (Full column rank). There exists a unique factorization:
      $$ \boxed{ A = Q R } $$
      where:
      <ul>
        <li>$Q \in \mathbb{R}^{m \times n}$ has <b>orthonormal columns</b> ($Q^\top Q = I_n$). This represents an isometric embedding.</li>
        <li>$R \in \mathbb{R}^{n \times n}$ is <b>upper triangular</b> with positive diagonal entries. This represents coordinates in the orthonormal basis.</li>
      </ul>
      <p><b>Proof by Geometry (Gram-Schmidt as Projection):</b> Let $a_1, \dots, a_n$ be the columns of $A$. At each step $k$, we seek a vector $q_k$ that captures the "new direction" of $a_k$ orthogonal to the previous subspace $W_{k-1} = \mathrm{span}(a_1, \dots, a_{k-1})$.
      <br>There is exactly one way to do this:
      $$ u_k = a_k - \operatorname{proj}_{W_{k-1}}(a_k) $$
      This residual $u_k$ is orthogonal to $W_{k-1}$ by the projection theorem. Normalizing it gives $q_k = u_k / \|u_k\|$.
      <br>Since $a_k$ is a linear combination of $q_k$ and the previous basis vectors ($q_1, \dots, q_{k-1}$), we have a triangular relation:
      $$ a_k = \sum_{j=1}^k r_{jk} q_j $$
      where $r_{jk} = \langle q_j, a_k \rangle$. This forces the matrix form $A=QR$, where $R$ is upper triangular. The diagonal entries $r_{kk} = \|u_k\|$ are positive, ensuring uniqueness.</p>

      <h3>4.2 Why QR is the "Correct" Way to Solve Least Squares</h3>
      <p>Consider $\min \|A\mathbf{x} - \mathbf{b}\|_2$.
      Using Normal Equations: $A^\top A \mathbf{x} = A^\top \mathbf{b}$. The condition number is $\kappa(A^\top A) = \kappa(A)^2$. This squaring doubles the number of digits of precision lost.
      <br>Using QR:
      $$ \|A\mathbf{x} - \mathbf{b}\|_2 = \|QR\mathbf{x} - \mathbf{b}\|_2 $$
      Since $Q$ preserves norms (on its range), we multiply the residual by $Q^\top$ (projecting onto the geometry):
      $$ R\mathbf{x} = Q^\top \mathbf{b} $$
      This is a triangular system. The condition number is $\kappa(R) = \kappa(A)$.
      <br><b>Conclusion:</b> QR is numerically stable because it separates the geometry ($Q$) from the conditioning ($R$). It never squares the condition number.</p>

      <h3>4.3 Full vs. Thin QR</h3>
      <p>
      <ul>
        <li><b>Thin QR ($Q \in \mathbb{R}^{m \times n}$):</b> Only computes an orthonormal basis for the column space. Sufficient for least squares.</li>
        <li><b>Full QR ($Q \in \mathbb{R}^{m \times m}$):</b> Extends $Q$ to a basis for the entire space $\mathbb{R}^m$. $A = [Q_1 \ Q_2] \begin{bmatrix} R \\ 0 \end{bmatrix}$. $Q_2$ forms a basis for the left nullspace $\mathcal{N}(A^\top)$.</li>
      </ul>
      </p>

      <h3>4.4 Householder Reflections: The Algorithmic Implementation</h3>

      <div class="insight">
        <h4>Why Not Gram-Schmidt in Practice?</h4>
        <p>The Gram-Schmidt algorithm described in Section 4.1 is conceptually elegant but <b>numerically unstable</b>. When vectors are nearly collinear, the computed $q_k$ vectors progressively lose orthogonality due to rounding errors. Modified Gram-Schmidt improves this somewhat, but the gold standard for computing QR is <b>Householder reflections</b> - a method that achieves backward stability.</p>
      </div>

      <p>The <b>Householder reflection</b> is a linear transformation that reflects a vector across a hyperplane. It provides a systematic way to "zero out" entries below the diagonal, column by column, transforming $A$ into upper triangular form $R$ while building $Q$ implicitly.</p>

      <h4>Definition: Householder Matrix</h4>
      <p>Given a unit vector $\mathbf{u} \in \mathbb{R}^m$ (the normal to the hyperplane), the <b>Householder matrix</b> is:
      $$ H = I - 2 \mathbf{u} \mathbf{u}^\top $$
      This matrix has three key properties:</p>
      <ul>
        <li><b>Symmetric:</b> $H^\top = H$ (since $\mathbf{u}\mathbf{u}^\top$ is symmetric)</li>
        <li><b>Orthogonal:</b> $H^\top H = H^2 = I$ (reflections applied twice return to the original)</li>
        <li><b>Reflection:</b> For any $\mathbf{x}$, $H\mathbf{x}$ is the reflection of $\mathbf{x}$ across the hyperplane $\{\mathbf{v} \mid \mathbf{u}^\top \mathbf{v} = 0\}$</li>
      </ul>

      <div class="proof-box">
        <h4>Verification of Orthogonality</h4>
        <p>
        $$ H^2 = (I - 2\mathbf{u}\mathbf{u}^\top)^2 = I - 4\mathbf{u}\mathbf{u}^\top + 4\mathbf{u}\mathbf{u}^\top \mathbf{u}\mathbf{u}^\top $$
        Since $\mathbf{u}$ is a unit vector, $\mathbf{u}^\top \mathbf{u} = 1$, so:
        $$ H^2 = I - 4\mathbf{u}\mathbf{u}^\top + 4\mathbf{u}\mathbf{u}^\top = I $$
        </p>
      </div>

      <h4>The Householder QR Algorithm</h4>
      <p>The algorithm constructs a sequence of Householder matrices $H_1, H_2, \ldots, H_n$ such that:
      $$ H_n \cdots H_2 H_1 A = R $$
      where $R$ is upper triangular. Since each $H_k$ is orthogonal and symmetric, we have:
      $$ A = (H_1 H_2 \cdots H_n) R = Q R $$
      with $Q = H_1 H_2 \cdots H_n$ orthogonal.</p>

      <div class="subsection">
        <h5>Step k: Zeroing the k-th Column</h5>
        <p>At step $k$, we've already zeroed columns $1, \ldots, k-1$. The matrix looks like:
        $$
        A^{(k-1)} = \begin{bmatrix}
        R_{1:k-1, 1:k-1} & * & \cdots & * \\
        0 & x_k & \cdots & * \\
        \vdots & x_{k+1} & & \vdots \\
        0 & x_m & \cdots & *
        \end{bmatrix}
        $$
        We want to choose $H_k$ to transform the $k$-th column below the diagonal into zeros.
        <br><br>
        Let $\mathbf{x} = (x_k, x_{k+1}, \ldots, x_m)^\top \in \mathbb{R}^{m-k+1}$ be the subvector below the diagonal. We seek a reflection that maps $\mathbf{x}$ to $\|\mathbf{x}\| \mathbf{e}_1 = (\|\mathbf{x}\|, 0, \ldots, 0)^\top$.
        <br><br>
        The normal vector $\mathbf{u}$ to the reflecting hyperplane is:
        $$ \mathbf{u} = \frac{\mathbf{x} - \|\mathbf{x}\| \mathbf{e}_1}{\|\mathbf{x} - \|\mathbf{x}\| \mathbf{e}_1\|} $$
        In practice, we choose the sign to avoid cancellation:
        $$ \mathbf{u} = \frac{\mathbf{x} + \mathrm{sign}(x_k) \|\mathbf{x}\| \mathbf{e}_1}{\|\mathbf{x} + \mathrm{sign}(x_k) \|\mathbf{x}\| \mathbf{e}_1\|} $$
        </p>
      </div>

      <div class="example-box">
        <h4>Example: QR via Householder for a 3×2 Matrix</h4>
        <p>Consider $A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}$.
        <br><br>
        <b>Step 1:</b> Zero out the first column below the diagonal.
        <br>$\mathbf{x} = (1, 1, 0)^\top$, $\|\mathbf{x}\| = \sqrt{2}$.
        <br>$\mathbf{v} = \mathbf{x} + \sqrt{2} \mathbf{e}_1 = (1+\sqrt{2}, 1, 0)^\top$.
        <br>Normalize: $\mathbf{u} = \frac{\mathbf{v}}{\|\mathbf{v}\|}$.
        <br>$H_1 = I - 2\mathbf{u}\mathbf{u}^\top$ transforms the first column to $(\sqrt{2}, 0, 0)^\top$.
        <br><br>
        <b>Step 2:</b> Apply $H_1$ to entire matrix, then construct $H_2$ to zero out the second column below diagonal (in the $2 \times 2$ submatrix).
        <br><br>
        <b>Result:</b> $A = QR$ where $Q = H_1 H_2$ and $R$ is upper triangular.
        </p>
      </div>

      <h4>Why Householder is Superior</h4>
      <div class="insight">
        <h4>Numerical Stability Comparison</h4>
        <table style="width: 100%; border-collapse: collapse; margin: 12px 0;">
          <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
            <th style="text-align: left; padding: 8px;"><b>Method</b></th>
            <th style="text-align: left; padding: 8px;"><b>Stability</b></th>
            <th style="text-align: left; padding: 8px;"><b>Cost</b></th>
          </tr>
          <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
            <td style="padding: 8px;">Classical Gram-Schmidt</td>
            <td style="padding: 8px;">❌ Unstable (loss of orthogonality)</td>
            <td style="padding: 8px;">$2mn^2$ flops</td>
          </tr>
          <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
            <td style="padding: 8px;">Modified Gram-Schmidt</td>
            <td style="padding: 8px;">⚠️ Better, but still issues</td>
            <td style="padding: 8px;">$2mn^2$ flops</td>
          </tr>
          <tr>
            <td style="padding: 8px;"><b>Householder Reflections</b></td>
            <td style="padding: 8px;">✅ <b>Backward stable</b></td>
            <td style="padding: 8px;">$2mn^2 - \frac{2}{3}n^3$ flops</td>
          </tr>
        </table>
        <p><b>Backward stability</b> means the computed $\hat{Q}$, $\hat{R}$ satisfy $\hat{Q}\hat{R} = A + E$ where $\|E\| = O(\epsilon_{\text{machine}} \|A\|)$. The algorithm computes the <i>exact</i> QR of a nearby matrix.</p>
      </div>

      <h4>Connection to Optimization</h4>
      <p>Householder reflections appear in several optimization contexts:</p>
      <ul>
        <li><b>Interior-point methods:</b> Forming and factoring the KKT system requires stable QR factorization</li>
        <li><b>Trust region methods:</b> The Steihaug-Toint conjugate gradient algorithm uses Householder to maintain orthogonality of search directions</li>
        <li><b>Constrained least squares:</b> Equality constraints $C\mathbf{x} = \mathbf{d}$ are handled by QR factorization of $C^\top$, yielding a reduced unconstrained problem</li>
      </ul>

    </section>

    <!-- SECTION 7: SVD -->
    <section class="section-card" id="section-svd">
      <h2>5. Singular Value Decomposition (SVD): The True Geometry</h2>
      <p>If the spectral theorem was "perfect geometry under symmetry," and QR was "geometry separated from coordinates," then <b>SVD is the fundamental theorem of linear algebra</b>. It applies to every matrix, square or rectangular, symmetric or not.</p>

      <h3>5.1 The Geometry of SVD: Rotate → Scale → Rotate</h3>
      <p><b>Theorem (SVD):</b> Any matrix $A \in \mathbb{R}^{m \times n}$ can be factored as:
      $$ \boxed{ A = U \Sigma V^\top } $$
      where:
      <ul>
        <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal (Left singular vectors, basis for codomain).</li>
        <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal (Right singular vectors, basis for domain).</li>
        <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with non-negative entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
      </ul>
      <p><b>Interpretation:</b>
      $$ x \xrightarrow{V^\top} \text{rotate coordinates} \xrightarrow{\Sigma} \text{independent scaling} \xrightarrow{U} \text{rotate to output basis} $$
      This proves that the image of the unit sphere under <i>any</i> linear map is a hyperellipse. The singular values are the lengths of the semi-axes.</p>

      <div class="insight">
        <h4>Why Singular Values = Roots of Eigenvalues of $A^\top A$</h4>
        <p>Using $A = U \Sigma V^\top$:
        $$ A^\top A = (V \Sigma^\top U^\top)(U \Sigma V^\top) = V (\Sigma^\top \Sigma) V^\top = V \Lambda V^\top $$
        Since $U^\top U = I$, we just multiply the diagonal matrices. $\Lambda = \text{diag}(\sigma_1^2, \dots, \sigma_n^2)$.
        <br>Thus:
        <ol>
            <li>The <b>right singular vectors</b> $V$ are the eigenvectors of $A^\top A$.</li>
            <li>The squared singular values $\sigma_i^2$ are the eigenvalues of $A^\top A$.</li>
            <li>This explains why $\|A\|_2 = \sigma_1 = \sqrt{\lambda_{\max}(A^\top A)}$.</li>
        </ol>
        </p>
      </div>

      <div class="proof-box">
        <h4>Rigorous Derivation via $A^\top A$</h4>
        <p>The existence of the SVD is a direct consequence of the Spectral Theorem applied to the symmetric positive semidefinite matrix $G = A^\top A$. This matrix captures the geometry of the domain metric induced by $A$.</p>
        <div class="proof-step">
            <strong>Step 1: Spectral Decomposition of $G$.</strong>
            Since $G = A^\top A$ is symmetric, there exists an orthonormal basis $\{v_1, \dots, v_n\}$ of eigenvectors with real eigenvalues $\lambda_1 \ge \dots \ge \lambda_n$.
            $$ A^\top A v_i = \lambda_i v_i $$
            Since $\|A v_i\|^2 = v_i^\top A^\top A v_i = \lambda_i \|v_i\|^2 = \lambda_i$, we must have $\lambda_i \ge 0$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Define Singular Values and Right Singular Vectors.</strong>
            Set $\sigma_i = \sqrt{\lambda_i}$. Let $r$ be the number of non-zero singular values (the rank).
            The matrix $V = [v_1 \dots v_n]$ is orthogonal.
        </div>
        <div class="proof-step">
            <strong>Step 3: Construct Left Singular Vectors.</strong>
            For $i \le r$, define $u_i = \frac{1}{\sigma_i} A v_i$.
            Check orthonormality:
            $$ u_i^\top u_j = \frac{1}{\sigma_i \sigma_j} v_i^\top A^\top A v_j = \frac{1}{\sigma_i \sigma_j} v_i^\top (\lambda_j v_j) = \frac{\sigma_j^2}{\sigma_i \sigma_j} \delta_{ij} = \delta_{ij} $$
            For $i > r$, $A v_i = 0$. We complete the set $\{u_1, \dots, u_r\}$ to an orthonormal basis $\{u_1, \dots, u_m\}$ for $\mathbb{R}^m$ (e.g., via Gram-Schmidt).
        </div>
        <div class="proof-step">
            <strong>Step 4: Matrix Factorization.</strong>
            Compute $A V$:
            $$ A v_i = \begin{cases} \sigma_i u_i & i \le r \\ 0 & i > r \end{cases} $$
            This is exactly the column-wise definition of $U \Sigma$. Thus $AV = U \Sigma$, or $A = U \Sigma V^\top$.
        </div>
      </div>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-wiki.png" alt="SVD Decomposition Wiki" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption><i>Figure 5a:</i> Standard SVD visualization (Wikimedia).</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-ellipse.png" alt="SVD Ellipse" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption><i>Figure 5b:</i> Unit circle to ellipse transformation.</figcaption>
        </figure>
      </div>

      <h3>5.2 Rank and the Four Fundamental Subspaces</h3>
      <p>SVD provides the optimal basis for all four fundamental subspaces of the matrix $A$. Let $r$ be the number of non-zero singular values.</p>
      <ul>
        <li><b>Range (Column Space):</b> Spanned by the first $r$ columns of $U$ (Left singular vectors).
        $$ \mathcal{R}(A) = \operatorname{span}\{u_1, \dots, u_r\} $$</li>
        <li><b>Nullspace (Kernel):</b> Spanned by the last $n-r$ columns of $V$ (Right singular vectors corresponding to $\sigma_i=0$).
        $$ \mathcal{N}(A) = \operatorname{span}\{v_{r+1}, \dots, v_n\} $$</li>
        <li><b>Row Space:</b> Spanned by the first $r$ columns of $V$ (Right singular vectors corresponding to $\sigma_i>0$).
        $$ \mathcal{R}(A^\top) = \operatorname{span}\{v_1, \dots, v_r\} $$</li>
        <li><b>Left Nullspace:</b> Spanned by the last $m-r$ columns of $U$.
        $$ \mathcal{N}(A^\top) = \operatorname{span}\{u_{r+1}, \dots, u_m\} $$</li>
      </ul>
      <p>This explicitly diagonalizes the geometry: $A$ maps the row space basis $\{v_1, \dots, v_r\}$ to the column space basis $\{u_1, \dots, u_r\}$ with scaling factors $\sigma_i$, and kills everything else.</p>

      <h3>5.3 Low-Rank Approximation (Eckart-Young-Mirsky)</h3>
      <p>SVD solves the problem of finding the best approximation of a matrix by one of lower rank.
      $$ A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top $$
      This matrix $A_k$ minimizes $\|A - X\|$ in both Spectral and Frobenius norms among all matrices of rank $k$.
      <br><b>Applications:</b> Image compression, PCA, Denoising.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> An image can be viewed as a matrix of pixel intensities. By computing its SVD and keeping only the largest singular values, we can reconstruct the image with far less data.</p>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD Geometry</h3>
        <p>Visualize the SVD decomposition $A = U \Sigma V^\top$ as a sequence of three geometric transformations: rotation ($V^\top$), axis-aligned scaling ($\Sigma$), and rotation ($U$).</p>
        <div id="widget-svd-geometry" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>5.4 The Schur Complement: Block Elimination</h3>
      <p>The <b>Schur Complement</b> arises naturally when solving block linear systems or analyzing positive definiteness. While often derived via Gaussian elimination, it has a deeper optimization interpretation: it is the result of <b>minimizing a quadratic form over a subset of variables</b>.</p>

      <div class="insight">
        <h4>Derivation via "Completing the Square"</h4>
        <p>Consider the quadratic form associated with a symmetric block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ (where $A \succ 0$):
        $$ f(x, y) = \begin{bmatrix} x \\ y \end{bmatrix}^\top \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = x^\top A x + 2 x^\top B y + y^\top C y $$
        We want to determine if $f(x, y) \ge 0$ for all $x, y$.
        <br>Minimizing with respect to $x$ (for fixed $y$) requires $\nabla_x f = 2Ax + 2By = 0$, so $x^* = -A^{-1}By$.
        <br>Substituting $x^*$ back into the expression completes the square:
        $$ f(x^*, y) = (-A^{-1}By)^\top A (-A^{-1}By) + 2 (-A^{-1}By)^\top B y + y^\top C y $$
        $$ = y^\top (B^\top A^{-1} A A^{-1} B) y - 2 y^\top B^\top A^{-1} B y + y^\top C y $$
        $$ = y^\top (B^\top A^{-1} B - 2 B^\top A^{-1} B + C) y = y^\top (C - B^\top A^{-1} B) y $$
        The matrix $S = C - B^\top A^{-1} B$ is the <b>Schur complement</b>.
        <br>Since $x^*$ minimizes the form, $f(x, y) \ge 0$ for all $x, y$ if and only if the minimum value $y^\top S y \ge 0$ for all $y$. Thus:
        $$ M \succeq 0 \iff A \succ 0 \text{ and } S \succeq 0 $$
        </p>
      </div>

      <div class="theorem-box">
        <h4>Theorem: Schur Complement Condition for PSD</h4>
        <p>If $A \succ 0$, then:
        $$ M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} \succeq 0 \iff C - B^\top A^{-1} B \succeq 0 $$
        This lemma is the "universal translator" for converting rational inequalities into Linear Matrix Inequalities (LMIs). For example:
        $$ x^\top P^{-1} x \le t \iff \begin{bmatrix} P & x \\ x^\top & t \end{bmatrix} \succeq 0 \quad (\text{for } P \succ 0) $$
        </p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: Schur Complement</h3>
        <p>Visualize the quadratic form of a block matrix. See how the Schur complement condition $s = a - b^2/d \ge 0$ determines positive semidefiniteness. The line $y = -(b/d)x$ corresponds to minimizing the quadratic over $y$ for a fixed $x$, which is the geometric operation underlying the Schur complement.</p>
        <div id="widget-schur-complement" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>5.5 Application: PCA as SVD of Data</h3>
      <p>Principal Component Analysis (PCA) is often presented as a statistical method. In reality, it is <b>pure geometry</b>: finding the best ellipsoidal approximation to a point cloud. We derive it from scratch using SVD, bypassing "covariance" jargon.</p>

      <h4>1) The Question</h4>
      <p>Given data points $\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^d$, PCA asks: <b>Along which orthogonal directions does the data spread the most?</b></p>

      <h4>2) Centering (Non-negotiable)</h4>
      <p>Define the mean $\mu = \frac{1}{N}\sum \mathbf{x}_i$. We work with centered data $\tilde{\mathbf{x}}_i = \mathbf{x}_i - \mu$.
      <br>Stack them into a data matrix $X \in \mathbb{R}^{N \times d}$ (rows are points):
      $$ X = \begin{bmatrix} \tilde{\mathbf{x}}_1^\top \\ \vdots \\ \tilde{\mathbf{x}}_N^\top \end{bmatrix} $$
      </p>

      <h4>3) Variance as a Quadratic Form</h4>
      <p>The variance along a unit direction $\mathbf{v}$ is the average squared projection:
      $$ \text{Var}(\mathbf{v}) = \frac{1}{N} \sum_{i=1}^N (\tilde{\mathbf{x}}_i^\top \mathbf{v})^2 = \frac{1}{N} \|X\mathbf{v}\|_2^2 = \mathbf{v}^\top \left( \frac{1}{N} X^\top X \right) \mathbf{v} $$
      Let $C = \frac{1}{N} X^\top X$ be the empirical covariance matrix. The problem is to maximize $\mathbf{v}^\top C \mathbf{v}$ subject to $\|\mathbf{v}\|=1$. This is exactly the Rayleigh Quotient maximization.</p>

      <h4>4) Enter SVD</h4>
      <p>Take the SVD of the centered data matrix: $\boxed{X = U \Sigma V^\top}$.
      <br>Then the covariance matrix is:
      $$ C = \frac{1}{N} (V \Sigma U^\top)(U \Sigma V^\top) = V \left( \frac{1}{N} \Sigma^2 \right) V^\top $$
      This is the eigendecomposition of $C$.
      </p>

      <h4>5) The Geometric Translation</h4>
      <ul>
        <li><b>Principal Components:</b> The columns of $V$ (Right singular vectors). These are the axes of the ellipse.</li>
        <li><b>Explained Variance:</b> The eigenvalues $\lambda_i = \sigma_i^2 / N$.</li>
        <li><b>Principal Scores:</b> The coordinates of the data in the new basis are given by $XV = U \Sigma$.</li>
      </ul>

      <h4>6) Dimensionality Reduction</h4>
      <p>The truncated SVD $X_k = U_k \Sigma_k V_k^\top$ gives the optimal rank-$k$ approximation of the data in the Frobenius norm. This means PCA minimizes the reconstruction error:
      $$ \sum_{i=1}^N \|\mathbf{x}_i - \text{proj}_k(\mathbf{x}_i)\|^2 $$
      PCA is not just a heuristic; it is the <b>optimal</b> linear compression scheme.</p>
    </section>

    <!-- SECTION 6: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>6. The Pseudoinverse and Condition Number</h2>

      <h3>6.1 The Moore-Penrose Pseudoinverse ($A^+$)</h3>
      <p>When $A$ is not square or not invertible, we cannot define $A^{-1}$. However, we can define a generalized inverse $A^+$ that "inverts" the action of $A$ on its range and sends the orthogonal complement to zero.</p>

      <h4>The Moore-Penrose Axioms</h4>
      <p>The pseudoinverse $A^+$ is the unique matrix satisfying these four algebraic conditions:
      <ol>
        <li>$A A^+ A = A$ ( $A A^+$ is a projection on the range).</li>
        <li>$A^+ A A^+ = A^+$ ( $A^+$ acts like an inverse on the range of $A$).</li>
        <li>$(A A^+)^\top = A A^+$ (Symmetry of projector).</li>
        <li>$(A^+ A)^\top = A^+ A$ (Symmetry of projector).</li>
      </ol>
      </p>

      <div class="proof-box">
        <h4>Proof: Uniqueness of the Pseudoinverse</h4>
        <p>Suppose $X$ and $Y$ both satisfy the four Moore-Penrose axioms. We show $X=Y$.
        <br>1. Start with $X = X A X$.
        <br>2. Use $A = A Y A$: $X = X (A Y A) X$.
        <br>3. Group terms: $X = (X A) Y (A X)$.
        <br>4. Use Axiom 3/4 to introduce symmetry: $X A = (X A)^\top = A^\top X^\top$ and $A X = (A X)^\top = X^\top A^\top$. (Wait, this direction is harder).
        <br><b>Alternative Clean Derivation:</b>
        $$ X = X A X = (X A) X = (X A)^\top X = A^\top X^\top X = (A Y A)^\top X^\top X = A^\top Y^\top A^\top X^\top X $$
        $$ = (Y A)^\top (X A)^\top X = Y A X A X = Y A X $$
        Symmetrically, $Y = Y A Y = Y A (X A Y) = Y A X A Y = (Y A X) A Y$.
        From above, $Y A X = X$, so $Y = X A Y$.
        <br>Now apply symmetry again:
        $X = Y A X = Y (A X) = Y (A X)^\top = Y X^\top A^\top = Y X^\top (A Y A)^\top = Y X^\top A^\top Y^\top A^\top$.
        This algebraic manipulation confirms that the geometric definition (via SVD) produces the <i>unique</i> matrix satisfying the algebraic axioms.
        </p>
      </div>

      <p><b>Definition via SVD:</b> If $A = U \Sigma V^\top$, then:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+$ is the $n \times m$ matrix obtained by transposing $\Sigma$ and inverting the non-zero singular values:
      $$
      \Sigma^+ = \begin{bmatrix}
      1/\sigma_1 & & & 0 & \dots \\
      & \ddots & & \vdots & \\
      & & 1/\sigma_r & 0 & \dots \\
      0 & \dots & 0 & 0 & \dots \\
      \vdots & & \vdots & \vdots & \ddots
      \end{bmatrix}
      $$
      (The non-zero block is diagonal with entries $1/\sigma_i$; all other entries are zero).</p>

      <h4>Properties and Applications</h4>
      <ul>
        <li><b>Minimum Norm Solution:</b> For any system $A\mathbf{x}=\mathbf{b}$ (or the least squares problem), the vector $\mathbf{x}^\star = A^+ \mathbf{b}$ is the solution that has the <b>minimum Euclidean norm</b>.
        <br>
        <div class="proof-box">
          <h4>Proof: $x_{LS} = A^+\mathbf{b}$ minimizes norm</h4>
        <p>The set of all least-squares solutions is given by $S_{LS} = \{\mathbf{x} \mid A^\top A \mathbf{x} = A^\top \mathbf{b}\}$. Since this is an affine set $x_p + \mathcal{N}(A)$, there is a unique element with minimum Euclidean norm. We show $\mathbf{x}^+ = A^+\mathbf{b}$ is that element.</p>

          <div class="proof-step">
          <strong>Step 1: Verification of Solution.</strong>
          First, we check that $\mathbf{x}^+$ actually solves the least squares problem (satisfies Normal Equations).
          Recall $A = U \Sigma V^\top$ and $A^+ = V \Sigma^+ U^\top$.
          $$ A \mathbf{x}^+ = U \Sigma V^\top V \Sigma^+ U^\top \mathbf{b} = U (\Sigma \Sigma^+) U^\top \mathbf{b} $$
          The product $\Sigma \Sigma^+$ is a diagonal projection matrix (1s for indices $1 \dots r$, 0s otherwise). It projects onto the range of $A$.
          Thus $A\mathbf{x}^+$ is the orthogonal projection of $\mathbf{b}$ onto $\mathcal{R}(A)$, which defines the least squares solution.
          </div>

          <div class="proof-step">
          <strong>Step 2: Orthogonality of $\mathbf{x}^+$.</strong>
          We check where $\mathbf{x}^+$ lies.
          $\mathbf{x}^+ = V (\Sigma^+ U^\top \mathbf{b})$. This vector is a linear combination of the columns of $V$ corresponding to non-zero singular values (indices $1 \dots r$).
          These columns span the <b>row space</b> $\mathcal{R}(A^\top)$.
          Thus, $\mathbf{x}^+ \in \mathcal{R}(A^\top)$.
          Recall the fundamental orthogonality: $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.
          </div>

          <div class="proof-step">
          <strong>Step 3: Pythagorean Minimization.</strong>
          Let $\mathbf{x}$ be any solution to the least squares problem. The set of all solutions is the affine set $S = \mathbf{x}^+ + \mathcal{N}(A)$.
          Any solution $\mathbf{x}$ can be written as $\mathbf{x} = \mathbf{x}^+ + \mathbf{z}$, where $\mathbf{z} \in \mathcal{N}(A)$.
          <br>We established in Step 2 that $\mathbf{x}^+ \in \mathcal{R}(A^\top)$.
          The Fundamental Theorem of Linear Algebra states that $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.
          Therefore, $\mathbf{x}^+ \perp \mathbf{z}$.
          <br>By the Pythagorean Theorem:
          $$ \|\mathbf{x}\|_2^2 = \|\mathbf{x}^+ + \mathbf{z}\|_2^2 = \|\mathbf{x}^+\|_2^2 + \|\mathbf{z}\|_2^2 $$
          Since $\|z\|_2^2 \ge 0$, the minimum possible value for $\|x\|_2^2$ occurs when $\|z\|_2^2 = 0$, i.e., $\mathbf{z}=0$.
          Thus, $\mathbf{x} = \mathbf{x}^+$ is the unique solution with minimum norm.
          </div>
        </div>
        </li>
        <li><b>Projectors:</b> $AA^+$ is the orthogonal projector onto the column space $\mathcal{R}(A)$. $A^+A$ is the orthogonal projector onto the row space $\mathcal{R}(A^\top)$.</li>
      </ul>

      <h3>6.2 Condition Number</h3>
      <p>The condition number $\kappa(A)$ measures the sensitivity of the solution of a linear system to perturbations in the data. Formally, it is defined as:</p>
      $$ \kappa(A) = \|A\|_2 \|A^+\|_2 = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} $$
      <p>where $\sigma_{\min}$ is the smallest <i>non-zero</i> singular value.</p>

      <ul>
        <li><b>$\kappa \approx 1$:</b> Well-conditioned. Errors are not amplified. Orthogonal matrices have $\kappa = 1$.</li>
        <li><b>$\kappa \gg 1$:</b> Ill-conditioned. Small errors in $\mathbf{b}$ can lead to massive errors in $\mathbf{x}$.</li>
      </ul>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/conditioning-geometry.png"
               alt="Well-conditioned versus ill-conditioned error amplification geometry"
               style="width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 6:</i> Condition number as error magnification: the same small uncertainty in $\mathbf{b}$ can map to a small or huge uncertainty in $\mathbf{x}$ depending on $\kappa(A)$.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/zig-zag-ill-conditioning.png"
               alt="Gradient descent zig-zagging on elongated contours in an ill-conditioned problem"
               style="width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 7:</i> Ill-conditioning creates elongated level sets; first-order methods like gradient descent can zig-zag across the valley, slowing convergence.</figcaption>
        </figure>
      </div>

      <div class="insight">
        <h4>Impact on Optimization Algorithms</h4>
        <p>The condition number is not just a numerical nuisance; it fundamentally limits the speed of optimization algorithms.
        For a quadratic function $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A \mathbf{x}$, Gradient Descent converges at a rate determined by $\frac{\kappa-1}{\kappa+1}$.
        <ul>
            <li>If $\kappa=1$ (spherical), convergence is instant.</li>
            <li>If $\kappa \gg 1$ (valley), convergence is arbitrarily slow.</li>
        </ul>
        This motivates <b>Newton's Method</b> and preconditioning, which effectively transform the problem to make $\kappa \approx 1$. We will study this in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>.</p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Stability</h3>
        <p><b>Visualize Ill-Conditioning:</b> Compare solving $A\mathbf{x}=\mathbf{b}$ for a well-conditioned matrix versus an ill-conditioned one. In the ill-conditioned case, the "ellipse" of the matrix is very skinny. A small change in the target vector $\mathbf{b}$ (moving the target slightly off the major axis) requires a massive change in $\mathbf{x}$ to compensate.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>6.3 Regularization: Controlled Bias for Stability</h3>
      <p>When a matrix is ill-conditioned, the pseudoinverse solution $x_{LS} = \sum \frac{\mathbf{u}_i^\top \mathbf{b}}{\sigma_i} \mathbf{v}_i$ becomes unstable because division by small $\sigma_i$ amplifies noise.
      <br><b>Regularization</b> stabilizes the inversion by filtering out these small singular values. The classic method is <b>Tikhonov Regularization</b> (Ridge Regression):</p>
      $$ \min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2 $$
      <p>The solution is $\mathbf{x}_{\text{ridge}} = (A^\top A + \lambda I)^{-1} A^\top \mathbf{b}$.</p>

      <h4>SVD Interpretation: Spectral Filtering</h4>
      <p>In terms of the SVD, Tikhonov regularization replaces the inversion $1/\sigma_i$ with a <b>filter factor</b> $f_i(\lambda)$:
      $$ \mathbf{x}_{\text{ridge}} = \sum_{i=1}^r \underbrace{\left( \frac{\sigma_i^2}{\sigma_i^2 + \lambda} \right)}_{\text{Filter } f_i} \frac{1}{\sigma_i} (\mathbf{u}_i^\top \mathbf{b}) \mathbf{v}_i $$
      Let's analyze the filter factor $f_i(\lambda) = \frac{\sigma_i^2}{\sigma_i^2+\lambda}$:
      <ul>
        <li><b>Signal Regime ($\sigma_i \gg \sqrt{\lambda}$):</b> $f_i \approx 1$. We keep the component (pseudoinverse behavior).</li>
        <li><b>Noise Regime ($\sigma_i \ll \sqrt{\lambda}$):</b> $f_i \approx \sigma_i^2/\lambda \to 0$. We suppress the component.</li>
      </ul>
      This acts as a "smooth low-pass filter" on the spectrum of the matrix, damping out high-frequency noise (small singular values) while preserving the dominant structure.</p>

      <h4>Truncated SVD (Hard Thresholding)</h4>
      <p>Another approach is to simply discard singular values below a threshold. This is the optimal low-rank approximation applied to the inversion process.
      $$ \mathbf{x}_{\text{trunc}} = \sum_{\sigma_i > \epsilon} \frac{1}{\sigma_i} (\mathbf{u}_i^\top \mathbf{b}) \mathbf{v}_i $$
      Both methods trade a small amount of bias (exactness) for a large reduction in variance (stability).</p>

      <div class="insight">
        <h4>The Irreducible Truth</h4>
        <p>You should now be able to see the single thread connecting these topics:
        <br><b>PCA, Least Squares, Pseudoinverses, and Regularization are all consequences of the SVD.</b>
        <br>The SVD exposes the intrinsic geometry of a linear map: which directions survive, which collapse, and which amplify noise. Every stable algorithm in optimization is, explicitly or implicitly, an SVD-aware algorithm.</p>
      </div>
    </section>

    <!-- SECTION 9: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>7. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>

      <h3>The Unified Picture</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>SVD Interpretation ($A=U\Sigma V^\top$)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>PCA</td>
            <td>SVD of centered data matrix</td>
          </tr>
          <tr>
            <td>Variance</td>
            <td>Squared singular values ($\sigma_i^2/N$)</td>
          </tr>
          <tr>
            <td>Least Squares</td>
            <td>Invert $\Sigma$ on range, zero on nullspace</td>
          </tr>
          <tr>
            <td>Ill-conditioning</td>
            <td>Decay of $\sigma_i$ to zero</td>
          </tr>
          <tr>
            <td>Regularization</td>
            <td>Smooth filtering of small $\sigma_i$</td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- SECTION 10: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 8. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.12 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Upper bound ($\le$):</strong> By Hölder's inequality, $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$.
          The definition of the dual norm is $\|y\|_* = \sup_{\|\mathbf{x}\|_p \le 1} \mathbf{x}^\top \mathbf{y}$.
          From Hölder, if $\|x\|_p \le 1$, then $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{y}\|_q$. Thus $\|y\|_* \le \|\mathbf{y}\|_q$.
        </div>
        <div class="proof-step">
          <strong>Lower bound ($\ge$):</strong> We need to construct a vector $\mathbf{x}$ that "aligns" with $\mathbf{y}$ to maximize the inner product.
          We want $x_i$ to have the same sign as $y_i$, and magnitudes such that equality holds in Hölder's.
          Choose $x_i = \mathrm{sign}(y_i) |y_i|^{q-1}$.
          <br>Check inner product: $\mathbf{x}^\top \mathbf{y} = \sum \mathrm{sign}(y_i)|y_i|^{q-1} y_i = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          <br>Check norm of $\mathbf{x}$: $\|x\|_p^p = \sum |x_i|^p = \sum |y_i|^{p(q-1)}$.
          Since $(p-1)q = p \implies p(q-1) = q$: $\|x\|_p^p = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          So $\|x\|_p = \|y\|_q^{q/p}$.
          <br>Now normalize to get a unit vector $\tilde{\mathbf{x}} = \mathbf{x} / \|\mathbf{x}\|_p = \mathbf{x} / \|\mathbf{y}\|_q^{q/p}$.
          $$ \tilde{\mathbf{x}}^\top \mathbf{y} = \frac{\|\mathbf{y}\|_q^q}{\|\mathbf{y}\|_q^{q/p}} = \|\mathbf{y}\|_q^{q - q/p} = \|\mathbf{y}\|_q^1 = \|\mathbf{y}\|_q $$
          Since we found a unit vector achieving this value, $\|y\|_* \ge \|\mathbf{y}\|_q$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Dual Norm:</b> Defined by $\|y\|_* = \sup \{ \mathbf{y}^\top \mathbf{x} \mid \|\mathbf{x}\| \le 1 \}$.
        <br><b>$\ell_p$ Dual:</b> The dual of the $\ell_p$ norm ($1 < p < \infty$) is the $\ell_q$ norm, where $1/p + 1/q = 1$.
        <br><b>Hölder's Inequality:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$. This is the generalized Cauchy-Schwarz inequality.
        <br><b>Alignment Condition:</b> Equality holds if $|x_i|^p$ is proportional to $|y_i|^q$ (and signs match). This is crucial for constructing worst-case examples in robustness analysis.</p>
      </div>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The Frobenius norm corresponds to the standard Euclidean norm of the vectorized matrix: $\|A\|_F = \|\mathrm{vec}(A)\|_2$.
        The inner product of matrices is $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.
        This is exactly the dot product of the vectorized forms: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        By the standard Cauchy-Schwarz inequality for vectors:
        $$ |\mathrm{vec}(A)^\top \mathrm{vec}(B)| \le \|\mathrm{vec}(A)\|_2 \|\mathrm{vec}(B)\|_2 $$
        Substituting back the matrix notation yields $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The $\mathrm{vec}$ Operator:</b> Stacks columns of a matrix into a single vector in $\mathbb{R}^{mn}$.
        <br><b>Isometry:</b> The mapping $A \mapsto \mathrm{vec}(A)$ is an isometry between $(\mathbb{R}^{m \times n}, \|\cdot\|_F)$ and $(\mathbb{R}^{mn}, \|\cdot\|_2)$.
        <br><b>Inner Product Consistency:</b> $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        <br><b>Result:</b> Cauchy-Schwarz for vectors immediately implies $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.</p>
      </div>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>1. Compute $A^\top A$ and its eigenvalues:</strong>
          $A^\top A = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$.
          Characteristic eq: $(1-\lambda)^2 - 1 = 0 \implies \lambda^2 - 2\lambda = 0 \implies \lambda_1=2, \lambda_2=0$.
          Singular values: $\sigma_1 = \sqrt{2}, \sigma_2 = 0$.
        </div>
        <div class="proof-step">
          <strong>2. Compute $V$ (eigenvectors of $A^\top A$):</strong>
          For $\lambda_1=2$: $\begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \mathbf{v} = 0 \implies v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
          For $\lambda_2=0$: $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
          $V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>3. Compute $U$:</strong>
          $u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
          Since $m=2$, we need a second vector $u_2$ orthogonal to $u_1$. $u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
          $U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>Result:</strong> $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \left(\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\right)^\top$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hand-Calculation Strategy for SVD:</b>
        <ol>
            <li>Compute the Gram matrix $G = A^\top A$.</li>
            <li>Find eigenvalues $\lambda_i$ of $G$. Singular values are $\sigma_i = \sqrt{\lambda_i}$.</li>
            <li>Find normalized eigenvectors $v_i$ of $G$ to form $V$.</li>
            <li>Compute $u_i = \frac{1}{\sigma_i} A v_i$ for non-zero $\sigma_i$.</li>
            <li>If $m > r$, complete $U$ using Gram-Schmidt or inspection (orthogonality).</li>
        </ol>
        <br><b>Check:</b> Verify $AV = U\Sigma$.</p>
      </div>

      <h3>P1.4 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, \mathbf{b} = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>(a) Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        $A^\top A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.
        $A^\top \mathbf{b} = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Solve $\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \mathbf{x} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Multiply by inverse $\frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$:
        $\mathbf{x} = \frac{1}{3} \begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 \\ 5 \end{bmatrix}$.</p>
        <p><b>(b) QR Factorization:</b>
        Gram-Schmidt on columns $a_1, a_2$.
        $u_1 = a_1 = (1, 0, 1)^\top$. $q_1 = (1/\sqrt{2}, 0, 1/\sqrt{2})^\top$.
        $v_2 = a_2 - \langle a_2, q_1 \rangle q_1 = (1, 1, 0)^\top - (1/\sqrt{2}) (1/\sqrt{2}, 0, 1/\sqrt{2})^\top = (1, 1, 0)^\top - (1/2, 0, 1/2)^\top = (1/2, 1, -1/2)^\top$.
        Normalize $v_2$: $\|v_2\|^2 = 1/4 + 1 + 1/4 = 1.5 = 3/2$. $\|v_2\| = \sqrt{3/2}$.
        $q_2 = \sqrt{2/3} (1/2, 1, -1/2)^\top$.
        Solve $R\mathbf{x} = Q^\top \mathbf{b}$.
        $R = \begin{bmatrix} \sqrt{2} & 1/\sqrt{2} \\ 0 & \sqrt{3/2} \end{bmatrix}$. $Q^\top \mathbf{b} = \begin{bmatrix} 3/\sqrt{2} \\ 4\sqrt{2/3}-1/\sqrt{6} \end{bmatrix}$.
        Back substitution yields same $\mathbf{x}$. QR avoids forming $A^\top A$ (better condition number).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Comparing Least Squares Algorithms:</b>
        <ul>
            <li><b>Normal Equations:</b>
                <ul><li>Cost: $O(n^2 m)$ (forming $A^\top A$) + $O(n^3)$ (Cholesky).</li>
                <li>Stability: Unstable. Dependence on $\kappa(A)^2$.</li></ul>
            </li>
            <li><b>QR Factorization:</b>
                <ul><li>Cost: $2 \times$ Normal Equations (roughly).</li>
                <li>Stability: Stable. Dependence on $\kappa(A)$. Standard choice.</li></ul>
            </li>
            <li><b>SVD:</b> Most expensive, but most robust (handles rank deficiency gracefully).</li>
        </ul></p>
      </div>

      <h3>P1.5 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $A = U \Sigma V^\top$. Then $A^+ = V \Sigma^+ U^\top$.
        $P = AA^+ = (U \Sigma V^\top)(V \Sigma^+ U^\top) = U (\Sigma \Sigma^+) U^\top$.
        $\Sigma \Sigma^+$ is a diagonal matrix with 1s for non-zero singular values and 0s otherwise.
        Let $U_r$ be the first $r$ columns of $U$. Then $P = U_r U_r^\top$.
        <br><b>Idempotent:</b> $P^2 = (U_r U_r^\top)(U_r U_r^\top) = U_r (U_r^\top U_r) U_r^\top = U_r I U_r^\top = P$.
        <br><b>Symmetric:</b> $P^\top = (U_r U_r^\top)^\top = (U_r^\top)^\top U_r^\top = U_r U_r^\top = P$.
        <br>Since the columns of $U_r$ form a basis for $\mathcal{R}(A)$, $P$ is the orthogonal projector onto $\mathcal{R}(A)$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Fundamental Projectors via SVD:</b>
        <ul>
            <li><b>Column Space Projector:</b> $P_{\mathcal{R}(A)} = A A^+ = U_r U_r^\top$.</li>
            <li><b>Row Space Projector:</b> $P_{\mathcal{R}(A^\top)} = A^+ A = V_r V_r^\top$.</li>
        </ul>
        <br><b>Intuition:</b> $A^+$ inverts the action on the range and kills the orthogonal complement. $AA^+$ maps range $\to$ range (identity) and orthogonal complement $\to$ 0.</p>
      </div>

      <h3>P1.6 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>We want to find the point $\mathbf{p} = A\mathbf{x}$ in the column space closest to $\mathbf{b}$.
        Problem: $\min_\mathbf{x} \|A\mathbf{x} - \mathbf{b}\|_2^2$.
        Gradient: $2A^\top (A\mathbf{x} - \mathbf{b}) = 0$.
        Normal eqs: $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        Since cols are independent, $A^\top A$ is invertible.
        Solution: $\mathbf{x}^* = (A^\top A)^{-1} A^\top \mathbf{b}$.
        Projection: $\mathbf{p} = A \mathbf{x}^* = A(A^\top A)^{-1} A^\top \mathbf{b}$.
        The matrix mapping $\mathbf{b}$ to $\mathbf{p}$ is $P = A(A^\top A)^{-1} A^\top$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The "Hat" Matrix:</b> $P = A(A^\top A)^{-1} A^\top$.
        <br><b>Requirements:</b> Valid only when $A$ has full column rank (so $A^\top A$ is invertible).
        <br><b>General Case:</b> If $A$ is rank-deficient, use the pseudoinverse form $P = AA^+$.
        <br><b>Properties:</b> $P^2=P$ and $P^\top=P$. The residual is $(I-P)\mathbf{b}$, which projects onto $\mathcal{N}(A^\top)$.</p>
      </div>

      <h3>P1.7 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Normal $a = (1, 1, 1)^\top$. Point $\mathbf{y} = (1, 2, 3)^\top$. Target $\mathbf{b}=3$.
        $\mathbf{p} = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a$.
        $a^\top \mathbf{y} = 1+2+3 = 6$. $\|a\|^2 = 3$.
        $\mathbf{p} = (1, 2, 3)^\top - \frac{6-3}{3} (1, 1, 1)^\top = (1, 2, 3)^\top - (1, 1, 1)^\top = (0, 1, 2)^\top$.
        Check: $0+1+2=3$. Correct.
        Distance: $\|y-p\| = \|(1, 1, 1)\| = \sqrt{3}$.
        Formula: $\frac{|a^\top \mathbf{y} - \mathbf{b}|}{\|a\|} = \frac{|6-3|}{\sqrt{3}} = \frac{3}{\sqrt{3}} = \sqrt{3}$. Matches.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Constraint Projection:</b> The projection of $\mathbf{y}$ onto the affine set $\{x \mid a^\top \mathbf{x} = \mathbf{b}\}$ is:
        $$ \Pi(\mathbf{y}) = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a $$
        <b>Optimization View:</b> This is the solution to $\min_\mathbf{x} \frac{1}{2}\|\mathbf{x}-\mathbf{y}\|^2$ s.t. $a^\top \mathbf{x} = \mathbf{b}$.
        <br><b>Lagrange Multiplier:</b> The term $\frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2}$ corresponds to the Lagrange multiplier $\nu$ associated with the constraint.</p>
      </div>

      <h3>P1.8 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The matrix is diagonal, so its singular values are the absolute values of diagonal entries: $1$ and $\epsilon$.
        Assuming $\epsilon < 1$: $\sigma_{\max} = 1, \sigma_{\min} = \epsilon$.
        $\kappa(A) = 1/\epsilon$.
        As $\epsilon \to 0$, $\kappa(A) \to \infty$. The matrix becomes ill-conditioned (nearly singular).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Condition Number $\kappa(A)$:</b> The ratio $\sigma_{\max}/\sigma_{\min}$ (for square invertible matrices).
        <br><b>Interpretation:</b> The aspect ratio of the hyperellipse image of the unit sphere.
        <br><b>Impact:</b>
        <ul>
            <li><b>Sensitivity:</b> Relative error in $\mathbf{x}$ $\le \kappa(A) \times$ Relative error in $\mathbf{b}$.</li>
            <li><b>Optimization:</b> Controls convergence rate of gradient descent. High $\kappa$ $\implies$ "Zig-zagging".</li>
        </ul></p>
      </div>

      <h3>P1.9 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = xy^\top$ where $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ are non-zero vectors. Show that $A^+ = \frac{A^\top}{\|\mathbf{x}\|_2^2 \|\mathbf{y}\|_2^2}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>SVD of rank-1 matrix: $A = \sigma \mathbf{u} \mathbf{v}^\top$.
        $\|A\|_F = \|x\|\|y\| = \sigma$.
        $\mathbf{u} = \mathbf{x}/\|\mathbf{x}\|$, $\mathbf{v} = \mathbf{y}/\|\mathbf{y}\|$.
        Check: $A = (\|\mathbf{x}\|\|\mathbf{y}\|) \frac{\mathbf{x}}{\|\mathbf{x}\|} \frac{\mathbf{y}^\top}{\|\mathbf{y}\|} = xy^\top$. Correct.
        Pseudoinverse: $A^+ = \frac{1}{\sigma} \mathbf{v} \mathbf{u}^\top = \frac{1}{\|\mathbf{x}\|\|\mathbf{y}\|} \frac{\mathbf{y}}{\|\mathbf{y}\|} \frac{\mathbf{x}^\top}{\|\mathbf{x}\|} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.
        Note that $A^\top = (xy^\top)^\top = yx^\top$.
        Thus $A^+ = \frac{A^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-1 SVD:</b> A matrix $A = xy^\top$ has exactly one non-zero singular value $\sigma_1 = \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$.
        <br><b>Singular Vectors:</b> $u_1 = \mathbf{x}/\|\mathbf{x}\|$, $v_1 = \mathbf{y}/\|\mathbf{y}\|$.
        <br><b>Pseudoinverse Formula:</b> For rank-1 matrices, $A^+ = \frac{A^\top}{\|A\|_F^2} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>

      <h3>P1.10 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$\|A\|_2^2 = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|^2}{\|\mathbf{x}\|^2} = \sup_{\mathbf{x} \ne 0} \frac{\mathbf{x}^\top A^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$.
        The term on the right is the Rayleigh quotient for the symmetric matrix $M = A^\top A$.
        By the variational characterization of eigenvalues, the maximum value of the Rayleigh quotient is $\lambda_{\max}(M)$.
        Thus $\|A\|_2^2 = \lambda_{\max}(A^\top A)$, so $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Variational Definition of Eigenvalues:</b> For symmetric $M$, $\lambda_{\max}(M) = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top M \mathbf{x}$.
        <br><b>Spectral Norm Connection:</b> $\|A\|_2^2 = \max_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|^2 = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top (A^\top A) \mathbf{x} = \lambda_{\max}(A^\top A)$.
        <br><b>Implication:</b> The spectral norm is the square root of the largest eigenvalue of the Gram matrix.</p>
      </div>

      <h3>P1.11 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Frobenius:</b> $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
        $\|UAV^\top\|_F^2 = \mathrm{tr}(V A^\top U^\top U A V^\top) = \mathrm{tr}(V A^\top A V^\top) = \mathrm{tr}(V^\top V A^\top A) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.
        <br><b>Spectral:</b> $\|UAV^\top\|_2 = \sup_{\|\mathbf{x}\|=1} \|UAV^\top \mathbf{x}\|$.
        Since $U$ is orthogonal, $\|Uy\| = \|y\|$. So we maximize $\|A (V^\top \mathbf{x})\|$.
        Since $V$ is orthogonal, as $\mathbf{x}$ ranges over the unit sphere, $\mathbf{y} = V^\top \mathbf{x}$ also ranges over the unit sphere.
        Thus $\sup_{\|\mathbf{x}\|=1} \|A V^\top \mathbf{x}\| = \sup_{\|\mathbf{y}\|=1} \|A\mathbf{y}\| = \|A\|_2$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Invariance:</b> A matrix norm is orthogonally invariant if $\|UAV^\top\| = \|A\|$ for all orthogonal $U, V$.
        <br><b>Schatten p-norms:</b> Defined as the $\ell_p$ norm of the singular values vector $\sigma(A)$.
        <ul>
            <li>$p=\infty$: Spectral Norm.</li>
            <li>$p=2$: Frobenius Norm.</li>
            <li>$p=1$: Nuclear Norm (Trace Norm).</li>
        </ul>
        <br>These are the "natural" norms for matrix analysis.</p>
      </div>

      <h3>P1.12 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Group:</b>
        1. Closure: $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.
        2. Inverse: $Q^{-1} = Q^\top$, which is orthogonal since $(Q^\top)^\top Q^\top = Q Q^\top = I$.
        3. Identity: $I \in O(n)$.
        <br><b>Compactness:</b>
        1. Bounded: The columns are unit vectors, so $\|Q\|_F = \sqrt{n}$. The set is bounded.
        2. Closed: The condition $Q^\top Q = I$ is a continuous equality constraint (level set of a continuous function). Thus the set is closed.
        By Heine-Borel, closed and bounded in finite dimensions implies compact.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The Orthogonal Group $O(n)$:</b> The set of all $n \times n$ orthogonal matrices.
        <br><b>Compactness Proof:</b>
        <ul>
            <li><b>Boundedness:</b> $\|Q\|_F = \sqrt{n}$, so $O(n)$ is contained in a ball of radius $\sqrt{n}$.</li>
            <li><b>Closedness:</b> It is the solution set of continuous polynomial equations $Q^\top Q - I = 0$.</li>
        </ul>
        <br><b>Optimization Consequence:</b> Any continuous function (like a norm or trace) maximized over orthogonal matrices <i>attains</i> its maximum. This is often used to prove existence of SVD.</p>
      </div>
</section>

    <!-- SECTION 11: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>9. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script type="module">
    import { initAdvancedLaDemos } from './widgets/js/advanced-la-demos.js';
    initAdvancedLaDemos('widget-svd-geometry', 'svd');
    initAdvancedLaDemos('widget-schur-complement', 'schur');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
