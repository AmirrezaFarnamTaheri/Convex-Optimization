<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>04. Convex Sets: Cones and Separation — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../03-convex-sets-geometry/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../05-convex-functions-basics/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>04. Convex Sets: Cones and Separation</h1>
      <div class="lecture-summary">
        <p>This lecture introduces the two geometric ideas that power much of modern convex optimization: (1) separation/support by hyperplanes and (2) convex cones and the generalized inequalities they induce. Separation theorems explain why convex problems admit certificates of optimality (duality), while cones provide the unified language behind SOCPs and SDPs.</p>
        <p><strong>Prerequisites:</strong> <a href="../03-convex-sets-geometry/index.html">Lecture 03</a> (convex sets and operations) and <a href="../00-linear-algebra-basics/index.html">Lecture 00</a> (inner products, orthogonality).</p>
        <p><strong>Forward Connections:</strong> Conic formulations (<a href="../08-convex-problems-conic/index.html">Lecture 08</a>) are built from cones, and duality/KKT theory (<a href="../09-duality/index.html">Lecture 09</a>) is built from separation.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Use separation correctly:</b> State and apply separating and supporting hyperplane theorems, including when strict separation is possible.</li>
        <li><b>Connect projection to separation:</b> Use the nearest-point (projection) characterization to derive a separating hyperplane.</li>
        <li><b>Work with cones:</b> Recognize proper cones, define generalized inequalities, and compute/interpret dual cones.</li>
        <li><b>See conic optimization coming:</b> Recognize how SOCP/SDP constraints are just “membership in a cone.”</li>
      </ul>
    </section>

    <!-- TRIFECTA NARRATIVE -->
    <section class="section-card">
      <h2><i data-feather="map"></i> The Arc from Sets to Functions</h2>

      <div class="insight">
        <h4>Where We Are in the Convexity Trifecta</h4>
        <p>In <a href="../03-convex-sets-geometry/index.html">Lecture 03</a>, we answered: <b>"WHAT are convex sets?"</b> We saw convex sets as geometric objects (polyhedra, ellipsoids, cones) and studied their properties (separating hyperplanes, projections).</p>

        <p>This lecture (Lecture 04) asks: <b>"What OPERATIONS preserve convexity?"</b> Specifically, we introduce <b>cones</b>—sets with special algebraic structure that enable <b>generalized inequalities</b> and <b>duality theory</b>.</p>

        <p>Next, <a href="../05-convex-functions-basics/index.html">Lecture 05</a> will ask: <b>"What FUNCTIONS have convex epigraphs?"</b> There, we'll shift from geometry (sets) to analysis (functions), verifying convexity via gradients and Hessians.</p>
      </div>

      <h3>Why Cones Matter for Optimization</h3>
      <p>Cones are not just geometric curiosities—they're <b>fundamental to three pillars</b> of convex optimization:</p>

      <ol>
        <li><b>Generalized Inequalities:</b> Just as $\mathbb{R}_+$ (nonnegative reals) defines the inequality $x \ge 0$, any proper cone $K$ defines a partial ordering $x \succeq_K y$ if $x - y \in K$. Examples:
          <ul style="margin-top: 0.5rem;">
            <li>$K = \mathbb{R}^n_+$ (nonnegative orthant) → componentwise inequality $x \ge y$</li>
            <li>$K = \mathcal{S}^n_+$ (PSD cone) → matrix inequality $X \succeq Y$ (positive semidefinite ordering)</li>
            <li>$K = \mathcal{Q}$ (second-order cone) → $\|x\|_2 \le t$ constraints in SOCP</li>
          </ul>
          These generalized inequalities are the constraints in <b>conic programs</b> (<a href="../08-convex-problems-conic/index.html">Lecture 08</a>).
        </li>

        <li><b>Duality Theory:</b> Every cone $K$ has a <b>dual cone</b> $K^* = \{y : x^\top y \ge 0 \ \forall x \in K\}$. The dual cone is the geometric foundation for:
          <ul style="margin-top: 0.5rem;">
            <li><b>Lagrange multipliers:</b> The dual variables in Lagrangian duality (<a href="../09-duality/index.html">Lecture 09</a>) live in the dual cone</li>
            <li><b>Strong duality theorems:</b> Conditions for primal and dual problems to have the same optimal value</li>
            <li><b>Separation theorems:</b> Dual cones provide the separating hyperplanes (from Lecture 03)</li>
          </ul>
        </li>

        <li><b>Conic Optimization (SOCP, SDP):</b> Modern optimization solvers work with conic programs. A conic program is:
          $$
          \begin{aligned}
          \text{minimize} \quad & c^\top x \\
          \text{subject to} \quad & Ax + b \in K \\
          & Fx = g
          \end{aligned}
          $$
          where $K$ is a cone. Choosing different cones gives different problem classes:
          <ul style="margin-top: 0.5rem;">
            <li>$K = \mathbb{R}^n_+$ → Linear Programming (LP)</li>
            <li>$K = \mathcal{Q}$ (second-order cone) → Second-Order Cone Programming (SOCP)</li>
            <li>$K = \mathcal{S}^n_+$ (PSD cone) → Semidefinite Programming (SDP)</li>
          </ul>
          You'll see these in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>.
        </li>
      </ol>

      <div class="insight">
        <h4>The Convexity Trifecta Roadmap</h4>
        <p><b>Lecture 03 (Sets):</b> Geometric foundations — what convex sets look like, separation theorems, projections.</p>
        <p><b>Lecture 04 (Cones):</b> Algebraic structure — operations that preserve convexity, dual cones, generalized inequalities.</p>
        <p><b>Lecture 05 (Functions):</b> Analytical tools — gradients, Hessians, composition rules, verifying convexity via calculus.</p>
        <p><b>Lectures 07-09 (Problems):</b> Putting it all together — standard forms (LP, QP, GP), conic forms (SOCP, SDP), and duality.</p>
        <p><b>The Thread:</b> Every convex function $f$ defines a convex set (its epigraph epi $f$). Every convex optimization problem can be viewed as minimizing over a convex set. <b>Cones</b> are the bridge between these perspectives—they enable both geometric (separation) and algebraic (duality) reasoning.</p>
      </div>
    </section>

    <article>
      <section class="section-card" id="section-1">
        <h2>1. Cones, Proper Cones, and Dual Cones</h2>

        <h3>1.1 Cones and Convex Cones</h3>

        <p>A set $K \subseteq \mathbb{R}^n$ is a <a href="#" class="definition-link">cone</a> if it is closed under non-negative scaling:</p>
        $$ x \in K, \ \alpha \ge 0 \implies \alpha x \in K $$
        <p><b>Note:</b> A cone can be wildly non-convex (e.g., a union of two lines). Optimization requires convexity.</p>

        <p>A <a href="#" class="definition-link">convex cone</a> is a cone that is also a convex set. Equivalently, it is closed under non-negative linear combinations (conic combinations):</p>
        $$ x, y \in K, \ \alpha, \beta \ge 0 \implies \alpha x + \beta y \in K $$
        <p><b>Why this definition is equivalent:</b>
        <br>($\Rightarrow$) If $K$ is a convex cone, scaling gives $\alpha x, \beta y \in K$. Convexity gives $\frac{1}{2}(\alpha x + \beta y) \in K$. Scaling by 2 gives $\alpha x + \beta y \in K$.
        <br>($\Leftarrow$) Setting $\beta=0$ gives the cone property. Setting $\alpha=\theta, \beta=1-\theta$ gives convexity.</p>

        <h4>The Big Three Cones (The "Atoms" of Conic Programming)</h4>
        <p>Most practical convex problems (LP, QP, QCQP, SOCP, SDP) can be written using just these three cones. Understanding their geometry is crucial.</p>

        <h5>(I) Nonnegative Orthant $\mathbb{R}^n_+$</h5>
        $$ \mathbb{R}^n_+ = \{x \in \mathbb{R}^n \mid x_i \ge 0 \ \forall i\} $$
        <p><b>Convexity Proof:</b> If $x_i \ge 0$ and $y_i \ge 0$, then $(\theta x + (1-\theta)y)_i = \theta x_i + (1-\theta)y_i \ge 0$ for $\theta \in [0,1]$.
        <br><b>Modeling:</b> Encodes linear inequalities $Ax \le b \iff b - Ax \in \mathbb{R}^m_+$.</p>

        <h5>(II) Second-Order Cone (SOC) $\mathcal{Q}^{n+1}$</h5>
        $$ \mathcal{Q}^{n+1} = \{(x, t) \in \mathbb{R}^n \times \mathbb{R} \mid \|x\|_2 \le t\} $$
        <p><b>Convexity Proof:</b> The norm is convex. $\mathcal{Q}^{n+1}$ is the epigraph of the Euclidean norm function $f(x)=\|x\|_2$. Epigraphs of convex functions are convex sets.
        <br><b>Modeling:</b> Encodes Euclidean distance constraints and robust linear constraints.</p>

        <h5>(III) Positive Semidefinite (PSD) Cone $\mathbb{S}^n_+$</h5>
        $$ \mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid z^\top X z \ge 0 \ \forall z \in \mathbb{R}^n\} $$
        <p><b>Convexity Proof:</b> If $X, Y \succeq 0$, then $z^\top (\theta X + (1-\theta)Y) z = \theta (z^\top X z) + (1-\theta) (z^\top Y z) \ge 0$.
        <br><b>Modeling:</b> Encodes eigenvalues, ellipses, and covariance matrices. (Warning: PSD is not elementwise non-negativity!).</p>

        <div class="theorem-box">
          <h4>Unified View: The Conic Program Template</h4>
          <p>LP, SOCP, and SDP are all instances of the same problem:
          $$ \min c^\top x \quad \text{s.t.} \quad Ax = b, \quad x \in K $$
          where $K$ is a product of these "Big Three" cones. Solvers like Mosek/SCS/ECOS are designed to handle exactly this structure efficiently.</p>
        </div>

        <div class="insight">
          <h4>Rotation: Connecting SOC and RSOC</h4>
          <p>The Rotated SOC is just a linear transformation of the standard SOC.
          Let $u = (y+z)/2$ and $v = (y-z)/2$. Then $y = u+v, z = u-v$.
          The inequality $\|x\|^2 \le yz$ becomes $\|x\|^2 \le u^2 - v^2$, or $\|x\|^2 + v^2 \le u^2$.
          This is exactly $\|(x, v)\|_2 \le u$, which is the standard SOC in coordinates $(x, v, u)$.
          <br>This cone is fundamental for modeling quadratic constraints like $x^T x \le yz$ (geometric mean bounds).</p>
        </div>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0; flex-wrap: wrap;">
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/second-order-cone.png"
                    alt="3D visualization of the second-order cone (ice cream cone)"
                    style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 5:</i> The Second-Order Cone in $\mathbb{R}^3$ (ice-cream cone) is defined by $\|x\|_2 \le t$.</figcaption>
             </figure>
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/hyperbolic-cone.png"
                    alt="3D visualization of a hyperbolic cone"
                    style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 6:</i> A Hyperbolic Cone. Cones are not limited to straight lines; they can have curved boundaries as long as they are closed under scaling.</figcaption>
             </figure>
        </div>

        <h3>1.2 Proper Cones</h3>

        <p>A <a href="#" class="definition-link">proper cone</a> is a convex cone $K$ that satisfies three additional properties:</p>
        <ol>
          <li><b>Closed:</b> $K$ contains its boundary.</li>
          <li><b>Pointed:</b> $K$ contains no lines ($K \cap -K = \{0\}$).</li>
          <li><b>Solid:</b> $K$ has a non-empty interior.</li>
        </ol>

        <p>Proper cones are geometrically "sharp" (pointed) and "full-dimensional" (solid). They are used to define generalized inequalities.</p>

        <div class="intuition-box">
          <p><b>Why these three conditions?</b> They make “$\preceq_K$” behave like a sensible notion of “$\le$”. <b>Pointed</b> prevents cycles (otherwise you could have $x \preceq_K y$ and $y \preceq_K x$ with $x\neq y$). <b>Solid</b> ensures there are truly “positive” directions (so strict inequality using $\mathrm{int}(K)$ is meaningful). <b>Closed</b> ensures the order is stable under limits (important in optimization where solutions arise as limits of approximations).</p>
        </div>

        <div class="interpretation-box">
          <p><b>Decision/economic view:</b> Think of $K$ as the set of "acceptable improvements" (e.g., more output, less cost, lower risk). Then $\mathbf{x} \preceq_K \mathbf{y}$ means "$\mathbf{y}$ dominates $\mathbf{x}$ up to an improvement in $K$." Properness ensures this dominance relation is non-degenerate and supports optimization and "Pareto" style comparisons.</p>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
             <img src="assets/cone-zoo.png"
                  alt="Examples of proper and improper cones"
                  style="max-width: 800px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
             <figcaption><i>Figure 7:</i> The "Cone Zoo". Left: A proper cone (closed, pointed, solid). Center: A wedge (not pointed, contains a line). Right: A flat slice (not solid, empty interior).</figcaption>
        </figure>

        <h3>1.3 Generalized Inequalities</h3>

        <p>A proper cone $K$ defines a partial ordering $\preceq_K$ on $\mathbb{R}^n$:</p>
        $$
        \mathbf{x} \preceq_K \mathbf{y} \iff \mathbf{y} - \mathbf{x} \in K
        $$
        <div class="insight">
          <h4>Breakdown: What does $x \preceq_K y$ mean?</h4>
          <p>This generalizes the scalar concept "$y \ge x$" (or $y-x \ge 0$).
          <br>Instead of requiring $y-x$ to be a non-negative number, we require the vector difference $y-x$ to land inside the "acceptable" set $K$.
          <br><b>Example (PSD Cone):</b> $A \preceq B$ means $B - A$ is a positive semidefinite matrix. It does <i>not</i> mean every entry of $B$ is larger than $A$. It means the "energy" $z^\top B z \ge z^\top A z$ for all directions $z$.
          </p>
        </div>
        <p>Strict inequality is defined using the interior of the cone:</p>
        $$
        \mathbf{x} \prec_K \mathbf{y} \iff \mathbf{y} - \mathbf{x} \in \text{int}(K)
        $$
        <p><b>Cone-Induced Order Properties:</b>
        <br>1. <b>Reflexive:</b> $x \preceq_K x$ (since $0 \in K$).
        <br>2. <b>Antisymmetric:</b> $x \preceq_K y$ and $y \preceq_K x \implies x=y$ (since $K$ is pointed).
        <br>3. <b>Transitive:</b> $x \preceq_K y$ and $y \preceq_K z \implies x \preceq_K z$ (since $K$ is closed under addition).
        <br>4. <b>Additivity:</b> $x \preceq_K y \implies x+z \preceq_K y+z$.
        <br>5. <b>Homogeneity:</b> $x \preceq_K y \implies \alpha x \preceq_K \alpha y$ for $\alpha \ge 0$.</p>

        <h4>Vector Optimization (Pareto Optimality)</h4>
        <p>This partial order allows us to define optimization for vector-valued functions, leading to the fundamental concept of <b>Pareto optimality</b> in multi-objective optimization.</p>

        <div class="theorem-box">
          <h4>Definition: Pareto Optimal Point</h4>
          <p>Given a vector-valued objective $f: \mathbb{R}^n \to \mathbb{R}^m$ and feasible set $\mathcal{X}$, a point $\mathbf{x}^* \in \mathcal{X}$ is <b>Pareto optimal</b> (or <b>efficient</b>) with respect to cone $K$ if there is no feasible $\mathbf{y} \in \mathcal{X}$ such that:
          $$ f(\mathbf{y}) \preceq_K f(\mathbf{x}^*) \quad \text{and} \quad f(\mathbf{y}) \neq f(\mathbf{x}^*) $$
          <b>Intuition:</b> You cannot improve any objective component (in the $K$ sense) without degrading at least one other component. The point is "undominated."
          </p>
        </div>

        <div class="example-box">
          <h4>Example: Portfolio Optimization (Risk vs. Return)</h4>
          <p>Consider minimizing $f(x) = (\text{risk}(x), -\text{return}(x))$ where $x$ is a portfolio allocation.
          <br>Using $K = \mathbb{R}^2_+$ (component-wise ordering):
          <ul>
            <li>Point $A$: Risk 5%, Return 3% (Low risk, low return)</li>
            <li>Point $B$: Risk 20%, Return 12% (High risk, high return)</li>
            <li>Point $C$: Risk 10%, Return 8% (Medium risk, medium return)</li>
          </ul>
          Is $C$ Pareto optimal? Yes, if there is no portfolio $D$ with Risk $\le 10\%$ AND Return $\ge 8\%$ (with at least one strict inequality).
          <br>If $C$ is Pareto optimal, you face a trade-off: to reduce risk below 10% (move towards $A$), you <i>must</i> accept a return lower than 8%. To get a return higher than 8% (move towards $B$), you <i>must</i> accept risk higher than 10%.
          <br><b>The Pareto frontier</b> is the set of all such "unbeatable" points.</p>
        </div>

        <div class="insight">
          <h4>Dominated vs. Pareto Optimal Points</h4>
          <p>A point $\mathbf{x}$ is <b>dominated</b> if there exists $\mathbf{y}$ with $f(\mathbf{y}) \preceq_K f(\mathbf{x})$ and $f(\mathbf{y}) \neq f(\mathbf{x})$. Dominated points are strictly worse—they can be improved in all objectives simultaneously (or at least some, without degrading others).
          <br><b>Pareto optimal points are precisely the non-dominated points.</b>
          <br>In practice, when objectives conflict (reduce cost vs. increase quality), the decision-maker must choose a point on the Pareto frontier based on preferences or trade-off weights.</p>
        </div>

        <p><b>Connection to Standard Optimization:</b> For $K = \mathbb{R}^n_+$ (component-wise ordering), this recovers standard multi-objective optimization. Scalarization (forming $\sum w_i f_i(x)$ with $w \geq 0$) produces Pareto optimal points by varying weights.</p>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0; flex-wrap: wrap;">
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/generalized-inequality-cone.png"
                    alt="Visualizing generalized inequality"
                    style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 8:</i> Visualizing $\mathbf{x} \preceq_K \mathbf{y}$. The point $\mathbf{x}$ must lie in the "shadow" of $\mathbf{y}$ cast by the cone $-K$.</figcaption>
             </figure>
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/matrix-inequality-cone.png"
                    alt="Matrix inequality ordering"
                    style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 9:</i> Partial ordering in the PSD cone. $A \preceq B$ means $B-A$ is in the PSD cone.</figcaption>
             </figure>
        </div>

        <div class="insight">
             <h4>Minimum vs. Minimal</h4>
             <p>Because the ordering is partial, we distinguish between a <b>minimum</b> element (smaller than everyone, e.g., the origin in $\mathbb{R}^n_+$) and a <b>minimal</b> element (nothing is smaller than it, but it might not compare to everyone).</p>
             <figure style="text-align: center; margin: 24px 0;">
                  <img src="assets/minimum-vs-minimal.png"
                       alt="Minimum vs minimal elements"
                       style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
                  <figcaption><i>Figure 10:</i> Left: Minimum element (unique). Right: Minimal elements (red boundary). In vector optimization (Pareto optimality), we seek minimal elements.</figcaption>
             </figure>
        </div>

        <h3>1.4 The Dual Cone</h3>

        <p>The <a href="#" class="definition-link">dual cone</a> of a cone $K$ is the set of all vectors making a non-obtuse angle with every vector in $K$:</p>
        $$
        K^* = \{y \in \mathbb{R}^n \mid y^\top x \ge 0 \ \forall x \in K\}
        $$

        <div class="example-box">
          <h4>Geometric Example: 2D Wedges</h4>
          <p>Consider a cone $K$ in $\mathbb{R}^2$ spanned by two vectors $v_1 = (1, 0)$ and $v_2 = (1, 1)$. This is a $45^\circ$ wedge.
          <br>The dual cone $K^*$ consists of all $y$ such that $y^\top v_1 \ge 0$ and $y^\top v_2 \ge 0$.
          <br>$\bullet$ $y^\top (1, 0) \ge 0 \implies y_1 \ge 0$.
          <br>$\bullet$ $y^\top (1, 1) \ge 0 \implies y_1 + y_2 \ge 0$.
          <br>This dual wedge is spanned by $(0, 1)$ (normal to $v_1$) and $(1, -1)$ (normal to $v_2$). Notice the angular relationship: if $K$ is "thin", $K^*$ is "fat".
          </p>
        </div>

        <div class="insight">
          <h4>Why Dual Cones Matter</h4>
          <p>The dual cone is the geometric equivalent of <b>non-negative Lagrange multipliers</b>.
          <br>In linear programming ($x \ge 0$), the dual variables must be non-negative ($y \ge 0$).
          <br>In conic programming ($x \in K$), the dual variables must live in the dual cone ($y \in K^*$).
          <br>This generalized non-negativity is what allows us to define valid lower bounds in duality theory.</p>
        </div>

        <p><b>Geometric Intuition (Non-Negative Normals):</b> If $K$ is the set of "allowable directions," $K^*$ is the set of "allowable gradients" that don't decrease the objective as you move along $K$. It generalizes the concept of the non-negative orthant. If $K$ is a "sharp" cone (pointed), $K^*$ is a "wide" cone (solid). If $K$ is a subspace (flat), $K^*$ is its orthogonal complement (normal).</p>
        <p>The dual cone $K^*$ is always a closed convex cone, regardless of whether $K$ is convex or closed.</p>

        <div class="theorem-box">
          <h4>Theorem (Bipolar Theorem)</h4>
          <p>If $K$ is a convex cone, then its second dual (bipolar) is its closure:</p>
          $$ K^{**} = \mathrm{cl}(K) $$
          <p>Consequently, if $K$ is a <b>closed convex cone</b>, then $K^{**} = K$. This is the "reflexive" property of cones, analogous to $(V^\perp)^\perp = V$ for subspaces.</p>
          <div class="proof-box">
            <h4>Proof Sketch</h4>
            <p>1. $K \subseteq K^{**}$: If $\mathbf{x} \in K$, then for all $\mathbf{y} \in K^*$, $\mathbf{y}^\top \mathbf{x} \ge 0$. Thus $\mathbf{x}$ makes a non-negative angle with everything in $K^*$, so $\mathbf{x} \in K^{**}$.</p>
            <p>2. $K^{**} \subseteq \mathrm{cl}(K)$: If $\mathbf{x} \notin \mathrm{cl}(K)$, by the Separating Hyperplane Theorem, there exists $\mathbf{y}$ strictly separating $\mathbf{x}$ from $\mathrm{cl}(K)$. This $\mathbf{y}$ can be shown to lie in $K^*$ while $\mathbf{y}^\top \mathbf{x} < 0$, proving $\mathbf{x} \notin K^{**}$.</p>
          </div>
        </div>

        <div class="insight">
          <h4>The Dual Cone Construction Kit</h4>
          <p>How do you find the dual of a complex cone? Use these calculus rules (valid for closed convex cones):</p>
          <table class="data-table">
            <thead>
              <tr>
                <th>Operation</th>
                <th>Primal Cone $K$</th>
                <th>Dual Cone $K^*$</th>
                <th>Intuition</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>Intersection</b></td>
                <td>$K_1 \cap K_2$</td>
                <td>$K_1^* + K_2^*$</td>
                <td>Dual of AND is OR (Minkowski sum).</td>
              </tr>
              <tr>
                <td><b>Sum</b></td>
                <td>$K_1 + K_2$</td>
                <td>$K_1^* \cap K_2^*$</td>
                <td>Dual of OR is AND.</td>
              </tr>
              <tr>
                <td><b>Linear Map</b></td>
                <td>$A(K) = \{Ax \mid x \in K\}$</td>
                <td>$(A^\top)^{-1}(K^*) = \{y \mid A^\top y \in K^*\}$</td>
                <td>Linear maps act as inverse transpose on duals.</td>
              </tr>
              <tr>
                <td><b>Inverse Map</b></td>
                <td>$A^{-1}(K) = \{x \mid Ax \in K\}$</td>
                <td>$A^\top (K^*) = \{A^\top y \mid y \in K^*\}$</td>
                <td>(assuming $A$ has full range).</td>
              </tr>
              <tr>
                <td><b>Product</b></td>
                <td>$K_1 \times K_2$</td>
                <td>$K_1^* \times K_2^*$</td>
                <td>Component-wise duality.</td>
              </tr>
            </tbody>
          </table>
        </div>

      <h4>The Polar Cone</h4>
      <p>The <a href="#" class="definition-link">polar cone</a> is the negative of the dual cone:</p>
      $$ K^\circ = -K^* = \{\mathbf{y} \in \mathbb{R}^n \mid \mathbf{y}^\top \mathbf{x} \le 0 \ \forall \mathbf{x} \in K\} $$
      <p>It contains all vectors making an obtuse angle with $K$. It is related to the normal cone at the origin: $K^\circ = N_K(0)$ for a convex cone $K$.</p>

      <h4>Geometric Normal Cones</h4>
      <p>For a general closed convex set $C$ and a point $\mathbf{x} \in C$, the <b>normal cone</b> $N_C(\mathbf{x})$ is the set of outward directions:</p>
      $$ N_C(\mathbf{x}) = \{\mathbf{g} \in \mathbb{R}^n \mid \mathbf{g}^\top(\mathbf{y} - \mathbf{x}) \le 0 \ \forall \mathbf{y} \in C\} $$
      <p>This generalizes the gradient of constraints. If $C$ is defined by smooth inequalities $f_i(\mathbf{x}) \le 0$, the normal cone is the cone generated by the gradients of the active constraints.</p>

        <div class="theorem-box">
          <h4>Subgradients via Epigraphs</h4>
          <p>This normal cone geometry is exactly where subgradients come from.
          <br>Let $f: \mathbb{R}^n \to \mathbb{R}$ be convex. A vector $g$ is a <b>subgradient</b> at $x_0$ if $(g, -1)$ is a normal to the epigraph of $f$ at $(x_0, f(x_0))$.
          $$ (g, -1)^\top (x - x_0, t - f(x_0)) \le 0 \quad \forall (x, t) \in \mathrm{epi}(f) $$
          Expanding this gives $g^\top (x-x_0) - (t - f(x_0)) \le 0 \implies t \ge f(x_0) + g^\top (x-x_0)$.
          Since $t \ge f(x)$, we recover the subgradient inequality $f(x) \ge f(x_0) + g^\top(x-x_0)$.</p>
        </div>

        <div class="intuition-box">
          <p><b>Geometric test:</b> A vector $\mathbf{y}$ is in $K^*$ if it can serve as an "outward normal" at the origin: the halfspace $\{\mathbf{x} \mid \mathbf{y}^\top \mathbf{x} \ge 0\}$ contains the cone $K$. In 2D, if $K$ is a wedge, then $K^*$ is the wedge of normals that still form an acute angle with every direction in $K$.</p>
        </div>

        <div class="interpretation-box">
          <p><b>Prices/supporting weights:</b> If $K$ represents "nonnegative" directions (improvements), then $K^*$ is the set of linear functionals that assign <i>nonnegative value</i> to every improvement: $\mathbf{y}^\top \mathbf{x} \ge 0$ for all $\mathbf{x}\in K$. A key consequence used later in duality is: if $\mathbf{x} \preceq_K \mathbf{y}$ then $\mathbf{w}^\top \mathbf{x} \le \mathbf{w}^\top \mathbf{y}$ for every $\mathbf{w} \in K^*$ (dual cone weights turn vector dominance into scalar inequalities).</p>
        </div>

        <figure style="text-align: center; margin: 24px 0;">
             <img src="assets/dual-cone-geometry.png"
                  alt="Geometry of primal and dual cones"
                  style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
             <figcaption><i>Figure 11:</i> The dual cone $K^*$ (red) consists of vectors that make an angle $\le 90^\circ$ with every vector in $K$ (blue).</figcaption>
        </figure>

        <div class="theorem-box">
            <h4>Important Dualities</h4>
            <ul>
                <li>$\mathbb{R}^n_+$ is <b>self-dual</b>: $(\mathbb{R}^n_+)^* = \mathbb{R}^n_+$</li>
                <li>$\mathbb{S}^n_+$ is <b>self-dual</b>: $(\mathbb{S}^n_+)^* = \mathbb{S}^n_+$ (under trace inner product). This property is crucial for the symmetry of primal and dual <b>Semidefinite Programs</b> (<a href="../09-duality/index.html">Lecture 09</a>).</li>
                <li>$\mathcal{Q}^{n+1}$ is <b>self-dual</b>: $(\mathcal{Q}^{n+1})^* = \mathcal{Q}^{n+1}$</li>
                <li>$(L_p \text{ norm cone})^* = L_q \text{ norm cone}$ where $1/p + 1/q = 1$.</li>
            </ul>
        </div>
        
        <div class="insight">
          <h4>Why the PSD Cone is the "King of Cones"</h4>
          <p>The Positive Semidefinite (PSD) cone $\mathbb{S}^n_+$ is a universe in itself.
          <ul>
            <li>It contains the non-negative orthant (diagonal matrices).</li>
            <li>It contains the second-order cone (via Schur complement).</li>
            <li>It allows us to model eigenvalues, singular values, and polynomials.</li>
          </ul>
          Its self-duality ($\mathrm{tr}(XY) \ge 0 \iff X, Y \succeq 0$) is the reason why SDPs have such symmetric primal-dual theories. Understanding the geometry of $\mathbb{S}^n_+$ is the key to advanced optimization.</p>
        </div>

        <div class="proof-box">
          <h4>Proof: Self-Duality of the Second-Order Cone</h4>
          <p>We prove $\mathcal{Q}^{n+1} = (\mathcal{Q}^{n+1})^*$ where $\mathcal{Q}^{n+1} = \{(x, t) \in \mathbb{R}^n \times \mathbb{R} : \|x\|_2 \le t\}$ under the standard inner product $\langle (x,t), (y,s) \rangle = x^\top y + ts$.</p>

          <div class="proof-step">
            <strong>Part 1: $\mathcal{Q} \subseteq \mathcal{Q}^*$ (Self-Polarity).</strong>
            <br>Let $(x, t), (y, s) \in \mathcal{Q}$, so $\|x\|_2 \le t$ and $\|y\|_2 \le s$.
            <br>We compute: $\langle (x,t), (y,s) \rangle = x^\top y + ts$.
            <br>By Cauchy-Schwarz: $x^\top y \ge -\|x\|_2 \|y\|_2$.
            <br>Thus $x^\top y + ts \ge ts - \|x\|_2 \|y\|_2$.
            <br>Since $t \ge \|x\|_2 \ge 0$ and $s \ge \|y\|_2 \ge 0$, we have $ts \ge \|x\|_2 \|y\|_2$.
            <br>Therefore $x^\top y + ts \ge 0$, proving every element of $\mathcal{Q}$ is in $\mathcal{Q}^*$.
          </div>

          <div class="proof-step">
            <strong>Part 2: $\mathcal{Q}^* \subseteq \mathcal{Q}$.</strong>
            <br>Let $(y, s) \in \mathcal{Q}^*$. By definition, $x^\top y + ts \ge 0$ for all $(x, t) \in \mathcal{Q}$.
            <br><b>Non-negativity of s:</b> Choose $(x, t) = (0, 1) \in \mathcal{Q}$. This gives $0^\top y + 1 \cdot s = s \ge 0$.
            <br><b>Norm bound:</b> If $y = 0$, then $(0, s) \in \mathcal{Q}$ trivially from $s \ge 0$.
            <br>If $y \neq 0$, choose the "opposing" test vector $x = -y/\|y\|_2$ with $t = 1$.
            <br>Since $\|x\|_2 = 1 \le t$, we have $(x, t) \in \mathcal{Q}$.
            <br>The dual condition gives:
            $$ x^\top y + ts = -\frac{y^\top y}{\|y\|_2} + s = -\frac{\|y\|_2^2}{\|y\|_2} + s = -\|y\|_2 + s \ge 0 $$
            <br>Thus $s \ge \|y\|_2$, which means $\|y\|_2 \le s$, so $(y, s) \in \mathcal{Q}$.
            <br>Since both inclusions hold, $\mathcal{Q}^* = \mathcal{Q}$.
          </div>
        </div>

        <div class="proof-box">
          <h4>Proof: Self-Duality of the PSD Cone</h4>
          <p>We want to show that $(\mathbb{S}^n_+)^* = \mathbb{S}^n_+$ under the trace inner product $\langle A, B \rangle = \mathrm{tr}(AB)$.</p>

          <div class="proof-step">
            <strong>Part 1: $\mathbb{S}^n_+ \subseteq (\mathbb{S}^n_+)^*$ (Primal implies Dual)</strong>
            <p>Assume $A \in \mathbb{S}^n_+$. We want to show it lies in the dual cone. By definition, this requires $\langle A, B \rangle \ge 0$ for every $B \in \mathbb{S}^n_+$.</p>
            <p><b>Step 1.1: Factorize A.</b> Since $A$ is symmetric positive semidefinite, we can write it as $A = L L^\top$ (using Cholesky factorization or eigenvalue decomposition $A = \sum \lambda_i v_i v_i^\top$).</p>
            <p><b>Step 1.2: Use Cyclic Property.</b>
            $$ \langle A, B \rangle = \mathrm{tr}(AB) = \mathrm{tr}(L L^\top B) $$
            Using the cyclic property of the trace ($\mathrm{tr}(XY) = \mathrm{tr}(YX)$), let $X=L$ and $Y=L^\top B$:
            $$ \mathrm{tr}(L (L^\top B)) = \mathrm{tr}((L^\top B) L) = \mathrm{tr}(L^\top B L) $$
            </p>
            <p><b>Step 1.3: Verify Non-negativity.</b> Let $M = L^\top B L$. Is its trace non-negative?
            <br>Consider the quadratic form for any vector $x$:
            $$ x^\top M x = x^\top L^\top B L x = (Lx)^\top B (Lx) $$
            Let $y = Lx$. Then $x^\top M x = y^\top B y$. Since $B \succeq 0$, we know $y^\top B y \ge 0$ for any vector $y$.
            <br>This implies $M$ is positive semidefinite ($M \succeq 0$). The trace of a PSD matrix is the sum of its eigenvalues, which are all non-negative. Thus $\mathrm{tr}(M) \ge 0$.
            <br><b>Conclusion:</b> $\mathrm{tr}(AB) \ge 0$ for all $B \succeq 0$, so $A \in (\mathbb{S}^n_+)^*$.</p>
          </div>

          <div class="proof-step">
            <strong>Part 2: $(\mathbb{S}^n_+)^* \subseteq \mathbb{S}^n_+$ (Dual implies Primal)</strong>
            <p>Assume $Y \in (\mathbb{S}^n_+)^*$. We must prove $Y$ is PSD. By definition, $Y$ must make a non-negative inner product with <i>every</i> matrix in the primal cone $\mathbb{S}^n_+$.</p>
            <p><b>Step 2.1: Choose Rank-1 Test Matrices.</b>
            The primal cone contains all matrices of the form $X = vv^\top$ for any vector $v \in \mathbb{R}^n$.
            <br>Check feasibility: Is $vv^\top \in \mathbb{S}^n_+$?
            $$ z^\top (vv^\top) z = (z^\top v)(v^\top z) = (v^\top z)^2 \ge 0 \quad \forall z $$
            Yes, rank-1 symmetric matrices are always PSD.
            </p>
            <p><b>Step 2.2: Apply the Dual Condition.</b>
            Since $Y$ is in the dual cone, $\mathrm{tr}(XY) \ge 0$ for our chosen $X=vv^\top$.
            $$ \mathrm{tr}(vv^\top Y) = \mathrm{tr}(v^\top Y v) = v^\top Y v $$
            Therefore, we have established that $v^\top Y v \ge 0$ for any vector $v$.
            </p>
            <p><b>Step 2.3: Conclusion.</b>
            The condition "$\forall v, v^\top Y v \ge 0$" is exactly the variational definition of a Positive Semidefinite matrix. Thus $Y \in \mathbb{S}^n_+$.
            </p>
          </div>

          <div class="proof-step">
            <strong>Conclusion:</strong> Since both inclusions hold, $(\mathbb{S}^n_+)^* = \mathbb{S}^n_+$.
          </div>
        </div>

        <div class="row" style="display: flex; gap: 20px; justify-content: center; margin: 24px 0; flex-wrap: wrap;">
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/self-dual-cones.png"
                    alt="The three major self-dual cones"
                    style="max-width: 800px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 12:</i> The Big Three self-dual cones: Nonnegative Orthant, Second-Order Cone, and PSD Cone.</figcaption>
             </figure>
             <figure style="text-align: center; flex: 1; min-width: 280px;">
               <img src="assets/dual-norm-cones.png"
                    alt="Dual relationship between L1 and L-infinity cones"
                    style="max-width: 600px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
               <figcaption><i>Figure 13:</i> Duality of Norm Cones: The dual of the $L_1$ cone is the $L_\infty$ cone.</figcaption>
             </figure>
        </div>

        <h3>1.5 Dual Norms: The Shadow Ruler</h3>
        <p>A specific and crucial application of duality is the concept of a <b>dual norm</b>. Just as dual cones define allowable gradients, dual norms define the "worst-case" alignment of a vector with the unit ball.</p>

        <p>For any norm $\|\cdot\|$ on $\mathbb{R}^n$, its dual norm $\|\cdot\|_*$ is defined by:</p>
        $$ \|u\|_* = \sup_{\|x\| \le 1} u^\top x $$
        <p>This is exactly the support function of the primal unit ball evaluated at $u$.</p>

        <div class="proof-box">
          <h4>Proof: The Dual Norm is a Norm</h4>
          <div class="proof-step">
            <strong>1. Definiteness:</strong> $\|u\|_* \ge u^\top 0 = 0$. If $\|u\|_*=0$, then $u^\top x=0$ for all $\|x\|\le 1$. By scaling, $u^\top y=0$ for all $y$, so $u=0$.
          </div>
          <div class="proof-step">
            <strong>2. Homogeneity:</strong> $\|\alpha u\|_* = \sup_{x \in B} (\alpha u)^\top x$. If $\alpha \ge 0$, $\alpha \sup u^\top x = \alpha \|u\|_*$. If $\alpha < 0$, let $\alpha = -|\alpha|$. $\sup (-|\alpha|u)^\top x = |\alpha| \sup u^\top (-x)$. Since the unit ball is symmetric, $\{-x : x \in B\} = B$, so this equals $|\alpha| \|u\|_*$.
          </div>
          <div class="proof-step">
            <strong>3. Triangle Inequality:</strong> $\|u+v\|_* = \sup_{x \in B} (u+v)^\top x \le \sup_{x \in B} u^\top x + \sup_{x \in B} v^\top x = \|u\|_* + \|v\|_*$.
          </div>
        </div>

        <div class="theorem-box">
          <h4>Properties of Dual Norms</h4>
          <ol>
            <li><b>Generalized Hölder Inequality:</b> For any $u, x$:
              $$ u^\top x \le \|u\|_* \|x\| $$
              <i>Proof:</i> If $x \neq 0$, let $z = x/\|x\|$. Then $\|z\|=1$. By definition, $u^\top z \le \|u\|_*$. Multiplying by $\|x\|$ gives $u^\top x \le \|u\|_* \|x\|$.
            </li>
            <li><b>Dual Ball is Polar:</b> The unit ball of the dual norm is the polar of the primal unit ball:
              $$ \{u : \|u\|_* \le 1\} = \{x : \|x\| \le 1\}^\circ $$
            </li>
            <li><b>Dual of Dual:</b> The dual of the dual norm is the primal norm: $(\|\cdot\|_* )_* = \|\cdot\|$.</li>
          </ol>
        </div>

        <h4>Calculated Dual Pairs (with explicit maximizers)</h4>
        <ul>
          <li><b>$\ell_2$ is self-dual:</b> $\|\cdot\|_{2,*} = \|\cdot\|_2$.
            <br><i>Maximizer:</i> For $u \neq 0$, choose $x = u/\|u\|_2$. Then $u^\top x = \|u\|_2$.
          </li>
          <li><b>$\ell_1$ dual is $\ell_\infty$:</b> $\|\cdot\|_{1,*} = \|\cdot\|_\infty$.
            <br><i>Maximizer:</i> To maximize $u^\top x$ subject to $\sum |x_i| \le 1$, we put all weight on the index $k$ where $|u_k|$ is maximal. Choose $x = \text{sign}(u_k) e_k$. Then $u^\top x = |u_k| = \|u\|_\infty$.
          </li>
          <li><b>$\ell_\infty$ dual is $\ell_1$:</b> $\|\cdot\|_{\infty,*} = \|\cdot\|_1$.
            <br><i>Maximizer:</i> To maximize $u^\top x$ subject to $|x_i| \le 1$, we set $x_i = \text{sign}(u_i)$ for all $i$. Then $u^\top x = \sum u_i \text{sign}(u_i) = \sum |u_i| = \|u\|_1$.
          </li>
        </ul>

        <h3>1.6 Cone Operations</h3>
        <p>Just as with convex sets, we can construct new cones from existing ones. This is particularly useful for modeling complex constraints in conic programming. Below we detail the primary operations and, crucially, how to compute their duals.</p>

        <h4>(a) Intersection</h4>
        <p>The intersection of any collection of cones is a cone. If the cones are convex, the intersection is a convex cone.</p>
        $$ K = \bigcap_{i \in I} K_i $$
        <p><b>Dual Property:</b> For closed convex cones, the dual of the intersection involves the Minkowski sum of the duals. However, this sum is not always closed, so we must take the closure:</p>
        $$ \left(\bigcap_{i} K_i\right)^* = \mathrm{cl}\left(\sum_{i} K_i^*\right) $$
        <div class="intuition-box">
          <h4>Why the closure?</h4>
          <p>Consider two "ice cream" cones in $\mathbb{R}^3$ that intersect only at a line. Their duals are also ice cream cones. The sum of these two dual cones might effectively cover an open half-space plus some boundary, but miss a specific ray on the boundary. The closure operation ensures that the resulting set is mathematically closed (contains all its limit points), which is required for it to be a valid dual cone.</p>
        </div>

        <h4>(b) Cartesian Product (Direct Sum)</h4>
        <p>The Cartesian product of cones $K_1 \subseteq \mathbb{R}^n$ and $K_2 \subseteq \mathbb{R}^m$ is a cone in the product space $\mathbb{R}^{n+m}$:</p>
        $$ K = K_1 \times K_2 = \{(\mathbf{x}, \mathbf{y}) \mid \mathbf{x} \in K_1, \mathbf{y} \in K_2\} $$
        <p>This is the standard way to build complex problems from simple blocks. For example, a Semidefinite Program (SDP) might require variable $\mathbf{x}$ to be non-negative ($\mathbf{x} \in \mathbb{R}^n_+$) AND a matrix $S$ to be PSD ($S \in \mathbb{S}^k_+$). The solver treats this as a single variable $(\mathbf{x}, \mathrm{vec}(S))$ living in the product cone $\mathbb{R}^n_+ \times \mathbb{S}^k_+$.</p>
        <p><b>Dual Property:</b> The dual of a product is simply the product of the duals:</p>
        $$ (K_1 \times K_2)^* = K_1^* \times K_2^* $$
        <p>This simplificity is why block-diagonal structure is so computationally efficient.</p>

        <h4>(c) Minkowski Sum</h4>
        <p>The Minkowski sum of two convex cones is defined as:</p>
        $$ K_1 + K_2 = \{x + y \mid x \in K_1, y \in K_2\} $$
        <p><b>Dual Property:</b> The dual of the sum is the <b>intersection</b> of the duals:</p>
        $$ (K_1 + K_2)^* = K_1^* \cap K_2^* $$
        <div class="proof-box">
          <h4>Proof of Dual Sum</h4>
          <p>Let $z \in (K_1 + K_2)^*$. By definition, $z^\top (x+y) \ge 0$ for all $x \in K_1, y \in K_2$.
          <br>Setting $y=0$, we get $z^\top x \ge 0 \forall x \in K_1 \implies z \in K_1^*$.
          <br>Setting $x=0$, we get $z^\top y \ge 0 \forall y \in K_2 \implies z \in K_2^*$.
          <br>Thus $z \in K_1^* \cap K_2^*$. The reverse inclusion holds by linearity.</p>
        </div>
        <p><b>Note:</b> Even if $K_1$ and $K_2$ are closed, their sum $K_1 + K_2$ might not be closed. This is a subtle topological issue that often necessitates "constraint qualifications" in duality theory.</p>

        <h4>(d) Linear Maps</h4>
        <p>Let $A \in \mathbb{R}^{m \times n}$ be a linear map.</p>
        <ul>
            <li><b>Image of a Cone:</b> If $K \subseteq \mathbb{R}^n$ is a cone, its image $A(K) = \{Ax \mid x \in K\}$ is a cone in $\mathbb{R}^m$.
            <br><b>Dual:</b> $(A(K))^* = (A^*)^{-1}(K^*) = \{y \mid A^\top y \in K^*\}$. (The preimage of the dual under the adjoint).</li>

            <li><b>Inverse Image (Preimage):</b> If $K \subseteq \mathbb{R}^m$ is a cone, its preimage $A^{-1}(K) = \{x \in \mathbb{R}^n \mid Ax \in K\}$ is a cone in $\mathbb{R}^n$.
            <br>This operation creates <b>Linear Matrix Inequalities (LMIs)</b>. For example, if $K = \mathbb{S}^k_+$, then $A^{-1}(K)$ is the set of $x$ such that the matrix combination $\sum x_i A_i$ is PSD.</li>
        </ul>

        <div class="theorem-box">
          <h4>Deep Dive: Dual of the Preimage (The Adjoint Rule)</h4>
          <p>For the preimage cone $C = \{x \mid Ax \in K\}$, what is its dual $C^*$?
          <br>Ideally, we want:
          $$ (A^{-1}(K))^* = A^\top K^* = \{A^\top y \mid y \in K^*\} $$
          However, similar to intersection, the set $A^\top K^*$ (image of a closed cone) is not necessarily closed. The rigorous statement requires closure:</p>
          $$ (A^{-1}(K))^* = \mathrm{cl}(A^\top K^*) $$
          <p><b>Constraint Qualifications:</b> If a condition like Slater's condition holds (e.g., there exists $x$ such that $Ax \in \mathrm{int}(K)$), then the closure is unnecessary, and we get the clean formula $(A^{-1}(K))^* = A^\top K^*$. This is equivalent to strong duality holding.</p>
        </div>

      </section>

          <section class="section-card" id="section-2">
      <h2>2. Separation and Supporting Hyperplanes</h2>
      <p>These theorems form the geometric backbone of duality theory. They formalize the intuition that convex sets can be "separated" from points outside them and "supported" by planes at their boundaries.</p>

      <h3>2.1 The Separating Hyperplane Theorem (Recap & Strict Separation)</h3>

      <div class="insight">
        <h4>Geometric Duality vs. Lagrangian Duality</h4>
        <p>The "dual cone" $K^*$ is <b>GEOMETRIC duality</b>—sets related by inner products. This is distinct from (but related to!) the <b>LAGRANGIAN duality</b> you'll see in <a href="../09-duality/index.html">Lecture 09</a>, where primal/dual are optimization problems.
        <br>The connection: <b>separation theorems</b> (geometric) underlie <b>strong duality theorems</b> (optimization). Every time you use a Lagrange multiplier, you are implicitly finding a separating hyperplane.</p>
      </div>

      <p>As introduced in <a href="../03-convex-sets-geometry/index.html">Lecture 03</a>, any two disjoint convex sets can be separated by a hyperplane. In this lecture, we deepen this result by focusing on <b>strict separation</b>, which is the engine behind strong duality.</p>

      <div class="theorem-box">
        <h4>Theorem (Strict Separation)</h4>
        <p>Let $C, D \subseteq \mathbb{R}^n$ be disjoint convex sets. If $C$ is <b>closed</b> and $D$ is <b>compact</b> (closed and bounded), then there exists a hyperplane that <b>strictly separates</b> them. Specifically, there exist $a \in \mathbb{R}^n \setminus \{0\}$ and $b \in \mathbb{R}$ such that:</p>
        $$
        \sup_{x \in C} a^\top x < b < \inf_{y \in D} a^\top y
        $$
        <p>This means there is a "gap" of positive width between the sets.</p>
      </div>

      <div class="proof-box">
        <h4>Proof Sketch (Constructive via Projection)</h4>
        <p>We construct the separating normal using the projection theorem.</p>
        <div class="proof-step">
            <strong>Step 1: Difference Set Geometry.</strong> Define the Minkowski difference $S = C - D = \{c - d \mid c \in C, d \in D\}$.
            <ul>
                <li>Since $C, D$ are disjoint, the origin $0$ is not in $S$.</li>
                <li>Since $C, D$ are convex, $S$ is convex.</li>
                <li>Since $C$ is closed and $D$ is compact, $S$ is closed (a sum of closed and compact sets is closed).</li>
            </ul>
            The problem reduces to separating the point $0$ from the closed convex set $S$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Projection onto S.</strong> Let $p = \Pi_S(0)$ be the unique point in $S$ closest to the origin.
            Since $S$ is closed and convex, this projection exists and is unique.
            Since $0 \notin S$, we have $\|p\| > 0$, so $p \neq 0$.
        </div>
        <div class="proof-step">
            <strong>Step 3: Variational Inequality.</strong>
            The projection theorem states that for any $z \in S$, the vector $p$ forms an obtuse angle with the path to $z$:
            $$ \langle z - p, 0 - p \rangle \le 0 \implies \langle z - p, -p \rangle \le 0 $$
            Rearranging: $\langle z, p \rangle \ge \|p\|^2$.
        </div>
        <div class="proof-step">
            <strong>Step 4: Separation.</strong>
            Set the normal vector $a = -p$. The inequality becomes $a^\top z \le -\|p\|^2$ for all $z \in S$.
            Substituting $z = c - d$:
            $$ a^\top (c - d) \le -\|p\|^2 < 0 \implies a^\top c < a^\top d - \|p\|^2 $$
            Let $b = \sup_{c \in C} a^\top c$. Then $b \le \inf_{d \in D} a^\top d - \|p\|^2 < \inf_{d \in D} a^\top d$.
            Thus, the hyperplane defined by $(a, b)$ strictly separates $C$ and $D$ with a gap of at least $\|p\|^2$.
        </div>
      </div>

      <h4>Phase 1.2 — Supporting Hyperplanes (The Skeleton)</h4>
      <p>This is the deep geometric backbone of optimization.</p>
      <div class="theorem-box">
        <h4>Supporting Hyperplane Theorem</h4>
        <p>A hyperplane $H = \{x \mid a^\top x = \alpha\}$ <b>supports</b> a set $C$ at a point $x_0 \in \partial C$ if:
        <ol>
            <li>$x_0$ lies on the hyperplane ($a^\top x_0 = \alpha$).</li>
            <li>The entire set $C$ lies in one of the halfspaces defined by $H$ (e.g., $a^\top x \le \alpha$ for all $x \in C$).</li>
        </ol>
        <b>Key Fact:</b> If $C$ is convex, then <i>every</i> point on its boundary has at least one supporting hyperplane.</p>
      </div>

      <p><b>Optimality Connection:</b> In optimization, we usually minimize $f_0(x)$ over a convex set $\mathcal{F}$.
      <br>At the optimal point $x^\star$, the level set $\{x \mid f_0(x) \le p^\star\}$ and the feasible set $\mathcal{F}$ touch but do not overlap (their interiors are disjoint).
      <br>Thus, there exists a <b>separating hyperplane</b> between them. This hyperplane supports both sets at $x^\star$.
      <br><b>Internalize:</b> Optimality = Existence of a hyperplane that supports the feasible set <i>and</i> the objective epigraph.</p>

      <div class="proof-box">
        <h4>Constructive Proof via Projection (Separation)</h4>
        <p>We use the result from 2.1 (Strict Separation) and a limiting argument.</p>
        <div class="proof-step">
          <strong>Step 1: Exterior Sequence.</strong>
          Since $x_0 \in \partial C$, there is a sequence of points $y_k \notin C$ such that $y_k \to x_0$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Projection & Normal.</strong>
          Let $x_k = \Pi_C(y_k)$. Since $y_k \notin C$, $x_k \neq y_k$.
          Define the normal $a_k = \frac{y_k - x_k}{\|y_k - x_k\|}$.
          By the projection inequality: $a_k^\top (z - x_k) \le 0$ for all $z \in C$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Convergence.</strong>
          The sequence $a_k$ lies on the unit sphere (compact), so there exists a convergent subsequence $a_{k_j} \to a$ with $\|a\|=1$.
          Also $x_{k_j} \to x_0$ (by continuity of projection).
        </div>
        <div class="proof-step">
          <strong>Step 4: Limit.</strong>
          Taking the limit of $a_k^\top (z - x_k) \le 0$:
          $$ a^\top (z - x_0) \le 0 \implies a^\top z \le a^\top x_0 $$
          for all $z \in C$. Thus $a$ defines a supporting hyperplane at $x_0$.
        </div>
      </div>

      <div class="example-box">
        <h4>Example: Supporting Hyperplanes of the Unit Ball</h4>
        <p>Consider $C = \{x \mid \|x\|_2 \le 1\}$ (unit ball). For any $x_0$ with $\|x_0\|_2 = 1$, the supporting hyperplane is:
        $$ \{x \mid x_0^\top x = 1\} $$
        <b>Normal vector:</b> $a = x_0$. This hyperplane is perpendicular to the radius at $x_0$ and tangent to the sphere.
        <br><b>Verification:</b> For any $x \in C$, by Cauchy-Schwarz: $x_0^\top x \le \|x_0\|_2 \|x\|_2 \le 1 \cdot 1 = 1$.
        </p>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/supporting-hyperplane-tangent.png"
             alt="Illustration of the Supporting Hyperplane Theorem"
             style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Figure 14:</i> A supporting hyperplane "kisses" the convex set at the boundary without cutting through it. The normal vector points outward.</figcaption>
      </figure>

      <h4>Phase 1.1 — Constraints as Geometry (Normal Cones)</h4>
      <p>This phase explains <i>why</i> Lagrange multipliers exist.</p>
      <p><b>Constraint Geometry:</b>
      <ul>
          <li>Inequality constraints $f_i(x) \le 0$ define curved regions (sublevel sets).</li>
          <li>Equality constraints $h_j(x) = 0$ define manifolds (surfaces).</li>
          <li><b>Active constraints</b> define the boundary where the optimum lives.</li>
      </ul>
      </p>
      <p><b>Normal Cone Definition:</b> For a convex set $C$ and a point $\mathbf{x} \in C$, the <b>normal cone</b> $N_C(\mathbf{x})$ describes the "outward" directions that are blocked by the boundary.
      $$ N_C(\mathbf{x}) = \{\mathbf{g} \in \mathbb{R}^n \mid \mathbf{g}^\top (\mathbf{y} - \mathbf{x}) \le 0 \text{ for all } \mathbf{y} \in C\} $$
      <b>Key Geometric Fact:</b> At the optimum $x^\star$, the negative gradient of the objective $-\nabla f_0(x^\star)$ points "into the wall". Formally, $-\nabla f_0(x^\star) \in N_C(x^\star)$.
      <br>This "pressure" against the boundary is what <b>dual variables</b> (Lagrange multipliers) measure.</p>

      <div class="insight">
        <h4>Linear Optimization Viewpoint: Support = Optimality</h4>
        <p>A point $x_0 \in C$ maximizes the linear function $f(x) = c^\top x$ over $C$ <b>if and only if</b> $c$ is a supporting normal at $x_0$.
        $$ x_0 \in \arg\max_{x \in C} c^\top x \iff c \in N_C(x_0) $$
        <p><i>Proof:</i> $c \in N_C(x_0) \iff c^\top(x-x_0) \le 0 \ \forall x \iff c^\top x \le c^\top x_0 \ \forall x$.
        <br>This is why <b>dual variables</b> (Lagrange multipliers) are geometric normals. They are the certificates that "push back" against the gradient of the objective function to keep it from improving further.</p>
      </div>

      <h3>2.4 Theorems of the Alternative (Farkas' Lemma)</h3>
      <p>Farkas' Lemma is the algebraic certificate of infeasibility for linear inequalities, derived directly from the Separating Hyperplane Theorem.</p>

      <div class="theorem-box">
        <h4>Farkas' Lemma</h4>
        <p>For $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$, exactly one of the following systems has a solution:</p>
        <ol>
          <li><b>Primal:</b> $Ax = b, \ x \ge 0$ (b is in the conic hull of columns of A)</li>
          <li><b>Dual (Alternative):</b> $A^\top y \ge 0, \ b^\top y < 0$ (separating hyperplane exists)</li>
        </ol>
      </div>

      <div class="intuition-box">
        <p><b>Geometric Meaning:</b> Farkas' Lemma is the "Separating Hyperplane Theorem for Cones."
        <br>Option 1 says $b$ is <i>inside</i> the cone generated by the columns of $A$.
        <br>Option 2 says there is a hyperplane (defined by normal $y$) that separates $b$ from the cone. The condition $A^\top y \ge 0$ means the hyperplane keeps the entire cone on one side (making acute angles with all generators), while $b^\top y < 0$ means $b$ is on the other side (making an obtuse angle).</p>
      </div>
    </section>

    <section class="section-card" id="section-3">
        <h2>3. Review & Cheat Sheet</h2>
        <h3>Separation Theorems</h3>
        <ul>
          <li><b>Separating Hyperplane:</b> Any two disjoint convex sets can be separated by a hyperplane ($a^\top x \le b \le a^\top y$).</li>
          <li><b>Strict Separation:</b> Requires disjoint, closed, and one set compact.</li>
          <li><b>Supporting Hyperplane:</b> At every boundary point of a convex set, there exists a hyperplane containing the set on one side.</li>
          <li><b>Farkas Lemma:</b> The fundamental theorem of the alternative for linear inequalities, derived from separation.</li>
        </ul>

        <h3>Cones and Duality</h3>
        <ul>
          <li><b>Proper Cone:</b> Convex, Closed, Pointed, Solid. Induces a partial order $\preceq_K$.</li>
          <li><b>Dual Cone ($K^*$):</b> The set of vectors making a non-obtuse angle with $K$.</li>
          <li><b>Self-Dual Cones:</b> $\mathbb{R}^n_+$, $\mathcal{Q}^{n+1}$ (SOC), $\mathbb{S}^n_+$ (PSD).</li>
          <li><b>Dual Operations:</b> $(K_1 + K_2)^* = K_1^* \cap K_2^*$ (for closed convex cones).</li>
        </ul>
      </section>

    <section class="section-card" id="section-4">
      <h2><i data-feather="edit-3"></i> 4. Exercises</h2>

<div class="problem">
  <h3>P4.2 — Dual of a Subspace</h3>
  <p>Let $V \subseteq \mathbb{R}^n$ be a linear subspace. Prove that its dual cone is the orthogonal complement $V^\perp$.</p>

      <div class="recap-box">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Subspace Duality:</b> For a subspace, $y^\top x \ge 0$ implies $y^\top x = 0$ because both $x$ and $-x$ are in $V$.</li>
            <li><b>Geometric Intuition:</b> A subspace is a "flat" cone. If you widen a cone until it becomes a plane, its dual cone narrows until it becomes the normal line (orthogonal complement).</li>
        </ul>
      </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
              <strong>($\subseteq$):</strong> If $y \in V^\perp$, then $y^\top x = 0 \ge 0$ for all $x \in V$. So $y \in V^*$.
    </div>
    <div class="proof-step">
              <strong>($\supseteq$):</strong> Suppose $y \in V^*$, so $y^\top x \ge 0$ for all $x \in V$. Since $V$ is a subspace, $-x \in V$. Thus $y^\top (-x) \ge 0 \implies y^\top x \le 0$.
              Combining $\ge 0$ and $\le 0$ gives $y^\top x = 0$. Thus $y \in V^\perp$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.3 — Self-Duality of Second-Order Cone</h3>
  <p>Prove that the second-order cone $\mathcal{Q} = \{(x, t) \in \mathbb{R}^{n+1} \mid \|x\|_2 \le t\}$ is self-dual: $\mathcal{Q}^* = \mathcal{Q}$.</p>

      <div class="recap-box">
        <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
        <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
            <li><b>Cauchy-Schwarz:</b> The inequality $u^\top v \ge -\|u\|\|v\|$ is central to proving this inclusion.</li>
        </ul>
      </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <p>Let $u=(x,t)$ and $v=(y,s)$.</p>
    <div class="proof-step">
      <strong>($\subseteq$):</strong> Let $v \in \mathcal{Q}$. For any $u \in \mathcal{Q}$, we have $\|x\| \le t$ and $\|y\| \le s$.
      We want to show $u^\top v \ge 0$.
      The standard Cauchy-Schwarz inequality states $|x^\top y| \le \|x\|\|y\|$, which implies $x^\top y \ge -|x^\top y| \ge -\|x\|\|y\|$.
      Thus:
      $$ u^\top v = x^\top y + ts \ge -\|x\|\|y\| + ts $$
      Since $t \ge \|x\|$ and $s \ge \|y\|$, we have $ts \ge \|x\|\|y\|$.
      Therefore:
      $$ -\|x\|\|y\| + ts \ge -\|x\|\|y\| + \|x\|\|y\| = 0 $$
      Thus $u^\top v \ge 0$ for all $u \in \mathcal{Q}$, so $v \in \mathcal{Q}^*$.
    </div>
    <div class="proof-step">
      <strong>($\supseteq$):</strong> Let $\mathbf{v} = (\mathbf{y},s) \in \mathcal{Q}^*$. We must show $\mathbf{v} \in \mathcal{Q}$, i.e., $s \ge \|\mathbf{y}\|$.
      <br>First, test with the vector $\mathbf{u}=(0,1)$. Since $\|0\| \le 1$, $\mathbf{u} \in \mathcal{Q}$.
      The dual condition implies $\mathbf{u}^\top \mathbf{v} = 0^\top \mathbf{y} + 1 \cdot s = s \ge 0$. So $s$ must be non-negative.
      <br>Now, consider the vector $\mathbf{y}$.
      <br><b>Case 1: $\mathbf{y} = 0$.</b> Then $\|\mathbf{y}\|=0$. Since we already showed $s \ge 0$, we have $s \ge \|\mathbf{y}\|$. Thus $(0, s) \in \mathcal{Q}$.
      <br><b>Case 2: $\mathbf{y} \neq 0$.</b> Construct the test vector $\mathbf{u} = (-\frac{\mathbf{y}}{\|\mathbf{y}\|}, 1)$.
      Check primal feasibility: the spatial part has norm $\left\|-\frac{\mathbf{y}}{\|\mathbf{y}\|}\right\| = 1$, which is $\le$ the time part $1$. So $\mathbf{u} \in \mathcal{Q}$.
      <br>Apply the dual condition $\mathbf{u}^\top \mathbf{v} \ge 0$:
      $$ -\frac{\mathbf{y}}{\|\mathbf{y}\|}^\top \mathbf{y} + 1 \cdot s \ge 0 \implies -\frac{\|\mathbf{y}\|^2}{\|\mathbf{y}\|} + s \ge 0 \implies -\|\mathbf{y}\| + s \ge 0 \implies s \ge \|\mathbf{y}\| $$
      Thus $v \in \mathcal{Q}$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.4 — Dual Cone Identities</h3>
  <p>For closed convex cones $K_1, K_2$, prove $(K_1 + K_2)^* = K_1^* \cap K_2^*$.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Duality of Operations:</b> Dual of sum is intersection.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>($\subseteq$):</strong>
      $y \in (K_1 + K_2)^*$ iff $y^\top (x_1+x_2) \ge 0$ for all $x_1 \in K_1, x_2 \in K_2$.
      Choosing $x_2=0$, $y^\top x_1 \ge 0 \implies y \in K_1^*$.
      Choosing $x_1=0$, $y^\top x_2 \ge 0 \implies y \in K_2^*$.
      Thus $y \in K_1^* \cap K_2^*$.
    </div>
    <div class="proof-step">
      <strong>($\supseteq$):</strong>
      Let $y \in K_1^* \cap K_2^*$. For any element $z \in K_1 + K_2$, we can write $z = x_1 + x_2$ with $x_1 \in K_1, x_2 \in K_2$.
      Then $y^\top z = y^\top x_1 + y^\top x_2$.
      Since $y \in K_1^*$, $y^\top x_1 \ge 0$. Since $y \in K_2^*$, $y^\top x_2 \ge 0$.
      Therefore $y^\top z \ge 0$ for all $z \in K_1 + K_2$, so $y \in (K_1 + K_2)^*$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.5 — Generalized Inequality Properties</h3>
  <p>Let $K$ be a proper cone. Prove $\preceq_K$ is antisymmetric: $x \preceq_K y \land y \preceq_K x \implies x=y$.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Pointedness:</b> $K \cap -K = \{0\}$ is the key property ensuring antisymmetry.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      $x \preceq_K y \implies y-x \in K$.
      $y \preceq_K x \implies x-y \in K \implies -(y-x) \in K$.
      So $y-x \in K \cap -K$. Since $K$ is proper (pointed), $K \cap -K = \{0\}$.
      Thus $y-x=0 \implies x=y$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.6 — Norm Cone (Epigraph of Norm)</h3>
  <p>Show that the set $C = \{(x,t) \mid \|x\| \le t\}$ is a convex cone.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Epigraphs of Convex Functions:</b> The set is the epigraph of the convex function $f(x)=\|x\|$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <b>Cone:</b> If $\|x\| \le t$, then $\|\alpha x\| = \alpha \|x\| \le \alpha t$. So $\alpha(x,t) \in C$ for $\alpha \ge 0$.
    </div>
    <div class="proof-step">
      <b>Convex:</b> If $(x,t), (y,s) \in C$, then $\|x+y\| \le \|x\|+\|y\| \le t+s$. So $(x+y, t+s) \in C$.
    </div>
  </div>
</div>


<div class="problem">
  <h3>P4.9 — Cones in $\mathbb{R}^2$</h3>
  <p>Describe all closed convex cones in $\mathbb{R}^2$.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Geometric Intuition:</b> In 2D, a cone is simply a "wedge" radiating from the origin.</li>
        <li><b>Dual Angle Relation:</b> If a cone has angular width $\alpha$, its dual cone has angular width $\pi - \alpha$. If the cone is "sharp" ($\alpha < \pi/2$), the dual is "blunt".</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Any closed convex cone $K \subset \mathbb{R}^2$ must fall into one of these categories:
      <ul>
        <li><b>Zero Dimensional:</b> The origin $\{0\}$. (Dual is $\mathbb{R}^2$).</li>
        <li><b>One Dimensional:</b> A single ray $\{t v \mid t \ge 0\}$. (Dual is a halfspace).</li>
        <li><b>Two Dimensional (Proper):</b> A "wedge" defined by two rays. Analytically, $K = \{x \mid a_1^\top x \ge 0, a_2^\top x \ge 0\}$. The angle between the bounding rays is less than $\pi$.</li>
        <li><b>Two Dimensional (Improper):</b> A half-plane (angle $\pi$) or the entire plane $\mathbb{R}^2$ (angle $2\pi$).</li>
      </ul>
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.10 — Properties of Dual Cones</h3>
  <p>Prove that if $\mathrm{int}(K) \neq \emptyset$, then $K^*$ is pointed.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Geometric Duality:</b> There is a duality between "solidness" and "pointedness". A fat cone (solid) forces its dual to be sharp (pointed). A flat cone (subspace) allows its dual to be wide (subspace).</li>
        <li><b>Lineality Space:</b> The set $K \cap -K$ is the lineality space. Pointed means the lineality space is $\{0\}$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Characterize the Lineality Space.</strong>
      $K^*$ is pointed if $K^* \cap -K^* = \{0\}$.
      Let $y \in K^* \cap -K^*$. This means $y^\top x \ge 0$ AND $(-y)^\top x \ge 0$ for all $x \in K$.
      Thus $y^\top x = 0$ for all $x \in K$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Use Solidness.</strong>
      Since $\mathrm{int}(K) \neq \emptyset$, the cone $K$ contains a basis of $\mathbb{R}^n$. Specifically, there exist $n$ linearly independent vectors $b_1, \dots, b_n \in K$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Orthogonality Argument.</strong>
      We know $y^\top b_i = 0$ for all $i=1\dots n$.
      Since the $b_i$ span $\mathbb{R}^n$, the only vector orthogonal to all of them is the zero vector.
      Therefore $y = 0$. This proves $K^*$ contains no lines, so it is pointed.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.11 — Dual Cone of Generated Cone</h3>
  <p>Let $K = \{Ax \mid x \ge 0\}$. Show $K^* = \{y \mid A^\top y \ge 0\}$.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Farkas Lemma Connection:</b> This result is the geometric heart of Farkas' Lemma. $\mathbf{y}$ is in the dual cone if it makes a non-negative angle with <i>all</i> vectors in $K$.</li>
        <li><b>Generators:</b> Since every vector in $K$ is a non-negative sum of the columns of $A$, checking non-negativity on the columns is sufficient.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Definition.</strong>
      $y \in K^* \iff y^\top z \ge 0$ for all $z \in K$.
      Any $z \in K$ can be written as $z = Ax$ for some $x \ge 0$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Adjoint Property.</strong>
      The condition becomes: $y^\top (Ax) \ge 0$ for all $x \ge 0$.
      Using the transpose: $(A^\top y)^\top x \ge 0$ for all $x \ge 0$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Self-Duality of Positive Orthant.</strong>
      Let $w = A^\top y$. The condition is $w^\top x \ge 0$ for all $x \in \mathbb{R}^n_+$.
      This is exactly the definition of the dual cone of $\mathbb{R}^n_+$. Since the orthant is self-dual, this means $w \in \mathbb{R}^n_+$, i.e., $w \ge 0$.
      Thus $A^\top y \ge 0$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.12 — The Monotone Nonnegative Cone</h3>
  <p>Find the dual of $K = \{x \in \mathbb{R}^n \mid x_1 \ge x_2 \ge \dots \ge 0\}$.</p>

  <div class="recap-box">
    <h4>Recap</h4>
    <p>The dual of the cone of monotonic sequences is the cone of sequences with non-negative partial sums. This duality is often used in isotonic regression.</p>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Using summation by parts (Abel's lemma), the condition $\sum x_i y_i \ge 0$ for all monotone $x$ requires the partial sums of $y$ to be non-negative.
      $K^* = \{y \mid \sum_{i=1}^k y_i \ge 0, k=1\dots n\}$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.13 — The Lexicographic Cone</h3>
  <p>Let $K_{lex} = \{0\} \cup \{x \mid \text{first nonzero of } x > 0\}$. Find $K_{lex}^*$.</p>

  <div class="recap-box">
    <h4>Recap</h4>
    <p>The lexicographic cone is not closed (it misses the boundary where the first coordinate is zero but the second is positive). Its dual is extremely thin (a single ray), illustrating that $K^{**} \neq K$ for non-closed cones.</p>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      The cone is "almost" a halfspace but includes boundaries delicately.
      Any vector $\mathbf{y}$ with a non-zero component at index $k > 1$ can form a negative dot product with some $\mathbf{x} \in K_{lex}$ (by dominating the first component with a large value at $k$).
      Thus $\mathbf{y}$ must be zero everywhere except index 1. $y_1$ must be non-negative.
      $K_{lex}^* = \{ \alpha e_1 \mid \alpha \ge 0 \}$ (The ray along the first axis).
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.14 — Dual of Intersection</h3>
  <p>Let $K_1, K_2$ be closed convex cones. Prove that $(K_1 \cap K_2)^* = \mathrm{cl}(K_1^* + K_2^*)$.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Duality Symmetry:</b> Since $K^{**} = K$ for closed convex cones, properties of the primal operations map to dual operations. Intersection in primal corresponds to sum in dual (with closure).</li>
        <li><b>Closure is Crucial:</b> The sum of two closed cones is not always closed. The closure operation is necessary for the equality to hold.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Use P4.4.</strong> From Problem 4.4, we know that for closed convex cones $A, B$, $(A + B)^* = A^* \cap B^*$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Apply to Duals.</strong> Let $A = K_1^*$ and $B = K_2^*$. Then $(K_1^* + K_2^*)^* = K_1^{**} \cap K_2^{**}$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Biconjugate.</strong> Since $K_i$ are closed convex cones, the Bipolar Theorem guarantees $K_i^{**} = K_i$.
      Substituting this into the intersection formula, we get $(K_1^* + K_2^*)^* = K_1 \cap K_2$.
      (Note: If $K_i$ were not closed, $K_i^{**}$ would be the closure of $K_i$).
    </div>
    <div class="proof-step">
      <strong>Step 4: Take Dual again.</strong> Taking the dual of both sides: $(K_1^* + K_2^*)^{**} = (K_1 \cap K_2)^*$.
      The biconjugate of a cone is its closure (if it's convex). Thus $\mathrm{cl}(K_1^* + K_2^*) = (K_1 \cap K_2)^*$.
    </div>
  </div>
</div>



<div class="problem">
  <h3>P4.22 — Separation by Projection</h3>
  <p>Let $C \subseteq \mathbb{R}^n$ be a closed convex set and $x \notin C$. Prove that there exists a hyperplane strictly separating $x$ from $C$. Specifically, show that with $p = \Pi_C(x)$, the hyperplane through $p$ with normal $x-p$ separates them.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Let $p = \Pi_C(x)$. Since $C$ is convex, the projection is unique.
      The variational characterization of projection states: $(z - p)^\top (x - p) \le 0$ for all $z \in C$.
    </div>
    <div class="proof-step">
      Let $a = x - p$. Then $a^\top (z - p) \le 0 \implies a^\top z \le a^\top p$ for all $z \in C$.
      For the point $x$ itself: $a^\top x = (x-p)^\top x = (x-p)^\top (x-p + p) = \|x-p\|^2 + a^\top p$.
      Since $x \notin C$, $\|x-p\| > 0$, so $a^\top x > a^\top p$.
      Thus the hyperplane $H = \{z \mid a^\top z = a^\top p\}$ separates $C$ (where $a^\top z \le a^\top p$) from $x$ (where $a^\top x > a^\top p$).
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.23 — Support Function Determines Set</h3>
  <p>Let $C, D$ be closed convex sets. Show that if their support functions are equal, $\sigma_C(a) = \sigma_D(a)$ for all $a$, then $C = D$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <p>A closed convex set is the intersection of its supporting halfspaces:
    $$ C = \bigcap_{a \in \mathbb{R}^n} \{x \mid a^\top x \le \sigma_C(a)\} $$
    If $\sigma_C = \sigma_D$, the intersections are identical, so $C = D$.</p>
  </div>
</div>

<div class="problem">
  <h3>P4.24 — Converse Supporting Hyperplane Theorem</h3>
  <p>Let $C$ be a connected closed set with non-empty interior. Prove that if every boundary point of $C$ has a supporting hyperplane, then $C$ is convex.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <p>This is the <b>Tietze-Nakajima Theorem</b>.
    <br><b>Intuition:</b> If a set is non-convex, it must have a "dent". At the deepest part of the dent (a point of non-convexity), any hyperplane passing through it would cut into the set, making it impossible to support the set.
    <br>However, this requires the set to be <b>connected</b>.
    <br><b>Counterexample (without connectedness):</b> Let $C$ be the union of two disjoint closed balls. Every boundary point has a supporting hyperplane (the tangent plane to the ball). However, $C$ is not convex.
    <br><b>Proof Sketch:</b>
    Assume $C$ is connected and has interior points. If $C$ is not convex, there exist $x, y \in C$ such that the segment $[x,y]$ is not contained in $C$.
    One can define a "visibility" function or use a topological argument to show that if every boundary point has a support, the "kernel" of the set (points that can see every other point) must be the whole set.</p>
  </div>
</div>

<div class="problem">
  <h3>P4.15 — Euclidean Distance Matrices (EDM)</h3>
  <p>A matrix $D \in \mathbb{S}^n$ is a Euclidean Distance Matrix if there exist points $x_1, \dots, x_n$ such that $D_{ij} = \|x_i - x_j\|_2^2$. Show that the set of all EDMs is a convex cone.</p>
  <div class="recap-box">
    <h4>Recap</h4>
    <p>EDMs are linear images of the PSD cone. Since linear maps preserve convexity and conical structure, the set of EDMs inherits the cone property from $\mathbb{S}^n_+$.</p>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Expand definition.</strong>
      $D_{ij} = \|x_i - x_j\|^2 = \|x_i\|^2 + \|x_j\|^2 - 2x_i^\top x_j$.
      Let $G$ be the Gram matrix where $G_{ij} = x_i^\top x_j$. Then $G \succeq 0$.
      Let $v$ be the vector where $v_i = G_{ii} = \|x_i\|^2$.
      We can write $D = v\mathbf{1}^\top + \mathbf{1}v^\top - 2G$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Identify Linear Structure.</strong>
      The mapping $\mathcal{L}(G) = \text{diag}(G)\mathbf{1}^\top + \mathbf{1}\text{diag}(G)^\top - 2G$ is a linear map from $\mathbb{S}^n$ to $\mathbb{S}^n$.
      The set of EDMs is the image of the PSD cone $\mathbb{S}^n_+$ under this linear map $\mathcal{L}$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Convexity.</strong>
      Since $\mathbb{S}^n_+$ is a convex cone and linear maps preserve convexity and conical structure, the set of EDMs is a convex cone.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.16 — SOCP Canonical Dual</h3>
  <p>Consider the canonical Second-Order Cone Program (SOCP):
  $$ \min c^\top x \quad \text{s.t.} \quad Ax = b, \quad \|D_i x + e_i\|_2 \le f_i^\top x + g_i $$
  Derive the dual problem and show it is also an SOCP. Use the fact that the second-order cone is self-dual.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Conic Formulation.</strong>
      Rewrite the SOC constraint as membership in the second-order cone $\mathcal{Q}_{k_i+1}$:
      $$ (D_i x + e_i, f_i^\top x + g_i) \in \mathcal{Q}_{k_i+1} $$
      Note the order: typically we write $(t, \mathbf{u}) \in \mathcal{Q}$ where $t \ge \|\mathbf{u}\|$. Here $t = f_i^\top x + g_i$ and $\mathbf{u} = D_i x + e_i$.
    </div>

    <div class="proof-step">
      <strong>Step 2: Dual Variables.</strong>
      Associate a dual variable $\nu \in \mathbb{R}^m$ with the equality $Ax=b$.
      Associate a dual variable $\mathbf{z}_i = (\mathbf{u}_i, t_i) \in \mathcal{Q}_{k_i+1}^*$ with each conic constraint.
      Since $\mathcal{Q}$ is self-dual, the condition is simply $\|\mathbf{u}_i\|_2 \le t_i$.
    </div>

    <div class="proof-step">
      <strong>Step 3: The Lagrangian.</strong>
      The Lagrangian is $L(x, \nu, \mathbf{z}) = c^\top x + \nu^\top (b - Ax) - \sum_{i=1}^m \mathbf{z}_i^\top (D_i x + e_i, f_i^\top x + g_i)$.
      <br>Recall the inner product in the cone is just the dot product of the vectors.
      $$ \mathbf{z}_i^\top (\dots) = \begin{bmatrix} \mathbf{u}_i \\ t_i \end{bmatrix}^\top \begin{bmatrix} D_i x + e_i \\ f_i^\top x + g_i \end{bmatrix} = \mathbf{u}_i^\top (D_i x + e_i) + t_i (f_i^\top x + g_i) $$
      We now collect all terms involving $x$ to isolate the coefficient vector:
      $$ = c^\top x + \nu^\top b - (A^\top \nu)^\top x - \sum (\mathbf{u}_i^\top D_i x + \mathbf{u}_i^\top e_i + t_i f_i^\top x + t_i g_i) $$
      $$ = \left( c - A^\top \nu - \sum_{i=1}^m (D_i^\top \mathbf{u}_i + t_i f_i) \right)^\top x + \left( \nu^\top b - \sum_{i=1}^m (\mathbf{u}_i^\top e_i + t_i g_i) \right) $$
    </div>

    <div class="proof-step">
      <strong>Step 4: The Dual Function.</strong>
      The dual function $g(\nu, \mathbf{z}) = \inf_x L(x, \nu, \mathbf{z})$.
      Since $L$ is affine in $x$, the infimum is $-\infty$ unless the coefficient of $x$ is zero.
      Dual constraint: $c - A^\top \nu - \sum (D_i^\top \mathbf{u}_i + t_i f_i) = 0$.
      If this holds, $g(\nu, \mathbf{z}) = \nu^\top b - \sum (\mathbf{u}_i^\top e_i + t_i g_i)$.
    </div>

    <div class="proof-step">
      <strong>Step 5: Final Dual Problem.</strong>
      Maximize the dual function subject to constraints:
      $$
      \begin{aligned}
      \max_{\nu, \mathbf{u}_i, t_i} \quad & b^\top \nu - \sum_{i=1}^m (e_i^\top \mathbf{u}_i + g_i t_i) \\
      \text{s.t.} \quad & A^\top \nu + \sum_{i=1}^m (D_i^\top \mathbf{u}_i + f_i t_i) = c \\
      & \|\mathbf{u}_i\|_2 \le t_i, \quad i=1,\dots,m
      \end{aligned}
      $$
      This is clearly another SOCP (linear objective, linear equalities, SOC constraints).
    </div>
  </div>
</div>

<div class="problem">
  <h3>P4.17 — Homogeneous Envelope and Directional Derivatives</h3>
  <p>Given a convex function $f$ with $f(0)=0$, define its <b>homogeneous envelope</b>:
  $$ g(x) = \inf_{\alpha > 0} \frac{f(\alpha x)}{\alpha} $$
  (a) Prove $g$ is positively homogeneous ($g(tx) = tg(x)$ for $t \ge 0$).
  <br>(b) Prove $g$ is the largest homogeneous underestimator of $f$.
  <br>(c) Prove $g$ is convex. (Hint: relate it to the directional derivative $f'(0; x)$).</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Directional Derivative:</b> For convex $f$ with $f(0)=0$, the directional derivative at 0 is $f'(0;x) = \inf_{t>0} \frac{f(tx)}{t}$. This matches the definition of $g(x)$.</li>
        <li><b>Support Function:</b> $g(x)$ is actually the support function of the subdifferential $\partial f(0)$. Support functions are always convex and homogeneous.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>(a) Homogeneity.</strong>
      For $t > 0$: $g(tx) = \inf_{\alpha > 0} \frac{f(\alpha t x)}{\alpha}$. Let $\beta = \alpha t$. Then $\alpha = \beta/t$.
      $$ g(tx) = \inf_{\beta > 0} \frac{f(\beta x)}{\beta/t} = t \inf_{\beta > 0} \frac{f(\beta x)}{\beta} = t g(x). $$
      For $t=0$, $g(0)=0$ (assuming properness), so it holds.
    </div>
    <div class="proof-step">
      <strong>(b) Largest Underestimator.</strong>
      First, $g(x) \le f(x)$ (take $\alpha=1$ in infimum).
      Second, let $h$ be homogeneous with $h \le f$. Then $h(x) = \frac{h(\alpha x)}{\alpha} \le \frac{f(\alpha x)}{\alpha}$ for all $\alpha$. Taking infimum, $h(x) \le g(x)$.
    </div>
    <div class="proof-step">
      <strong>(c) Convexity via Directional Derivative.</strong>
      Since $f$ is convex with $f(0)=0$, the difference quotient $\frac{f(\alpha x)}{\alpha} = \frac{f(0+\alpha x)-f(0)}{\alpha}$ is non-decreasing in $\alpha$.
      Thus the infimum is the limit as $\alpha \downarrow 0$, which is the directional derivative $f'(0; x)$.
      Directional derivatives of convex functions are sublinear (convex):
      $$ f'(0; x+y) \le f'(0; x) + f'(0; y) \quad \text{(Subadditivity)} $$
      Combined with homogeneity (part a), $g$ is convex.
    </div>
  </div>
</div>

</div>

    </section>

    <section class="section-card" id="section-5">
      <h2>5. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required Reading:</strong> Boyd & Vandenberghe, Chapter 2 (Convex Sets, specifically Cones and Dual Cones).</li>
        <li><strong>Supplementary:</strong> Rockafellar, <em>Convex Analysis</em>, Sections 11 (Separation Theorems) and 14 (Cones).</li>
        <li><strong>Interactive:</strong> Use the Separating Hyperplane Explorer below to test intuition.</li>
        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Demo: Separating Hyperplanes</h3>
          <p>This widget demonstrates the Separating Hyperplane Theorem in 2D. Add points to two sets, compute their convex hulls, and find the separating line.</p>
          <div id="widget-separating-hyperplane" style="width: 100%; height: 600px; position: relative; max-width: 900px; margin: 0 auto; border-radius: 8px;"></div>
        </div>
      </ul>
    </section>

    <section class="section-card" id="section-6">
      <h2>6. Recap &amp; What's Next</h2>
      <div class="recap-box">
        <ul style="margin: 0 0 0 20px;">
          <li><b>Separation:</b> convex sets can be separated from disjoint convex sets by a hyperplane; boundary points admit supporting hyperplanes.</li>
          <li><b>Projection viewpoint:</b> projecting onto a closed convex set produces a normal direction that defines a separating/supporting hyperplane.</li>
          <li><b>Cones:</b> a cone is closed under nonnegative scaling; a proper cone is closed, pointed, and solid and induces a generalized inequality.</li>
          <li><b>Dual cones:</b> $K^* = \{y \mid y^\top x \ge 0 \ \forall x \in K\}$ encodes which linear functionals are “nonnegative” on $K$.</li>
        </ul>
      </div>
      <div class="interpretation-box">
        <p style="margin: 0;"><b>Forward look:</b> In <a href="../08-convex-problems-conic/index.html">Lecture 08</a> you will see constraints written as “$(x,t)\in K$,” where $K$ is a cone like the SOC or PSD cone. In <a href="../09-duality/index.html">Lecture 09</a> you will see separation reappear as dual variables and optimality certificates.</p>
      </div>
    </section>

    </article>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSeparatingHyperplane } from './widgets/js/separating-hyperplane.js';
    initSeparatingHyperplane('widget-separating-hyperplane');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
