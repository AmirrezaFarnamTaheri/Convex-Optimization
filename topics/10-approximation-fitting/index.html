<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>06. Applications I: Approximation & Fitting — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="section-card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">06. Applications I: Approximation & Fitting</h1>
      <div class="meta">
        Date: 2025-11-25 · Duration: 90 min · Tags: applications, fitting
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> Least squares, robust regression, sparse recovery, regularization.</p>
        <p><strong>Prerequisites:</strong> <a href="../07-convex-problems-standard/index.html">Lecture 07-08</a></p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you should understand:</p>
      <ul style="line-height: 1.8;">
        <li>How to formulate and solve least squares problems.</li>
        <li>The use of regularization to prevent overfitting.</li>
        <li>How to formulate and solve robust regression problems.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>1. Least Squares Problems</h3>
      <p>The basic least squares problem minimizes $\|Ax-b\|_2^2$ to find the best fit in the column space of $A$. The solution is $x^* = (A^\top A)^{-1}A^\top b$ when $A$ has full column rank, and can be computed efficiently via QR factorization or normal equations.</p>

      <!-- Include images as needed -->
      <figure style="margin: 16px 0; text-align: center;">
        <img src="../../static/assets/topics/10-approximation-fitting/least-squares-fit.gif" alt="Animation of a least squares fit" style="max-width: 500px; height: auto; border-radius: 8px;" />
        <figcaption style="font-size: 13px; color: var(--muted); margin-top: 8px;">
          An animation showing a line being fit to a set of data points using the method of least squares. Source: USGS.
        </figcaption>
      </figure>

      <h3>2. Weighted Least Squares</h3>
      <p>When measurements have different reliabilities, weighted least squares minimizes $\sum_i w_i(a_i^\top x - b_i)^2$. This reduces to $\|W^{1/2}(Ax-b)\|_2^2$ where $W$ is a diagonal weight matrix, allowing different confidence levels for each data point.</p>

      <h3>3. Regularization Techniques</h3>
      <p>Regularization prevents overfitting by adding a penalty term: $\|Ax-b\|_2^2 + \lambda R(x)$. Common choices include Ridge ($R(x) = \|x\|_2^2$), LASSO ($R(x) = \|x\|_1$), and Elastic Net (combination of both), each encouraging different solution properties.</p>

      <h3>4. Robust Regression</h3>
      <p>Robust methods reduce sensitivity to outliers by using norms other than $\ell_2$. The $\ell_1$ norm $\|Ax-b\|_1$ provides robustness, while Huber loss combines quadratic behavior for small residuals with linear growth for large ones.</p>

      <h3>5. Sparse Signal Recovery</h3>
      <p>Compressed sensing seeks sparse solutions via $\ell_1$ minimization: $\min \|x\|_1$ subject to $Ax=b$. Under certain conditions on $A$ (RIP property), this recovers the sparsest solution, enabling signal reconstruction from fewer measurements than traditionally required.</p>

      <h3>6. Total Variation Denoising</h3>
      <p>For piecewise-constant signals or images, total variation regularization $\sum_i |x_{i+1}-x_i|$ preserves edges while removing noise. This formulation is widely used in image processing and can be solved efficiently as a convex optimization problem.</p>

      <h3>7. Basis Pursuit and Variants</h3>
      <p>Basis pursuit solves $\min \|x\|_1$ subject to $Ax=b$ exactly, while basis pursuit denoising relaxes to $\min \|x\|_1 + \frac{1}{2\mu}\|Ax-b\|_2^2$. These formulations connect to compressed sensing and dictionary learning applications.</p>

      <h3>8. Matrix Approximation</h3>
      <p>Low-rank matrix approximation seeks $X$ minimizing $\|A-X\|_F$ subject to $\text{rank}(X) \leq k$. The optimal solution is given by truncated SVD, and nuclear norm relaxation $\|X\|_*$ provides a convex surrogate for rank minimization in matrix completion problems.</p>

      <h3>9. Polynomial and Function Fitting</h3>
      <p>Fitting polynomials or other basis functions to data leads to least squares with design matrix $A_{ij} = \phi_j(t_i)$. Choosing appropriate basis functions and regularization prevents oscillations and overfitting in interpolation tasks.</p>
    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively. Try tweaking parameters to build intuition.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Least Squares Regularization</h3>
        <p>Explore the effect of L1 and L2 regularization on the solution of a least squares problem.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Robust Regression</h3>
        <p>Compare the solution of a least squares problem with the solution of a robust regression problem in the presence of outliers.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Sparse Recovery Demo</h3>
        <p>Demonstrates how L1 regularization can recover a sparse signal from a limited number of measurements.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 4 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Matrix Completion Visualizer</h3>
        <p>Users can hide entries of a low-rank matrix (e.g., an image) and watch an algorithm recover the missing values.</p>
        <div id="widget-4" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 5 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Regularization Theory Tool</h3>
        <p>Visualizes the geometry of different regularizers (L1, L2, Elastic Net).</p>
        <div id="widget-5" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 6 — Approximation and Fitting</li>
        <li><strong>Course slides:</strong> [Link if available]</li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <h3>Example 6.1: Ridge Regression with Explicit Solution</h3>
      <p><strong>Problem:</strong> Derive the closed-form solution for Ridge regression $\min_x \|Ax-b\|_2^2 + \lambda\|x\|_2^2$ and explain how $\lambda$ affects the condition number.</p>
      <p><em>[Detailed worked solution to be added: Shows $x^* = (A^\top A + \lambda I)^{-1}A^\top b$ and condition number improvement from $\kappa(A)^2$ to $(\kappa(A)^2 + \lambda)/\lambda$]</em></p>

      <h3>Example 6.2: LASSO Path and Sparsity Pattern</h3>
      <p><strong>Problem:</strong> For a simple 2D problem with $A = I$, sketch the LASSO solution path as $\lambda$ varies and identify where variables become zero.</p>
      <p><em>[Detailed worked solution to be added: Geometric analysis showing diamond-shaped $\ell_1$ ball intersecting level sets of $\|Ax-b\|_2^2$, deriving soft-thresholding solution]</em></p>

      <h3>Example 6.3: Weighted Least Squares for Heteroscedastic Data</h3>
      <p><strong>Problem:</strong> Given measurements with variance $\sigma_i^2 \propto i$, formulate the weighted least squares problem and compare to ordinary least squares.</p>
      <p><em>[Detailed worked solution to be added: Constructs weight matrix $W = \text{diag}(1/i)$, shows improved fit at high-precision points]</em></p>

      <h3>Example 6.4: Robust Regression with Huber Loss</h3>
      <p><strong>Problem:</strong> Formulate the Huber regression problem for data with 20% outliers and express it as a convex optimization problem.</p>
      <p><em>[Detailed worked solution to be added: Defines Huber function, reformulates as QP or SOCP, compares to $\ell_1$ and $\ell_2$ solutions]</em></p>

      <h3>Example 6.5: Sparse Signal Recovery via Basis Pursuit</h3>
      <p><strong>Problem:</strong> A 100-dimensional signal with 10 nonzero entries is measured via 30 random Gaussian measurements. Set up the basis pursuit problem to recover the signal.</p>
      <p><em>[Detailed worked solution to be added: Formulates $\min \|x\|_1$ s.t. $Ax=b$, discusses RIP conditions, shows recovery with high probability]</em></p>

      <h3>Example 6.6: Total Variation Denoising of Piecewise Constant Signal</h3>
      <p><strong>Problem:</strong> Apply TV denoising to a step function corrupted by Gaussian noise, formulating as an LP.</p>
      <p><em>[Detailed worked solution to be added: Constructs difference matrix $D$, minimizes $\|Dx\|_1 + \frac{1}{2\mu}\|x-y\|_2^2$, shows edge preservation]</em></p>

      <h3>Example 6.7: Low-Rank Matrix Approximation via SVD</h3>
      <p><strong>Problem:</strong> Find the best rank-2 approximation to a $4 \times 4$ matrix in Frobenius norm.</p>
      <p><em>[Detailed worked solution to be added: Computes full SVD, truncates to 2 singular values, verifies optimality via Eckart-Young theorem]</em></p>

      <h3>Example 6.8: Polynomial Fitting with Regularization</h3>
      <p><strong>Problem:</strong> Fit a degree-10 polynomial to 15 noisy data points, comparing unregularized fit to Ridge-regularized version.</p>
      <p><em>[Detailed worked solution to be added: Constructs Vandermonde matrix, shows overfitting without regularization, demonstrates smoothing effect of Ridge penalty]</em></p>
    </section>

    <!-- Exercises (optional) -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Exercises</h2>
      <p>Try these problems to deepen your understanding. Solutions are provided at the end of the course.</p>
      <ol style="line-height: 2;">
        <li>Show that the solution to the ridge regression problem $\min_x \|Ax-b\|_2^2 + \lambda\|x\|_2^2$ is unique for any $\lambda > 0$, even when $A$ is rank-deficient.</li>
        <li>Prove that the LASSO estimator $\hat{x} = \arg\min_x \|Ax-b\|_2^2 + \lambda\|x\|_1$ is equivalent to the constrained problem $\min_x \|Ax-b\|_2^2$ subject to $\|x\|_1 \leq t$ for some appropriate $t$ (Lagrange duality).</li>
        <li>For weighted least squares with diagonal weight matrix $W$, show that the solution is $x^* = (A^\top W A)^{-1}A^\top W b$. How does this relate to maximum likelihood estimation under heteroscedastic Gaussian noise?</li>
        <li>Derive the subdifferential of the $\ell_1$ norm $f(x) = \|x\|_1$ and use it to characterize the optimality conditions for basis pursuit denoising.</li>
        <li>Show that minimizing $\|Ax-b\|_1$ (robust regression) can be reformulated as a linear program. Write out the LP explicitly in standard form.</li>
        <li>For total variation denoising $\min_x \sum_{i=1}^{n-1}|x_{i+1}-x_i| + \frac{1}{2\mu}\|x-y\|_2^2$, explain why this preserves edges better than Ridge regularization $\|Dx\|_2^2$.</li>
        <li>Prove the Eckart-Young theorem: the best rank-$k$ approximation to matrix $A$ in Frobenius norm is given by truncating the SVD to the $k$ largest singular values.</li>
        <li>Show that the nuclear norm $\|X\|_* = \sum_i \sigma_i(X)$ is the convex envelope of the rank function over the unit ball. Why does this make it a good convex relaxation for rank minimization?</li>
        <li>For polynomial fitting with degree $d$, explain the bias-variance tradeoff. How does Ridge regularization affect this tradeoff?</li>
        <li>Implement coordinate descent for LASSO and verify the soft-thresholding update: $x_j \leftarrow \mathcal{S}_{\lambda/L}(x_j - \frac{1}{L}\nabla_j f(x))$ where $\mathcal{S}_\tau(z) = \text{sign}(z)\max(|z|-\tau, 0)$.</li>
      </ol>
    </section>
  </main></div>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></a> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initLeastSquaresPlayground } from './widgets/js/least-squares-regularization.js';
    initLeastSquaresPlayground('widget-1');
  </script>
  <script type="module">
    import { initRobustRegression } from './widgets/js/robust-regression.js';
    initRobustRegression('widget-2');
  </script>
  <script type="module">
    import { initSparseRecoveryDemo } from './widgets/js/sparse-recovery.js';
    initSparseRecoveryDemo('widget-3');
  </script>
  <script type="module">
    import { initMatrixCompletionVisualizer } from './widgets/js/matrix-completion.js';
    initMatrixCompletionVisualizer('widget-4');
  </script>
  <script type="module">
    import { initRegularizationTheoryTool } from './widgets/js/regularization-theory.js';
    initRegularizationTheoryTool('widget-5');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
<script src="../../static/js/ui.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
