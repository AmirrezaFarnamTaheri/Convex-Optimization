<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>13. Algorithms I: Unconstrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="section-card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">13. Algorithms I: Unconstrained Minimization</h1>
      <div class="meta">
        Date: 2025-12-16 · Duration: 90 min · Tags: algorithms, unconstrained
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> Gradient descent, Newton's method, step size selection, convergence rates, self-concordance.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05</a></p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you should understand:</p>
      <ul style="line-height: 1.8;">
        <li>The convergence properties of gradient descent for strongly convex and merely convex functions.</li>
        <li>How to implement backtracking line search and why it is preferred over exact search.</li>
        <li>The derivation and quadratic convergence of Newton's method.</li>
        <li>The concept of self-concordance and its role in Newton's method analysis.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>0. Setup and Assumptions</h3>
      <p>We consider the unconstrained minimization problem $\min_x f(x)$ where $f: \mathrm{dom}\,f \to \mathbb{R}$ is convex and twice continuously differentiable ($C^2$) on an open domain. We assume the problem is solvable with optimal value $p^\star = f(x^\star)$.</p>
      <p>The analysis typically relies on the initial sublevel set $S = \{x \in \mathrm{dom}\,f \mid f(x) \le f(x^{(0)})\}$. Descent methods ensure $x^{(k)} \in S$ for all $k$. We often assume $S$ is closed (implies compactness if bounded).</p>

      <h3>1. Strong Convexity and Conditioning</h3>
      <p>We assume strong convexity on $S$: there exists $m > 0$ such that $\nabla^2 f(x) \succeq mI$ for all $x \in S$. This implies a quadratic lower bound on growth:
      $$f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|_2^2$$
      Minimizing the RHS gives a bound on suboptimality in terms of the gradient norm:
      $$f(x) - p^\star \le \frac{1}{2m}\|\nabla f(x)\|_2^2$$
      This justifies using $\|\nabla f(x)\| \le \epsilon$ as a stopping criterion.
      </p>
      <p>We also assume an upper bound on curvature (smoothness): $\nabla^2 f(x) \preceq MI$ for all $x \in S$. The ratio $\kappa = M/m \ge 1$ is the <strong>condition number</strong> of the sublevel sets. A large $\kappa$ implies the level sets are long and skinny ("valleys"), which causes zigzagging in gradient descent.</p>

      <h3>2. Descent Methods and Line Search</h3>
      <p>A generic descent method iterates $x^{(k+1)} = x^{(k)} + t^{(k)}\Delta x^{(k)}$.</p>
      <ul>
        <li><strong>Exact Line Search:</strong> $t = \text{argmin}_{s \ge 0} f(x + s\Delta x)$.</li>
        <li><strong>Backtracking (Armijo) Line Search:</strong> Start with $t=1$ and reduce by factor $\beta \in (0,1)$ until sufficient decrease is met:
        $$f(x+t\Delta x) \le f(x) + \alpha t \nabla f(x)^T \Delta x$$
        where $\alpha \in (0, 0.5)$. This guarantees termination and sufficient progress.</li>
      </ul>

      <h3>3. Gradient Descent Analysis</h3>
      <p>For GD ($\Delta x = -\nabla f(x)$), smoothness $\nabla^2 f \preceq MI$ implies a quadratic upper bound:
      $$f(y) \le f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}\|y-x\|_2^2$$
      This guarantees a decrease of at least $\frac{1}{2M}\|\nabla f(x)\|^2$ per step. Combined with strong convexity, we get <strong>linear convergence</strong>:
      $$f(x^{(k)}) - p^\star \le c^k (f(x^{(0)}) - p^\star), \quad c = 1 - \frac{m}{M}$$
      The convergence rate depends directly on the condition number $\kappa = M/m$. For ill-conditioned problems (large $\kappa$), $c \approx 1$, and convergence is very slow.</p>

      <h3>4. Steepest Descent & Geometry</h3>
      <p>"Steepest" depends on how you measure length. The <strong>normalized steepest descent direction</strong> for a norm $\|\cdot\|$ is:
      $$\Delta x_{\mathrm{nsd}} = \text{argmin}_{\|v\| \le 1} \nabla f(x)^T v$$
      Using the <strong>dual norm</strong> $\|g\|_* = \sup_{\|v\| \le 1} g^T v$, the best rate of decrease is $-\|\nabla f(x)\|_*$.
      Different norms yield different algorithms:</p>
      <ul>
        <li><strong>Euclidean Norm ($\ell_2$):</strong> $\Delta x_{sd} = -\nabla f(x)$. (Gradient Descent)</li>
        <li><strong>Quadratic Norm ($\|v\|_P = \sqrt{v^T P v}$):</strong> $\Delta x_{sd} = -P^{-1}\nabla f(x)$. (Preconditioned GD). This corresponds to GD in the coordinate system $\bar{x} = P^{1/2}x$.</li>
        <li><strong>$\ell_1$ Norm:</strong> $\Delta x_{sd} = -(\partial f/\partial x_i) e_i$. (Coordinate Descent). Updates the coordinate with the largest partial derivative.</li>
      </ul>

      <h3>5. Newton's Method</h3>
      <p>Newton's step $\Delta x_{nt} = -\nabla^2 f(x)^{-1}\nabla f(x)$ has two powerful interpretations:</p>
      <ol>
        <li><strong>Steepest Descent in Hessian Norm:</strong> It is steepest descent using the local geometry defined by the Hessian $\|\cdot\|_{\nabla^2 f(x)}$. This effectively "preconditions" the problem to have condition number 1 locally.</li>
        <li><strong>Quadratic Model Minimization:</strong> It minimizes the second-order Taylor approximation:
        $$\hat{f}(x+v) = f(x) + \nabla f(x)^T v + \frac{1}{2}v^T \nabla^2 f(x) v$$</li>
      </ol>
      <p>The <strong>Newton Decrement</strong> is $\lambda(x) = (\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x))^{1/2}$. It represents the approximate suboptimality: $f(x) - \inf_y \hat{f}(y) = \lambda(x)^2/2$. It is affine invariant.</p>

      <h3>6. Newton Convergence Phases</h3>
      <p>Newton's method (with backtracking) typically exhibits two phases:</p>
      <ul>
        <li><strong>Damped Phase:</strong> When far from optimum, backtracking selects $t < 1$. The algorithm behaves like steepest descent with guaranteed decrease.</li>
        <li><strong>Quadratically Convergent Phase:</strong> Once sufficiently close (where the Hessian is stable, captured by self-concordance or Lipschitz assumptions), backtracking accepts $t=1$. The error squres at each step:
        $$\|\nabla f(x^{(k+1)})\| \le C \|\nabla f(x^{(k)})\|^2$$
        This leads to extremely high precision in very few steps (e.g., doubling digits of accuracy).</li>
      </ul>

      <h3>7. Self-Concordance</h3>
      <p>A function is self-concordant if $|f'''(x)| \le 2 f''(x)^{3/2}$. For such functions, Newton's method can be analyzed without unknown constants like $L, \mu$. The analysis guarantees convergence based purely on the Newton decrement $\lambda(x)$. Standard barriers like $-\sum \log x_i$ are self-concordant.</p>

      <h3>8. Quasi-Newton Methods</h3>
      <p>Methods like BFGS approximate the Hessian (or its inverse) using gradient differences $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$ and steps $s_k = x_{k+1} - x_k$, satisfying the secant equation $B_{k+1} s_k = y_k$. They achieve superlinear convergence without $O(n^3)$ Hessian inversion.</p>
    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Gradient Descent Visualizer</h3>
        <p>Visualize the trajectory of gradient descent for a 2D quadratic function.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">GD vs. Newton Race</h3>
        <p>Compare the convergence of gradient descent and Newton's method.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 (if exists) -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Convergence Rate Comparison</h3>
        <p>Plots the convergence rates of different first-order methods.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 9 — Unconstrained Minimization</li>
        <li><strong>Course slides:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">PDF</a></li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <div class="example-problem">
        <h3>Example 13.1: Gradient Descent on Quadratic</h3>
        <p><strong>Problem:</strong> Apply GD to $f(x) = \frac{1}{2}(x_1^2 + \gamma x_2^2)$ with $\gamma = 10$. Start at $(10, 1)$. Estimate steps to reach $\|x\| \le 10^{-3}$.</p>
        <p><strong>Solution:</strong></p>
        <p>Condition number $\kappa = 10$. Optimal step rate $t = 2/(L+\mu) = 2/(10+1) \approx 0.18$.
        Convergence factor $\rho = (\kappa-1)/(\kappa+1) = 9/11 \approx 0.818$.
        Error decays as $(0.818)^k$.
        Need $(0.818)^k \cdot \sqrt{101} \le 10^{-3}$.
        $k \log(0.818) \le \log(10^{-4})$. $k \approx 45$.
        With exact line search, zigzagging occurs.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.2: Newton's Method for Logistic Regression</h3>
        <p><strong>Problem:</strong> Formulate Newton step for logistic regression.</p>
        <p><strong>Solution:</strong></p>
        <p>$f(w) = \sum \log(1 + e^{z_i}) - y_i z_i$ where $z_i = w^\top x_i$.
        Gradient $\nabla f = X^\top (p - y)$ where $p_i = \sigma(z_i)$.
        Hessian $H = X^\top D X$ where $D_{ii} = p_i(1-p_i)$.
        Newton step $\Delta w = -H^{-1} g = -(X^\top D X)^{-1} X^\top (p-y)$.
        This is iteratively reweighted least squares (IRLS).</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.3: Backtracking Line Search</h3>
        <p><strong>Problem:</strong> Describe the backtracking condition for $f(x) = x^2$ with $\Delta x = -1$ at $x=2$.</p>
        <p><strong>Solution:</strong></p>
        <p>Condition: $f(x+t\Delta x) \le f(x) + \alpha t f'(x)\Delta x$.
        $f(2)=4, f'(2)=4, \Delta x=-1$.
        $(2-t)^2 \le 4 + \alpha t (4)(-1) = 4 - 4\alpha t$.
        $4 - 4t + t^2 \le 4 - 4\alpha t$.
        $t^2 \le 4t(1-\alpha) \implies t \le 4(1-\alpha)$.
        For small $t$, this always holds. We start with $t=1$. If $1 > 4(1-\alpha)$, we reduce $t$.
        E.g., if $\alpha=0.25$, limit is $3$. $t=1$ is accepted immediately.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.4: Conjugate Gradient Direction</h3>
        <p><strong>Problem:</strong> For $f(x) = \frac{1}{2}x^\top A x - b^\top x$, how is the search direction updated?</p>
        <p><strong>Solution:</strong></p>
        <p>$d_{k+1} = -g_{k+1} + \beta_k d_k$.
        For quadratic CG, $\beta_k = \frac{g_{k+1}^\top g_{k+1}}{g_k^\top g_k}$ (Fletcher-Reeves).
        Ensures $d_{k+1}^\top A d_k = 0$ ($A$-conjugacy).
        This allows exact minimization in $n$ steps.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.5: BFGS Update Rank</h3>
        <p><strong>Problem:</strong> Show that the BFGS update is a rank-2 update.</p>
        <p><strong>Solution:</strong></p>
        <p>Update: $B_{k+1} = B_k + \frac{y y^\top}{y^\top s} - \frac{B s s^\top B}{s^\top B s}$.
        This adds two rank-1 matrices.
        It preserves positive definiteness if $y^\top s > 0$ (curvature condition).
        Avoids $O(n^3)$ cost of inverting Hessian.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.6: Convergence Rate Comparison</h3>
        <p><strong>Problem:</strong> Compare GD and Newton on $f(x) = x^4$.</p>
        <p><strong>Solution:</strong></p>
        <p>GD: $x_{k+1} = x_k - t 4 x_k^3$. For convergence, need small $t$. Rate is sublinear (slow).
        Newton: $\Delta x = -f'/f'' = -4x^3 / 12x^2 = -x/3$.
        $x_{k+1} = x_k - x_k/3 = (2/3)x_k$.
        Linear convergence with rate $2/3$.
        Note: $x^4$ is strictly convex but not strongly convex at 0 ($f''(0)=0$). Newton is not quadratic here!</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.7: Nesterov Acceleration</h3>
        <p><strong>Problem:</strong> Write the update equations for Nesterov's accelerated gradient.</p>
        <p><strong>Solution:</strong></p>
        <p>$x_k$: current position, $y_k$: momentum variable.
        1. $x_{k+1} = y_k - t \nabla f(y_k)$
        2. $y_{k+1} = x_{k+1} + \frac{k-1}{k+2} (x_{k+1} - x_k)$
        Step 1 is standard GD step from the lookahead position $y_k$.
        Step 2 adds momentum.
        Converges as $1/k^2$ for smooth convex functions.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.8: Newton Decrement Property</h3>
        <p><strong>Problem:</strong> Show $\lambda(x)^2 = \nabla f(x)^\top \Delta x_{nt}$.</p>
        <p><strong>Solution:</strong></p>
        <p>$\Delta x_{nt} = -H^{-1} g$.
        $\lambda(x)^2 = g^\top H^{-1} g$.
        Inner product: $g^\top \Delta x_{nt} = g^\top (-H^{-1} g) = -g^\top H^{-1} g = -\lambda(x)^2$.
        So $\lambda^2 = -g^\top \Delta x_{nt}$ (directional derivative in Newton direction is $-\lambda^2$).</p>
      </div>
    </section>

    <!-- Exercises -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Exercises</h2>
      <ol style="line-height: 2;">
        <li>
          <strong>Gradient Descent Rate:</strong><br>
          Prove that for $f(x) = \frac{1}{2}x^\top Q x$, GD with optimal constant step size converges as $\|x_{k+1}\| \le \frac{\kappa-1}{\kappa+1}\|x_k\|$.
          <br><em>Solution:</em>
          $x_{k+1} = (I - tQ)x_k$. Minimize spectral radius $\rho(I - tQ) = \max_i |1 - t\lambda_i|$.
          Optimal $t = 2/(\lambda_{\min} + \lambda_{\max})$.
          $\rho = \frac{\lambda_{\max} - \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}} = \frac{\kappa-1}{\kappa+1}$.
        </li>
        <li>
          <strong>Affine Invariance of Newton's Method:</strong><br>
          Let $y = Tx$. Define $g(y) = f(Ty) = f(x)$. Show that the Newton step for $g$ at $y$ corresponds to the Newton step for $f$ at $x$ transformed by $T^{-1}$.
          <br><em>Solution:</em>
          $\nabla g(y) = T^\top \nabla f(x)$. $\nabla^2 g(y) = T^\top \nabla^2 f(x) T$.
          $\Delta y = -[T^\top H T]^{-1} T^\top g = -T^{-1} H^{-1} T^{-\top} T^\top g = -T^{-1} H^{-1} g = T^{-1} \Delta x$.
          So updates are consistent: $y + \Delta y = T(x + \Delta x)$.
        </li>
        <li>
          <strong>Backtracking Termination:</strong><br>
          Prove that backtracking line search terminates if $f$ has Lipschitz gradient.
          <br><em>Solution:</em>
          Lipschitz gradient implies $f(y) \le f(x) + \nabla f(x)^\top(y-x) + \frac{L}{2}\|y-x\|^2$.
          With $y = x - t\nabla f$, $f(y) \le f(x) - t\|\nabla f\|^2 + \frac{Lt^2}{2}\|\nabla f\|^2$.
          We want $f(y) \le f(x) - \alpha t \|\nabla f\|^2$.
          Satisfied if $-t + Lt^2/2 \le -\alpha t \implies Lt/2 \le 1-\alpha \implies t \le \frac{2(1-\alpha)}{L}$.
          Since we reduce $t$ by factor $\beta$, we eventually drop below this threshold.
        </li>
        <li>
          <strong>Conjugate Gradient Finite Termination:</strong><br>
          Explain why CG terminates in $n$ steps for $n$-dimensional quadratic problems.
          <br><em>Solution:</em>
          CG generates search directions $d_0, \dots, d_{n-1}$ that are $Q$-conjugate ($d_i^\top Q d_j = 0$).
          These form a basis for $\mathbb{R}^n$.
          Minimizing a convex quadratic sequentially along conjugate directions is equivalent to minimizing over their span.
          After $n$ steps, we minimize over full $\mathbb{R}^n$.
        </li>
        <li>
          <strong>BFGS Positive Definiteness:</strong><br>
          Show that if $B_k \succ 0$ and $y_k^\top s_k > 0$, then $B_{k+1} \succ 0$.
          <br><em>Solution:</em>
          $B_{k+1} = B_k + \frac{y y^\top}{y^\top s} - \frac{B s s^\top B}{s^\top B s}$.
          Evaluate quadratic form $z^\top B_{k+1} z$.
          Let $a = y^\top s > 0, b = s^\top B s > 0$.
          Use Cauchy-Schwarz type inequality or determinant formula.
          Intuitively, we add a rank-1 PSD term and subtract a rank-1 term, but in a way that preserves PD nature if curvature condition holds.
          Formal proof uses product form of inverse update (DFP) or determinant update.
        </li>
      </ol>
    </section>
  </main></div>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></a> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initGradientDescentVisualizer } from './widgets/js/gradient-descent-visualizer.js';
    initGradientDescentVisualizer('widget-1');
  </script>
  <script type="module">
    import { initGDvsNewton } from './widgets/js/gd-vs-newton.js';
    initGDvsNewton('widget-2');
  </script>
  <script type="module">
    import { initConvergenceRate } from './widgets/js/convergence-rate.js';
    initConvergenceRate('widget-3');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
<script src="../../static/js/ui.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
