
**Image 1: Determinant as Volume Scaling**
> **Description:** A 2D transformation visualization.
> * **Left:** A unit square with area 1, spanned by standard basis vectors $e_1, e_2$.
> * **Right:** The image of the square under a linear transformation $A$. It is a parallelogram spanned by $Ae_1, Ae_2$.
> * **Label:** The area of the parallelogram is explicitly labeled $|\det(A)|$.
> * **Sign:** A circular arrow indicates orientation. If the orientation flips (e.g., clockwise to counter-clockwise), the determinant is negative.

**Image 2: Operator Norm Geometry**
> **Description:** A visualization of $\|A\|_2$ as the maximum stretch.
> * **Input Space:** A unit circle is shown in $\mathbb{R}^2$.
> * **Output Space:** The image of the circle under $A$ is an ellipse.
> * **Max Stretch:** A vector $v$ on the input circle maps to a vector $Av$ on the ellipse. The longest semi-axis of the ellipse is highlighted in red.
> * **Label:** The length of this longest semi-axis is labeled $\|A\|_2 = \sigma_{\max}$.
> * **Comparison:** Other vectors on the ellipse are shorter, visually confirming the definition $\max_{\|x\|=1} \|Ax\|$.


**Image 3: The SVD Transformation Sequence**
> **Description:** A step-by-step breakdown of $A = U \Sigma V^T$.
> * **Step 1 ($V^T$):** A unit circle with an embedded "F" shape (to show orientation) is rotated. The shape remains a circle.
> * **Step 2 ($\Sigma$):** The rotated circle is stretched along the x and y axes by factors $\sigma_1$ and $\sigma_2$, becoming an axis-aligned ellipse. The "F" is stretched.
> * **Step 3 ($U$):** The ellipse is rotated to its final orientation.
> * **Result:** The final shape represents the image of the unit circle under $A$. The axes of the final ellipse are the left singular vectors $u_1, u_2$ scaled by $\sigma_1, \sigma_2$.

**Image 4: SVD vs. Eigendecomposition**
> **Description:** A comparison for a non-symmetric matrix.
> * **Left (Eigen):** Shows eigenvectors $v$ where $Av$ is parallel to $v$ (just scaled). The vectors might not be orthogonal.
> * **Right (SVD):** Shows the orthogonal input basis vectors $v_i$ mapping to the orthogonal output basis vectors $u_i$. $Av_i = \sigma_i u_i$.
> * **Key Difference:** Highlights that SVD maps one orthogonal frame to another, while eigendecomposition looks for invariant directions.



**Image 5: The Zig-Zag of Ill-Conditioning**
> **Description:** A comparison of optimization paths on contour maps.
> * **Left (Well-Conditioned, $\kappa \approx 1$):** Circular contours. The gradient descent path (red arrows) moves straight toward the center minimum. Fast convergence.
> * **Right (Ill-Conditioned, $\kappa \gg 1$):** Very elongated elliptical contours. The gradient is perpendicular to the contours, not pointing at the minimum. The path (red arrows) zig-zags back and forth across the "valley," taking many steps to reach the center.
> * **Connection:** Connects the ratio $\sigma_{\max}/\sigma_{\min}$ directly to algorithm speed.


**Image 6: Maximizing Inner Product (L1 vs L-infinity)**
> **Description:** A 2D visualization of the duality between $\ell_1$ and $\ell_\infty$.
> * **Center:** The unit ball of the $\ell_1$ norm (a blue diamond with corners at $(\pm 1, 0)$ and $(0, \pm 1)$).
> * **Vector y:** A vector $y = (0.5, 0.8)$ is drawn.
> * **Optimization:** To evaluate $\|y\|_\infty$, we maximize $x^T y$ where $x$ is in the diamond.
> * **Visual Result:** The level sets of the linear function $x^T y = c$ (straight lines perpendicular to $y$) move across the diamond. The maximum value occurs exactly at the top vertex $(0,1)$, illustrating that the dual of the $\ell_1$ ball "picks out" the largest component index.


**Image 7: Least Squares as Orthogonal Projection**
> **Description:** A 3D geometry visualization.
> * **Subspace:** A flat 2D plane passing through the origin represents the Column Space $\mathcal{R}(A)$.
> * **Target:** A vector $b$ points away from the plane into 3D space (indicating $Ax=b$ has no solution).
> * **Projection:** A vector $p = Ax^*$ lies on the plane directly "underneath" $b$.
> * **Residual:** A dashed red line connects $p$ to $b$. This is the residual $r = b - Ax^*$.
> * **Orthogonality:** A right-angle symbol marks the angle between $r$ and the plane, visually proving the Normal Equation principle $A^T r = 0$.



**Image 8: Projection onto an Affine Set**
> **Description:** A visualization distinguishing subspace vs. affine projection.
> * **Affine Set:** A floating 2D plane that *does not* pass through the origin (e.g., $x_1 + x_2 + x_3 = 1$).
> * **Input:** A point $y$ in space.
> * **Step 1:** A "Particular Solution" vector $x_0$ points from the origin to the plane.
> * **Step 2:** The projection is shown as dropping a perpendicular line from $y$ to the floating plane.
> * **Calculation:** Ghosted lines show the vector $y - x_0$ being projected onto the parallel linear subspace (Nullspace), then shifted back by $x_0$.


**Image 9: Well-Conditioned vs. Ill-Conditioned Geometry**

**Title:** A comparative two-panel diagram titled **"Visualizing Condition Number ($\kappa$): Error Amplification in Linear Systems ($A\mathbf{x}=\mathbf{b}$)"**.

**Overall Layout:** Two side-by-side Cartesian coordinate plots separated by a central vertical divider. The background is clean white. The aesthetic is modern, clean, and professional, using semi-transparent colors for regions.

#### **Left Panel: "Well-Conditioned Case ($\kappa(A) \approx 1$)"**

* **Subtitle:** "Small input error leads to small output error. Stable."
* **Left Sub-plot (Input Space):**
    * **Axes:** Standard axes labeled at the origin. The plot area is labeled **"Input Space (contains $\mathbf{b}$)"** at the bottom.
    * **Content:** A central point labeled $\mathbf{b}$. Surrounding $\mathbf{b}$ is a small, perfect circle filled with semi-transparent blue.
    * **Labels:** An arrow points to the blue circle boundary labeled **"Input Uncertainty ($\|\delta \mathbf{b}\| \leq \epsilon$)"**.
* **Mapping Arrow:** A prominent, thick, arcing grey arrow points from the Input Space plot to the Solution Space plot. Over this arrow is the text label: **"Mapping: $\mathbf{x} = A^{-1}\mathbf{b}$"**.
* **Right Sub-plot (Solution Space):**
    * **Axes:** Standard axes. The plot area is labeled **"Solution Space (contains $\mathbf{x}$)"** at the bottom.
    * **Content:** A central point labeled $\mathbf{x}$. Surrounding $\mathbf{x}$ is a slightly elliptical region (almost circular) filled with semi-transparent orange. It is roughly the same relative size as the blue input circle.
    * **Labels:** An arrow points to the orange boundary labeled **"Resulting Output Error ($\delta \mathbf{x}$)"**. A small double-headed arrow inside the ellipse shows the major axis is only slightly longer than the minor axis.

#### **Right Panel: "Ill-Conditioned Case ($\kappa(A) \gg 1$)"**

* **Subtitle:** "Small input error leads to HUGE output error. Unstable."
* **Left Sub-plot (Input Space):**
    * **Axes & Label:** Same as the left panel: **"Input Space (contains $\mathbf{b}$)"**.
    * **Content:** **Crucial:** An *identical* small, semi-transparent blue circle around point $\mathbf{b}$ as seen in the Well-Conditioned panel.
    * **Labels:** Same label: **"Input Uncertainty ($\|\delta \mathbf{b}\| \leq \epsilon$)"**.
* **Mapping Arrow:** A prominent, thick, arcing grey arrow points to the solution space, labeled: **"Mapping: $\mathbf{x} = A^{-1}\mathbf{b}$"**.
* **Right Sub-plot (Solution Space):**
    * **Axes & Label:** Same as the left panel: **"Solution Space (contains $\mathbf{x}$)"**.
    * **Content:** A central point labeled $\mathbf{x}$. Surrounding $\mathbf{x}$ is an extremely elongated, thin ellipse filled with semi-transparent orange. It stretches diagonally across almost the entire plot area.
    * **Labels:** An arrow points to the far tip of the long axis of the ellipse labeled **"Massive Error Amplification"**. Another label points to the ellipse boundary generally: **"Resulting Output Error ($\delta \mathbf{x}$)"**.
    * **Vector Detail:** Inside this long ellipse, a distinct red vector arrow originates from $\mathbf{x}$ and extends along the major axis to the furthest point of the ellipse. This red vector is labeled **"Max $\|\delta \mathbf{x}\|$ direction (related to small singular values)"**.

#### **Bottom Annotation (Spanning both panels):**

At the very bottom center, in a clear mathematical font, include the governing inequality that links the diagram to the math:

**"The Governing Inequality: $\frac{\|\delta \mathbf{x}\|}{\|\mathbf{x}\|} \leq \kappa(A) \cdot \frac{\|\delta \mathbf{b}\|}{\|\mathbf{b}\|}$"**
*(A small note below it reads: "The condition number $\kappa(A)$ is the magnifying glass for relative error.")*


**Image 10: Low-Rank Image Approximation**
> **Description:** A visual demonstration of SVD for data compression.
> * **Left:** A crisp, high-resolution grayscale photograph (Rank 500).
> * **Middle:** The same photo reconstructed using only the top 50 singular values (Rank 50). It looks nearly identical but slightly softer.
> * **Right:** The photo using only the top 5 singular values (Rank 5). It is very blurry and "blocky," showing only the main shapes (light/dark regions) without detail.
> * Text labels show the "Energy Retained" percentage decreasing from left to right.



