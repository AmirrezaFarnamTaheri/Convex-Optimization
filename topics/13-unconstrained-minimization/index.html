<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>09. Algorithms I: Unconstrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="section-card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">09. Algorithms I: Unconstrained Minimization</h1>
      <div class="meta">
        Date: 2025-12-16 · Duration: 90 min · Tags: algorithms, unconstrained
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> Gradient descent, Newton's method, step size selection, convergence rates.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05-06</a></p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you should understand:</p>
      <ul style="line-height: 1.8;">
        <li>The gradient descent algorithm and its convergence properties.</li>
        <li>Newton's method and its convergence properties.</li>
        <li>The trade-offs between gradient descent and Newton's method.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>1. Gradient Descent (GD)</h3>
      <p>The gradient descent algorithm iterates $x_{k+1} = x_k - t_k \nabla f(x_k)$ with step size $t_k$. For strongly convex $f$ with condition number $\kappa$, GD converges linearly at rate $(1 - 1/\kappa)$, making it efficient for well-conditioned problems but slow for ill-conditioned ones.</p>

      <!-- Include images as needed -->
      <figure style="margin: 16px 0; text-align: center;">
        <img src="../../static/assets/topics/13-unconstrained-minimization/gd-trajectory.svg" alt="The trajectory of gradient descent" style="max-width: 100%; height: auto;" />
        <figcaption style="font-size: 13px; color: var(--muted); margin-top: 8px;">
          Gradient descent converges to a local minimum.
        </figcaption>
      </figure>

      <h3>2. Step Size Selection</h3>
      <p>Common step size strategies: (1) exact line search $t_k = \arg\min_t f(x_k - t\nabla f(x_k))$, (2) backtracking with Armijo condition $f(x + t\Delta x) \leq f(x) + \alpha t \nabla f(x)^\top \Delta x$, (3) constant step $t_k = 1/L$ where $L$ is Lipschitz constant of $\nabla f$.</p>

      <h3>3. Newton's Method</h3>
      <p>Newton's method uses second-order information: $x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1}\nabla f(x_k)$. Near the optimum, it exhibits quadratic convergence: $\|x_{k+1} - x^*\| \leq c\|x_k - x^*\|^2$, converging much faster than GD but requiring Hessian computation and inversion.</p>

      <h3>4. Quasi-Newton Methods</h3>
      <p>BFGS and limited-memory BFGS (L-BFGS) approximate the Hessian inverse using gradient information only, avoiding expensive Hessian computation. BFGS achieves superlinear convergence while maintaining $O(n^2)$ storage (vs. $O(n)$ for L-BFGS), balancing Newton's speed with GD's low memory footprint.</p>

      <h3>5. Conjugate Gradient Method</h3>
      <p>For quadratic $f(x) = \frac{1}{2}x^\top A x - b^\top x$, conjugate gradient finds the minimum in at most $n$ steps by choosing search directions $A$-orthogonal. For general convex $f$, nonlinear CG uses Polak-Ribière or Fletcher-Reeves formulas to update directions, often outperforming GD without Hessian computation.</p>

      <h3>6. Accelerated Gradient Methods</h3>
      <p>Nesterov's accelerated gradient (NAG) achieves optimal $O(1/k^2)$ convergence for smooth convex functions via momentum: $y_{k+1} = x_k - t\nabla f(x_k)$, $x_{k+1} = y_{k+1} + \frac{k-1}{k+2}(y_{k+1} - y_k)$. This improves on GD's $O(1/k)$ rate without using second-order information.</p>

      <h3>7. Convergence Analysis and Rates</h3>
      <p>Convergence rates classify algorithms: sublinear ($O(1/k)$ for GD on convex), linear ($O(\rho^k)$ for GD on strongly convex, $\rho < 1$), superlinear (BFGS), quadratic (Newton). The choice depends on problem structure, dimension, and computational budget per iteration.</p>

      <h3>8. Self-Concordance and Damped Newton</h3>
      <p>For self-concordant functions, the Newton decrement $\lambda(x) = (\nabla f(x)^\top [\nabla^2 f(x)]^{-1} \nabla f(x))^{1/2}$ provides a stopping criterion and affine-invariant measure of optimality. Damped Newton with backtracking ensures global convergence followed by locally quadratic convergence.</p>

      <h3>9. Preconditioning and Scaling</h3>
      <p>Preconditioning transforms $f(x)$ to $f(P^{-1}x)$ where $P$ approximates $\nabla^2 f$, improving condition number. Diagonal preconditioning (rescaling variables), incomplete Cholesky, and L-BFGS approximations reduce iteration count significantly for ill-conditioned problems.</p>
    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively. Try tweaking parameters to build intuition.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Gradient Descent Visualizer</h3>
        <p>Visualize the trajectory of gradient descent for a 2D quadratic function.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">GD vs. Newton Race</h3>
        <p>Compare the convergence of gradient descent and Newton's method.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 (if exists) -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Convergence Rate Comparison</h3>
        <p>Plots the convergence rates of different first-order methods.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 9 — Unconstrained Minimization</li>
        <li><strong>Course slides:</strong> [Link if available]</li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <h3>Example 9.1: Gradient Descent on Quadratic Function</h3>
      <p><strong>Problem:</strong> Apply GD with exact line search to $f(x) = \frac{1}{2}x^\top \begin{bmatrix} 4 & 0 \\ 0 & 1 \end{bmatrix} x$ starting from $x_0 = [1, 1]^\top$.</p>
      <p><em>[Detailed worked solution to be added: Computes gradient, derives optimal step size, shows zigzagging behavior due to condition number $\kappa=4$]</em></p>

      <h3>Example 9.2: Newton's Method for Logistic Regression</h3>
      <p><strong>Problem:</strong> Implement Newton's method for logistic regression with 3 data points, show quadratic convergence.</p>
      <p><em>[Detailed worked solution to be added: Derives Hessian $\nabla^2 \ell = X^\top D X$ where $D = \text{diag}(p_i(1-p_i))$, shows 3-4 iterations suffice]</em></p>

      <h3>Example 9.3: Backtracking Line Search</h3>
      <p><strong>Problem:</strong> For $f(x) = e^{x_1} + x_1^2 + x_2^2$, implement backtracking with $\alpha=0.3, \beta=0.8$ from $x_0=[2,2]^\top$.</p>
      <p><em>[Detailed worked solution to be added: Shows step size sequence, verifies Armijo condition, compares to exact line search]</em></p>

      <h3>Example 9.4: Conjugate Gradient for Linear System</h3>
      <p><strong>Problem:</strong> Solve $Ax = b$ for $3 \times 3$ SPD matrix using CG, verify convergence in 3 iterations.</p>
      <p><em>[Detailed worked solution to be added: Computes conjugate directions, shows $A$-orthogonality, verifies finite termination]</em></p>

      <h3>Example 9.5: BFGS Update</h3>
      <p><strong>Problem:</strong> Given $x_k, x_{k+1}, \nabla f(x_k), \nabla f(x_{k+1})$, compute the BFGS approximation $B_{k+1}$ to the Hessian.</p>
      <p><em>[Detailed worked solution to be added: Applies BFGS formula with $s_k = x_{k+1}-x_k$, $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$]</em></p>

      <h3>Example 9.6: Convergence Rate Comparison</h3>
      <p><strong>Problem:</strong> For $f(x) = \frac{1}{2}\|Ax-b\|_2^2$ with $\kappa(A)=100$, compare iterations required for GD vs. Newton to achieve $\|x_k - x^*\| \leq 10^{-6}$.</p>
      <p><em>[Detailed worked solution to be added: Estimates GD iterations $\approx 100\log(10^6)$, Newton $\approx 5-6$ iterations]</em></p>

      <h3>Example 9.7: Nesterov Acceleration</h3>
      <p><strong>Problem:</strong> Implement accelerated GD for $f(x) = \frac{1}{2}x^\top A x - b^\top x$, compare convergence to standard GD.</p>
      <p><em>[Detailed worked solution to be added: Shows momentum term, verifies $O(1/k^2)$ vs. $O(1/k)$ convergence empirically]</em></p>

      <h3>Example 9.8: Newton Decrement and Stopping Criterion</h3>
      <p><strong>Problem:</strong> For self-concordant $f$, compute $\lambda(x) = (\nabla f^\top [\nabla^2 f]^{-1} \nabla f)^{1/2}$ and show $\lambda(x) \leq \epsilon$ implies near-optimality.</p>
      <p><em>[Detailed worked solution to be added: Derives bound $f(x) - f(x^*) \leq \lambda^2/2$, uses as stopping criterion]</em></p>
    </section>

    <!-- Exercises (optional) -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Exercises</h2>
      <p>Try these problems to deepen your understanding. Solutions are provided at the end of the course.</p>
      <ol style="line-height: 2;">
        <li>Prove that for strongly convex $f$ with strong convexity constant $m$ and Lipschitz constant $L$, gradient descent with step size $t = 2/(m+L)$ converges at rate $(1 - m/L)^k = (1 - 1/\kappa)^k$.</li>
        <li>Show that Newton's method is affine-invariant: if $x_{k+1}$ is the Newton update for $f(x)$, then $Tx_{k+1}$ is the Newton update for $f(T^{-1}x)$ for nonsingular $T$.</li>
        <li>Derive the backtracking line search algorithm and prove that it terminates in finite iterations under Lipschitz continuous gradient assumption.</li>
        <li>For the quadratic $f(x) = \frac{1}{2}x^\top A x - b^\top x$, show that conjugate gradient finds the exact minimizer in at most $n$ iterations, where $n = \dim(x)$.</li>
        <li>Prove the BFGS update formula maintains positive definiteness: if $B_k \succ 0$ and $s_k^\top y_k > 0$, then $B_{k+1} \succ 0$.</li>
        <li>Show that Nesterov's accelerated gradient achieves $f(x_k) - f(x^*) \leq \frac{2L\|x_0-x^*\|^2}{(k+1)^2}$ for $L$-smooth convex $f$.</li>
        <li>For self-concordant $f$, prove that the Newton decrement $\lambda(x)$ satisfies $f(x) - f(x^*) \leq \lambda(x)^2/2$ when $\lambda(x) < 1$.</li>
        <li>Implement L-BFGS with memory size $m=10$ and compare memory usage to full BFGS for $n=1000$ dimensional problems.</li>
        <li>Show that preconditioning with $P = \text{diag}(\nabla^2 f(x^*))$ can reduce the condition number from $\kappa$ to approximately $\sqrt{\kappa}$ for certain problem classes.</li>
        <li>Prove that exact line search in gradient descent guarantees $\nabla f(x_k)^\top \nabla f(x_{k+1}) = 0$ (consecutive gradients are orthogonal).</li>
      </ol>
    </section>
  </main></div>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></a> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initGradientDescentVisualizer } from './widgets/js/gradient-descent-visualizer.js';
    initGradientDescentVisualizer('widget-1');
  </script>
  <script type="module">
    import { initGDvsNewton } from './widgets/js/gd-vs-newton.js';
    initGDvsNewton('widget-2');
  </script>
  <script type="module">
    import { initConvergenceRate } from './widgets/js/convergence-rate.js';
    initConvergenceRate('widget-3');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
<script src="../../static/js/ui.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
