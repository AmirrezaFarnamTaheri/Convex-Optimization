<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>00. Linear Algebra Basics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../01-linear-algebra-advanced/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>00. Linear Algebra Basics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-14</a>
        <span>Duration: 90 min</a>
        <span>Tags: prerequisites, review, linear-algebra, foundational</a>
      </div>
      <div class="lecture-summary">
        <p>This lecture establishes the linear algebra foundations required for convex optimization. We begin by defining the core objects—vectors, matrices, and their operations—and progress to the geometry of high-dimensional spaces through inner products, norms, and orthogonality. We then explore the crucial concept of Positive Semidefinite (PSD) matrices, which generalize non-negative numbers to the matrix setting and underpin convexity. Finally, we derive the method of least squares from a geometric perspective via orthogonal projections. This material provides the necessary language to describe convex sets, functions, and optimization problems with precision.</p>
        <p><strong>Prerequisites:</strong> Basic multivariable calculus and familiarity with standard matrix notation.</p>
        <p><strong>Forward Connections:</strong> The projection techniques introduced here are essential for the geometric interpretation of constrained optimization. PSD matrices are the cornerstone of convex quadratic programs (QP) and Semidefinite Programming (SDP) covered in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The four fundamental subspaces provide the geometric intuition for duality theory (<a href="../09-duality/index.html">Lecture 09</a>). Advanced topics such as QR factorization, SVD, and pseudoinverses are covered in <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Analyze Fundamental Subspaces:</b> Identify and compute the dimensions of the four fundamental subspaces of a matrix ($\mathcal{R}(A), \mathcal{N}(A), \mathcal{R}(A^\top), \mathcal{N}(A^\top)$) using the Rank-Nullity Theorem.</li>
        <li><b>Manipulate Norms and Inner Products:</b> Apply standard and generalized inner products, vector norms ($\ell_1, \ell_2, \ell_\infty$), and their duals to establish inequalities like Cauchy-Schwarz and Hölder.</li>
        <li><b>Perform Geometric Operations:</b> Construct orthonormal bases via Gram-Schmidt and compute orthogonal projections onto subspaces and affine sets.</li>
        <li><b>Characterize PSD Matrices:</b> Determine positive semidefiniteness using eigenvalues, variational forms ($\mathbf{x}^\top A\mathbf{x} \ge 0$), and the Schur Complement lemma.</li>
        <li><b>Solve Least Squares:</b> Derive and solve the normal equations for overdetermined systems and interpret the solution geometrically as an orthogonal projection.</li>
        <li><b>Apply Matrix Calculus:</b> Compute gradients and Hessians for linear, quadratic, and log-determinant functions, which are essential for first- and second-order optimization algorithms.</li>
      </ul>
    </section>

    <!-- SECTION 1: NOTATION -->
    <section class="section-card" id="section-notation">
      <h2>1. Mathematical Atoms: From Fields to Functions</h2>

      <h3>1.1 The Geometric "Atoms" of Optimization</h3>
      <p>Optimization is "glue math": we compose maps, stack constraints, take gradients, and build Lagrangians. To do this without errors, we must treat mathematics as a <b>typed language</b>. Every object has a declared type, and operations are only defined when types match.</p>
      <ul>
        <li><b>Scalar:</b> $a \in \mathbb{R}$ (or $\mathbb{C}$)</li>
        <li><b>Vector:</b> $x \in \mathbb{R}^n$ (Column vector)</li>
        <li><b>Matrix:</b> $A \in \mathbb{R}^{m \times n}$ (Linear map $\mathbb{R}^n \to \mathbb{R}^m$)</li>
        <li><b>Set:</b> $C \subseteq \mathbb{R}^n$ (Boolean predicate on space)</li>
        <li><b>Function:</b> $f: D \to Y$ (Map with explicit domain)</li>
      </ul>
      <p>In convex optimization, we view algebraic problems through a geometric lens. Throughout this course, you will repeatedly perform three fundamental operations:</p>
      <ol>
        <li><b>Mixing (Combinations):</b> Optimization often involves "averaging" or interpolating between candidate solutions.
          <br><i>Example:</i> A portfolio is a weighted mix of assets. The rules for these weights (sum to 1? non-negative?) determine whether we are working with <b>subspaces</b>, <b>affine sets</b>, or <b>convex sets</b>.</li>
        <li><b>Measuring (Norms & Inner Products):</b> We need precise tools to quantify "error" and "similarity".
          <br><i>Example:</i> The objective function minimizes a residual vector. <b>Norms</b> measure lengths (error size), while <b>inner products</b> measure angles (alignment).</li>
        <li><b>Pullback (Preimages):</b> Constraints $g(\mathbf{x}) \le 0$ define sets implicitly.
          <br><i>Example:</i> Understanding constraints as <b>preimages</b> allows us to deduce the shape of the feasible region directly from the properties of the function $\mathbf{g}$.</li>
      </ol>

      <h3>1.2 Scalars and Fields: The Rulebook</h3>
      <p>Before "vectors" exist, linear algebra is already quietly assuming something about the <i>numbers</i> you’re allowed to use as coefficients. Those numbers are the <b>scalars</b>. A <b>field</b> $\mathbb{F}$ is the rulebook that tells you how those scalars behave. In convex optimization, our scalars are almost always $a \in \mathbb{R}$.</p>
      <p><b>Why "field" matters:</b> When you later do manipulations like $a(x+y) = ax+ay$ or $(a+b)x=ax+bx$, you are using distributivity and associativity in $\mathbb{R}$. If scalars lived in something weaker than a field (e.g., a ring without inverses), many optimization arguments would break.</p>
      <p><b>Extended Reals:</b> In optimization, we often need the <b>extended reals</b> $\overline{\mathbb{R}} := \mathbb{R} \cup \{+\infty\}$. This allows us to treat "constraints" as part of the objective function (e.g., indicator functions where $\delta_C(x) = +\infty$ if $x \notin C$).</p>

      <p>You can think of a field as: "a number system where you can add, subtract, multiply, and divide (except by 0), and the usual algebra works." Most linear algebra courses live over $\mathbb{R}$ (real numbers) or $\mathbb{C}$ (complex numbers), but the definitions are built to work over any field $\mathbb{F}$.</p>

      <h4>The Field Axioms (From Scratch)</h4>
      <p>A <b>field</b> $\mathbb{F}$ is a set equipped with two operations—addition ($+$) and multiplication ($\cdot$)—satisfying three bundles of axioms:</p>
      <ul>
        <li><b>A) Addition ($\mathbb{F}, +$ is an abelian group):</b>
          <ul>
            <li><b>Closure:</b> $a+b \in \mathbb{F}$.</li>
            <li><b>Associativity:</b> $(a+b)+c = a+(b+c)$.</li>
            <li><b>Commutativity:</b> $a+b = b+a$.</li>
            <li><b>Additive Identity:</b> There exists $0 \in \mathbb{F}$ such that $a+0=a$. (Unique).</li>
            <li><b>Additive Inverse:</b> For every $a$, there exists $-a$ such that $a+(-a)=0$. This allows subtraction: $a-b := a+(-b)$.</li>
          </ul>
        </li>
        <li><b>B) Multiplication ($\mathbb{F} \setminus \{0\}, \cdot$ is an abelian group):</b>
          <ul>
            <li><b>Closure:</b> $ab \in \mathbb{F}$.</li>
            <li><b>Associativity:</b> $(ab)c = a(bc)$.</li>
            <li><b>Commutativity:</b> $ab = ba$. (Required for fields, unlike skew-fields).</li>
            <li><b>Multiplicative Identity:</b> There exists $1 \in \mathbb{F}, 1 \neq 0$ such that $a \cdot 1 = a$. (Unique).</li>
            <li><b>Multiplicative Inverse:</b> For every $a \neq 0$, there exists $a^{-1}$ such that $aa^{-1}=1$. This allows division: $a/b := ab^{-1}$.</li>
          </ul>
        </li>
        <li><b>C) The Bridge Axiom (Distributivity):</b> $a(b+c) = ab + ac$. This ensures multiplication respects addition.</li>
      </ul>

      <p><b>Why linear algebra needs a field:</b> To make linear combinations $\sum \alpha_i v_i$ work, scalars must allow scaling, undoing scaling (inverses), and distribution. The hidden reason is solving equations: $\alpha x = y \implies x = \alpha^{-1}y$ requires inverses. Without them, Gaussian elimination fails.</p>

      <h4>Derived Laws (Proven Consequences)</h4>
      <p>From these axioms alone, we can prove facts you use constantly:</p>
      <ul>
        <li><b>$0 \cdot a = 0$:</b> Proof: $0a = (0+0)a = 0a + 0a \implies 0 = 0a$ (by additive cancellation).</li>
        <li><b>$(-1)a = -a$:</b> Proof: $a + (-1)a = 1a + (-1)a = (1-1)a = 0a = 0$. So $(-1)a$ is the additive inverse of $a$.</li>
        <li><b>No Zero Divisors:</b> If $ab=0$ and $a \neq 0$, then $b=0$. (Multiply by $a^{-1}$). This is why we can solve by factoring.</li>
        <li><b>Uniqueness:</b> The identities $0, 1$ and inverses $-a, a^{-1}$ are unique.</li>
      </ul>

      <h4>Concrete Examples & The Failure Museum</h4>
      <ul>
        <li><b>Fields (Good Citizens):</b> $\mathbb{Q}$ (rationals), $\mathbb{R}$ (reals), $\mathbb{C}$ (complex), and finite fields $\mathbb{F}_p$ (where $p$ is prime).</li>
        <li><b>$\mathbb{Z}$ (Integers):</b> NOT a field. It is a <b>Ring</b>. You cannot solve $2x=1$ in integers because 2 has no multiplicative inverse ($1/2 \notin \mathbb{Z}$). Linear algebra over $\mathbb{Z}$ becomes "Module Theory".</li>
        <li><b>$\mathbb{Z}_n$ (Integers mod $n$):</b> A field if and only if $n$ is prime. If $n=6$, then $2 \cdot 3 = 0 \pmod 6$. This implies we have <b>zero divisors</b> (non-zero numbers multiply to zero), destroying the cancellation law ($ab=ac \implies b=c$). In a field, scaling by a non-zero scalar is always injective (preserves information).</li>
      </ul>

      <h3>1.3 Start with $\mathbb{R}^n$ as a Concrete Model</h3>
      <p>We define $\mathbb{R}^n$ not just as "tuples" but as functions $x: \{1,\dots,n\} \to \mathbb{R}$. This makes it clear that operations are defined <b>componentwise</b>, inheriting properties directly from the field $\mathbb{R}$.</p>

      <p><b>$\mathbb{R}^n$ is a Type, Not Decoration:</b> A vector $x \in \mathbb{R}^n$ is an ordered $n$-tuple. This asserts two things:
      <br>1. <b>Shape:</b> $x$ has exactly $n$ coordinates.
      <br>2. <b>Ambient Space:</b> $x$ lives in a space where addition and scaling are defined.
      <br><b>Equality is Coordinatewise:</b> $x=y \iff \forall i, x_i=y_i$. This is the bridge that converts $n$ scalar equalities into one vector equality.</p>

      <p><b>Vector Operations (Forced by Consistency):</b>
      <br>1. <b>Addition:</b> $(x+y)_i := x_i + y_i$. We want addition to commute and reduce to scalar addition in each slot.
      <br>2. <b>Scalar Multiplication:</b> $(ax)_i := a x_i$. We want scaling to distribute: $a(x+y) = ax + ay$.
      <br>This is why operations on $\mathbb{R}^n$ are "boring"—they are literally scalar algebra applied $n$ times.</p>

      <h4>The Axioms Verified (Why it works)</h4>
      <p>For $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ and $\alpha \in \mathbb{R}$, we define $(\mathbf{x}+\mathbf{y})_i = x_i+y_i$ and $(\alpha \mathbf{x})_i = \alpha x_i$. This structure satisfies all 8 vector space axioms because each axiom holds in $\mathbb{R}$ at every coordinate.</p>
      <ul>
        <li><b>Additive Inverse:</b> The vector $-\mathbf{x} = (-x_1, \dots, -x_n)$ satisfies $\mathbf{x} + (-\mathbf{x}) = \mathbf{0}$. Uniqueness is guaranteed by componentwise uniqueness.</li>
        <li><b>Derived Facts:</b> $0 \cdot \mathbf{x} = \mathbf{0}$, $(-1)\mathbf{x} = -\mathbf{x}$. These are not assumptions; they are theorems derived from the axioms.</li>
        <li><b>Standard Basis:</b> The vectors $e_j$ (1 in pos $j$, 0 elsewhere) allow any vector to be written as a linear combination: $\mathbf{x} = \sum x_i e_i$. This means coordinates are literally coefficients in a linear combination.</li>
      </ul>

      <h3>1.4 Geometry as a Representation (Points vs. Vectors)</h3>
      <p>In elementary math, "point $(1,1)$" and "vector $(1,1)$" are treated as the same thing. In optimization, the distinction is critical.
      <br><b>Vectors</b> are displacements (elements of a vector space $V$). You can add them and scale them.
      <br><b>Points</b> are locations (elements of an affine space $A$). You cannot add points intrinsically. Adding locations depends on where you put the origin.</p>

      <h4>The Affine Rules</h4>
      <ul>
        <li><b>Point - Point = Vector:</b> $q - p$ is the unique displacement from $p$ to $q$.</li>
        <li><b>Point + Vector = Point:</b> $p + v$ is the translation of $p$ by $v$.</li>
        <li><b>Point + Point = Illegal:</b> $(1,1) + (2,2) = (3,3)$ is only meaningful if we secretly treat them as displacements from $(0,0)$. Change the origin, and the sum changes.</li>
        <li><b>Affine Combinations:</b> $\sum \alpha_i p_i$ is a valid point if and only if $\sum \alpha_i = 1$. This makes the result origin-independent.</li>
      </ul>
      <p><b>Why this matters:</b> The solution set to $Ax=0$ is a <b>subspace</b> (contains 0, vectors). The solution set to $Ax=b$ ($b \ne 0$) is an <b>affine set</b> (does not contain 0, points). Mixing them up leads to errors in duality and tangent space calculations.</p>

      <h3>1.5 Linear Combinations: The Fundamental Constructor</h3>
      <p>Linear algebra is built on one atomic operation: the <b>finite linear combination</b>.
      $$ \mathbf{y} = \sum_{i=1}^k \alpha_i \mathbf{v}_i $$
      Every concept in the course reduces to this.</p>

      <h4>(A) Systems of Linear Equations as Linear Maps</h4>
      <p>The equation $A\mathbf{x} = \mathbf{b}$ is usually taught as "a system of $m$ equations in $n$ unknowns." That description is correct but shallow. The more powerful interpretation is this:
      <br><b>$A$ defines a linear map, and $A\mathbf{x}=\mathbf{b}$ asks whether $\mathbf{b}$ lies in its image.</b>
      <br>Everything else—Gaussian elimination, pivots, ranks—is machinery for answering that one question.</p>

      <p><b>1. Interpret $A$ as a function, not a table of numbers:</b> A matrix $A \in \mathbb{R}^{m\times n}$ defines a linear transformation $T_A : \mathbb{R}^n \to \mathbb{R}^m, \ T_A(\mathbf{x}) = A\mathbf{x}$. This is literally a function with two defining properties:
      <ul>
        <li>$T_A(\mathbf{x}+\mathbf{y}) = T_A(\mathbf{x}) + T_A(\mathbf{y})$</li>
        <li>$T_A(\alpha \mathbf{x}) = \alpha T_A(\mathbf{x})$</li>
      </ul>
      Those properties mean the entire behavior of $T_A$ is determined by what it does to basis vectors, and concretely, to the <b>columns of $A$</b>.</p>

      <p><b>2. The "Fundamental Identity" (Column-space interpretation):</b> Write the columns of $A$ as $A = [\mathbf{a}_1 \dots \mathbf{a}_n]$. Then for any $\mathbf{x} = (x_1,\dots,x_n)$,
      $$ \boxed{ A\mathbf{x} = x_1 \mathbf{a}_1 + x_2 \mathbf{a}_2 + \dots + x_n \mathbf{a}_n } $$
      This identity is not a trick. It <i>is</i> matrix–vector multiplication. So now the equation $A\mathbf{x}=\mathbf{b}$ becomes: <i>Does there exist a choice of scalars $x_1,\dots,x_n$ such that $\mathbf{b} = x_1 \mathbf{a}_1 + \cdots + x_n \mathbf{a}_n$?</i> That is exactly the definition of <b>membership in a span</b>.</p>

      <p><b>3. The Column Space Answers Consistency:</b> Define the <b>column space</b> of $A$: $\mathcal{R}(A) = \operatorname{span}\{\mathbf{a}_1,\dots,\mathbf{a}_n\} \subseteq \mathbb{R}^m$. Now the fundamental equivalence:
      $$ \boxed{ A\mathbf{x} = \mathbf{b} \text{ is solvable } \iff \mathbf{b} \in \mathcal{R}(A). } $$
      Nothing more, nothing less. If $\mathbf{b}$ lies in the column space, the system is <b>consistent</b>. If $\mathbf{b}$ lies outside it, the system is <b>inconsistent</b>, and no amount of algebra will fix that.</p>

      <h4>(B) Row Reduction as Invertible Transformation</h4>
      <p>Gaussian elimination is often taught as a sequence of mechanical steps. However, it has a precise algebraic meaning: <b>Row operations are left-multiplications by invertible matrices.</b></p>
      <p>If we perform a row operation on the augmented matrix $[A \mid \mathbf{b}]$, we are effectively multiplying the system $A\mathbf{x} = \mathbf{b}$ by an elementary matrix $E$:
      $$ E A \mathbf{x} = E \mathbf{b} $$
      Since every elementary row operation (swap, scale, add) is reversible, the matrix $E$ is <b>invertible</b>.
      <br><b>Theorem:</b> If $E$ is invertible, the solution sets of $A\mathbf{x}=\mathbf{b}$ and $(EA)\mathbf{x} = E\mathbf{b}$ are identical.
      <br><i>Proof:</i> If $A\mathbf{x}=\mathbf{b}$, then $EA\mathbf{x}=E\mathbf{b}$. Conversely, if $EA\mathbf{x}=E\mathbf{b}$, multiplying by $E^{-1}$ yields $A\mathbf{x}=\mathbf{b}$.
      <br>Thus, row reduction transforms the system into a "simpler" linear map (RREF) without changing the solution set. It changes the <i>coordinate representation</i> of the constraints, not the geometry of the solution space.</p>

      <div class="insight">
        <h4>Immediate Consequences (Geometry vs. Algebra)</h4>
        <ol>
            <li><b>Why inconsistent systems exist:</b> If $\mathcal{R}(A)$ is a proper subspace of $\mathbb{R}^m$ (rank < m), then "most" vectors $\mathbf{b}$ are outside it. Inconsistency is the generic case, not a pathology.</li>
            <li><b>Why adding columns can make systems solvable:</b> Adding a column to $A$ enlarges $\mathcal{R}(A)$. You are literally adding a new direction you can combine.</li>
            <li><b>Why rank matters:</b> The rank of $A$ is $\operatorname{rank}(A) = \dim(\mathcal{R}(A))$. Rank measures how "big" the image of the linear map is.</li>
            <li><b>Why overdetermined systems often fail:</b> If $m > n$, the column space lives in $\mathbb{R}^m$ but has dimension at most $n$. It cannot fill the space. Many $\mathbf{b}$’s are unreachable.</li>
        </ol>
        <h4>The Mental Model to Keep Forever</h4>
        <p>Do not think: <i>"I have equations and unknowns."</i></p>
        <p>Think instead: <i>"I have a linear map. What outputs can it produce?"</i></p>
        <p>Solving $A\mathbf{x}=\mathbf{b}$ is not algebra first and geometry later. It is <b>geometry first, algebra as a tool</b>.</p>
      </div>

      <h4>(B) Linear Independence as "No Redundancy"</h4>
      <p>Linear independence is the rule that says: “none of these vectors is secretly unnecessary.” We define it precisely to ensure coordinates are unique.</p>

      <p><b>Definition:</b> A list of vectors $\mathbf{v}_1,\dots,\mathbf{v}_k$ is <b>linearly independent</b> if the only way to combine them to get the zero vector is the trivial way:
      $$ \alpha_1 \mathbf{v}_1+\cdots+\alpha_k \mathbf{v}_k = 0 \quad\Longrightarrow\quad \alpha_1=\cdots=\alpha_k=0. $$
      If there exists a non-trivial combination (some $\alpha_i \neq 0$) summing to zero, the vectors are <b>linearly dependent</b>.</p>

      <p><b>Why check for zero?</b> You might wonder: why not check if two combinations produce the same vector? Because those are the same question.
      $$ \sum \alpha_i \mathbf{v}_i = \sum \beta_i \mathbf{v}_i \iff \sum (\alpha_i - \beta_i) \mathbf{v}_i = 0 $$
      Thus, <b>Independence $\iff$ Uniqueness of Representation</b>. Once you choose independent vectors as building blocks, every vector in their span has a unique recipe (coordinates).</p>

      <div class="proof-box">
        <h4>Proposition: Independence $\iff$ Uniqueness of Representation</h4>
        <p>Independence is the "secret meaning" of coordinate uniqueness.
        <br><b>1. Independence $\implies$ Uniqueness:</b>
        <br>Suppose $\mathbf{v}_1,\dots,\mathbf{v}_k$ are linearly independent. If a vector $\mathbf{w}$ has two representations:
        $$ \mathbf{w} = \sum_{i=1}^k \alpha_i \mathbf{v}_i = \sum_{i=1}^k \beta_i \mathbf{v}_i $$
        Subtracting the equations gives $\sum (\alpha_i - \beta_i)\mathbf{v}_i = \mathbf{0}$.
        By independence, the only linear combination that yields $\mathbf{0}$ is the trivial one, so $\alpha_i - \beta_i = 0$ for all $i$. Thus $\alpha_i = \beta_i$. The coordinates are unique.
        <br><b>2. Uniqueness $\implies$ Independence:</b>
        <br>Assume every vector in the span has a unique representation. The zero vector $\mathbf{0}$ is in the span and has the trivial representation $\mathbf{0} = \sum 0\mathbf{v}_i$. If there were <i>any</i> other representation $\sum c_i \mathbf{v}_i = \mathbf{0}$ with non-zero $c_i$, uniqueness would be violated. Thus, the only representation of $\mathbf{0}$ is the trivial one, which is the definition of independence.</p>
      </div>

      <div class="theorem-box">
        <h4>Theorem: The Redundancy Test</h4>
        <p>A set of vectors is linearly independent if and only if <b>no vector in the set is redundant</b> (i.e., lies in the span of the others).
        <br><i>Proof:</i>
        <br>($\Rightarrow$) <b>Independence implies No Redundancy:</b> Assume the set is independent. Suppose, for contradiction, that $v_j$ is redundant: $v_j = \sum_{i \neq j} c_i v_i$. Then $1 \cdot v_j - \sum_{i \neq j} c_i v_i = 0$. This is a non-trivial linear combination summing to zero (the coefficient of $v_j$ is 1), which contradicts independence.
        <br>($\Leftarrow$) <b>No Redundancy implies Independence:</b> Assume no vector is in the span of the others. Suppose the set is dependent: $\sum_{i=1}^k \alpha_i v_i = 0$ with some coefficients not zero. Pick an index $j$ where $\alpha_j \neq 0$. We can isolate $v_j$:
        $$ \alpha_j v_j = -\sum_{i \neq j} \alpha_i v_i \implies v_j = \sum_{i \neq j} \left( -\frac{\alpha_i}{\alpha_j} \right) v_i $$
        This expresses $v_j$ as a linear combination of the other vectors, meaning $v_j$ is redundant. This contradicts the assumption. Thus, the set must be independent.</p>
      </div>

      <p><b>Geometric Intuition:</b>
      <br>In $\mathbb{R}^2$: Two vectors are independent iff they are not collinear.
      <br>In $\mathbb{R}^3$: Three vectors are independent iff they do not lie in the same plane through the origin.
      <br><b>Redundancy:</b> If a set is dependent, at least one vector adds no new dimension to the span.</p>

      <h4>(C) Algorithmic Check: The Pivot Column Method</h4>
      <p>How do we practically check for independence or finding a basis? We use the <b>Pivot Column Algorithm</b> (Gaussian Elimination).</p>
      <ol>
        <li><b>Form a Matrix:</b> Stack the vectors as columns of a matrix $A = [\mathbf{v}_1 \dots \mathbf{v}_k]$.</li>
        <li><b>Row Reduce:</b> Compute the Row Reduced Echelon Form (RREF) of $A$.</li>
        <li><b>Identify Pivots:</b> A column with a pivot (leading 1) corresponds to a vector that adds a new dimension. A column <i>without</i> a pivot is linearly dependent on the previous pivot columns.</li>
      </ol>
      <p><b>Why this works:</b> Row operations preserve linear relationships between columns. If column 3 is the sum of columns 1 and 2 in the RREF, it was the sum of columns 1 and 2 in the original matrix.</p>

      <h3>1.6 Functions: Linear vs. Affine Maps</h3>
      <p>Linear algebra is not just about sets of vectors; it is about the functions that respect their structure. We distinguish two key types:</p>

      <h4>Linear Maps: The Origin-Anchored Structure</h4>
      <p>A map $T: V \to W$ is <b>linear</b> if it preserves linear combinations: $T(ax+by) = aT(x) + bT(y)$.
      <br><b>Consequences:</b>
      <br>1. $T(0) = 0$. Linear maps must fix the origin.
      <br>2. <b>Matrix Representation:</b> Every linear map $L: \mathbb{R}^n \to \mathbb{R}^m$ is of the form $L(x) = Ax$.
      <br>3. <b>Geometry:</b> Linear maps send subspaces to subspaces.</p>

      <h4>Affine Maps: Linear + Translation</h4>
      <p>A map $f: \mathbb{R}^n \to \mathbb{R}^m$ is <b>affine</b> if it has the form $f(x) = Ax + b$.
      <br><b>Characterization:</b> A map is affine iff it preserves <b>affine combinations</b> (where $\sum \theta_i = 1$):
      $$ f(\theta x + (1-\theta)y) = \theta f(x) + (1-\theta)f(y) \quad \forall x,y, \theta \in \mathbb{R} $$
      <b>Geometry:</b>
      <br>1. <b>Lines to Lines:</b> Affine maps send lines to lines (or points). They preserve "straightness".
      <br>2. <b>Parallelism:</b> Parallel lines map to parallel lines.
      <br>3. <b>Convexity Invariance:</b> The image and preimage of a convex set under an affine map are convex. This is the single most-used convexity fact in modeling. $Ax=b$ defines a convex set because it is the preimage of $\{b\}$ (convex) under the affine map $Ax$.</p>

      <div class="theorem-box">
        <h4>Theorem: Basis Determines Linear Maps</h4>
        <p>Let $\mathcal{B} = \{b_1, \dots, b_n\}$ be a basis of $V$. A linear transformation $T: V \to W$ is <b>completely determined</b> by its values on the basis vectors $T(b_1), \dots, T(b_n)$.
        <br><i>Proof:</i> Take any vector $v \in V$. Since $\mathcal{B}$ is a basis, there exist <b>unique</b> scalars $c_i$ such that $v = \sum_{i=1}^n c_i b_i$.
        Applying $T$:
        $$ T(v) = T\left(\sum_{i=1}^n c_i b_i\right) = \sum_{i=1}^n c_i T(b_i) $$
        There is no freedom left. Once the images of the basis vectors are chosen, the image of every other vector is forced by linearity. This explains why an infinite number of input-output pairs can be encoded by a finite matrix.</p>
      </div>

      <h3>1.7 Matrix Algebra from Linear Maps</h3>
      <p>A matrix $A \in \mathbb{R}^{m \times n}$ is best treated as a <b>typed operator</b>: $A: \mathbb{R}^n \to \mathbb{R}^m$. The number of columns $n$ is the input dimension, and the number of rows $m$ is the output dimension. This prevents shape errors: $BA$ is only defined if the output space of $A$ matches the input space of $B$.</p>

      <p><b>Matrix-Vector Multiplication Definition:</b>
      $$ (Ax)_i := \sum_{j=1}^n A_{ij} x_j $$
      This definition is not arbitrary. It is forced by the requirement of linearity ($A(ax+by) = aAx + bAy$) and compatibility with composition. It allows two powerful interpretations:</p>

      <ul>
        <li><b>Row View (Measurement):</b> Let $A_{i:}$ be the $i$-th row. Then $(Ax)_i = A_{i:} x = \langle A_{i:}^\top, x \rangle$. Each row is a linear functional that "measures" the input $x$.</li>
        <li><b>Column View (Synthesis):</b> Let $A_{:j}$ be the $j$-th column. Then $Ax = \sum_{j=1}^n x_j A_{:j}$. The output is a linear combination of the columns, with coefficients given by $x$. This is the view used for range, span, and feasibility.</li>
      </ul>

      <p><b>Coordinate Representation:</b>
      <br>Let $V$ be a vector space with basis $B = \{b_1,\dots,b_n\}$ and $W$ be a vector space with basis $C = \{c_1,\dots,c_m\}$.
      <br>For a linear map $T: V \to W$, we want a matrix $A$ that transforms coordinates in $B$ to coordinates in $C$:
      $$ [T(v)]_C = A [v]_B $$
      This leads to the constructive definition of a matrix:</p>

      <div class="insight">
        <h4>Construction of the Matrix</h4>
        <p><b>The $j$-th column of the matrix $A$ is the coordinate vector of the image of the $j$-th basis vector.</b>
        $$ \text{Column } j = [T(b_j)]_C $$
        <br><i>Why this works:</i>
        $$ T(v) = T\left(\sum x_j b_j\right) = \sum x_j T(b_j) $$
        Taking coordinates in $C$:
        $$ [T(v)]_C = \sum x_j [T(b_j)]_C = \sum x_j (\text{Column } j) $$
        This is exactly the definition of matrix-vector multiplication: a linear combination of columns.</p>
      </div>

      <h4>Composition = Matrix Multiplication</h4>
      <p>Why do we multiply matrices using the "row-by-column" rule? It is forced by the composition of linear maps.
      <br>If $S: U \to V$ (Matrix $A$) and $T: V \to W$ (Matrix $B$), then $T \circ S$ corresponds to $BA$.
      $$ [T(S(u))]_D = B [S(u)]_C = B (A [u]_B) = (BA) [u]_B $$
      We derive the entry-wise formula $(BA)_{ij} = \sum_k B_{ik} A_{kj}$ not as a definition, but as the inevitable algebraic consequence of tracking these coordinates.
      <br><b>Consequences:</b>
      <ul>
        <li><b>Associativity:</b> Matrix multiplication is associative ($(AB)C = A(BC)$) because function composition is associative.</li>
        <li><b>Non-Commutativity:</b> It is not commutative ($AB \neq BA$) because the order of operations matters (rotating then projecting is different from projecting then rotating).</li>
      </ul>
      </p>

      <h3>1.8 Basis and Dimension: Coordinates and Degrees of Freedom</h3>
      <p>A <b>basis</b> is where linear algebra stops being about abstract sets and becomes a working coordinate system. It combines the two key properties we've studied: existence (spanning) and uniqueness (independence).</p>

      <h4>(A) Basis: A Coordinate System that Actually Works</h4>
      <p>A set of vectors $\mathcal{B} = \{b_1, \dots, b_n\}$ is a <b>basis</b> for $V$ if:
      <ol>
        <li><b>Spanning:</b> $\mathcal{B}$ generates $V$. Every vector can be built from $\mathcal{B}$.</li>
        <li><b>Independence:</b> $\mathcal{B}$ has no redundancy. Every vector is built in a <i>unique</i> way.</li>
      </ol>
      </p>

      <div class="theorem-box">
        <h4>The Basis Theorem</h4>
        <p>If $\mathcal{B}$ is a basis, then every vector $\mathbf{v} \in V$ can be written in <b>exactly one way</b> as a linear combination:
        $$ \mathbf{v} = c_1 b_1 + \dots + c_n b_n $$
        The unique scalars $(c_1, \dots, c_n)$ are the <b>coordinates</b> of $\mathbf{v}$ in basis $\mathcal{B}$, denoted $[\mathbf{v}]_\mathcal{B}$.
        <br>This creates a perfect dictionary (bijection) between the abstract space $V$ and the concrete space $\mathbb{F}^n$.
        </p>
      </div>

      <h4>(B) Dimension: The Invariant Number</h4>
      <p>A vector space can have infinitely many different bases. However, a structural theorem ensures they all share one property: the <b>number of elements</b>. This invariant is the <b>dimension</b> of $V$.</p>

      <div class="proof-box">
        <h4>Lemma: Steinitz Exchange Lemma (Proof)</h4>
        <p>Let $V$ be a vector space. Suppose:
        <ul>
            <li>$S = \{s_1,\dots,s_m\}$ spans $V$.</li>
            <li>$L = \{\ell_1,\dots,\ell_k\}$ is linearly independent in $V$.</li>
        </ul>
        Then <b>$k \le m$</b>, and we can replace $k$ vectors in $S$ with $L$ to form a new spanning set.</p>

        <h4>Proof by Step-by-Step Replacement</h4>
        <p>We proceed by induction to inject elements of $L$ into $S$ one by one.</p>
        <div class="proof-step">
            <strong>Step 0:</strong> Start with $S^{(0)} = S$, which spans $V$.
        </div>
        <div class="proof-step">
            <strong>Inductive Step:</strong> Assume $S^{(j-1)}$ spans $V$ and contains $\{\ell_1, \dots, \ell_{j-1}\}$. We want to insert $\ell_j$.
            <br>Since $S^{(j-1)}$ spans, write $\ell_j$ as a linear combination of vectors in $S^{(j-1)}$.
            $$ \ell_j = \sum_{u \in S^{(j-1)}} c_u u $$
            Rearranging: $0 = \ell_j - \sum c_u u$.
            <br>Crucially, at least one vector $u^* \in S^{(j-1)}$ <b>not</b> in $\{\ell_1, \dots, \ell_{j-1}\}$ must have a non-zero coefficient $c_{u^*} \neq 0$.
            <br><i>Why?</i> If only $\ell$'s had non-zero coefficients, then $\ell_j$ would be in the span of previous $\ell$'s, contradicting independence of $L$.
        </div>
        <div class="proof-step">
            <strong>Replacement:</strong> Solve for $u^*$:
            $$ u^* = \frac{1}{c_{u^*}} \ell_j - \sum \dots $$
            This shows $u^*$ is in the span of the new set $S^{(j)} = (S^{(j-1)} \setminus \{u^*\}) \cup \{\ell_j\}$.
            Thus, the new set still spans $V$.
        </div>
        <div class="proof-step">
            <strong>Conclusion:</strong> We can continue this for all $k$ elements of $L$. Since each step consumes a distinct vector from the original $S$ (and never one of the inserted $\ell$'s), we must have enough capacity in $S$. Thus $k \le m$.
        </div>
      </div>

      <div class="theorem-box">
        <h4>Theorem: Characterizations of Dimension</h4>
        <p>Let $V$ be finite-dimensional with dimension $n$. The following are equivalent:</p>
        <ol>
            <li>Every basis has exactly $n$ vectors.</li>
            <li>The largest possible size of a linearly independent set is $n$.</li>
            <li>The smallest possible size of a spanning set is $n$.</li>
        </ol>
        <p><i>Proof:</i>
        <br>If $B$ is a basis ($|B|=n$), then any independent set $L$ satisfies $|L| \le |B|=n$ (Steinitz).
        <br>Any spanning set $S$ satisfies $|S| \ge |B|=n$ (Steinitz with $L=B, S=S$).
        <br>Thus $n$ is the unique crossover point where max-independence meets min-spanning.</p>
      </div>

      <h3>1.9 Functions and Preimages</h3>
      <p>A function $f: \mathcal{X} \to \mathcal{Y}$ maps inputs to outputs.
      <br>The <b>Preimage</b> $f^{-1}(B) = \{\mathbf{x} \in \mathcal{X} : f(\mathbf{x}) \in B\}$ is the set of inputs mapping to $B$.
      <br><b>Crucial Property:</b> Preimages preserve set operations (union, intersection, complement). This means if we define a feasible region by intersecting constraints $g_i(\mathbf{x}) \le 0$, the geometry of the region is the intersection of the preimages of $(-\infty, 0]$. Since preimages of convex sets under convex functions are convex, this explains why "convex constraints define convex sets".</p>

    </section>

    <!-- SECTION 2: SUBSPACES -->
    <section class="section-card" id="section-subspaces">
      <h2>2. Subspaces and the Four Fundamental Spaces</h2>

      <p>A <a href="#" class="definition-link">linear subspace</a> is a set of vectors that is closed under addition and scalar multiplication. When a linear map $T: V \to W$ acts, it decomposes the domain and codomain based on <b>what information is lost</b> and <b>what survives</b>.</p>

      <h3>2.1 Kernel and Image: Where Information Dies and Survives</h3>
      <p>For a linear map $T: \mathbb{R}^n \to \mathbb{R}^m$ represented by matrix $A$:</p>
      <ul>
        <li><b>Kernel (Nullspace):</b> $\mathcal{N}(A) = \{\mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = 0\}$.
          <br><i>Meaning:</i> <b>Information Loss.</b> Two inputs $\mathbf{u}, \mathbf{v}$ produce the same output ($A\mathbf{u}=A\mathbf{v}$) if and only if their difference lies in the kernel ($\mathbf{u}-\mathbf{v} \in \mathcal{N}(A)$). It measures the <b>indistinguishable</b> inputs.</li>
        <li><b>Image (Column Space):</b> $\mathcal{R}(A) = \{A\mathbf{x} \mid \mathbf{x} \in \mathbb{R}^n\}$.
          <br><i>Meaning:</i> <b>Information Survival.</b> These are the outputs that can actually be produced. The equation $A\mathbf{x}=\mathbf{b}$ is solvable iff $\mathbf{b} \in \mathcal{R}(A)$.</li>
      </ul>

      <div class="theorem-box">
        <h4>Structure of Solution Sets: Affine Geometry</h4>
        <p>$$ \boxed{ \{\text{solutions of } A\mathbf{x}=\mathbf{b}\} = \mathbf{x}_p + \mathcal{N}(A) } $$
        <br><b>Why every consistent linear system produces an affine space:</b>
        <br>Assume the system is consistent and fix <b>one</b> particular solution $\mathbf{x}_p$ such that $A\mathbf{x}_p = \mathbf{b}$.
        <br><b>1. Closure (Translating by Kernel):</b> Take any $\mathbf{x}_h \in \mathcal{N}(A)$. Then $A(\mathbf{x}_p + \mathbf{x}_h) = A\mathbf{x}_p + A\mathbf{x}_h = \mathbf{b} + 0 = \mathbf{b}$. Every vector of the form $\mathbf{x}_p + \mathbf{x}_h$ is a solution.
        <br><b>2. Exhaustion (No other solutions):</b> Let $\mathbf{x}$ be <i>any</i> solution ($A\mathbf{x}=\mathbf{b}$). Then $A(\mathbf{x} - \mathbf{x}_p) = A\mathbf{x} - A\mathbf{x}_p = \mathbf{b} - \mathbf{b} = 0$. So $\mathbf{x} - \mathbf{x}_p \in \mathcal{N}(A)$, which means $\mathbf{x} = \mathbf{x}_p + \mathbf{x}_h$ for some null-space vector.
        <br><b>Conclusion:</b> The solution set is a translation of the subspace $\mathcal{N}(A)$.
        <ul>
            <li>If $\mathbf{b}=0$, the set is a subspace (contains 0).</li>
            <li>If $\mathbf{b} \ne 0$, the set is an <b>affine subspace</b> (does not contain 0).</li>
        </ul>
        This implies that $\dim(\text{solution set}) = \dim(\mathcal{N}(A))$. The nullspace controls the <b>multiplicity</b> (degrees of freedom), while the column space controls the <b>existence</b>.</p>

        <h4>Theorem: Injectivity $\iff$ Trivial Kernel</h4>
        <p>A linear map is injective (one-to-one) if and only if its kernel contains only the zero vector.
        <br><i>Proof:</i>
        <br>($\Rightarrow$) Assume injective. Since $T(0)=0$, if $v \in \mathcal{N}(T)$ then $T(v)=0=T(0)$, so $v=0$.
        <br>($\Leftarrow$) Assume $\mathcal{N}(T)=\{0\}$. If $T(u)=T(v)$, then $T(u-v)=0$, so $u-v \in \mathcal{N}(T)$. Thus $u-v=0 \implies u=v$.
        </p>
      </div>

      <h3>2.2 Rank-Nullity: Conservation of Degrees of Freedom</h3>
      <p>The <b>Rank-Nullity Theorem</b> is the fundamental conservation law of linear algebra. It states that a linear map cannot create or destroy dimension arbitrarily. Every input degree of freedom ($n$) must either be annihilated (go to Nullspace) or preserved (go to Rank).</p>
      $$ \boxed{ n = \dim(\mathcal{N}(A)) + \dim(\mathcal{R}(A)) } $$
      <p><i>Interpretation:</i>
      <br><b>Rank ($r$):</b> Number of independent constraints the system <i>can</i> impose (or independent directions it can reach).
      <br><b>Nullity ($n-r$):</b> Number of free variables. This measures the dimension of the solution space for $A\mathbf{x}=0$.
      <br><b>Consequence:</b> You cannot map a high-dimensional space injectively into a low-dimensional one (nullity must be positive), nor surjectively map a low-dimensional space onto a high-dimensional one.</p>

      <h3>2.3 The Four Fundamental Subspaces: Invariant Anatomy</h3>
      <p>Every linear map $A: \mathbb{R}^n \to \mathbb{R}^m$ induces four intrinsic subspaces. These are not artifacts of calculation; they are the <b>invariant anatomy</b> of the map. They live in two different ambient spaces ($\mathbb{R}^n$ and $\mathbb{R}^m$) and are paired by orthogonality.</p>

      <h4>The Map Spaces (Primal)</h4>
      <ul>
        <li><b>Column Space ($\mathcal{R}(A)$):</b> The image of the map $T_A$.
          $$ \mathcal{R}(A) = \{A\mathbf{x} : \mathbf{x} \in \mathbb{R}^n\} \subseteq \mathbb{R}^m $$
          It answers: <i>Which vectors in the output space are reachable?</i>
        </li>
        <li><b>Nullspace ($\mathcal{N}(A)$):</b> The kernel of the map $T_A$.
          $$ \mathcal{N}(A) = \{\mathbf{x} \in \mathbb{R}^n : A\mathbf{x} = 0\} \subseteq \mathbb{R}^n $$
          It answers: <i>Which input directions are annihilated by the map?</i>
        </li>
      </ul>

      <h4>The Dual Spaces (Transpose)</h4>
      <ul>
        <li><b>Row Space ($\mathcal{R}(A^\top)$):</b> The image of the dual map $A^\top$.
          $$ \mathcal{R}(A^\top) = \{A^\top \mathbf{y} : \mathbf{y} \in \mathbb{R}^m\} \subseteq \mathbb{R}^n $$
          It answers: <i>Which linear constraints are actually active?</i> Each row represents a measurement direction; dependencies among rows mean redundant measurements.
        </li>
        <li><b>Left Nullspace ($\mathcal{N}(A^\top)$):</b> The kernel of the dual map.
          $$ \mathcal{N}(A^\top) = \{\mathbf{y} \in \mathbb{R}^m : A^\top \mathbf{y} = 0\} \subseteq \mathbb{R}^m $$
          It answers: <i>Which combinations of equations sum to $0=0$?</i> A non-zero vector here represents a dependency among the constraints.
        </li>
      </ul>

      <table class="data-table">
        <thead>
          <tr>
            <th>Space</th>
            <th>Definition</th>
            <th>Lives in</th>
            <th>Geometric Meaning</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Column Space</td>
            <td>$\mathrm{Im}(A)$</td>
            <td>$\mathbb{R}^m$</td>
            <td>Reachable outputs (Consistency)</td>
          </tr>
          <tr>
            <td>Null Space</td>
            <td>$\mathrm{Ker}(A)$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Invisible inputs (Non-uniqueness)</td>
          </tr>
          <tr>
            <td>Row Space</td>
            <td>$\mathrm{Im}(A^\top)$</td>
            <td>$\mathbb{R}^n$</td>
            <td>Active measurement directions</td>
          </tr>
          <tr>
            <td>Left Null Space</td>
            <td>$\mathrm{Ker}(A^\top)$</td>
            <td>$\mathbb{R}^m$</td>
            <td>Redundant constraints</td>
          </tr>
        </tbody>
      </table>

      <h3>2.4 Orthogonality Relations (The Fundamental Theorem)</h3>
      <p>The four subspaces are not independent. They are locked together by orthogonality. This is the geometric core of the Fundamental Theorem of Linear Algebra.</p>

      <div class="theorem-box">
        <h4>Theorem: Orthogonal Complements</h4>
        <p>
        1. <b>In the Domain $\mathbb{R}^n$:</b> The nullspace is the orthogonal complement of the row space.
        $$ \mathcal{N}(A) = \mathcal{R}(A^\top)^\perp $$
        2. <b>In the Codomain $\mathbb{R}^m$:</b> The left nullspace is the orthogonal complement of the column space.
        $$ \mathcal{N}(A^\top) = \mathcal{R}(A)^\perp $$
        </p>
      </div>

      <div class="proof-box">
        <h4>Proof: $\mathcal{N}(A) = \mathcal{R}(A^\top)^\perp$</h4>
        <p>We prove this by showing double inclusion.</p>
        <div class="proof-step">
          <strong>Step 1: $\mathcal{N}(A) \subseteq \mathcal{R}(A^\top)^\perp$.</strong>
          Let $\mathbf{x} \in \mathcal{N}(A)$. Then $A\mathbf{x} = 0$.
          Write the rows of $A$ as $r_1^\top, \dots, r_m^\top$. Then $A\mathbf{x}=0$ means $r_i^\top \mathbf{x} = 0$ for all $i$.
          Since $\mathbf{x}$ is orthogonal to every row, it is orthogonal to any linear combination of rows.
          Thus $\mathbf{x} \perp \mathcal{R}(A^\top)$.
        </div>
        <div class="proof-step">
          <strong>Step 2: $\mathcal{R}(A^\top)^\perp \subseteq \mathcal{N}(A)$.</strong>
          Let $\mathbf{x} \in \mathcal{R}(A^\top)^\perp$. By definition, $\mathbf{x}$ is orthogonal to every vector in the row space, including each row $r_i$.
          So $r_i^\top \mathbf{x} = 0$ for all $i$.
          Stacking these equations gives $A\mathbf{x} = 0$, so $\mathbf{x} \in \mathcal{N}(A)$.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> The sets are identical. This geometric fact ($\perp$) implies the algebraic fact that constraints kill degrees of freedom.
        </div>
      </div>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/linear-subspaces.svg"
             alt="Visual representation of intersecting planes showing linear subspaces"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 4:</i> The orthogonal decomposition of the domain and codomain.</figcaption>
      </figure>

      <h4>The Fredholm Alternative (Theorem of Alternatives)</h4>
      <p>A crucial consequence of the orthogonality $\mathcal{R}(A) = \mathcal{N}(A^\top)^\perp$ is a solvability criterion for linear systems, known as the <b>Fredholm Alternative</b>. For any $A \in \mathbb{R}^{m \times n}$ and $\mathbf{b} \in \mathbb{R}^m$, exactly one of the following two statements is true:</p>
      <ol>
        <li><b>Primal Feasibility:</b> The system $A\mathbf{x} = \mathbf{b}$ has a solution.</li>
        <li><b>Dual Certificate of Infeasibility:</b> There exists a vector $\mathbf{y}$ such that $A^\top \mathbf{y} = 0$ but $\mathbf{b}^\top \mathbf{y} \neq 0$.</li>
      </ol>
      <p><i>Interpretation:</i> If $A\mathbf{x}=\mathbf{b}$ has no solution, then $\mathbf{b}$ does not lie in $\mathcal{R}(A)$. Since $\mathcal{R}(A)^\perp = \mathcal{N}(A^\top)$, there must be a component of $\mathbf{b}$ that lies in $\mathcal{N}(A^\top)$. This component $\mathbf{y}$ serves as a witness (or certificate) that proving $\mathbf{b}$ is unreachable by $A$. This logic generalizes to <b>Farkas' Lemma</b> and strong duality in later lectures.</p>

      <div class="example">
        <h4>Geometric Example: $\mathbb{R}^3$</h4>
        <p>Let $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}$.
        <br><b>Column Space $\mathcal{R}(A)$:</b> The span of $(1,0,0)^\top$ and $(0,1,0)^\top$, i.e., the $xy$-plane ($\mathbf{z}=0$).
        <br><b>Left Nullspace $\mathcal{N}(A^\top)$:</b> We solve $A^\top \mathbf{y} = 0$.
        $$ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \implies y_1 = 0, y_2 = 0 $$
        So $\mathbf{y} = (0, 0, y_3)^\top$. This is the $\mathbf{z}$-axis.
        <br><b>Orthogonality:</b> The $xy$-plane is clearly orthogonal to the $\mathbf{z}$-axis.</p>
      </div>

      <h3>Rank and Rank–Nullity Theorem</h3>
      <p>$\mathrm{rank}(A) = \dim \mathcal{R}(A)$. For $A \in \mathbb{R}^{m \times n}$:
      $$ \dim \mathcal{N}(A) + \mathrm{rank}(A) = n $$
      This fundamental result relates the dimensions of the domain's subspaces to the rank of the mapping.</p>

      <div class="proof-box">
        <h4>Proof of the Rank-Nullity Theorem</h4>

        <div class="proof-step">
          <strong>Step 1: Construct a basis for the Nullspace.</strong> Let $k = \dim(\mathcal{N}(A))$ be the nullity. Let $\{v_1, \dots, v_k\}$ be a basis for the nullspace $\mathcal{N}(A)$. These are $k$ linearly independent vectors in $\mathbb{R}^n$ such that $Av_i = 0$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Extend to a basis for $\mathbb{R}^n$.</strong> By the Basis Extension Theorem, we can always extend a linearly independent set (like the nullspace basis) to a full basis for the vector space. We add $r = n-k$ vectors $\{w_1, \dots, w_r\}$ such that the full set $B = \{v_1, \dots, v_k, w_1, \dots, w_r\}$ is a basis for $\mathbb{R}^n$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Analyze the image of the basis vectors.</strong> Consider the image of an arbitrary vector $\mathbf{x} \in \mathbb{R}^n$. Since $B$ is a basis, we can write $\mathbf{x}$ uniquely as:
          $$ \mathbf{x} = \sum_{i=1}^k c_i v_i + \sum_{j=1}^{r} d_j w_j $$
          Applying the linear map $A$:
          $$ A\mathbf{x} = \sum_{i=1}^k c_i \underbrace{A v_i}_{0} + \sum_{j=1}^{r} d_j A w_j = \sum_{j=1}^{r} d_j A w_j $$
          This equation shows that any vector in the column space ($A\mathbf{x}$) can be written as a linear combination of the vectors $\{Aw_1, \dots, Aw_r\}$. Thus, these $r$ vectors span the column space $\mathcal{R}(A)$.
        </div>

        <div class="proof-step">
          <strong>Step 4: Prove Linear Independence in the Range.</strong> To verify that $\{Aw_1, \dots, Aw_r\}$ form a basis for $\mathcal{R}(A)$, we must show they are linearly independent. Suppose we have a linear combination summing to zero:
          $$ \sum_{j=1}^{r} \alpha_j A w_j = 0 $$
          By linearity, this implies $A \left( \sum_{j=1}^{r} \alpha_j w_j \right) = 0$.
          Let $\mathbf{u} = \sum_{j=1}^{r} \alpha_j w_j$. Since $A\mathbf{u} = 0$, the vector $\mathbf{u}$ must lie in the nullspace $\mathcal{N}(A)$.
        </div>

        <div class="proof-step">
          <strong>Step 5: Prove Independence via Basis Uniqueness.</strong> Since $\mathbf{u} \in \mathcal{N}(A)$, it can be written as a linear combination of the nullspace basis $\{v_i\}$:
          $$ \mathbf{u} = \sum_{i=1}^k \beta_i v_i $$
          Equating the two expressions for $\mathbf{u}$:
          $$ \sum_{j=1}^{r} \alpha_j w_j = \sum_{i=1}^k \beta_i v_i \quad \implies \quad \sum_{j=1}^{r} \alpha_j w_j - \sum_{i=1}^k \beta_i v_i = 0 $$
          This equation is a linear combination of the full basis set $\{v_1, \dots, v_k, w_1, \dots, w_r\}$ summing to zero. Since these vectors form a basis for $\mathbb{R}^n$, they are linearly independent, implying all coefficients must be zero. Specifically, $\alpha_j = 0$ for all $j=1, \dots, r$.
          <br>Since the only solution to $\sum \alpha_j A w_j = 0$ is the trivial solution $\alpha = 0$, the set $\{Aw_1, \dots, Aw_r\}$ is linearly independent.
        </div>

        <div class="proof-step">
          <strong>Step 6: Count Dimensions.</strong> Since $\{Aw_1, \dots, Aw_r\}$ is a linearly independent spanning set for $\mathcal{R}(A)$, it is a basis.
          Thus, the dimension of the column space (rank) is exactly $r$.
          From Step 2, we defined $r = n - k$. Rearranging gives the theorem:
          $$ \dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A)) = r + k = n $$
        </div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Rank & Nullspace</h3>
        <p><b>Explore the Four Fundamental Subspaces:</b> Define a 2×3 or 3×2 matrix and visualize how its column space, row space, nullspace, and left nullspace relate to each other. Try creating rank-deficient matrices (e.g., with linearly dependent columns) to see the Rank-Nullity Theorem in action—observe how the dimensions of the nullspace and column space always sum to the number of columns.</p>
        <p><i>Key insight:</i> The orthogonality relationships between these subspaces are fundamental to understanding linear transformations and will appear throughout convex optimization.</p>
        <div id="widget-rank-nullspace" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 3: ALGEBRAIC INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>3. Algebraic Invariants: Determinant, Trace, and Eigenvalues</h2>

      <p>While a matrix $A$ represents a linear map in a specific basis, we are often interested in properties that are intrinsic to the map itself, independent of the basis. These are called <b>invariants</b>.</p>

      <h3>3.1 The "Big Three" Invariants</h3>
      <p>For a square matrix $A \in \mathbb{R}^{n \times n}$ with eigenvalues $\lambda_1, \dots, \lambda_n$ (counted with algebraic multiplicity):</p>
      <ul>
        <li><b>Trace:</b> $\mathrm{tr}(A) = \sum_{i=1}^n A_{ii} = \sum_{i=1}^n \lambda_i$.</li>
        <li><b>Determinant:</b> $\det(A) = \prod_{i=1}^n \lambda_i$.</li>
        <li><b>Eigenvalues:</b> Roots of the characteristic polynomial $p_A(\lambda) = \det(A - \lambda I)$.</li>
      </ul>

      <div class="insight">
        <h4>Geometric Interpretation</h4>
        <ul>
          <li><b>Determinant as Volume:</b> $|\det(A)|$ is the factor by which the linear map scales volume. If $S$ is a set with volume $V$, then $A(S)$ has volume $|\det(A)| V$. The sign indicates orientation (preservation vs. reversal).</li>
          <li><b>Trace as Derivative:</b> The trace is the "infinitesimal change in volume." Specifically, $\det(I + \epsilon A) \approx 1 + \epsilon \mathrm{tr}(A)$ for small $\epsilon$. This connects the trace to the derivative of the determinant, a fact we used in the matrix calculus derivation.</li>
          <li><b>Eigenvalues as Stretch Factors:</b> If $A\mathbf{x} = \lambda \mathbf{x}$, the map scales the vector $\mathbf{x}$ by $\lambda$. If a matrix has a full set of eigenvectors, it maps a unit circle to an ellipse whose semiaxes are aligned with the eigenvectors and have lengths $|\lambda_i|$.</li>
        </ul>
      </div>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/determinant-volume-scaling.png"
             alt="A unit square mapped by a linear transformation to a parallelogram with area |det(A)|"
             style="max-width: 90%; height: auto; border-radius: 8px;" />
        <figcaption><i>Figure 5:</i> Determinant as volume scaling: a unit square maps to a parallelogram of area $|\det(A)|$; the sign of $\det(A)$ tracks whether orientation is preserved or flipped.</figcaption>
      </figure>


      <h3>3.2 The "Isotropic Scaling" Lemma</h3>
      <p>While eigenvalues describe the behavior of $A$ along specific directions, sometimes the matrix acts uniformly in all directions. A classic result connects this geometric isotropy to the algebraic structure of the matrix. We ask: what if <i>every</i> vector is an eigenvector?</p>
      <div class="theorem-box">
        <h4>Lemma: Scalar Matrices</h4>
        <p>If every non-zero vector $\mathbf{x} \in \mathbb{R}^n$ is an eigenvector of $A$ (i.e., $A\mathbf{x} = \lambda(\mathbf{x})\mathbf{x}$), then $A$ must be a scalar multiple of the identity: $A = \lambda I$.</p>
      </div>
      <div class="proof-box">
        <h4>Proof of the Isotropic Scaling Lemma</h4>
        <div class="proof-step">
            <strong>Step 1: Linearly Independent Vectors Share Eigenvalues.</strong>
            Let $\mathbf{u}, \mathbf{v}$ be linearly independent vectors. By assumption, they are eigenvectors: $A\mathbf{u} = \lambda_u \mathbf{u}$ and $A\mathbf{v} = \lambda_v \mathbf{v}$.
            Consider their sum $\mathbf{w} = \mathbf{u}+\mathbf{v}$. Since every vector is an eigenvector, $\mathbf{w}$ must also be one: $A(\mathbf{u}+\mathbf{v}) = \lambda_w (\mathbf{u}+\mathbf{v})$.
            By linearity:
            $$ \lambda_w \mathbf{u} + \lambda_w \mathbf{v} = \lambda_w(\mathbf{u}+\mathbf{v}) = A(\mathbf{u}+\mathbf{v}) = A\mathbf{u} + A\mathbf{v} = \lambda_u \mathbf{u} + \lambda_v \mathbf{v} $$
            Rearranging terms to group by vector:
            $$ (\lambda_w - \lambda_u)\mathbf{u} + (\lambda_w - \lambda_v)\mathbf{v} = 0 $$
            Since $\mathbf{u}$ and $\mathbf{v}$ are linearly independent, the only solution to this linear combination is the trivial one. Thus, their coefficients must vanish:
            $$ \lambda_w - \lambda_u = 0 \quad \text{and} \quad \lambda_w - \lambda_v = 0 $$
            This implies $\lambda_u = \lambda_w = \lambda_v$. Conclusion: Any two linearly independent vectors must correspond to the same eigenvalue.
        </div>
        <div class="proof-step">
            <strong>Step 2: Linearly Dependent Vectors Share Eigenvalues.</strong>
            Suppose $\mathbf{v}$ and $\mathbf{u}$ are linearly dependent, meaning $\mathbf{v} = cu$ for some scalar $c \neq 0$. We cannot use the independence argument directly on the pair $(\mathbf{u}, \mathbf{v})$.
            <br>However, assuming the dimension $n \ge 2$, the space is not just a line. We can choose a third vector $\mathbf{z}$ that is linearly independent of $\mathbf{u}$. Since $\mathbf{v}$ is a multiple of $\mathbf{u}$, $\mathbf{z}$ is also linearly independent of $\mathbf{v}$.
            <br>Applying the result from Step 1 to the pair $(\mathbf{u}, \mathbf{z})$, we get $\lambda_u = \lambda_z$.
            <br>Applying it to the pair $(\mathbf{v}, \mathbf{z})$, we get $\lambda_v = \lambda_z$.
            <br>By transitivity, $\lambda_u = \lambda_v$.
        </div>
        <div class="proof-step">
            <strong>Step 3: Conclusion.</strong>
            Since all non-zero vectors share the same eigenvalue $\lambda$, we have $A\mathbf{x} = \lambda \mathbf{x}$ for all $\mathbf{x}$. This is exactly the definition of the scalar matrix $A = \lambda I$.
        </div>
      </div>
      <p><b>Geometric Intuition:</b> A general symmetric matrix maps the unit sphere to an ellipsoid. The eigenvectors correspond to the axes of the ellipsoid. If <i>every</i> vector is an eigenvector, then every direction is a principal axis. The only ellipsoid with this symmetry is a sphere (isotropic scaling).</p>

      <h3>3.3 Algebraic Properties</h3>

      <h4>Linearity of Trace</h4>
      <p>The trace is a linear functional:
      $$ \mathrm{tr}(A + B) = \mathrm{tr}(A) + \mathrm{tr}(B) \quad \text{and} \quad \mathrm{tr}(cA) = c\,\mathrm{tr}(A). $$
      </p>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>By definition, $\mathrm{tr}(A) = \sum_{i=1}^n a_{ii}$. Then:</p>
        $$ \mathrm{tr}(A+B) = \sum_{i=1}^n (a_{ii} + b_{ii}) = \sum_{i=1}^n a_{ii} + \sum_{i=1}^n b_{ii} = \mathrm{tr}(A) + \mathrm{tr}(B) $$
      </div>

      <h4>Multiplicativity of Determinant</h4>
      <p>The determinant is multiplicative:
      $$ \det(AB) = \det(A)\det(B). $$
      This reflects the geometric fact that the volume scaling of a composite map is the product of the individual scalings. Note that $\det(AB) = \det(BA)$ even if $AB \neq BA$.
      </p>

      <h4>Trace of a Product (Cyclic Property)</h4>
      <p>In general, $\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$. However, the trace is <b>cyclic</b>:
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA). $$
      More generally, $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) = \mathrm{tr}(CAB)$. This property is crucial for deriving matrix gradients.
      </p>

      <div class="proof-box">
        <h4>Proof: $\mathrm{tr}(AB) = \mathrm{tr}(BA)$</h4>
        <div class="proof-step">
          <strong>Coordinate Definition:</strong> Let $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times m}$. The diagonal elements of the product $AB$ are:
          $$ (AB)_{ii} = \sum_{k=1}^n a_{ik} b_{ki} $$
        </div>
        <div class="proof-step">
          <strong>Summing the diagonal:</strong>
          $$ \mathrm{tr}(AB) = \sum_{i=1}^m (AB)_{ii} = \sum_{i=1}^m \sum_{k=1}^n a_{ik} b_{ki} $$
        </div>
        <div class="proof-step">
          <strong>Switching order:</strong> Since the sums are finite, we can swap the order of summation. We also commute the scalar terms $a_{ik} b_{ki} = b_{ki} a_{ik}$:
          $$ \sum_{i=1}^m \sum_{k=1}^n a_{ik} b_{ki} = \sum_{k=1}^n \sum_{i=1}^m b_{ki} a_{ik} $$
        </div>
        <div class="proof-step">
          <strong>Recognize the diagonal of BA:</strong> The inner sum $\sum_{i=1}^m b_{ki} a_{ik}$ is exactly the element $(BA)_{kk}$ (the $k$-th diagonal entry of the matrix product $BA$).
          $$ \sum_{k=1}^n \left( \sum_{i=1}^m b_{ki} a_{ik} \right) = \sum_{k=1}^n (BA)_{kk} $$
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong> The sum of the diagonal entries is the trace.
          $$ \sum_{k=1}^n (BA)_{kk} = \mathrm{tr}(BA) $$
        </div>
      </div>


      <div class="example">
        <h4>Worked Example: Trace and Determinant Caveats</h4>
        <p>While $\mathrm{tr}(A+B) = \mathrm{tr}(A) + \mathrm{tr}(B)$, other properties are subtler.</p>

        <p><b>1. Trace of Product:</b> $\mathrm{tr}(AB) \neq \mathrm{tr}(A)\mathrm{tr}(B)$.
        <br>Let $A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and $B = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$.
        <br>$\mathrm{tr}(A)=1, \mathrm{tr}(B)=1 \implies \mathrm{tr}(A)\mathrm{tr}(B)=1$.
        <br>But $AB = 0$, so $\mathrm{tr}(AB) = 0$. Thus multiplicativity fails.</p>

        <p><b>2. Determinant of Sum:</b> $\det(A+B) \neq \det(A) + \det(B)$.
        <br>A nice conceptual counterexample: Take $A = I$ and $B = -I$ (for even $n$).
        <br>$\det(A) = 1$, $\det(B) = (-1)^n = 1$.
        <br>$\det(A+B) = \det(0) = 0$.
        <br>But $\det(A) + \det(B) = 1 + 1 = 2 \neq 0$.
        <br>This inequality is true generically; $\det$ is multiplicative, not additive.</p>
      </div>

      <h3>3.4 Similarity and Basis Independence</h3>
      <p>Two matrices $A$ and $B$ are <b>similar</b> if $B = P A P^{-1}$ for some invertible $P$. This represents the same linear operator in a different basis. The mechanics of this coordinate transformation and its connection to diagonalization are covered in depth in <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a>.</p>

      <div class="theorem-box">
        <h4>Theorem: Invariance under Similarity</h4>
        <p>If $A$ and $B$ are similar, they share the same algebraic invariants:</p>
        <ul>
          <li>$\det(PAP^{-1}) = \det(P)\det(A)\det(P^{-1}) = \det(A)$.</li>
          <li>$\mathrm{tr}(PAP^{-1}) = \mathrm{tr}(AP^{-1}P) = \mathrm{tr}(A)$.</li>
          <li>$p_{PAP^{-1}}(\lambda) = \det(PAP^{-1} - \lambda I) = \det(P(A-\lambda I)P^{-1}) = p_A(\lambda)$.</li>
        </ul>
        <p>Thus, they have the <b>same eigenvalues</b>.</p>
      </div>

      <h3>3.5 Spectral Shift</h3>
      <p>Adding a multiple of the identity shifts the eigenvalues but preserves eigenvectors.
      If $A\mathbf{x} = \lambda \mathbf{x}$, then:
      $$ (A + tI)\mathbf{x} = A\mathbf{x} + tI\mathbf{x} = (\lambda + t)\mathbf{x}. $$
      Thus, the eigenvalues of $A+tI$ are $\{\lambda_i + t\}$.
      <br><b>Warning:</b> In general, $\det(A+B) \neq \det(A) + \det(B)$. For example, $\det(I+I) = 2^n \neq 1+1$.
      </p>
    </section>


    <section class="section-card" id="section-norms">
      <h2>4. Inner Products & Norms: Geometry from Algebra</h2>
      <p>Inner products and norms bridge the gap between abstract vector spaces and geometry. They allow us to talk about <b>angles</b>, <b>lengths</b>, <b>distances</b>, and <b>projections</b>. Without these, a vector space is just a "floppy" elastic sheet; with them, it becomes a rigid structure capable of supporting optimization.</p>

      <h3>4.1 Inner Products: The Source of Angles</h3>
      <p>An <b>inner product</b> on a real vector space $V$ is a function $\langle \cdot, \cdot \rangle: V \times V \to \mathbb{R}$ satisfying three axioms for all $x, y, z \in V$ and $\alpha \in \mathbb{R}$:</p>
      <ol>
        <li><b>Symmetry:</b> $\langle x, y \rangle = \langle y, x \rangle$.</li>
        <li><b>Linearity (in first argument):</b> $\langle \alpha x + y, z \rangle = \alpha \langle x, z \rangle + \langle y, z \rangle$. (Combined with symmetry, this implies bilinearity).</li>
        <li><b>Positive Definiteness:</b> $\langle x, x \rangle \ge 0$, and $\langle x, x \rangle = 0 \iff x = 0$.</li>
      </ol>

      <h4>Verification for the Dot Product</h4>
      <p>For $\langle x,y \rangle = x^\top y$:
      <br>1. <b>Symmetry:</b> $\sum x_i y_i = \sum y_i x_i$.
      <br>2. <b>Linearity:</b> $\sum (\alpha x_i + \beta z_i) y_i = \alpha \sum x_i y_i + \beta \sum z_i y_i$.
      <br>3. <b>Positivity:</b> $\sum x_i^2 \ge 0$. Sum of squares is zero iff every $x_i=0$.
      <br>Thus, the dot product transforms $\mathbb{R}^n$ into a <b>Euclidean Space</b>. Any symmetric positive definite matrix $M$ defines a generalized inner product $\langle x, y \rangle_M = x^\top M y$.</p>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/vector-addition-parallelogram.png"
             alt="Parallelogram law for vector addition"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 12px; background: white;" />
        <figcaption><i>Figure 3:</i> The parallelogram law, an identity satisfied only by norms induced by inner products.</figcaption>
      </figure>

      <h3>4.2 Geometry Extracted from Axioms</h3>
      <p>Once you have an inner product, you get geometry for free:</p>

      <h4>Proof of Norm Properties</h4>
      <ol>
        <li><b>Definiteness:</b> $\|x\|=0 \iff \sqrt{\langle x,x \rangle} = 0 \iff \langle x,x \rangle = 0 \iff x=0$ (by inner product axioms).</li>
        <li><b>Homogeneity:</b> $\|\alpha x\|^2 = \langle \alpha x, \alpha x \rangle = \alpha^2 \langle x,x \rangle$. Taking square roots gives $\sqrt{\alpha^2}\|x\| = |\alpha| \|x\|$.</li>
        <li><b>Triangle Inequality:</b> $\|x+y\| \le \|x\| + \|y\|$. This requires the Cauchy-Schwarz inequality.</li>
      </ol>

        <h4>The Cauchy-Schwarz Inequality (Rigorous Proof)</h4>
        <p><b>Theorem:</b> $|\langle x, y \rangle| \le \|x\| \|y\|$. Equality holds iff $x$ and $y$ are collinear.
        <br><i>Proof:</i> Consider the function $\phi(t) = \|x - ty\|^2 = \langle x - ty, x - ty \rangle$. By positive definiteness, $\phi(t) \ge 0$ for all $t$.
        Expanding using bilinearity:
        $$ \phi(t) = \langle x, x \rangle - 2t\langle x, y \rangle + t^2\langle y, y \rangle = \|x\|^2 - 2t\langle x, y \rangle + t^2\|y\|^2 $$
        This is a quadratic polynomial $at^2 + bt + c$ with $a=\|y\|^2, b=-2\langle x,y\rangle, c=\|x\|^2$.
        Since the parabola never dips below zero, its discriminant must be non-positive ($b^2 - 4ac \le 0$).
        $$ (-2\langle x, y \rangle)^2 - 4(\|y\|^2)(\|x\|^2) \le 0 $$
        $$ 4\langle x, y \rangle^2 \le 4\|x\|^2 \|y\|^2 \implies |\langle x, y \rangle| \le \|x\| \|y\| $$
        If the discriminant is 0, the parabola touches 0, meaning $\phi(t)=0$ for some $t$, implying $x=ty$ (collinear).</p>

        <h4>Derivation: Triangle Inequality</h4>
        <p>Now we can prove the final norm axiom.
        $$ \begin{aligned}
        \|x+y\|^2 &= \langle x+y, x+y \rangle = \|x\|^2 + 2\langle x, y \rangle + \|y\|^2 \\
        &\le \|x\|^2 + 2|\langle x, y \rangle| + \|y\|^2 \quad (\text{by Cauchy-Schwarz}) \\
        &\le \|x\|^2 + 2\|x\|\|y\| + \|y\|^2 \\
        &= (\|x\| + \|y\|)^2
        \end{aligned} $$
        Taking square roots yields $\|x+y\| \le \|x\| + \|y\|$.
        <br><b>Conclusion:</b> Every inner product space is a normed space. The converse is false (only norms satisfying the Parallelogram Law come from inner products).
        </p>

          <h4>Proof: Norm Balls are Convex</h4>
          <p>Let $x, y \in B(x_c, r)$ and $\theta \in [0,1]$. Then $\|x - x_c\| \le r$ and $\|y - x_c\| \le r$.</p>
          $$
          \begin{aligned}
          \|(\theta x + (1-\theta)y) - x_c\| &= \|\theta(x - x_c) + (1-\theta)(y - x_c)\| \\
          &\le \theta\|x - x_c\| + (1-\theta)\|y - x_c\| \\
          &\le \theta r + (1-\theta)r = r
          \end{aligned}
          $$
          <p>Thus the convex combination is in the ball.</p>

      <h3>4.3 Norms: The Mental Model and Axioms</h3>

      <h4>The mental model: what structure are norms trying to capture?</h4>
      <p>A norm is a rule that measures "size" of vectors in a way that is compatible with two primitive operations you can’t avoid in linear algebra and optimization:</p>
      <ul>
        <li><b>Scaling:</b> you can multiply a vector by a scalar.</li>
        <li><b>Addition:</b> you can add vectors.</li>
      </ul>
      <p>If "size" is to behave sanely, it must interact with these operations in a controlled way:</p>
      <ul>
        <li><b>Scaling sanity $\Rightarrow$ Homogeneity:</b> Scaling the vector scales its size proportionally.</li>
        <li><b>Addition sanity $\Rightarrow$ Triangle Inequality:</b> The size of a sum is not bigger than the sum of sizes (no shortcuts via detours).</li>
        <li><b>Non-degeneracy sanity $\Rightarrow$ Definiteness:</b> Only the zero vector has zero size.</li>
      </ul>
      <p>These are not arbitrary. They are exactly what you need for:</p>
      <ul>
        <li>"Balls" to be convex (set geometry),</li>
        <li>The norm function to be convex (function geometry),</li>
        <li>Stability/continuity (numerics and optimization),</li>
        <li>A clean link to support functions and dual norms (duality / robust constraints).</li>
      </ul>

      <p>These requirements are formalized in the definition of a norm $\|\cdot\|:\mathbb{R}^n\to\mathbb{R}$:</p>

      <div class="definition-box">
        <h4>(N1) Nonnegativity + Definiteness</h4>
        <p>1. $\|x\| \ge 0$.</p>
        <p>2. $\|x\| = 0 \iff x = 0$.</p>
        <p><i>What it enforces:</i> "Non-negative length" prevents physical nonsense. Definiteness forbids "invisible nonzero vectors"—if a nonzero vector had zero cost, optimization would break.</p>
        <p><i>What happens if you drop the "$\Rightarrow$" direction?</i> If you keep $\|x\|\ge 0$ and $\|0\|=0$ but allow $\|x\|=0$ for some nonzero $x$, you have a <b>seminorm</b>. Example: $\|x\| := |a^\top x|$ is zero for every $x$ orthogonal to $a$. Geometrically: everything on a whole hyperplane is "free."</p>
      </div>

      <div class="definition-box">
        <h4>(N2) Positive Homogeneity</h4>
        <p>$$ \|\alpha x\| = |\alpha| \, \|x\| $$</p>
        <p><i>What it enforces:</i> If you double the vector, you double its size. The absolute value matters: direction reversal ($\alpha < 0$) should not change size.</p>
      </div>

      <div class="definition-box">
        <h4>(N3) Triangle Inequality</h4>
        <p>$$ \|x+y\| \le \|x\| + \|y\| $$</p>
        <p><i>What it enforces:</i> The "no cheating by zig-zagging" law. The straight path is always the shortest. This axiom is the main engine behind convexity.</p>
        <p><i>What breaks if you drop it?</i> You can create "size" functions that are homogeneous and definite but wildly nonconvex, with nonconvex "balls," which instantly kills convex optimization use.</p>
      </div>

      <h3>4.4 Core Lemmas: Reusable Proof Templates</h3>
      <p>From these three axioms, we can derive powerful properties that you will use constantly in proofs.</p>

      <h4>Lemma A: $\|0\|=0$</h4>
      <p>Take any $x$. Then $0 = 0 \cdot x$. By (N2), $\|0\| = \|0 \cdot x\| = |0| \, \|x\| = 0$. This doesn't need to be an axiom; it's a consequence.</p>

      <h4>Lemma 1: Symmetry $\|x\| = \|-x\|$</h4>
      <p>By (N2) with $\alpha = -1$: $\|-x\| = \|(-1)x\| = |-1| \, \|x\| = \|x\|$. Norms are centrally symmetric functions.</p>

      <h4>Lemma 2: Reverse Triangle Inequality</h4>
      <p>$$ \big| \|x\| - \|y\| \big| \le \|x - y\| $$</p>
      <p>This says that the norm function is <b>1-Lipschitz</b> with respect to itself. Norms cannot change faster than the distance they induce.</p>
      <div class="proof-box">
        <h4>Proof of Reverse Triangle Inequality</h4>
        <div class="proof-step">
          <strong>Step 1:</strong> Write $x = (x-y) + y$. Apply Triangle Inequality:
          $$ \|x\| = \|(x-y) + y\| \le \|x-y\| + \|y\| \implies \|x\| - \|y\| \le \|x-y\| $$
        </div>
        <div class="proof-step">
          <strong>Step 2:</strong> Swap $x$ and $y$.
          $$ \|y\| - \|x\| \le \|y-x\| = \|x-y\| $$
        </div>
        <div class="proof-step">
          <strong>Step 3:</strong> Combine.
          $$ -\|x-y\| \le \|x\| - \|y\| \le \|x-y\| \iff \big| \|x\| - \|y\| \big| \le \|x-y\| $$
        </div>
      </div>

      <h4>Lemma 3: "Two-point Jensen" (Convexity Inequality)</h4>
      <p>For $\theta \in [0, 1]$:</p>
      $$ \|\theta x + (1-\theta)y\| \le \theta \|x\| + (1-\theta) \|y\| $$
      <p>This proves that <b>all norms are convex functions</b>. The proof is purely algebraic:</p>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>$$ \|\theta x + (1-\theta)y\| \le \|\theta x\| + \|(1-\theta)y\| \quad \text{(Triangle Inequality)} $$</p>
        <p>$$ = |\theta| \|x\| + |1-\theta| \|y\| \quad \text{(Homogeneity)} $$</p>
        <p>Since $\theta \in [0, 1]$, $|\theta|=\theta$ and $|1-\theta|=1-\theta$. Thus:</p>
        <p>$$ = \theta \|x\| + (1-\theta) \|y\| $$</p>
      </div>

      <h3>4.5 Norms Induce Metrics</h3>
      <p>Given any norm $\|\cdot\|$, we can define a distance function (metric) $d(x, y) := \|x - y\|$. This metric inherits all required properties:</p>
      <ol>
        <li><b>Non-negativity:</b> $d(x,y) \ge 0$, and $0$ iff $x=y$ (from N1).</li>
        <li><b>Symmetry:</b> $d(x,y) = \|x-y\| = \|-(y-x)\| = \|y-x\| = d(y,x)$ (from N2).</li>
        <li><b>Triangle Inequality:</b> $d(x,z) = \|x-z\| = \|(x-y)+(y-z)\| \le \|x-y\| + \|y-z\| = d(x,y) + d(y,z)$ (from N3).</li>
      </ol>
      <p>So norms are not just "sizes"; they generate "distances." This is why we can talk about convergence, limits, and continuity in vector spaces.</p>

      <h3>4.6 Standard Norms ($\ell_2, \ell_1, \ell_\infty$)</h3>
      <p>Not all norms come from inner products. The $\ell_1$ and $\ell_\infty$ norms are crucial in optimization (sparsity, robustness) but do not satisfy the Parallelogram Law.</p>

      <div class="example">
        <h4>Example 1: Euclidean Norm ($\ell_2$)</h4>
        <p>$$ \|x\|_2 = \sqrt{\sum x_i^2} $$</p>
        <ul>
          <li><b>N1 & N2:</b> Trivial from square root properties.</li>
          <li><b>N3 (Triangle Inequality):</b> Follows from the Cauchy-Schwarz inequality ($\mathbf{x}^\top \mathbf{y} \le \|x\|_2 \|y\|_2$).</li>
        </ul>
      </div>

      <div class="example">
        <h4>Example 2: Manhattan Norm ($\ell_1$)</h4>
        <p>$$ \|x\|_1 = \sum |x_i| $$</p>
        <ul>
          <li><b>N1 & N2:</b> Trivial.</li>
          <li><b>N3:</b> Follows from the scalar triangle inequality $|a+b| \le |a| + |b|$ applied coordinate-wise.</li>
        </ul>
      </div>

      <div class="example">
        <h4>Example 3: Chebyshev Norm ($\ell_\infty$)</h4>
        <p>$$ \|x\|_\infty = \max_i |x_i| $$</p>
        <ul>
          <li><b>N1 & N2:</b> Trivial.</li>
          <li><b>N3:</b> $|x_i + y_i| \le |x_i| + |y_i| \le \|x\|_\infty + \|y\|_\infty$. Taking the max over $i$ proves it.</li>
        </ul>
      </div>

      <h3>4.7 Linear Functionals and Riesz Representation</h3>
      <p>A <b>linear functional</b> is a linear map from vector space $V$ to scalars $\mathbb{R}$.
      $$ f: \mathbb{R}^n \to \mathbb{R} $$
      Example: $f(x) = 3x_1 - 5x_2$.
      <br><b>Theorem (Riesz Representation):</b> In finite dimensions, <i>every</i> linear functional can be represented as an inner product with a unique vector $a$.
      $$ f(x) = \langle a, x \rangle $$
      <b>Why this matters:</b> This is why gradients are vectors. The derivative $Df(x)$ is technically a linear functional (it eats a direction and spits out a rate of change). The <b>gradient</b> $\nabla f(x)$ is the unique vector that represents this functional: $Df(x)[v] = \langle \nabla f(x), v \rangle$.</p>

      <h3>4.8 Dual Norms and Polar Sets</h3>
      <ul>
        <li><b>$\ell_1$ (Manhattan):</b> $\|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|$. Unit ball is a diamond (polyhedron).</li>
        <li><b>$\ell_\infty$ (Chebyshev):</b> $\|\mathbf{x}\|_\infty = \max_{i} |x_i|$. Unit ball is a square (polyhedron).</li>
      </ul>

      <figure style="text-align: center;">
        <img src="assets/norm-balls.png"
             alt="Comparison of L1, L2, and Infinity norm unit balls"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 6:</i> The "unit ball" $\{\mathbf{x} \mid \|\mathbf{x}\| \le 1\}$ for the $\ell_1$ (diamond), $\ell_2$ (circle), and $\ell_\infty$ (square) norms. All are convex sets.</figcaption>
      </figure>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: The Geometry of Norms</h3>
        <p><b>Visualize Norm Unit Balls:</b> Explore the unit balls for the $\ell_1$, $\ell_2$, and $\ell_\infty$ norms. Note that the $\ell_1$ ball has "corners" on the axes (promoting sparsity), while $\ell_2$ is isotropic.</p>
        <div id="widget-norm-geometry" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>Dual Norms and Polar Sets</h3>
      <p>The <b>dual norm</b> provides a variational definition of size. For any norm $\|\cdot\|$, its dual $\|\cdot\|_*$ is:
      $$ \|\mathbf{y}\|_* = \sup_{\|\mathbf{x}\| \le 1} \mathbf{x}^\top \mathbf{y} $$
      This measures the maximum "correlation" $\mathbf{y}$ can have with any unit vector.</p>

      <div class="proof-box">
        <h4>Theorem: The Dual Norm is a Norm</h4>
        <div class="proof-step">
          <strong>1. Positive Definiteness:</strong> $\|\mathbf{y}\|_* \ge \mathbf{y}^\top 0 = 0$. If $\|\mathbf{y}\|_*=0$, then $\sup \mathbf{x}^\top \mathbf{y} = 0$. By taking $\mathbf{x} = \mathbf{y}/\|\mathbf{y}\|$ (if norm exists) or coordinate vectors, we see $\mathbf{y}$ must be 0.
        </div>
        <div class="proof-step">
          <strong>2. Homogeneity:</strong> $\|\alpha \mathbf{y}\|_* = \sup_{\|\mathbf{x}\|\le 1} \mathbf{x}^\top (\alpha \mathbf{y}) = \alpha \sup \mathbf{x}^\top \mathbf{y}$ (if $\alpha \ge 0$). Symmetry of the unit ball handles $\alpha < 0$.
        </div>
        <div class="proof-step">
          <strong>3. Triangle Inequality:</strong>
          $$ \|\mathbf{y}+\mathbf{z}\|_* = \sup_{\|\mathbf{x}\|\le 1} (\mathbf{y}+\mathbf{z})^\top \mathbf{x} \le \sup_{\|\mathbf{x}\|\le 1} \mathbf{y}^\top \mathbf{x} + \sup_{\|\mathbf{x}\|\le 1} \mathbf{z}^\top \mathbf{x} = \|\mathbf{y}\|_* + \|\mathbf{z}\|_*. $$
        </div>
      </div>

      <h4>Generalized Hölder's Inequality</h4>
      <p>From the definition, we immediately get the fundamental inequality:
      $$ \mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\| \|\mathbf{y}\|_* $$
      <i>Proof:</i> If $\mathbf{x}=0$, trivial. If $\mathbf{x} \neq 0$, let $\mathbf{u} = \mathbf{x}/\|\mathbf{x}\|$. Then $\|\mathbf{u}\|=1$, so $\mathbf{u}^\top \mathbf{y} \le \|\mathbf{y}\|_* \implies (\mathbf{x}/\|\mathbf{x}\|)^\top \mathbf{y} \le \|\mathbf{y}\|_* \implies \mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\| \|\mathbf{y}\|_*$.</p>

      <h4>Canonical Dual Pairs</h4>
      <ul>
        <li><b>$\ell_2$ is self-dual:</b> $\|\mathbf{y}\|_{2,*} = \|\mathbf{y}\|_2$. (Aligned by $\mathbf{x} = \mathbf{y}/\|\mathbf{y}\|_2$).</li>
        <li><b>$\ell_1$ dual is $\ell_\infty$:</b> $\|\mathbf{y}\|_{1,*} = \|\mathbf{y}\|_\infty$. (Maximize $\mathbf{x}^\top \mathbf{y}$ on diamond $\to$ pick corner).</li>
        <li><b>$\ell_\infty$ dual is $\ell_1$:</b> $\|y\|_{\infty,*} = \|\mathbf{y}\|_1$. (Maximize $\mathbf{x}^\top \mathbf{y}$ on box $\to$ match signs).</li>
      </ul>

      <div class="theorem-box">
        <h4>Lemma: Conjugate of a Norm</h4>
        <p>This lemma is crucial for dual derivations. Let $\|\cdot\|$ be any norm and $\|\cdot\|_*$ its dual norm. The conjugate of the scaled norm $f(\mathbf{x}) = \rho \|\mathbf{x}\|$ is the indicator function of the dual ball:
        $$ (\rho\|\mathbf{x}\|)^*(\mathbf{z}) = I_{\{\|\mathbf{z}\|_* \le \rho\}}(\mathbf{z}) = \begin{cases} 0 & \|\mathbf{z}\|_* \le \rho \\ \infty & \text{otherwise} \end{cases} $$
        <i>Proof:</i> We need to calculate the conjugate of $f(\mathbf{x}) = \rho \|\mathbf{x}\|$, which is $f^*(\mathbf{z}) = \sup_{\mathbf{x}} (\mathbf{z}^\top \mathbf{x} - \rho \|\mathbf{x}\|)$.
        <br><b>Case 1: $\|\mathbf{z}\|_* \le \rho$.</b> By definition of dual norm, $\mathbf{z}^\top \mathbf{x} \le \|\mathbf{z}\|_* \|\mathbf{x}\|$.
        $$ \mathbf{z}^\top \mathbf{x} - \rho \|\mathbf{x}\| \le (\|\mathbf{z}\|_* - \rho) \|\mathbf{x}\| $$
        Since $\|\mathbf{z}\|_* \le \rho$, the coefficient $(\|\mathbf{z}\|_* - \rho)$ is non-positive. Since $\|\mathbf{x}\| \ge 0$, the maximum value is 0 (achieved at $\mathbf{x}=0$).
        <br><b>Case 2: $\|\mathbf{z}\|_* > \rho$.</b> There exists a vector $\mathbf{x}_0$ such that $\|\mathbf{x}_0\| \le 1$ and $\mathbf{z}^\top \mathbf{x}_0 = \|\mathbf{z}\|_*$ (or arbitrarily close).
        Let $\mathbf{x} = t \mathbf{x}_0$ for $t > 0$.
        $$ \mathbf{z}^\top (t \mathbf{x}_0) - \rho \|t \mathbf{x}_0\| = t (\mathbf{z}^\top \mathbf{x}_0 - \rho \|\mathbf{x}_0\|) = t (\|\mathbf{z}\|_* - \rho) $$
        Since $\|\mathbf{z}\|_* > \rho$, this expression goes to $+\infty$ as $t \to \infty$.
        <br>Thus, the conjugate is the indicator of the dual ball.
        </p>
      </div>

      <h4>Polar Sets</h4>
      <p>The <b>polar</b> of a convex set $C$ is $C^\circ = \{\mathbf{y} : \mathbf{y}^\top \mathbf{x} \le 1 \ \forall \mathbf{x} \in C\}$.
      <br>For a unit ball $B = \{\mathbf{x} : \|\mathbf{x}\| \le 1\}$, the polar is exactly the dual unit ball:
      $$ B^\circ = \{\mathbf{y} : \|\mathbf{y}\|_* \le 1\} $$
      This geometric duality underpins the duality of optimization problems.</p>

      <h3>Spectral Norm and Convexity</h3>

      <h4>Induced Matrix Norms</h4>
      <p>The spectral norm is a specific instance of an <b>induced norm</b> (or operator norm). For any vector norms $\|\cdot\|_a$ on the domain and $\|\cdot\|_b$ on the codomain, the induced norm of a matrix $X$ is:
      $$ \|X\|_{a,\mathbf{b}} = \sup_{\mathbf{v} \neq 0} \frac{\|X\mathbf{v}\|_b}{\|\mathbf{v}\|_a} = \sup_{\|\mathbf{v}\|_a = 1} \|X\mathbf{v}\|_b $$
      This measures the maximum amplification of the $\mathbf{b}$-norm of the output relative to the $a$-norm of the input.</p>

      <h4>Common Operator Norms</h4>
      <p>When the domain and codomain are equipped with the same $\ell_p$ norm ($a=\mathbf{b}=\mathbf{p}$), we write $\|X\|_p$.</p>
      <ul>
        <li><b>Spectral Norm ($\mathbf{p}=2$):</b> $\|X\|_2 = \sigma_{\max}(X)$. (Max stretch of Euclidean length).</li>
        <li><b>Max Column Sum Norm ($\mathbf{p}=1$):</b> The operator norm induced by the $\ell_1$ norm is the maximum absolute column sum.
        $$ \|X\|_1 = \max_{j} \sum_{i} |X_{ij}| $$
        <i>Intuition:</i> The unit ball of $\ell_1$ is a diamond with vertices at $e_j$. The maximum stretch occurs at one of these vertices $X e_j$ (the $j$-th column).
        </li>
        <li><b>Max Row Sum Norm ($\mathbf{p}=\infty$):</b> The operator norm induced by the $\ell_\infty$ norm is the maximum absolute row sum.
        $$ \|X\|_\infty = \max_{i} \sum_{j} |X_{ij}| $$
        <i>Intuition:</i> This is the dual of the 1-norm case.
        </li>
      </ul>

      <p><b>Convexity:</b> All induced norms are convex functions of the matrix $X$. This follows because $\|X\|_{a,b} = \sup_{\|\mathbf{u}\|_{\mathbf{b}^*} \le 1, \|\mathbf{v}\|_a \le 1} \mathbf{u}^\top X \mathbf{v}$, which is a pointwise supremum of linear functions.</p>

      <h4>Spectral Norm Details</h4>
      <p>The <b>spectral norm</b> (or operator norm) of a matrix $X \in \mathbb{R}^{m \times n}$ is induced by the Euclidean vector norm:
      $$ \|X\|_2 = \sup_{\mathbf{v} \neq 0} \frac{\|X\mathbf{v}\|_2}{\|\mathbf{v}\|_2} = \sup_{\|\mathbf{v}\|_2=1} \|X\mathbf{v}\|_2 $$
      <p><b>Intuition: Worst-Case Stretch.</b> The operator norm measures the maximum factor by which the matrix $X$ can "stretch" a vector. It corresponds to the "Lipschitz constant" of the linear map. Geometrically, it is the length of the longest semi-axis of the ellipsoid formed by mapping the unit sphere.</p>
      It turns out this equals the maximum singular value:
      $$ \|X\|_2 = \sigma_{\max}(X) = \sqrt{\lambda_{\max}(X^\top X)} $$
      Since $X^\top X$ is always Positive Semidefinite ($\mathbf{v}^\top X^\top X \mathbf{v} = \|X\mathbf{v}\|_2^2 \ge 0$), its eigenvalues are non-negative.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/operator-norm-geometry.png"
             alt="A unit circle mapped to an ellipse illustrating the spectral norm as maximum stretch"
             style="max-width: 90%; height: auto; border-radius: 8px;" />
        <figcaption><i>Figure 7:</i> Operator-norm geometry: $\|A\|_2$ is the maximum stretch of the unit circle under $A$, equal to the longest semi-axis length (the largest singular value).</figcaption>
      </figure>

      <div class="proof-box">
        <h4>Proof: $\|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)}$</h4>
        <div class="proof-step">
          <strong>Step 1: Squared Norm.</strong>
          $$ \|X\|_2^2 = \sup_{\mathbf{v} \neq 0} \frac{\|X\mathbf{v}\|_2^2}{\|\mathbf{v}\|_2^2} = \sup_{\mathbf{v} \neq 0} \frac{\mathbf{v}^\top X^\top X \mathbf{v}}{\mathbf{v}^\top \mathbf{v}} $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Rayleigh Quotient.</strong> The expression $\frac{\mathbf{v}^\top M \mathbf{v}}{\mathbf{v}^\top \mathbf{v}}$ for a symmetric matrix $M$ is the Rayleigh Quotient. By the Spectral Theorem, we can diagonalize $M$ in an orthonormal basis of eigenvectors. The maximum value of the quotient is achieved when $\mathbf{v}$ is the eigenvector corresponding to the largest eigenvalue. Thus, the maximum is exactly $\lambda_{\max}(M)$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          $$ \|X\|_2^2 = \lambda_{\max}(X^\top X) \implies \|X\|_2 = \sqrt{\lambda_{\max}(X^\top X)} $$
        </div>
      </div>

      <p><b>Dual Representation (Crucial for Convexity Proofs):</b> Similar to the max eigenvalue, the spectral norm can be written as a supremum:
      $$ \|X\|_2 = \sup_{\|\mathbf{u}\|_2=1, \|\mathbf{v}\|_2=1} \mathbf{u}^\top X \mathbf{v} $$
      Since $f(X) = \mathbf{u}^\top X \mathbf{v}$ is a <b>linear function</b> of $X$, and $\|X\|_2$ is the supremum of these linear functions, $\|X\|_2$ is a <b>convex function</b>.</p>

      <h3>Matrix Inner Product and Frobenius Norm</h3>
      <p>For matrices $X, Y \in \mathbb{R}^{m \times n}$, the <b>trace inner product</b> (also called the Hilbert-Schmidt inner product) is:
      $$ \langle X, Y \rangle = \mathrm{tr}(X^\top Y) = \sum_{i=1}^m \sum_{j=1}^n X_{ij} Y_{ij} $$
      The <b>Frobenius norm</b> (or Hilbert-Schmidt norm) is $\|X\|_F = \sqrt{\langle X, X \rangle}$.</p>

      <div class="proof-box">
        <h4>Derivation: Trace and Frobenius Norm</h4>
        <p>We show that the Frobenius norm $\|A\|_F = \sqrt{\sum_{i,j} A_{ij}^2}$ is induced by the trace inner product $\langle A, B \rangle = \mathrm{tr}(A^\top B)$.</p>
        <div class="proof-step">
            <strong>Step 1: Compute Diagonal Elements of $A^\top A$.</strong>
            Let $P = A^\top A$. By definition of matrix multiplication, the $(j, j)$ entry is the dot product of the $j$-th row of $A^\top$ and the $j$-th column of $A$.
            Since the $j$-th row of $A^\top$ is the transpose of the $j$-th column of $A$:
            $$ P_{jj} = \sum_{k=1}^m (A^\top)_{jk} A_{kj} = \sum_{k=1}^m A_{kj} A_{kj} = \sum_{k=1}^m A_{kj}^2 $$
            This represents the squared Euclidean norm of the $j$-th column of $A$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Sum the Diagonal (Trace).</strong>
            The trace is the sum of diagonal elements:
            $$ \mathrm{tr}(A^\top A) = \sum_{j=1}^n P_{jj} = \sum_{j=1}^n \left( \sum_{k=1}^m A_{kj}^2 \right) $$
        </div>
        <div class="proof-step">
            <strong>Step 3: Relate to Element-wise Sum.</strong>
            The double sum $\sum_{j=1}^n \sum_{k=1}^m A_{kj}^2$ simply iterates over every entry in the matrix, squaring it and adding it up.
            This is exactly the definition of the squared Frobenius norm:
            $$ \sum_{j=1}^n \sum_{k=1}^m A_{kj}^2 = \sum_{i,j} A_{ij}^2 = \|A\|_F^2 $$
            Thus, $\|A\|_F = \sqrt{\mathrm{tr}(A^\top A)}$.
        </div>
      </div>

      <h4>Submultiplicativity of the Frobenius Norm</h4>
      <p>For compatible matrices $A, B$, we have the inequality $\|AB\|_F \le \|A\|_F \|B\|_F$. This property is crucial for analyzing the convergence of matrix iterations.</p>

      <div class="proof-box">
        <h4>Proof via Column/Row Factorization</h4>
        <p>We prove this inequality by decomposing the matrix multiplication into vector inner products and applying Cauchy-Schwarz.</p>

        <div class="proof-step">
          <strong>Step 1: Setup.</strong>
          Let $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times \mathbf{p}}$. Let $C = AB$.
          <br>Denote the $i$-th <b>column</b> of $A^\top$ (which is the $i$-th row of $A$) as $A_i \in \mathbb{R}^n$.
          <br>Denote the $j$-th <b>column</b> of $B$ as $B^j \in \mathbb{R}^n$.
          <br>The $(i,j)$ entry of $C$ is the inner product: $C_{ij} = \langle A_i, B^j \rangle$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Expand Squared Norm.</strong>
          $$ \|C\|_F^2 = \sum_{i=1}^m \sum_{j=1}^p C_{ij}^2 = \sum_{i=1}^m \sum_{j=1}^p \langle A_i, B^j \rangle^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Apply Cauchy-Schwarz.</strong>
          For each term, $\langle A_i, B^j \rangle^2 \le \|A_i\|_2^2 \|B^j\|_2^2$.
          $$ \|C\|_F^2 \le \sum_{i=1}^m \sum_{j=1}^p \|A_i\|_2^2 \|B^j\|_2^2 $$
        </div>

        <div class="proof-step">
          <strong>Step 4: Factorization Trick.</strong>
          Define $\alpha_i = \|A_i\|_2^2$ and $\beta_j = \|B^j\|_2^2$. The double sum splits because the terms are independent:
          $$ \sum_{i,j} \alpha_i \beta_j = \sum_i \alpha_i \left(\sum_j \beta_j\right) = \left(\sum_i \alpha_i\right) \left(\sum_j \beta_j\right) $$
          Substituting back:
          $$ \sum_{i,j} \|A_i\|_2^2 \|B^j\|_2^2 = \left(\sum_{i=1}^m \|A_i\|_2^2\right) \left(\sum_{j=1}^p \|B^j\|_2^2\right) $$
        </div>

        <div class="proof-step">
          <strong>Step 5: Identify Norms.</strong>
          The sum of squared norms of rows of $A$ is exactly $\|A\|_F^2$.
          The sum of squared norms of columns of $B$ is exactly $\|B\|_F^2$.
          $$ \|AB\|_F^2 \le \|A\|_F^2 \|B\|_F^2 $$
          Taking square roots gives the result: $\boxed{\|AB\|_F \le \|A\|_F \|B\|_F}$.
        </div>
      </div>
    </section>

    <!-- SECTION 5: ORTHOGONALITY -->
    <section class="section-card" id="section-orthogonality">
      <h2>5. Orthogonality and Projections</h2>

      <p>Inner products allow us to define <b>Orthogonality</b>, which is the geometric concept of "perpendicularity". This structure turns the abstract vector space into a tool for optimization, enabling us to solve the problem of finding the "closest" vector in a subspace.</p>

      <h3>5.1 Orthogonality and Complements</h3>
      <p><b>Definition:</b> Two vectors are orthogonal ($u \perp v$) if $\langle u, v \rangle = 0$.
      <br><b>Pythagorean Theorem:</b> If $u \perp v$, then $\|u+v\|^2 = \|u\|^2 + \|v\|^2$.
      <br><i>Proof:</i> $\|u+v\|^2 = \langle u+v, u+v \rangle = \|u\|^2 + 2\langle u, v \rangle + \|v\|^2$. Since the cross term vanishes, the identity holds.</p>

      <h4>Orthogonal Complement ($W^\perp$)</h4>
      <p>For a subspace $W \subseteq V$, the set of all vectors orthogonal to every vector in $W$ is called the orthogonal complement:
      $$ W^\perp := \{v \in V \mid \langle v, w \rangle = 0 \ \forall w \in W\} $$
      <b>Theorem:</b> $W^\perp$ is always a subspace of $V$.
      <br><i>Proof:</i>
      1. <b>Zero:</b> $\langle 0, w \rangle = 0$ for all $w$, so $0 \in W^\perp$.
      2. <b>Additivity:</b> If $v_1, v_2 \in W^\perp$, then $\langle v_1+v_2, w \rangle = \langle v_1, w \rangle + \langle v_2, w \rangle = 0+0=0$.
      3. <b>Scaling:</b> If $v \in W^\perp$, $\langle \alpha v, w \rangle = \alpha \langle v, w \rangle = 0$.
      </p>

      <h3>5.2 Orthogonal Projection: The Closest Point</h3>
      <p>This is the central problem of approximation theory: Given $x \in V$ and a subspace $W$, find the vector $p \in W$ that is closest to $x$.
      $$ \min_{w \in W} \|x - w\| $$
      <b>The Solution (Orthogonal Decomposition):</b> Every vector $x$ can be uniquely decomposed as $x = p + z$, where $p \in W$ and $z \in W^\perp$.
      <br>The vector $p$ is called the <b>orthogonal projection</b> of $x$ onto $W$, denoted $\operatorname{proj}_W(x)$.</p>

      <div class="proof-box">
        <h4>Proof: Optimality of the Projection</h4>
        <p>Why is the projection the minimizer?
        <br>Suppose we have the decomposition $x = p + z$ with $p \in W, z \in W^\perp$. Let $w' \in W$ be any candidate.
        <br>We can write the error vector as:
        $$ x - w' = (p + z) - w' = (p - w') + z $$
        Since $p, w' \in W$, their difference $(p-w') \in W$. By definition, $z \in W^\perp$. Thus $(p-w') \perp z$.
        <br>Applying the Pythagorean Theorem:
        $$ \|x - w'\|^2 = \|(p-w') + z\|^2 = \|p-w'\|^2 + \|z\|^2 $$
        The term $\|z\|^2$ is fixed (depends only on $x$). To minimize the error, we must minimize $\|p-w'\|^2$. Since norms are non-negative, the minimum occurs exactly when $\|p - w'\| = 0$, i.e., $w' = p$.
        <br><b>Conclusion:</b> The unique closest point is the orthogonal projection $p$.</p>
      </div>

      <p><b>Linearity:</b> The map $P: x \mapsto \operatorname{proj}_W(x)$ is a linear operator. It satisfies $P^2=P$ (idempotent) and $P^\top=P$ (self-adjoint).</p>

      <h3>5.3 Orthonormal Bases: Geometry-Compatible Coordinates</h3>
      <p>An <b>orthonormal basis</b> is a coordinate system that preserves lengths, angles, and inner products exactly.
      <br><b>Definition:</b> A basis $\{q_1, \dots, q_n\}$ is orthonormal if $\langle q_i, q_j \rangle = \delta_{ij}$.
      <br><b>Why this is special:</b> In an arbitrary basis, calculating lengths requires a metric tensor ($\mathbf{x}^\top G \mathbf{x}$). In an orthonormal basis, geometry simplifies to the Pythagorean sum of squares.</p>

      <h4>The Expansion Formula (No Solving Required)</h4>
      <p>If $\{q_1, \dots, q_n\}$ is an orthonormal basis for $V$, then for every $\mathbf{v} \in V$:
      $$ \boxed{ \mathbf{v} = \sum_{i=1}^n \langle q_i, \mathbf{v} \rangle q_i } $$
      <b>Proof:</b> Let $\mathbf{v} = \sum c_j q_j$. Take the inner product with $q_k$:
      $$ \langle q_k, \mathbf{v} \rangle = \sum c_j \langle q_k, q_j \rangle = \sum c_j \delta_{kj} = c_k $$
      Thus, coordinates are simply inner products. No linear system needs to be solved.</p>

      <h4>Parseval's Identity</h4>
      <p>$$ \|\mathbf{v}\|^2 = \sum_{i=1}^n |\langle q_i, \mathbf{v} \rangle|^2 $$
      The norm of a vector equals the Euclidean norm of its coordinate vector. This is why orthonormal coordinate changes are <b>isometries</b>.</p>

      <h3>5.4 The Gram-Schmidt Process</h3>
      <p>Gram-Schmidt is not just an algorithm; it is the <b>constructive proof</b> that every finite-dimensional subspace admits an orthonormal basis. It systematically manufactures geometry from algebra.</p>

      <div class="insight">
        <h4>Gram-Schmidt is "Projection Theorem in Disguise"</h4>
        <p>At step $j$, we want a vector in $\operatorname{span}(a_1, \dots, a_j)$ that is orthogonal to $\operatorname{span}(a_1, \dots, a_{j-1})$.
        <br>There is exactly <b>one</b> way to do this:
        <ul>
            <li>Take $a_j$.</li>
            <li>Subtract its <b>orthogonal projection</b> onto the subspace spanned by the previous vectors.</li>
        </ul>
        This residual $u_j = a_j - \operatorname{proj}_{W_{j-1}}(a_j)$ captures exactly the "new information" in $a_j$ orthogonal to what we already know. Normalizing it gives the basis vector.</p>
      </div>

      <p><b>The Idea:</b> Given linearly independent vectors $a_1, \dots, a_k$, we construct orthonormal vectors $q_1, \dots, q_k$ such that for each $j$, $\operatorname{span}(q_1, \dots, q_j) = \operatorname{span}(a_1, \dots, a_j)$.
      <br><b>Step-by-step Construction:</b>
      1. <b>First Vector:</b> $q_1 = a_1 / \|a_1\|$.
      2. <b>Subsequent Vectors:</b> To find $q_j$, take $a_j$ and subtract its projection onto the subspace spanned by the previous vectors $W_{j-1} = \operatorname{span}(q_1, \dots, q_{j-1})$.
      $$ u_j = a_j - \operatorname{proj}_{W_{j-1}}(a_j) = a_j - \sum_{i=1}^{j-1} \langle q_i, a_j \rangle q_i $$
      3. <b>Normalize:</b> $q_j = u_j / \|u_j\|$.
      </p>
      <p>The vector $u_j$ is guaranteed to be non-zero because $a_j$ is linearly independent of the previous vectors. It is orthogonal to all previous $q_i$ by construction.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/gram-schmidt-orthogonalization.png"
             alt="Step-by-step geometric illustration of the Gram-Schmidt orthogonalization process"
             style="max-width: 95%; height: auto; border-radius: 8px;" />
        <figcaption><i>Figure 8:</i> Gram-Schmidt orthogonalization: subtract projections to remove components in previously constructed directions.</figcaption>
      </figure>

      <h3>5.5 Orthogonal Matrices and Isometries</h3>
      <p>A square matrix $Q$ is <b>orthogonal</b> if its columns form an orthonormal basis. This is algebraically equivalent to $Q^\top Q = I$.
      <br><b>Properties of Orthogonal Matrices:</b>
      <ul>
        <li><b>Isometry:</b> $\|Q\mathbf{x}\| = \|\mathbf{x}\|$ and $\langle Q\mathbf{x}, Q\mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{y} \rangle$. They represent rigid motions (rotations and reflections).</li>
        <li><b>Inverse:</b> $Q^{-1} = Q^\top$. This makes solving linear systems trivial: $Q\mathbf{x}=\mathbf{b} \implies \mathbf{x} = Q^\top \mathbf{b}$.</li>
        <li><b>Stability:</b> Multiplication by orthogonal matrices does not amplify numerical errors ($\kappa(Q)=1$).</li>
      </ul>
      </p>

      <h4>The QR Decomposition (Preview)</h4>
      <p>The Gram-Schmidt process can be written in matrix form as $A = QR$.
      <ul>
        <li>$Q$ has orthonormal columns (the geometry).</li>
        <li>$R$ is upper triangular (the coordinates).</li>
      </ul>
      This factorization is the foundation of stable least squares algorithms, covered in depth in <a href="../01-linear-algebra-advanced/index.html">Lecture 01</a>.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Orthogonality & Projections</h3>
        <p><b>Visualize Vector Relationships:</b> Drag two vectors in the 2D plane and observe how their geometric relationships change in real-time. The tool displays:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Dot product ($\mathbf{x}^\top \mathbf{y}$):</b> Becomes zero when vectors are orthogonal (perpendicular)</li>
          <li><b>Angle:</b> Computed using $\cos \theta = \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{x}\|_2 \|\mathbf{y}\|_2}$ via the Cauchy-Schwarz inequality</li>
          <li><b>Orthogonal projection:</b> The shadow of one vector onto another—this is the foundation of least squares!</li>
        </ul>
        <p><i>Key concept:</i> Projection is everywhere in optimization—from computing least squares solutions to understanding constraint gradients. The projection of $\mathbf{b}$ onto the column space of $A$ gives us the best least-squares fit.</p>
        <div id="widget-orthogonality" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- SECTION 6: MATRIX CALCULUS -->
    <section class="section-card" id="section-matrix-calculus">
      <h2>6. Matrix Calculus Basics</h2>
      <p>Gradient-based optimization relies heavily on the computation of gradients and Hessians. Computing derivatives with respect to vectors and matrices is therefore a core skill. Unlike scalar calculus, where order doesn't matter, in matrix calculus we must carefully preserve non-commutativity and track dimensions. The "master key" to matrix calculus is the <b>differential</b>.</p>

      <p>For a function $f: \mathbb{R}^{n \times m} \to \mathbb{R}$, the gradient $\nabla f(X)$ is defined as the unique matrix satisfying the first-order approximation:
      $$ f(X + \Delta) = f(X) + \langle \nabla f(X), \Delta \rangle + o(\|\Delta\|) $$
      where $\langle A, B \rangle = \mathrm{tr}(A^\top B)$ is the standard trace inner product. This definition provides a systematic algorithm for finding gradients:</p>
      <ol>
        <li>Perturb $X$ by a small direction $\Delta$.</li>
        <li>Expand $f(X + \Delta)$ using Taylor series or algebraic properties.</li>
        <li>Isolate the term that is <i>linear</i> in $\Delta$.</li>
        <li>Rewrite that linear term in the form $\langle G, \Delta \rangle = \mathrm{tr}(G^\top \Delta)$. Then $G$ is the gradient.</li>
      </ol>

      <h4>Gradients of Linear and Quadratic Forms</h4>
      <p>For $\mathbf{x} \in \mathbb{R}^n$, $A \in \mathbb{R}^{n \times n}$, and $\mathbf{b} \in \mathbb{R}^n$:</p>
      <table class="data-table">
        <thead>
          <tr>
            <th>Function $f(\mathbf{x})$</th>
            <th>Gradient $\nabla f(\mathbf{x})$</th>
            <th>Hessian $\nabla^2 f(\mathbf{x})$</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>$\mathbf{b}^\top \mathbf{x}$</td>
            <td>$\mathbf{b}$</td>
            <td>$0$</td>
            <td>Linear function</td>
          </tr>
          <tr>
            <td>$\mathbf{x}^\top A \mathbf{x}$</td>
            <td>$(A + A^\top)\mathbf{x}$</td>
            <td>$A + A^\top$</td>
            <td>General quadratic</td>
          </tr>
          <tr>
            <td>$\mathbf{x}^\top A \mathbf{x}$</td>
            <td>$2A\mathbf{x}$</td>
            <td>$2A$</td>
            <td>If $A$ is symmetric ($A=A^\top$)</td>
          </tr>
          <tr>
            <td>$\|Ax - b\|_2^2$</td>
            <td>$2A^\top(A\mathbf{x} - \mathbf{b})$</td>
            <td>$2A^\top A$</td>
            <td>Least Squares objective</td>
          </tr>
        </tbody>
      </table>

      <div class="proof-box">
        <h4>Derivation: Gradient of Quadratic Form</h4>
        <p>Let $f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$. We apply the perturbation method.</p>
        <div class="proof-step">
          <strong>Step 1: Perturb.</strong> Let $\mathbf{x} \to \mathbf{x} + h$.
          $$ f(\mathbf{x}+h) = (\mathbf{x}+h)^\top A (\mathbf{x}+h) = \mathbf{x}^\top A \mathbf{x} + \mathbf{x}^\top A h + h^\top A \mathbf{x} + h^\top A h $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Linearize.</strong>
          Identify terms linear in $h$: $L(h) = \mathbf{x}^\top A h + h^\top A \mathbf{x}$.
          The term $h^\top A h$ is quadratic (second-order) and vanishes as $h \to 0$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Rearrange to Inner Product form.</strong>
          Recall that a scalar equals its transpose: $h^\top A \mathbf{x} = (h^\top A \mathbf{x})^\top = \mathbf{x}^\top A^\top h$.
          Thus, $L(h) = \mathbf{x}^\top A h + \mathbf{x}^\top A^\top h = \mathbf{x}^\top (A + A^\top) h$.
          Rewriting as an inner product:
          $$ L(h) = \langle (A + A^\top)\mathbf{x}, h \rangle $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Identify Gradient.</strong>
          By definition, the vector multiplying $h$ in the inner product is the gradient.
          $$ \nabla f(\mathbf{x}) = (A + A^\top)\mathbf{x} $$
          If $A$ is symmetric, $A = A^\top$, so $\nabla f(\mathbf{x}) = 2A\mathbf{x}$.
        </div>
      </div>

      <h4>Gradients with Respect to Matrices</h4>
      <p>For a matrix variable $X \in \mathbb{R}^{m \times n}$, the gradient is an $m \times n$ matrix. The definition is consistent with the scalar directional derivative.</p>
      <ul>
        <li><b>Rule 1: Linear Trace:</b> $\nabla_X \mathrm{tr}(A^\top X) = A$.
        <br><i>Proof:</i> $\mathrm{tr}(A^\top X) = \langle A, X \rangle$. The function is linear: $f(X+\Delta) = \langle A, X+\Delta \rangle = f(X) + \langle A, \Delta \rangle$. The gradient is the coefficient of $\Delta$, which is $A$.</li>
        <li><b>Rule 2: Quadratic Trace:</b> $\nabla_X \mathrm{tr}(X^\top A X) = (A + A^\top)X$. If $A$ is symmetric, this is $2AX$.
        <br><i>Note:</i> This corresponds to $\frac{d}{dx}(ax^2) = 2ax$. For vector $\mathbf{x}$, $\nabla (\mathbf{x}^\top A\mathbf{x}) = (A+A^\top)\mathbf{x}$. For matrix $X$, the columns transform independently if $A$ is scalar, but coupled if $A$ is a matrix.
        <br><i>Derivation:</i>
        $$ \begin{aligned}
        f(X+\Delta) &= \mathrm{tr}((X+\Delta)^\top A (X+\Delta)) \\
        &= \mathrm{tr}(X^\top AX + \Delta^\top AX + X^\top A\Delta + \Delta^\top A\Delta) \\
        &\approx f(X) + \mathrm{tr}(\Delta^\top AX) + \mathrm{tr}(X^\top A \Delta) \\
        &= f(X) + \mathrm{tr}(\Delta^\top AX) + \mathrm{tr}(\Delta^\top A^\top X) \quad \text{(using } \mathrm{tr}(M)=\mathrm{tr}(M^\top)) \\
        &= f(X) + \mathrm{tr}(\Delta^\top (A+A^\top)X) \\
        &= f(X) + \langle (A+A^\top)X, \Delta \rangle
        \end{aligned} $$
        Thus $\nabla f(X) = (A+A^\top)X$.
        </li>
        <li><b>Rule 3: Log Determinant:</b> $\nabla_X \log \det X = X^{-\top}$. If $X \in \mathbb{S}^n_{++}$ (symmetric positive definite), then $\nabla_X \log \det X = X^{-1}$.
        <br>
        <div class="proof-box">
          <h4>Derivation: Gradient of Log Determinant</h4>
          <p>We derive the gradient of $f(X) = \log \det X$ (defined for $\det X > 0$) by identifying the linear term in the Taylor expansion of $f(X+\Delta)$ around $X$. This step-by-step derivation uses the perturbation method, which is robust and general.</p>
          <div class="proof-step">
            <strong>Step 1: Factor out X.</strong>
            We want to isolate the perturbation $\Delta$. Since $X$ is invertible, we can write $X+\Delta = X(I + X^{-1}\Delta)$.
            $$ \log \det (X + \Delta) = \log \det (X(I + X^{-1}\Delta)) $$
            Using the multiplicative property of the determinant ($\det(AB) = \det(A)\det(B)$) and the additive property of the logarithm ($\log(ab) = \log a + \log \mathbf{b}$):
            $$ = \log (\det X \cdot \det(I + X^{-1}\Delta)) = \log \det X + \log \det(I + X^{-1}\Delta) $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Relate Determinant to Trace via Eigenvalues.</strong>
            Let $E = X^{-1}\Delta$. We need to approximate $\log \det(I + E)$ for small $E$.
            Let $\lambda_i$ be the eigenvalues of $E$. The eigenvalues of $I+E$ are then $1+\lambda_i$.
            $$ \det(I + E) = \prod_{i=1}^n (1 + \lambda_i) $$
            Taking the natural logarithm turns the product into a sum:
            $$ \log \det(I + E) = \sum_{i=1}^n \log(1 + \lambda_i) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Linearize via Taylor Series.</strong>
            We use the scalar Taylor series expansion $\log(1+\mathbf{z}) = \mathbf{z} - \frac{\mathbf{z}^2}{2} + O(\mathbf{z}^3)$ for small $\mathbf{z}$.
            Applying this to each term $\log(1+\lambda_i)$:
            $$ \sum_{i=1}^n \log(1 + \lambda_i) = \sum_{i=1}^n \left( \lambda_i - \frac{\lambda_i^2}{2} + \dots \right) = \sum_{i=1}^n \lambda_i - \frac{1}{2}\sum_{i=1}^n \lambda_i^2 + \dots $$
            The linear term is the sum of eigenvalues, which is the <b>trace</b> of the matrix $E$:
            $$ \sum_{i=1}^n \lambda_i = \mathrm{tr}(E) = \mathrm{tr}(X^{-1}\Delta) $$
            Thus, to first order, $\log \det(I + E) \approx \mathrm{tr}(E)$.
          </div>
          <div class="proof-step">
            <strong>Step 4: Extract the Gradient.</strong>
            Substituting back, the first-order approximation of the function is:
            $$ f(X+\Delta) \approx f(X) + \mathrm{tr}(X^{-1}\Delta) $$
            The gradient $\nabla f(X)$ is defined as the matrix $G$ such that the linear term is $\langle G, \Delta \rangle = \mathrm{tr}(G^\top \Delta)$.
            We rewrite our linear term to match this form:
            $$ \mathrm{tr}(X^{-1}\Delta) = \mathrm{tr}((X^{-\top})^\top \Delta) = \langle X^{-\top}, \Delta \rangle $$
            Comparing terms, we identify $G = X^{-\top}$.
            <br><b>Result:</b> $\nabla_X \log \det X = X^{-\top}$.
            <br>If $X$ is symmetric ($X=X^\top$), then $\nabla f(X) = X^{-1}$.
          </div>
        </div>
        </li>
      </ul>
      <div class="insight">
        <h4>💡 Chain Rule for Matrix Functions</h4>
        <p>If $f(X) = g(h(X))$, then the differential is $df = \mathbf{g}'(h(X)) \circ dh$. For example, to differentiate $\|Ax - b\|_2^2$:</p>
        <ol>
          <li>Let $\mathbf{r} = A\mathbf{x} - \mathbf{b}$. Then $f = \mathbf{r}^\top \mathbf{r}$.</li>
          <li>$df = 2\mathbf{r}^\top dr$.</li>
          <li>$dr = A dx$.</li>
          <li>Substitute: $df = 2\mathbf{r}^\top A dx = (2A^\top \mathbf{r})^\top dx$.</li>
          <li>The gradient is the transpose of the coefficient of $dx^\top$ (or the vector multiplying $dx$), so $\nabla f = 2A^\top \mathbf{r} = 2A^\top(A\mathbf{x}-\mathbf{b})$.</li>
        </ol>
      </div>

      <div class="proof-box">
        <h4>Derivation: Hessian of Least Squares</h4>
        <p>Let $f(\mathbf{x}) = \|A\mathbf{x} - \mathbf{b}\|_2^2$. We show $\nabla^2 f(\mathbf{x}) = 2A^\top A$ and prove its convexity.</p>

        <div class="proof-step">
          <strong>Step 1: Expand.</strong>
          $$ f(\mathbf{x}) = (A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = \mathbf{x}^\top A^\top A \mathbf{x} - 2\mathbf{b}^\top A \mathbf{x} + \mathbf{b}^\top \mathbf{b} $$
        </div>

        <div class="proof-step">
          <strong>Step 2: Gradient.</strong>
          Using the rules derived above:
          <ul>
            <li>$\nabla (\mathbf{x}^\top (A^\top A) \mathbf{x}) = 2A^\top A \mathbf{x}$ (since $A^\top A$ is symmetric).</li>
            <li>$\nabla (-2(A^\top \mathbf{b})^\top \mathbf{x}) = -2A^\top \mathbf{b}$.</li>
            <li>$\nabla (\mathbf{b}^\top \mathbf{b}) = 0$.</li>
          </ul>
          Summing these: $\nabla f(\mathbf{x}) = 2A^\top A \mathbf{x} - 2A^\top \mathbf{b} = 2A^\top (A\mathbf{x} - \mathbf{b})$.
        </div>

        <div class="proof-step">
          <strong>Step 3: Hessian.</strong>
          Differentiating $\nabla f(\mathbf{x})$ with respect to $\mathbf{x}$:
          $$ \nabla^2 f(\mathbf{x}) = \nabla_x (2A^\top A \mathbf{x}) = 2A^\top A $$
          The Hessian is constant, characteristic of quadratic functions.
        </div>

        <div class="proof-step">
          <strong>Step 4: Convexity (PSD Check).</strong>
          For any vector $\mathbf{v} \in \mathbb{R}^n$:
          $$ \mathbf{v}^\top \nabla^2 f(\mathbf{x}) \mathbf{v} = \mathbf{v}^\top (2A^\top A) \mathbf{v} = 2(A\mathbf{v})^\top (A\mathbf{v}) = 2\|A\mathbf{v}\|_2^2 \ge 0 $$
          Since the quadratic form is always non-negative, the Hessian is Positive Semidefinite (PSD). Thus, the least squares objective is always <b>convex</b>.
        </div>
      </div>

      <div class="theorem-box">
        <h4>Lemma: Quadratic Minimization (Completion of the Square)</h4>
        <p>This result is the "workhorse" of convex optimization derivation. Let $H \in \mathbb{S}_{++}^n$ (symmetric positive definite) and $\mathbf{g} \in \mathbb{R}^n$. Then the unique minimizer of the quadratic form:
        $$ \inf_{\mathbf{x}\in\mathbb R^n}\left\{\frac12 \mathbf{x}^\top H\mathbf{x} + \mathbf{g}^\top \mathbf{x}\right\} $$
        is given by $\boxed{\mathbf{x}^\star = -H^{-1}\mathbf{g}}$, and the optimal value is $\boxed{-\frac12 \mathbf{g}^\top H^{-1}\mathbf{g}}$.</p>
        <p><i>Proof:</i> Complete the square. Let $\mathbf{x} = \mathbf{z} - H^{-1}\mathbf{g}$.
        $$ \frac12 (\mathbf{z} - H^{-1}\mathbf{g})^\top H (\mathbf{z} - H^{-1}\mathbf{g}) + \mathbf{g}^\top (\mathbf{z} - H^{-1}\mathbf{g}) $$
        Expanding this yields $\frac12 \mathbf{z}^\top H \mathbf{z} - \frac12 \mathbf{g}^\top H^{-1}\mathbf{g}$. Since $H \succ 0$, the minimum is at $\mathbf{z}=0$.
        </p>
      </div>
    </section>

    <!-- SECTION 7: PSD MATRICES -->
    <section class="section-card" id="section-psd">
      <h2>7. Positive Semidefinite Matrices</h2>

      <h3>7.1 The Space of Symmetric Matrices ($\mathbb{S}^n$)</h3>
      <p>In convex optimization, a central object is the Hessian matrix $\nabla^2 f(\mathbf{x})$. By Schwarz's Theorem, if a function is twice continuously differentiable, its Hessian is symmetric. A matrix $A \in \mathbb{R}^{n \times n}$ is symmetric if $A = A^\top$. The set of such matrices is denoted $\mathbb{S}^n$.</p>

      <h4>The Spectral Theorem</h4>
      <p>If $A \in \mathbb{S}^n$, then:</p>
      <ol>
        <li>All <b>eigenvalues</b> ($\lambda_1, \dots, \lambda_n$) are <b>real numbers</b>.</li>
        <li>There exists a set of <b>orthonormal eigenvectors</b> $q_1, \dots, q_n$.</li>
        <li>$A$ can be diagonalized as $A = Q \Lambda Q^\top$, where $Q$ is an orthogonal matrix ($Q^\top Q = I$) containing the eigenvectors, and $\Lambda$ is a diagonal matrix containing the eigenvalues.</li>
      </ol>

      <h4>Why are Eigenvectors of Symmetric Matrices Orthogonal?</h4>
      <p>The Spectral Theorem guarantees orthogonal eigenvectors for symmetric matrices. The proof relies on the adjoint property.</p>
      <div class="proof-box">
        <h4>Proof of Orthogonality</h4>
        <p>Let $A$ be symmetric ($A^\top = A$). Let $v_1, v_2$ be eigenvectors with distinct eigenvalues $\lambda_1 \neq \lambda_2$.
        $$ A v_1 = \lambda_1 v_1, \quad A v_2 = \lambda_2 v_2 $$
        Consider the inner product $\langle A v_1, v_2 \rangle$. We can evaluate it two ways:</p>
        <ol>
          <li>$\langle A v_1, v_2 \rangle = \langle \lambda_1 v_1, v_2 \rangle = \lambda_1 \langle v_1, v_2 \rangle$</li>
          <li>$\langle A v_1, v_2 \rangle = \langle v_1, A^\top v_2 \rangle = \langle v_1, A v_2 \rangle = \langle v_1, \lambda_2 v_2 \rangle = \lambda_2 \langle v_1, v_2 \rangle$</li>
        </ol>
        <p>Subtracting gives $(\lambda_1 - \lambda_2) \langle v_1, v_2 \rangle = 0$. Since $\lambda_1 \neq \lambda_2$, we must have $\langle v_1, v_2 \rangle = 0$.</p>
      </div>

      <p><b>Implication for convexity:</b> The "shape" of a multivariable function (bowl vs. saddle) is entirely determined by the signs of the eigenvalues of its Hessian.</p>

      <h3>7.2 Positive Semidefinite (PSD) Matrices</h3>
      <p>This definition is central to the course. It provides the matrix equivalent of a "non-negative number" and underpins the definition of <b>Convex Functions</b> (<a href="../05-convex-functions-basics/index.html">Lecture 05</a>) and <b>Semidefinite Programming</b> (<a href="../07-convex-problems-standard/index.html">Lecture 08</a>).</p>

      <div class="insight">
        <h4>Forward Connection: The PSD Cone</h4>
        <p>The set of all PSD matrices forms a convex cone, denoted $\mathbb{S}^n_+$. This geometric object is the foundation of <b>Semidefinite Programming (SDP)</b>, a powerful generalization of linear programming that we will study in <a href="../07-convex-problems-standard/index.html">Lecture 08</a>. As LP optimizes over the non-negative orthant, SDP optimizes over the PSD cone.</p>
      </div>

      <h4>Definition A: The Variational Definition</h4>
      <p>A symmetric matrix $A \in \mathbb{S}^n$ is <a href="#" class="definition-link" data-term="positive semidefinite">Positive Semidefinite (PSD)</a>, denoted as $A \succeq 0$, if:</p>
      $$ \mathbf{x}^\top A \mathbf{x} \ge 0 \quad \text{for all } \mathbf{x} \in \mathbb{R}^n $$
      <p><b>Geometric Intuition (Generalized Non-Negative Numbers):</b> Just as a non-negative number $a \ge 0$ allows us to define convex quadratic functions like $f(\mathbf{x}) = ax^2$, a PSD matrix allows us to define convex quadratic forms in higher dimensions. If $A \succeq 0$, the function $f(\mathbf{x}) = \mathbf{x}^\top A \mathbf{x}$ looks like a bowl. If $A$ has a negative eigenvalue, the function looks like a saddle (curves down in some directions), breaking convexity.</p>
      <p><b>Curvature:</b> If $A$ is the Hessian $\nabla^2 f(\mathbf{x})$, the term $\mathbf{x}^\top A \mathbf{x}$ is the second derivative along the line defined by vector $\mathbf{x}$. $A \succeq 0$ means the function has non-negative curvature in <i>every</i> possible direction.</p>

      <h4>Definition B: The Eigenvalue Definition</h4>
      <p>A symmetric matrix $A \in \mathbb{S}^n$ is PSD if and only if all its eigenvalues are non-negative:</p>
      $$ \lambda_i(A) \ge 0 \quad \text{for all } i = 1, \dots, n $$

      <h4>Factorization of PSD Matrices (Gram Matrix)</h4>
      <p>A symmetric matrix $A$ is PSD if and only if it can be written as a Gram matrix: $A = B^\top B$ for some matrix $B$ (not necessarily square).
      <br><i>Proof:</i> If $A \succeq 0$, spectral decomposition gives $A = Q \Lambda Q^\top = (Q \Lambda^{1/2}) (Q \Lambda^{1/2})^\top$. Let $B = (Q \Lambda^{1/2})^\top$. This $B$ is a "square root" of $A$.
      Conversely, if $A = B^\top B$, then $\mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top B^\top B \mathbf{x} = (B\mathbf{x})^\top (B\mathbf{x}) = \|B\mathbf{x}\|_2^2 \ge 0$.
      </p>

      <div class="proof-box">
        <h4>Proof: Equivalence of Definitions</h4>
        <p>We prove that $(\mathbf{x}^\top A \mathbf{x} \ge 0) \iff (\lambda_i \ge 0)$.</p>

        <div class="proof-step">
          <strong>Step 1: $(\Rightarrow)$</strong> Assume $\mathbf{x}^\top A \mathbf{x} \ge 0$ for all vectors $\mathbf{x}$. Let $\mathbf{v}$ be an eigenvector of $A$ with eigenvalue $\lambda$.
          $$ A \mathbf{v} = \lambda \mathbf{v} $$
          Substitute $\mathbf{x} = \mathbf{v}$ into the variational definition:
          $$ \mathbf{v}^\top A \mathbf{v} = \mathbf{v}^\top (\lambda \mathbf{v}) = \lambda (\mathbf{v}^\top \mathbf{v}) = \lambda \|\mathbf{v}\|_2^2 $$
          Since $\mathbf{x}^\top A \mathbf{x} \ge 0$, we have $\lambda \|\mathbf{v}\|_2^2 \ge 0$. Since eigenvectors are non-zero, $\|v\|_2^2 > 0$. Therefore, <b>$\lambda \ge 0$</b>.
        </div>

        <div class="proof-step">
          <strong>Step 2: $(\Leftarrow)$</strong> Assume all $\lambda_i \ge 0$. Use the Spectral Decomposition $A = Q \Lambda Q^\top$. For any arbitrary vector $\mathbf{x}$:
          $$ \mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top (Q \Lambda Q^\top) \mathbf{x} = (Q^\top \mathbf{x})^\top \Lambda (Q^\top \mathbf{x}) $$
          Let $\mathbf{y} = Q^\top \mathbf{x}$. Then:
          $$ \mathbf{x}^\top A \mathbf{x} = \mathbf{y}^\top \Lambda \mathbf{y} = \sum_{i=1}^n \lambda_i y_i^2 $$
          Since $\lambda_i \ge 0$ and $y_i^2 \ge 0$, the sum is non-negative. Thus <b>$\mathbf{x}^\top A \mathbf{x} \ge 0$</b>.
        </div>
      </div>

      <h3>7.3 Geometry of PSD Matrices: Ellipsoids</h3>
      <p>Positive Definite (PD) matrices define the shape of <b>ellipsoids</b>. For a matrix $P \in \mathbb{S}^n_{++}$, the set:
      $$ \mathcal{E} = \{ \mathbf{x} \in \mathbb{R}^n \mid \mathbf{x}^\top P \mathbf{x} \le 1 \} $$
      is an ellipsoid centered at the origin.
      </p>
      <ul>
        <li><b>Axes alignment:</b> The axes of the ellipsoid are aligned with the eigenvectors of $P$.</li>
        <li><b>Axis lengths:</b> The semi-axis lengths are $1/\sqrt{\lambda_i}$, where $\lambda_i$ are the eigenvalues of $P$. Note the inverse relationship: a <i>large</i> eigenvalue means steep curvature in the quadratic bowl, which corresponds to a <i>short</i> axis in the level set ellipsoid (the function rises quickly).</li>
      </ul>
      <p>This geometric link explains why "conditioning" matters. If the ratio $\lambda_{\max}/\lambda_{\min}$ is large, the ellipsoid is very long and thin (a narrow valley in one direction, flat in another), making optimization difficult.</p>

      <h3>7.4 The Rayleigh Quotient and Eigenvalues</h3>
      <p>The <b>maximum eigenvalue</b> $\lambda_{\max}(A)$ can be defined as an optimization problem:</p>
      $$ \lambda_{\max}(A) = \sup_{\mathbf{x} \neq 0} \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}} = \sup_{\|\mathbf{x}\|_2 = 1} \mathbf{x}^\top A \mathbf{x} $$
      <p>This is the <b>Rayleigh Quotient</b>. This definition proves that $\lambda_{\max}(A)$ is a convex function of $A$. Notice that for a fixed $\mathbf{x}$, the function $g(A) = \mathbf{x}^\top A \mathbf{x}$ is <b>linear</b> in $A$. Since $\lambda_{\max}(A)$ is the <b>pointwise supremum</b> of a family of linear functions (indexed by $\mathbf{x}$), it is a convex function.</p>

      <h3>7.5 The Schur Complement Lemma</h3>
      <p>A powerful tool for handling block matrices in optimization is the Schur Complement. It allows us to convert complicated nonlinear constraints into Linear Matrix Inequalities (LMIs) and provides a systematic way to check positive semidefiniteness.</p>

      <div class="insight">
        <h4>💡 Intuition: Completing the Square</h4>
        <p>The Schur Complement is the matrix generalization of "completing the square". Consider a scalar quadratic form in two variables $\mathbf{x}, \mathbf{y}$:</p>
        $$ q(\mathbf{x}, \mathbf{y}) = ax^2 + 2bxy + cy^2 $$
        <p>To determine if this is non-negative for all $\mathbf{x}, \mathbf{y}$, we complete the square with respect to $\mathbf{x}$ (assuming $a > 0$):</p>
        $$ a(\mathbf{x} + \frac{\mathbf{b}}{a}\mathbf{y})^2 + (c - \frac{\mathbf{b}^2}{a})\mathbf{y}^2 $$
        <p>The first term is always non-negative. The sign of the entire expression depends on the second term coefficient: $S = c - \mathbf{b}^2/a$.
        <br>In the matrix case $\begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$, the term $S = C - B^\top A^{-1} B$ plays exactly the same role. The block diagonal decomposition $ME$ derived below is the matrix equivalent of this algebraic rearrangement.</p>
      </div>

      <div class="insight">
        <h4>Why is this useful?</h4>
        <p>Often in optimization we encounter nonlinear inequalities like $\frac{\mathbf{x}^2}{\mathbf{y}} \le t$ or $\|Ax\|_2^2 \le c$. These look difficult to handle with linear algebra. The Schur Complement allows us to rewrite them as <b>linear</b> constraints on a matrix variable. For example, $\mathbf{x}^2 \le ty$ (with $\mathbf{y}>0$) is equivalent to the matrix condition $\begin{bmatrix} \mathbf{y} & \mathbf{x} \\ \mathbf{x} & t \end{bmatrix} \succeq 0$. This "lifting" trick is the key to Semidefinite Programming.</p>
      </div>

      <figure style="text-align: center;">
        <img src="assets/schur_complement_block.png"
             alt="Schur Complement Block Matrix Transformation Visualization"
             style="max-width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white;" />
        <figcaption><i>Figure 9:</i> Visualization of the Schur complement process. Left: Original block matrix. Center: Elimination matrix. Right: Resulting block upper-triangular matrix where the top-left block has been transformed into the Schur complement.</figcaption>
      </figure>

      <div class="proof-box">
        <h4>1. Block Matrix Setup and Dimensions</h4>
        <p>Consider a block matrix $M$ partitioned as follows:</p>
        $$ M = \begin{bmatrix} A & B \\ C & D \end{bmatrix} $$
        <p>where $A \in \mathbb{R}^{\mathbf{p} \times \mathbf{p}}$, $B \in \mathbb{R}^{\mathbf{p} \times \mathbf{q}}$, $C \in \mathbb{R}^{\mathbf{q} \times \mathbf{p}}$, and $D \in \mathbb{R}^{\mathbf{q} \times \mathbf{q}}$. Assume the bottom-right block $D$ is invertible.</p>
        <p>The <b>Schur complement</b> of $D$ in $M$ is the matrix defined as:</p>
        $$ S := A - B D^{-1} C $$

        <h4>2. Constructing the Elimination Matrix</h4>
        <p>To understand the origin of $S$, we apply <b>Block Gaussian Elimination</b>. The goal is to zero out the $C$ block (bottom-left) to obtain a block upper-triangular matrix, from which properties like determinant and invertibility are easily read.</p>
        <p>We achieve this by post-multiplying $M$ by an elimination matrix $E$:</p>
        $$ E = \begin{bmatrix} I_p & 0 \\ -D^{-1}C & I_q \end{bmatrix} $$
        <p>This matrix $E$ is block lower-triangular with identity matrices on the diagonal, so $\det(E) = 1$. It acts as a "column operation": it subtracts $D^{-1}C$ times the second column-block from the first column-block.</p>

        <p>Multiplying $M$ by $E$ yields the factorization:</p>
        $$ ME = \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} I_p & 0 \\ -D^{-1}C & I_q \end{bmatrix} = \begin{bmatrix} A - BD^{-1}C & B \\ 0 & D \end{bmatrix} = \begin{bmatrix} S & B \\ 0 & D \end{bmatrix} $$

        <div class="proof-box">
          <h4>Line-by-Line Verification of Block Multiplication</h4>
          <p>We verify this multiplication explicitly using the standard block multiplication rule $\begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} X & Y \\ Z & W \end{bmatrix} = \begin{bmatrix} AX+BZ & AY+BW \\ CX+DZ & CY+DW \end{bmatrix}$.</p>
          <p>Here we set $X=I_p, Y=0, Z=-D^{-1}C, W=I_q$.</p>
          <ul>
            <li><b>Top-left (The Pivot):</b> $A(I_p) + B(-D^{-1}C) = A - BD^{-1}C = S$. This term is exactly the "pivot" remaining after elimination.</li>
            <li><b>Top-right:</b> $A(0) + B(I_q) = B$. (Unchanged).</li>
            <li><b>Bottom-left (The Zero):</b> $C(I_p) + D(-D^{-1}C) = C - C = 0$. (This was the specific design goal of the elimination step).</li>
            <li><b>Bottom-right:</b> $C(0) + D(I_q) = D$. (Unchanged).</li>
          </ul>
        </div>

        <div class="insight">
          <h4>💡 Choice of Elimination Matrix</h4>
          <p>The goal is to eliminate the bottom-left block $C$, analogous to Gaussian elimination. We require the $(2,1)$ block of the product to be zero:
          $$ C \cdot I_p + D \cdot Z = 0 \implies D Z = -C \implies Z = -D^{-1}C $$
          This determines the entry $-D^{-1}C$ in the elimination matrix $E$.</p>
        </div>

        <p>Since $ME$ is block upper-triangular, its determinant is the product of the determinants of its diagonal blocks: $\det(ME) = \det(S)\det(D)$. Because $\det(E) = 1$, we derive the <b>Determinant Identity</b>:</p>
        $$ \det(M) = \det(D) \det(A - B D^{-1} C) $$

        <div class="example">
          <h4>Numerical Example (Sanity Check)</h4>
          <p>Let $A=[2], B=[1], C=[3], D=[4]$. Then $M = \begin{bmatrix} 2 & 1 \\ 3 & 4 \end{bmatrix}$.</p>
          <ul>
            <li><b>Direct Determinant:</b> $\det(M) = 2(4) - 1(3) = 8 - 3 = 5$.</li>
            <li><b>Schur Complement:</b> $S = A - B D^{-1} C = 2 - 1(\frac{1}{4})3 = 2 - 0.75 = 1.25$.</li>
            <li><b>Formula Check:</b> $\det(D)\det(S) = 4(1.25) = 5$.</li>
          </ul>
          <p>It matches perfectly.</p>
        </div>

        <h4>3. Symmetric Variant: Schur Complement of $A$</h4>
        <p>Similarly, if $A$ is invertible, we can eliminate $C$ using the Schur complement of $A$, defined as $D - C A^{-1} B$. The determinant identity becomes:</p>
        $$ \det(M) = \det(A) \det(D - C A^{-1} B) $$

        <h4>4. Schur Complement and Positive Semidefiniteness</h4>
        <p>This is the crucial property for convex optimization. Let $M$ be a symmetric matrix ($C = B^\top$):</p>
        $$ M = \begin{bmatrix} A & B \\ B^\top & D \end{bmatrix} $$
        <p><b>Theorem:</b> If $D \succ 0$, then $M \succeq 0$ if and only if the Schur complement $S = A - B D^{-1} B^\top \succeq 0$.</p>

        <div class="proof-step">
          <strong>Proof ($\Rightarrow$):</strong> Assume $M \succeq 0$.
          We need to show that the Schur complement is PSD, i.e., $\mathbf{x}^\top S \mathbf{x} \ge 0$ for any $\mathbf{x}$.
          Since $M \succeq 0$, we know that $\mathbf{z}^\top M \mathbf{z} \ge 0$ for <i>any</i> vector $\mathbf{z}$. The trick is to choose a specific test vector $\mathbf{z}$ that isolates $S$.
          <br>Let $\mathbf{z} = \begin{bmatrix} \mathbf{x} \\ \mathbf{y} \end{bmatrix}$. The quadratic form is:
          $$ \mathbf{z}^\top M \mathbf{z} = \mathbf{x}^\top A \mathbf{x} + 2 \mathbf{x}^\top B \mathbf{y} + \mathbf{y}^\top D \mathbf{y} $$
          We want to choose $\mathbf{y}$ to minimize this expression for a fixed $\mathbf{x}$, effectively "concentrating" the condition onto $\mathbf{x}$. Setting the gradient with respect to $\mathbf{y}$ to zero:
          $$ \nabla_y (\mathbf{z}^\top M \mathbf{z}) = 2B^\top \mathbf{x} + 2D\mathbf{y} = 0 \implies \mathbf{y} = -D^{-1}B^\top \mathbf{x} $$
          Now, we substitute this optimal $\mathbf{y}$ back into the quadratic form:
          $$ \mathbf{z}^\top M \mathbf{z} = \mathbf{x}^\top A \mathbf{x} + 2 \mathbf{x}^\top B (-D^{-1}B^\top \mathbf{x}) + (-D^{-1}B^\top \mathbf{x})^\top D (-D^{-1}B^\top \mathbf{x}) $$
          $$ = \mathbf{x}^\top A \mathbf{x} - 2 \mathbf{x}^\top B D^{-1} B^\top \mathbf{x} + \mathbf{x}^\top B D^{-1} D D^{-1} B^\top \mathbf{x} $$
          $$ = \mathbf{x}^\top A \mathbf{x} - 2 \mathbf{x}^\top B D^{-1} B^\top \mathbf{x} + \mathbf{x}^\top B D^{-1} B^\top \mathbf{x} $$
          $$ = \mathbf{x}^\top (A - B D^{-1} B^\top) \mathbf{x} = \mathbf{x}^\top S \mathbf{x} $$
          Since $M \succeq 0$, we must have $\mathbf{z}^\top M \mathbf{z} \ge 0$. Therefore, $\mathbf{x}^\top S \mathbf{x} \ge 0$ for all $\mathbf{x}$, which means $S \succeq 0$.
        </div>

        <div class="proof-step">
          <strong>Proof ($\Leftarrow$):</strong> Assume $S \succeq 0$ and $D \succ 0$.
          Take any $\mathbf{z} = [\mathbf{x}^\top, \mathbf{y}^\top]^\top$. We can factor $M$ using the elimination matrix from the proof of the lemma:
          $$ \mathbf{z}^\top M \mathbf{z} = \mathbf{x}^\top S \mathbf{x} + (\mathbf{y} + D^{-1}B^\top \mathbf{x})^\top D (\mathbf{y} + D^{-1}B^\top \mathbf{x}) $$
          Since $S \succeq 0$, the first term is non-negative. Since $D \succ 0$, the second term is non-negative. Thus $\mathbf{z}^\top M \mathbf{z} \ge 0$ for all $\mathbf{z}$.
        </div>
      <div class="example">
        <h4>Example: Determinant of Symmetric Block Matrix</h4>
        <p>Consider the matrix $M = \begin{bmatrix} Y & \mathbf{x} \\ \mathbf{x}^\top & t \end{bmatrix}$ where $Y \succ 0$.
        Using the general determinant formula with $A=Y, B=\mathbf{x}, C=\mathbf{x}^\top, D=t$:
        $$ \det M = \det Y \cdot \det(t - \mathbf{x}^\top Y^{-1} \mathbf{x}) = (\det Y)(t - \mathbf{x}^\top Y^{-1} \mathbf{x}) $$
        This scalar formula is frequently used to check if $M \succ 0$: we need $\det Y > 0$ (given) and the Schur complement $t - \mathbf{x}^\top Y^{-1} \mathbf{x} > 0$.</p>
      </div>

      </div>

      <p><b>Forward Connection:</b> This result can also be viewed as minimizing the quadratic form $f(\mathbf{x}, \mathbf{y}) = [\mathbf{x}^\top \ \mathbf{y}^\top] M [\mathbf{x}^\top \ \mathbf{y}^\top]^\top$ with respect to $\mathbf{y}$ (partial minimization), yielding a new quadratic form in $\mathbf{x}$ defined by the Schur complement matrix. This perspective is explored in <a href="../05-convex-functions-basics/index.html">Lecture 05</a>.</p>

        <div class="insight">
          <h4>The Triangle of Equivalence</h4>
          <p>For a positive definite matrix $Y \succ 0$, the following three conditions are equivalent. They represent three different "faces" of the same object:</p>
          $$
          \boxed{
          \begin{matrix}
          \textbf{Scalar Inequality} & & \textbf{Block Matrix LMI} \\
          t \ge \mathbf{x}^\top Y^{-1} \mathbf{x} & \iff & \begin{bmatrix} Y & \mathbf{x} \\ \mathbf{x}^\top & t \end{bmatrix} \succeq 0 \\
          & \Updownarrow & \\
          & \textbf{Rank-1 Update} & \\
          & tY - xx^\top \succeq 0 &
          \end{matrix}
          }
          $$
          <ul>
            <li><b>Scalar:</b> Useful for epigraphs of quadratic-over-linear functions.</li>
            <li><b>Block Matrix:</b> Useful for Semidefinite Programming (SDP) constraints.</li>
            <li><b>Rank-1 Update:</b> Useful for algebraic manipulation ($tY \succeq xx^\top$).</li>
          </ul>
          <div class="proof-box">
            <h4>Derivation: From Rank-1 to Scalar</h4>
            <p>We want to show $tY - xx^\top \succeq 0 \iff t \ge \mathbf{x}^\top Y^{-1} \mathbf{x}$.</p>
            <div class="proof-step">
                <strong>Step 1: Variational Definition.</strong>
                $tY - xx^\top \succeq 0$ means for all $\mathbf{v} \in \mathbb{R}^n$:
                $$ \mathbf{v}^\top (tY - xx^\top) \mathbf{v} \ge 0 \implies t (\mathbf{v}^\top Y \mathbf{v}) - (\mathbf{v}^\top \mathbf{x})(\mathbf{x}^\top \mathbf{v}) \ge 0 \implies (\mathbf{x}^\top \mathbf{v})^2 \le t (\mathbf{v}^\top Y \mathbf{v}) $$
            </div>
            <div class="proof-step">
                <strong>Step 2: Change of Variables.</strong>
                Since $Y \succ 0$, it has a square root $Y^{1/2}$. Let $\mathbf{u} = Y^{1/2} \mathbf{v}$. Then $\mathbf{v} = Y^{-1/2} \mathbf{u}$.
                The term $\mathbf{v}^\top Y \mathbf{v} = \mathbf{u}^\top Y^{-1/2} Y Y^{-1/2} \mathbf{u} = \mathbf{u}^\top \mathbf{u} = \|\mathbf{u}\|_2^2$.
                The term $\mathbf{x}^\top \mathbf{v} = \mathbf{x}^\top Y^{-1/2} \mathbf{u} = (Y^{-1/2} \mathbf{x})^\top \mathbf{u}$. Let $\mathbf{w} = Y^{-1/2} \mathbf{x}$. Then $\mathbf{x}^\top \mathbf{v} = \mathbf{w}^\top \mathbf{u}$.
            </div>
            <div class="proof-step">
                <strong>Step 3: Apply Cauchy-Schwarz.</strong>
                The inequality becomes $(\mathbf{w}^\top \mathbf{u})^2 \le t \|\mathbf{u}\|_2^2$ for all $\mathbf{u}$.
                We want to find the minimum $t$ that satisfies this. This is equivalent to:
                $$ t \ge \sup_{\mathbf{u} \neq 0} \frac{(\mathbf{w}^\top \mathbf{u})^2}{\|\mathbf{u}\|_2^2} $$
                By the Cauchy-Schwarz inequality, $(\mathbf{w}^\top \mathbf{u})^2 \le \|\mathbf{w}\|_2^2 \|\mathbf{u}\|_2^2$, with equality when $\mathbf{u}$ is parallel to $\mathbf{w}$.
                Thus, the supremum is $\|w\|_2^2$.
            </div>
            <div class="proof-step">
                <strong>Step 4: Conclusion.</strong>
                $$ t \ge \|\mathbf{w}\|_2^2 = \mathbf{w}^\top \mathbf{w} = (Y^{-1/2} \mathbf{x})^\top (Y^{-1/2} \mathbf{x}) = \mathbf{x}^\top Y^{-1} \mathbf{x} $$
            </div>
          </div>
        </div>

      <p><b>Application:</b> This lemma turns the nonlinear constraint $\|Bx\|_2^2 \le c$ (equivalent to $\mathbf{x}^\top B^\top B \mathbf{x} \le c$) into an LMI:
      $$ \begin{bmatrix} I & B\mathbf{x} \\ (B\mathbf{x})^\top & c \end{bmatrix} \succeq 0 $$
      This is central to Semidefinite Programming.</p>

      <p>A positive definite matrix $Q$ can be used to define a generalized norm, $\|x\|_Q = \sqrt{\mathbf{x}^\top Q \mathbf{x}}$. The unit ball for this norm, $\{x \mid \mathbf{x}^\top Q \mathbf{x} \le 1\}$, is an ellipsoid, whose geometry is determined by the eigenvalues and eigenvectors of $Q$.</p>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/eigenvalues-psd.png"
             alt="Eigenvalue visualization for PSD matrices"
             style="max-width: 80%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <figcaption><i>Figure 10:</i> Eigenvalues and positive semidefiniteness—the signs determine the matrix's curvature properties.</figcaption>
      </figure>

      <figure style="text-align: center;">
        <img src="../../static/assets/topics/00-linear-algebra-basics/eigenvectors.gif"
             alt="Animated eigenvector visualization"
             style="max-width: 70%; height: auto; border: 1px solid var(--border); border-radius: 8px;" />
        <figcaption><i>Figure 11:</i> Eigenvectors in action—how matrices transform space along principal directions.</figcaption>
      </figure>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Explorer: Matrix Geometry & Definiteness</h3>
        <p><b>Explore Linear Maps and Quadratic Forms:</b> This interactive tool connects the linear transformation $A\mathbf{x}$ with the quadratic form $\mathbf{x}^\top A\mathbf{x}$. Toggle "Force Symmetric" to explore the specific properties relevant to optimization (Hessians).</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Linear Map:</b> See how the unit circle transforms into an ellipse. The axes of the ellipse correspond to eigenvectors (for symmetric matrices).</li>
          <li><b>Quadratic Form:</b> Visualize the level sets of $\mathbf{z} = \mathbf{x}^\top A\mathbf{x}$.
            <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
              <li><b>Positive Definite:</b> Ellipses (Convex Bowl)</li>
              <li><b>Indefinite:</b> Hyperbolas (Saddle Point)</li>
            </ul>
          </li>
        </ul>
        <div id="widget-matrix-geometry" style="width: 100%; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Hessian Landscape Visualizer</h3>
        <p><b>Connect Eigenvalues to Function Curvature:</b> This 3D visualizer renders the surface of a quadratic function $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top Q \mathbf{x}$ and displays its Hessian matrix $Q$. The eigenvalues of the Hessian directly control the curvature:</p>
        <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
          <li><b>Large positive eigenvalues:</b> Steep curvature (fast convergence in optimization)</li>
          <li><b>Small positive eigenvalues:</b> Flat directions (slow convergence)</li>
          <li><b>Condition number ($\kappa = \lambda_{\max}/\lambda_{\min}$):</b> When this ratio is large, gradient descent converges slowly</li>
        </ul>
        <p><i>Practical insight:</i> This visualization explains why preconditioning (transforming to balance eigenvalues) dramatically speeds up iterative solvers!</p>
        <div id="widget-hessian-landscape" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>7.6 The Loewner Order</h3>
      <p>For symmetric matrices $X, Y \in \mathbb{S}^n$, we write <b>$X \succeq Y$</b> if and only if $X - Y$ is <b>positive semidefinite (PSD)</b>, meaning:</p>
      $$
      \mathbf{v}^\top (X - Y) \mathbf{v} \ge 0 \quad \text{for all } \mathbf{v} \in \mathbb{R}^n
      $$

      <div class="proof-box">
        <h4>Key Properties of the Loewner Order</h4>
        <ul>
          <li><b>Eigenvalue characterization:</b> $X \succeq 0$ if and only if all eigenvalues of $X$ are nonnegative.</li>
          <li><b>Inner product property:</b> If $X \succeq 0$ and $Y \succeq 0$, then:
            $$ \langle X, Y \rangle = \mathrm{tr}(XY) \ge 0 $$
            (This follows because $XY$ has nonnegative trace when both are PSD.)
          </li>
          <li><b>Partial order:</b> The relation $\succeq$ is reflexive, transitive, and antisymmetric (hence a partial order).</li>
        </ul>
      </div>

      <p>The Loewner order is the language of:</p>
      <ul>
        <li><b>Ellipsoids</b> (defined by PSD matrices in quadratic forms)</li>
        <li><b>PSD cone</b> $\mathbb{S}^n_+ = \{X \in \mathbb{S}^n \mid X \succeq 0\}$ in <a href="../03-convex-sets-geometry/index.html">Lecture 03</a></li>
        <li><b>Semidefinite programs (SDPs)</b> in <a href="../08-convex-problems-conic/index.html">Lecture 08</a></li>
      </ul>
    </section>

    <!-- SECTION 8: PROJECTIONS -->
    <section class="section-card" id="section-projections">
      <h2>8. Projections onto Subspaces and Affine Sets</h2>

      <p>The concept of "finding the closest point" is the geometric heart of optimization. Whether solving least squares, applying constraints, or analyzing duality, we are often just computing an orthogonal projection.</p>

      <h3>8.1 Projection onto a Line (1D Case)</h3>
      <p>Before tackling general subspaces, let's derive the projection of a vector $\mathbf{x}$ onto the line spanned by a single non-zero vector $a \in \mathbb{R}^n$. This simple case contains all the intuition needed for the general theory.</p>
      <p>We seek the scalar $\alpha$ that minimizes the squared distance $\|x - \alpha a\|_2^2$.
      $$ f(\alpha) = \|\mathbf{x} - \alpha a\|_2^2 = (\mathbf{x} - \alpha a)^\top (\mathbf{x} - \alpha a) = \|\mathbf{x}\|_2^2 - 2\alpha (a^\top \mathbf{x}) + \alpha^2 \|a\|_2^2 $$
      This is a simple convex quadratic in $\alpha$. Setting the derivative to zero:
      $$ f'(\alpha) = -2(a^\top \mathbf{x}) + 2\alpha \|a\|_2^2 = 0 \implies \alpha^\star = \frac{a^\top \mathbf{x}}{\|a\|_2^2} $$
      The projection is the vector $\mathbf{p} = \alpha^\star a$:
      $$ \mathbf{p} = \frac{a^\top \mathbf{x}}{a^\top a} a = \left( \frac{a a^\top}{a^\top a} \right) \mathbf{x} $$
      </p>
      <div class="insight">
        <h4>Geometric Insight: Orthogonality of the Residual</h4>
        <p>Let $\mathbf{r} = \mathbf{x} - \mathbf{p}$ be the error (residual) vector. Check its inner product with $a$:
        $$ a^\top \mathbf{r} = a^\top \left( \mathbf{x} - \frac{a^\top \mathbf{x}}{a^\top a} a \right) = a^\top \mathbf{x} - \frac{a^\top \mathbf{x}}{a^\top a} (a^\top a) = a^\top \mathbf{x} - a^\top \mathbf{x} = 0 $$
        The error is orthogonal to the subspace (line). This <b>Orthogonality Principle</b> is universal: the optimal error is always perpendicular to the feasible set (for subspaces and affine sets).</p>
      </div>

      <h3>8.2 Orthogonal Projection onto a General Subspace</h3>
      <p>Let $\mathcal{S} \subseteq \mathbb{R}^m$ be a subspace and $\mathbf{b} \in \mathbb{R}^m$. The <b>orthogonal projection</b> $\mathbf{p} \in \mathcal{S}$ of $\mathbf{b}$ is the unique vector in $\mathcal{S}$ minimizing $\|b - p\|_2$. It is characterized by the condition that the residual is orthogonal to the entire subspace:
      $$ \mathbf{b} - \mathbf{p} \perp \mathcal{S} \quad \iff \quad \mathbf{v}^\top(\mathbf{b} - \mathbf{p}) = 0 \ \ \forall \mathbf{v} \in \mathcal{S} $$</p>

      <h3>Projection via a Basis</h3>
      <p>If $Q \in \mathbb{R}^{m \times k}$ has orthonormal columns spanning $\mathcal{S}$, the projector is:
      $$ P = QQ^\top \quad \text{and} \quad \mathbf{p} = P\mathbf{b} = QQ^\top \mathbf{b} $$
      Check: $P^2 = P$ (idempotent) and $P^\top = P$ (symmetric)—that's what makes it an <b>orthogonal</b> projector.</p>

      <h3>Projection onto $\mathcal{R}(A)$ using $A$</h3>
      <p>If $A \in \mathbb{R}^{m \times n}$ has full column rank:
      $$ P = A(A^\top A)^{-1} A^\top \quad \text{and} \quad \mathbf{p} = P\mathbf{b} $$
      </p>

      <h3>8.3 Projection onto Closed Convex Sets</h3>
      <p>The concept of projection extends beyond subspaces. For any nonempty, closed convex set $C$, and any point $y$, the projection $\Pi_C(y)$ is the unique point in $C$ closest to $y$.
      $$ \Pi_C(y) = \operatorname{argmin}_{x \in C} \|x - y\|_2 $$
      <b>Existence & Uniqueness:</b> Existence follows from $C$ being closed (and the norm being coercive). Uniqueness follows from the strict convexity of the Euclidean norm and the convexity of $C$.
      <br><b>The Projection Inequality:</b> The defining characteristic of the projection $x^* = \Pi_C(y)$ is the obtuse angle condition:
      $$ \langle y - x^*, z - x^* \rangle \le 0 \quad \forall z \in C $$
      Geometrically, the vector from the projection $x^*$ to the target $y$ makes an obtuse angle with any vector pointing from $x^*$ into the set. This condition is the ancestor of KKT stationarity and separating hyperplanes.</p>

      <h3>8.4 Projection onto an Affine Set</h3>
      <p>Consider an affine set defined by $\mathcal{A} = \{ \mathbf{x} \in \mathbb{R}^n \mid F\mathbf{x} = \mathbf{g} \}$, where $F \in \mathbb{R}^{\mathbf{p} \times n}$ has full row rank and $\mathbf{g} \in \mathbb{R}^p$. Projecting onto this set reduces to subspace projection by translating.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/projection-onto-affine-set.png"
             alt="Projection onto an affine set visualized as translation to a subspace, projection, then translation back"
             style="max-width: 95%; height: auto; border-radius: 8px;" />
        <figcaption><i>Figure 12:</i> Projection onto an affine set: translate to a parallel subspace, project orthogonally, then shift back by a particular solution.</figcaption>
      </figure>

      <h4>Explicit Formula via Least Norm</h4>
      <p>We solve the problem $\min \|\mathbf{x} - \mathbf{y}\|_2^2$ s.t. $F\mathbf{x} = \mathbf{g}$. The KKT conditions yield the linear system:
      $$ \begin{bmatrix} I & F^\top \\ F & 0 \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ \nu \end{bmatrix} = \begin{bmatrix} \mathbf{y} \\ \mathbf{g} \end{bmatrix} $$
      Solving for $\mathbf{x}$ gives the explicit projection formula:
      $$ \Pi_{\mathcal{A}}(\mathbf{y}) = \mathbf{y} - F^\top (F F^\top)^{-1} (F \mathbf{y} - \mathbf{g}) $$
      This is often more practical than constructing a nullspace basis $Z$.</p>

      <h4>Parametric Representation</h4>
      <p>Every point in $\mathcal{A}$ can be written as $\mathbf{x} = x_0 + Zt$, where:</p>
      <ul>
        <li>$x_0$ is any particular solution satisfying $Fx_0 = \mathbf{g}$.</li>
        <li>$Z$ is a matrix whose columns form a basis for $\mathcal{N}(F)$ (the nullspace of $F$).</li>
        <li>$t \in \mathbb{R}^k$ where $k = n - \mathbf{p}$ (dimension of the nullspace).</li>
      </ul>

      <h4>Euclidean Projection Formula (Explicit via KKT)</h4>
      <p>Alternatively, using Lagrange multipliers (a technique we will explore in Lecture 09), we can derive the projection without forming a nullspace basis.</p>
      $$
      \Pi_{\mathcal{A}}(\mathbf{y}) = \mathbf{y} - F^\top (F F^\top)^{-1} (F \mathbf{y} - \mathbf{g})
      $$

      <div class="proof-box">
        <h4>Derivation via Lagrange Multipliers</h4>
        <div class="proof-step">
          <strong>Step 1: Formulation.</strong> Minimize $\frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2$ subject to $F\mathbf{x} = \mathbf{g}$.
          The Lagrangian is $L(\mathbf{x}, \nu) = \frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2 + \nu^\top (F\mathbf{x} - \mathbf{g})$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Optimality Conditions.</strong>
          Gradient w.r.t $\mathbf{x}$: $\mathbf{x} - \mathbf{y} + F^\top \nu = 0 \implies \mathbf{x} = \mathbf{y} - F^\top \nu$.
          Primal feasibility: $F\mathbf{x} = \mathbf{g}$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Solve for Dual Variable $\nu$.</strong>
          Substitute $\mathbf{x}$ into the constraint:
          $$ F(\mathbf{y} - F^\top \nu) = \mathbf{g} \implies F\mathbf{y} - F F^\top \nu = \mathbf{g} \implies F F^\top \nu = F\mathbf{y} - \mathbf{g} $$
          Assuming $F$ has full row rank, $F F^\top$ is invertible:
          $$ \nu = (F F^\top)^{-1} (F\mathbf{y} - \mathbf{g}) $$
        </div>
        <div class="proof-step">
          <strong>Step 4: Substitute back.</strong>
          $$ \mathbf{x} = \mathbf{y} - F^\top (F F^\top)^{-1} (F\mathbf{y} - \mathbf{g}) $$
        </div>
      </div>

      <h4>Euclidean Projection Formula (Nullspace)</h4>
      <p>The <b>Euclidean projection</b> of any point $\mathbf{y} \in \mathbb{R}^n$ onto $\mathcal{A}$ can also be computed via the nullspace basis:</p>
      $$
      \Pi_{\mathcal{A}}(\mathbf{y}) = \mathbf{x}_0 + Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - \mathbf{x}_0)
      $$

      <div class="proof-box">
        <h4>Geometric Interpretation and Derivation</h4>
        <p>This formula says: "Project $(\mathbf{y} - x_0)$ onto the nullspace $\mathcal{N}(F)$, then shift back by $x_0$."</p>

        <div class="proof-step">
          <strong>Step 1: Translate to subspace problem.</strong>
          We want to find $\mathbf{x} \in \mathcal{A}$ minimizing $\|x - y\|_2$. Let $\mathbf{x} = x_0 + \mathbf{w}$, where $\mathbf{w} \in \mathcal{N}(F)$.
          The problem becomes minimizing $\|(x_0 + w) - y\|_2 = \|w - (y - x_0)\|_2$ subject to $\mathbf{w} \in \mathcal{N}(F)$.
        </div>

        <div class="proof-step">
          <strong>Step 2: Project onto nullspace.</strong>
          This is now a projection of the vector $(\mathbf{y} - x_0)$ onto the subspace $\mathcal{N}(F)$.
          Since the columns of $Z$ form a basis for $\mathcal{N}(F)$, the projection matrix onto this subspace is $P_{\mathcal{N}(F)} = Z(Z^\top Z)^{-1} Z^\top$.
          Thus, the optimal $\mathbf{w}^*$ is:
          $$ \mathbf{w}^* = \Pi_{\mathcal{N}(F)}(\mathbf{y} - x_0) = Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - x_0) $$
        </div>

        <div class="proof-step">
          <strong>Step 3: Shift back.</strong>
          The projection onto the affine set is $\mathbf{x}^* = x_0 + \mathbf{w}^*$.
          $$ \Pi_{\mathcal{A}}(\mathbf{y}) = x_0 + Z(Z^\top Z)^{-1} Z^\top (\mathbf{y} - x_0) $$
        </div>
      </div>

      <div class="proof-box">
        <h3>Example 1: Projection onto a Line</h3>
        <p>We compute the projection of the vector $\mathbf{b} = (2, 3)^\top$ onto the line spanned by the vector $\mathbf{u} = (1, 1)^\top$. The projection $\mathbf{p}$ is given by the formula:
        $$ \mathbf{p} = \frac{\mathbf{u}^\top \mathbf{b}}{\mathbf{u}^\top \mathbf{u}} \mathbf{u} = \frac{(1)(2) + (1)(3)}{(1)(1) + (1)(1)} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{5}{2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 2.5 \\ 2.5 \end{pmatrix} $$
        The residual vector is $\mathbf{r} = \mathbf{b} - \mathbf{p} = (2 - 2.5, 3 - 2.5)^\top = (-0.5, 0.5)^\top$. We verify that the residual is orthogonal to the line: $\mathbf{u}^\top \mathbf{r} = (1)(-0.5) + (1)(0.5) = 0$.
        </p>
      </div>

    </section>

    <!-- SECTION 9: LEAST SQUARES -->
    <section class="section-card" id="section-least-squares">
      <h2>9. The Method of Least Squares</h2>

      <h3>The Problem: Overdetermined Systems</h3>
      <p>In practice, we often encounter linear systems $A\mathbf{x}=\mathbf{b}$ with no exact solution. This typically occurs when $m > n$ (more equations than unknowns) and $\mathbf{b} \notin \mathcal{R}(A)$. The "least squares" approach finds the best approximate solution by minimizing the sum of squared errors—the squared Euclidean norm of the residual $r = A\mathbf{x}-\mathbf{b}$. This formulates the canonical <b>unconstrained optimization problem</b>:</p>
      $$ \min_{\mathbf{x} \in \mathbb{R}^n} \|A\mathbf{x} - \mathbf{b}\|_2^2 $$

      <div class="insight">
        <h4>Forward Connection: Optimization Formulation</h4>
        <p>This problem is an instance of <b>Unconstrained Convex Optimization</b>. It minimizes a convex quadratic loss function. The solution methods we derive here (analytical gradients) generalize to the iterative descent methods (Gradient Descent, Newton's Method) covered in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>. Furthermore, it is a specific case of <b>Quadratic Programming (QP)</b> (<a href="../07-convex-problems-standard/index.html">Lecture 07</a>), which adds linear constraints to the problem.</p>
      </div>

      <h3>Geometric Interpretation: Projection</h3>
      <p>The solution to the least squares problem has a clean geometric interpretation. The vector $A\mathbf{x}$ lies in the column space of $A$, denoted $\mathcal{R}(A)$. The problem $\min_x \|A\mathbf{x} - \mathbf{b}\|_2$ is therefore equivalent to finding the point $\mathbf{p} \in \mathcal{R}(A)$ that is closest to $\mathbf{b}$ in Euclidean distance.</p>

      <h4>The 1D Case: Projection onto a Vector</h4>
      <p>Before solving the general case, consider projecting $\mathbf{x}$ onto the line spanned by a vector $\mathbf{y} \neq 0$. We want to find $t$ to minimize $\|x - ty\|_2^2$.
      $$ \phi(t) = \|\mathbf{x} - ty\|_2^2 = \|\mathbf{x}\|_2^2 - 2t(\mathbf{x}^\top \mathbf{y}) + t^2 \|\mathbf{y}\|_2^2 $$
      Taking the derivative with respect to $t$:
      $$ \phi'(t) = -2(\mathbf{x}^\top \mathbf{y}) + 2t \|\mathbf{y}\|_2^2 $$
      Setting $\phi'(t) = 0$ gives the optimal scaling:
      $$ t^\star = \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{y}\|_2^2} $$
      The projection is $\mathbf{p} = t^\star \mathbf{y}$.
      <br><i>Orthogonality Check:</i> Let the residual be $\mathbf{r} = \mathbf{x} - \mathbf{p}$. Then $\mathbf{y}^\top \mathbf{r} = \mathbf{y}^\top \mathbf{x} - t^\star \mathbf{y}^\top \mathbf{y} = \mathbf{y}^\top \mathbf{x} - \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{y}\|^2} \|\mathbf{y}\|^2 = 0$. The residual is orthogonal to $\mathbf{y}$. This intuition generalizes to subspaces.</p>

      <p>By the Pythagorean theorem, the closest point $\mathbf{p}$ must satisfy a specific geometric condition: the error vector $\mathbf{b} - \mathbf{p}$ must be orthogonal to the subspace $\mathcal{R}(A)$. <b>Think of dropping a perpendicular:</b> The shortest path from a point to a plane (subspace) is the straight line perpendicular to that plane. If the error vector were not orthogonal, we could project it onto the subspace to find a "correction" that brings us closer to $\mathbf{b}$, contradicting the optimality of $\mathbf{p}$. Thus, the optimal $\mathbf{p} = A\mathbf{x}^\star$ is the <b>orthogonal projection</b> of $\mathbf{b}$ onto $\mathcal{R}(A)$.</p>

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/least-squares-orthogonal-projection.png"
             alt="Least squares as orthogonal projection of b onto the column space of A with residual orthogonal to the subspace"
             style="max-width: 95%; height: auto; border-radius: 8px;" />
        <figcaption><i>Figure 13:</i> Least squares geometry: the fitted vector $\mathbf{p}=A\mathbf{x}^\star$ is the projection of $\mathbf{b}$ onto $\mathcal{R}(A)$, and the residual $\mathbf{r}=\mathbf{b}-\mathbf{p}$ is orthogonal to the subspace.</figcaption>
      </figure>

      <h3>The Normal Equations</h3>
      <p>The orthogonality condition, $(\mathbf{b} - A\mathbf{x}^\star) \perp \mathcal{R}(A)$, implies that the residual vector must be orthogonal to every basis vector of the subspace. Since the columns of $A$ span $\mathcal{R}(A)$, the residual must be orthogonal to each column of $A$. This can be expressed compactly as:
      $$ A^\top (\mathbf{b} - A\mathbf{x}^\star) = 0 $$
      Expanding this yields the <b>normal equations</b>:
      $$ A^\top A \mathbf{x}^\star = A^\top \mathbf{b} $$
      If the matrix $A$ has linearly independent columns (full column rank), then $A^\top A$ is invertible, and we can write the unique solution as $\mathbf{x}^\star = (A^\top A)^{-1} A^\top \mathbf{b}$.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Visualizer: Least Squares Projection</h3>
        <p><b>See the Geometry in 3D:</b> The solution $A\mathbf{x}^\star$ is the orthogonal projection of the target vector $\mathbf{b}$ onto the subspace spanned by the columns of $A$ (the plane). The residual vector $\mathbf{r} = \mathbf{b} - A\mathbf{x}^\star$ connects the projection to the target and is perpendicular (orthogonal) to the plane.</p>
        <div id="widget-least-squares" style="width: 100%; height: 500px; position: relative;"></div>
      </div>

      <h3>Uniqueness of the Solution</h3>
      <ul>
        <li>If $A$ has full column rank ($\mathrm{rank}(A) = n$), the solution $\mathbf{x}^\star$ is unique.</li>
        <li>If $A$ is rank-deficient ($\mathrm{rank}(A) < n$), there are infinitely many solutions. In this case, the pseudoinverse is used to find the solution with the minimum norm.</li>
      </ul>

      <div class="proof-box">
        <h3>Example 2: Solving Least Squares with Normal Equations</h3>
        <p>Consider the least squares problem for the system $A\mathbf{x}=\mathbf{b}$ with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} $$
        First, we form the normal equations $A^\top A \mathbf{x} = A^\top \mathbf{b}$:
        $$ A^\top A = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} $$
        $$ A^\top \mathbf{b} = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 3 \end{pmatrix} $$
        We solve the system $\begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \mathbf{x} = \begin{pmatrix} 3 \\ 3 \end{pmatrix}$, which yields the solution $\mathbf{x}^\star = (3/4, 3/4)^\top$.
        The projection is $\mathbf{p} = A\mathbf{x}^\star = (3/2, 0, 3/2)^\top$, and the residual is $\mathbf{r} = \mathbf{b} - \mathbf{p} = (1/2, 0, -1/2)^\top$. We can verify the orthogonality condition: $A^\top \mathbf{r} = (0, 0)^\top$.
        </p>
      </div>

      <div class="proof-box">
        <h3>Example 3: Rank-Deficient Case</h3>
        <p>Consider the system with:
        $$ A = \begin{pmatrix} 1 & 1 \\ 2 & 2 \end{pmatrix}, \quad \mathbf{b} = \begin{pmatrix} 3 \\ 6 \end{pmatrix} $$
        The columns of $A$ are linearly dependent, so the system is rank-deficient. The vector $\mathbf{b}$ is in the column space of $A$, so there are infinitely many solutions. The normal equations are $A^\top A \mathbf{x} = \begin{pmatrix} 5 & 5 \\ 5 & 5 \end{pmatrix} \mathbf{x} = \begin{pmatrix} 15 \\ 15 \end{pmatrix}$. This system has infinitely many solutions of the form $x_1 + x_2 = 3$. The pseudoinverse would provide the minimum-norm solution, $x_1 = x_2 = 1.5$.
        </p>
      </div>

      <h3>8.4 Variants: Weighted and Constrained Least Squares</h3>


      <h3>(a) Weighted Least Squares (WLS)</h3>
      <p>Given SPD weight $W \in \mathbb{R}^{m \times m}$:
      $$ \min_x \|A\mathbf{x} - \mathbf{b}\|_W^2 := (A\mathbf{x} - \mathbf{b})^\top W(A\mathbf{x} - \mathbf{b}) $$
      Let $C$ be a matrix such that $W = C^\top C$ (e.g., via Cholesky decomposition). Then the problem is equivalent to ordinary least squares in the <b>whitened</b> system, minimizing $\|C(Ax - b)\|_2^2 = \|CAx - Cb\|_2^2$. The normal equations are $A^\top W A \mathbf{x} = A^\top W \mathbf{b}$.</p>

      <h3>(b) Constrained to an Affine Set</h3>
      <p>Solve:
      $$ \min_x \|A\mathbf{x} - \mathbf{b}\|_2^2 \quad \text{s.t.} \quad F\mathbf{x} = \mathbf{g} $$
      One method: parametrize $\mathbf{x} = x_0 + Z\mathbf{y}$, where $Fx_0 = \mathbf{g}$ and columns of $Z$ form a basis for $\mathcal{N}(F)$. Then minimize $\|A(x_0 + Zy) - b\|_2^2$ over $\mathbf{y}$ (an unconstrained LS). QR on $AZ$ is typically best.</p>

    </section>

    <!-- SECTION 10: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>10. Review & Cheat Sheet</h2>
      <div class="lecture-summary" style="margin-bottom: 20px;">
        <p>This section condenses the lecture into a quick-reference format for definitions, properties, and standard results.</p>
      </div>

      <h3>Definitions</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>Definition</th>
            <th>Key Properties</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>Subspace</b></td>
            <td>Set $S$ closed under addition and scalar multiplication</td>
            <td>Contains $\mathbf{0}$. Intersection of subspaces is a subspace.</td>
          </tr>
          <tr>
            <td><b>Inner Product</b></td>
            <td>$\langle \mathbf{x}, \mathbf{y} \rangle$</td>
            <td>Bilinear, Symmetric, Positive Definite. Enables angles and projection.</td>
          </tr>
          <tr>
            <td><b>Norm</b></td>
            <td>$\|x\|$</td>
            <td>Positivity, Homogeneity, Triangle Inequality.</td>
          </tr>
          <tr>
            <td><b>PSD Matrix</b></td>
            <td>$A \succeq 0$</td>
            <td>$\mathbf{x}^\top A \mathbf{x} \ge 0 \ \forall \mathbf{x}$. Eigenvalues $\lambda_i \ge 0$.</td>
          </tr>
          <tr>
            <td><b>Orthogonal Matrix</b></td>
            <td>$Q^\top Q = I$</td>
            <td>Preserves norms/angles. Columns are orthonormal.</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Theorems</h3>
      <ul>
        <li><b>Rank-Nullity:</b> $\dim \mathcal{R}(A) + \dim \mathcal{N}(A) = n$ (for $A \in \mathbb{R}^{m \times n}$).</li>
        <li><b>Spectral Theorem:</b> Real symmetric matrices have real eigenvalues and orthogonal eigenvectors.</li>
        <li><b>Schur Complement:</b> $M \succeq 0 \iff D \succ 0$ and $A - BD^{-1}B^\top \succeq 0$.</li>
        <li><b>Fundamental Theorem of Linear Algebra:</b> $\mathcal{R}(A) \perp \mathcal{N}(A^\top)$ and $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.</li>
      </ul>

      <h3>Standard Formulas</h3>
      <ul>
        <li><b>Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$</li>
        <li><b>Projection onto Subspace:</b> $P = A(A^\top A)^{-1}A^\top$ (if $A$ full rank)</li>
        <li><b>Cauchy-Schwarz:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$</li>
        <li><b>Gradient of Quadratic:</b> $\nabla (\mathbf{x}^\top A \mathbf{x}) = (A+A^\top)\mathbf{x}$</li>
      </ul>
    </section>

        <!-- SECTION 11: EXERCISES -->
    <section class="section-card" id="section-exercises">
      <h2><i data-feather="edit-3"></i> 11. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises reinforce the foundational tools of linear algebra used in optimization. Focus on the geometry of subspaces, the calculus of gradients (crucial for finding optimality conditions), and the properties of PSD matrices (essential for convexity).</p>
      </div>
<h3>P0.1 — Linear Independence</h3>
      <p>Determine whether the following sets of vectors are linearly independent. If dependent, exhibit a linear combination summing to zero.</p>
      <ol type="a">
        <li>$v_1 = (1, 2, 3)^\top, v_2 = (4, 5, 6)^\top, v_3 = (7, 8, 9)^\top$.</li>
        <li>$v_1 = (1, 0, 0)^\top, v_2 = (1, 1, 0)^\top, v_3 = (1, 1, 1)^\top$.</li>
        <li>The columns of an upper triangular matrix with non-zero diagonal entries.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Dependent.</b> Notice that the vectors are in an arithmetic progression. $v_2 - v_1 = (3, 3, 3)^\top$ and $v_3 - v_2 = (3, 3, 3)^\top$. Thus $v_2 - v_1 = v_3 - v_2$, which rearranges to $v_1 - 2v_2 + v_3 = 0$. This is a non-zero linear combination summing to zero.</li>
          <li><b>Independent.</b> Form the matrix $A = [v_1, v_2, v_3]$. It is lower triangular with non-zero diagonal entries (all 1s). The determinant is the product of the diagonal entries, which is 1. Since $\det(A) \neq 0$, the columns are linearly independent.</li>
          <li><b>Independent.</b> An upper triangular matrix $U$ with non-zero diagonal entries has determinant $\prod u_{ii} \neq 0$. Thus, its columns form a basis and are linearly independent.</li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Definition:</b> A set of vectors $\{v_1, \dots, v_k\}$ is <b>linearly independent</b> if the only linear combination summing to zero is the trivial one ($\sum c_i v_i = 0 \implies c_i = 0 \ \forall i$).
        <br><b>Matrix View:</b> The matrix $A = [v_1 \dots v_k]$ formed by these vectors has full column rank ($\mathrm{rank}(A) = k$) if and only if $\mathcal{N}(A) = \{0\}$.
        <br><b>Determinant Test (Square Case):</b> For $n$ vectors in $\mathbb{R}^n$, they form a <b>basis</b> (independent and spanning) if and only if $\det([v_1 \dots v_n]) \neq 0$.
        <br><b>Geometric Intuition:</b> Linearly dependent vectors are "redundant"; one can be written as a combination of the others, collapsing the span into a lower-dimensional subspace.</p>
      </div>




<h3>P0.2 — The Rank-Nullity Theorem</h3>
      <p>Let $A$ be a $10 \times 15$ matrix.</p>
      <ol type="a">
        <li>What is the maximum possible rank of $A$?</li>
        <li>If the rank of $A$ is 8, what is the dimension of the nullspace $\mathcal{N}(A)$?</li>
        <li>If $A \mathbf{x} = 0$ has only the solution $\mathbf{x}=0$, is this possible? Explain.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>The rank is bounded by the dimensions: $\mathrm{rank}(A) \le \min(m, n) = \min(10, 15) = 10$.</li>
          <li>By the Rank-Nullity Theorem, $\dim(\mathcal{N}(A)) + \mathrm{rank}(A) = n$. Here $n=15$ (number of columns). So $\dim(\mathcal{N}(A)) = 15 - 8 = 7$.</li>
          <li>The condition "only the solution $\mathbf{x}=0$" means $\mathcal{N}(A) = \{0\}$, so $\dim(\mathcal{N}(A)) = 0$. By Rank-Nullity, this would imply $\mathrm{rank}(A) = 15 - 0 = 15$. However, we established in (a) that the maximum rank is 10. Thus, this is <b>impossible</b>. An underdetermined system ($m < n$) always has a non-zero nullspace.</li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-Nullity Theorem:</b> For any matrix $A \in \mathbb{R}^{m \times n}$, the dimension of the domain splits into the nullspace and the row space:
        $$ \dim \mathcal{R}(A) + \dim \mathcal{N}(A) = n $$
        <br><b>Rank Inequalities:</b> $\mathrm{rank}(A) \le \min(m, n)$. Also, $\mathrm{rank}(AB) \le \min(\mathrm{rank}(A), \mathrm{rank}(B))$ and $\mathrm{rank}(A+B) \le \mathrm{rank}(A) + \mathrm{rank}(B)$.
        <br><b>Full Rank Conditions:</b>
        <ul>
            <li><b>Full Column Rank ($m \ge n$):</b> $\mathrm{rank}(A) = n \iff \mathcal{N}(A) = \{0\} \iff A^\top A$ is invertible.</li>
            <li><b>Full Row Rank ($m \le n$):</b> $\mathrm{rank}(A) = m \iff \mathcal{R}(A) = \mathbb{R}^m \iff AA^\top$ is invertible.</li>
        </ul></p>
      </div>




<h3>P0.3 — Trace and Determinant</h3>
      <ol type="a">
        <li>Show that $\mathrm{tr}(ABC) = \mathrm{tr}(BCA)$ but generally $\mathrm{tr}(ABC) \neq \mathrm{tr}(BAC)$. Construct a $2 \times 2$ counterexample.</li>
        <li>Let $A \in \mathbb{R}^{n \times n}$ be skew-symmetric ($A^\top = -A$). Show that $\mathbf{x}^\top A \mathbf{x} = 0$ for all $\mathbf{x}$.</li>
        <li>Use the result from (b) to prove that if $n$ is odd, $\det(A) = 0$. (Hint: $\det(A^\top) = \det(-A)$).</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Cyclic Property:</b> Let $X = AB$. Then $\mathrm{tr}(XC) = \mathrm{tr}(CX)$ (basic cyclic property). Substituting $X=AB$, we get $\mathrm{tr}((AB)C) = \mathrm{tr}(C(AB)) = \mathrm{tr}(CAB)$. Applying it again gives $\mathrm{tr}(BCA)$.
          <br><b>Counterexample for Non-Cyclic:</b> Let $A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$, $B = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$, $C = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$.
          <br>$ABC = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \implies \mathrm{tr}=1$.
          <br>$BAC = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \mathbf{0} \implies \mathrm{tr}=0$.
          </li>
          <li><b>Skew-Symmetric Form:</b> The scalar $\mathbf{x}^\top A \mathbf{x}$ is its own transpose.
          $$ \mathbf{x}^\top A \mathbf{x} = (\mathbf{x}^\top A \mathbf{x})^\top = \mathbf{x}^\top A^\top \mathbf{x} $$
          Since $A^\top = -A$, we have $\mathbf{x}^\top A \mathbf{x} = \mathbf{x}^\top (-A) \mathbf{x} = -\mathbf{x}^\top A \mathbf{x}$.
          The only number equal to its negative is 0. Thus $\mathbf{x}^\top A \mathbf{x} = 0$.
          </li>
          <li><b>Determinant of Skew-Symmetric:</b>
          $$ \det(A) = \det(A^\top) = \det(-A) = (-1)^n \det(A) $$
          If $n$ is odd, $(-1)^n = -1$. So $\det(A) = -\det(A)$, which implies $2\det(A) = 0 \implies \det(A) = 0$.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Trace Properties:</b>
        <ul>
            <li><b>Linearity:</b> $\mathrm{tr}(\alpha A + \beta B) = \alpha \mathrm{tr}(A) + \beta \mathrm{tr}(B)$.</li>
            <li><b>Cyclic Invariance:</b> $\mathrm{tr}(ABC) = \mathrm{tr}(BCA) = \mathrm{tr}(CAB)$. Note: $\mathrm{tr}(ABC) \neq \mathrm{tr}(BAC)$ in general.</li>
        </ul>
        <br><b>Determinant Properties:</b>
        <ul>
            <li><b>Multiplicativity:</b> $\det(AB) = \det(A)\det(B)$.</li>
            <li><b>Transpose:</b> $\det(A^\top) = \det(A)$.</li>
        </ul>
        <br><b>Skew-Symmetric Matrices ($A^\top = -A$):</b>
        <ul>
            <li><b>Quadratic Form:</b> $\mathbf{x}^\top A \mathbf{x} = 0$ for all $\mathbf{x} \in \mathbb{R}^n$.</li>
            <li><b>Eigenvalues:</b> Purely imaginary or zero. If $n$ is odd, at least one eigenvalue is 0 ($\det(A)=0$).</li>
        </ul></p>
      </div>




<h3>P0.4 — Norm Equivalence</h3>
      <p>In finite dimensions, all norms are equivalent. For $\mathbf{x} \in \mathbb{R}^n$, prove the following inequalities:</p>
      <ol type="a">
        <li>$\|x\|_\infty \le \|\mathbf{x}\|_2 \le \sqrt{n} \|\mathbf{x}\|_\infty$</li>
        <li>$\|x\|_2 \le \|\mathbf{x}\|_1 \le \sqrt{n} \|\mathbf{x}\|_2$</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Left inequality:</b> $\|x\|_\infty^2 = \max_i |x_i|^2 \le \sum_i x_i^2 = \|\mathbf{x}\|_2^2$. Taking square roots gives $\|x\|_\infty \le \|\mathbf{x}\|_2$.
          <br><b>Right inequality:</b> $\|x\|_2^2 = \sum_i x_i^2 \le \sum_i (\max_j |x_j|)^2 = \sum_i \|\mathbf{x}\|_\infty^2 = n \|\mathbf{x}\|_\infty^2$. Taking square roots gives $\|x\|_2 \le \sqrt{n}\|\mathbf{x}\|_\infty$.
          </li>
          <li><b>Left inequality:</b> Square $\|x\|_1$: $\|x\|_1^2 = (\sum |x_i|)^2 = \sum x_i^2 + \sum_{i \ne j} |x_i||x_j| = \|\mathbf{x}\|_2^2 + \text{non-negative terms} \ge \|\mathbf{x}\|_2^2$. Thus $\|x\|_1 \ge \|\mathbf{x}\|_2$.
          <br><b>Right inequality:</b> Use Cauchy-Schwarz with the vector of ones $\mathbf{1}$ and the vector $|\mathbf{x}| = (|x_1|, \dots, |x_n|)$.
          $$ \|\mathbf{x}\|_1 = \sum |x_i| \cdot 1 = |\mathbf{x}|^\top \mathbf{1} \le \||\mathbf{x}|\|_2 \|\mathbf{1}\|_2 = \|\mathbf{x}\|_2 \sqrt{n} $$
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Norm Equivalence Theorem:</b> In finite-dimensional vector spaces, all norms are equivalent. For any two norms $\|\cdot\|_a, \|\cdot\|_b$, there exist constants $c, C > 0$ such that $c \|\mathbf{x}\|_a \le \|\mathbf{x}\|_b \le C \|\mathbf{x}\|_a$. This implies they define the same topology (convergence sequences are the same).
        <br><b>Standard Inequalities:</b>
        <ul>
            <li>$\|x\|_\infty \le \|\mathbf{x}\|_2 \le \|\mathbf{x}\|_1$ (Hierarchy of norms)</li>
            <li>$\|x\|_2 \le \sqrt{n} \|\mathbf{x}\|_\infty$</li>
            <li>$\|x\|_1 \le \sqrt{n} \|\mathbf{x}\|_2$ (Cauchy-Schwarz with $\mathbf{1}$)</li>
        </ul>
        <br><b>Geometry:</b>
        <ul>
            <li>$\ell_1$ ball: Cross-polytope (Diamond).</li>
            <li>$\ell_2$ ball: Sphere.</li>
            <li>$\ell_\infty$ ball: Hypercube.</li>
        </ul></p>
      </div>




<h3>P0.5 — Least Squares from Scratch</h3>
      <p>Consider the function $f(\mathbf{x}) = \frac{1}{2} \|A\mathbf{x} - \mathbf{b}\|_2^2$.</p>
      <ol type="a">
        <li>Expand the squared norm into terms involving $\mathbf{x}^\top A^\top A \mathbf{x}$, etc.</li>
        <li>Compute the gradient $\nabla f(\mathbf{x})$ step-by-step.</li>
        <li>Set the gradient to zero to derive the Normal Equations.</li>
        <li>Show that if $\mathcal{N}(A) = \{0\}$, the Hessian is positive definite, ensuring a unique global minimum.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>$f(\mathbf{x}) = \frac{1}{2}(A\mathbf{x} - \mathbf{b})^\top (A\mathbf{x} - \mathbf{b}) = \frac{1}{2}(\mathbf{x}^\top A^\top - \mathbf{b}^\top)(A\mathbf{x} - \mathbf{b}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \frac{1}{2} \mathbf{x}^\top A^\top \mathbf{b} - \frac{1}{2} \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b}$.
          Since scalar transpose is identity ($\mathbf{x}^\top A^\top \mathbf{b} = \mathbf{b}^\top A \mathbf{x}$), we simplify to:
          $$ f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top A^\top A \mathbf{x} - \mathbf{b}^\top A \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b} $$
          </li>
          <li><b>Gradient Derivation:</b>
            <br>Term 1: $\frac{1}{2} \mathbf{x}^\top (A^\top A) \mathbf{x}$. Let $Q = A^\top A$ (symmetric).
            $$ \nabla (\frac{1}{2} \mathbf{x}^\top Q \mathbf{x}) = \frac{1}{2} (Q + Q^\top) \mathbf{x} = \frac{1}{2} (2Q) \mathbf{x} = Q\mathbf{x} = A^\top A \mathbf{x} $$
            <br>Term 2: $-\mathbf{b}^\top A \mathbf{x}$. This is linear in $\mathbf{x}$. We can rewrite it as $-(A^\top \mathbf{b})^\top \mathbf{x}$.
            The gradient of $c^\top \mathbf{x}$ is $c$. Here $c = -(A^\top \mathbf{b})$.
            $$ \nabla (-\mathbf{b}^\top A \mathbf{x}) = -A^\top \mathbf{b} $$
            <br>Term 3: $\frac{1}{2} \mathbf{b}^\top \mathbf{b}$. Constant w.r.t $\mathbf{x}$, gradient is 0.
            <br><b>Result:</b> $\nabla f(\mathbf{x}) = A^\top A \mathbf{x} - A^\top \mathbf{b} = A^\top (A\mathbf{x} - \mathbf{b})$.
          </li>
          <li>$\nabla f(\mathbf{x}) = 0 \implies A^\top A \mathbf{x} - A^\top \mathbf{b} = 0 \implies A^\top A \mathbf{x} = A^\top \mathbf{b}$. These are the Normal Equations.
          </li>
          <li>$\nabla^2 f(\mathbf{x}) = A^\top A$.
          If $\mathcal{N}(A) = \{0\}$, then for any $\mathbf{v} \neq 0$, $A\mathbf{v} \neq 0$.
          $$ \mathbf{v}^\top A^\top A \mathbf{v} = (A\mathbf{v})^\top (A\mathbf{v}) = \|A\mathbf{v}\|_2^2 > 0 $$
          Thus the Hessian is positive definite, which guarantees strict convexity and a unique global minimum.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Least Squares Objective:</b> $f(\mathbf{x}) = \frac{1}{2}\|A\mathbf{x} - \mathbf{b}\|_2^2$ is a convex quadratic function.
        <br><b>Gradients:</b>
        <ul>
            <li>$\nabla (\frac{1}{2} \mathbf{x}^\top Q \mathbf{x}) = Q\mathbf{x}$ (for symmetric $Q$).</li>
            <li>$\nabla (c^\top \mathbf{x}) = c$.</li>
        </ul>
        <br><b>Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$. This system represents the condition $A^\top (A\mathbf{x} - \mathbf{b}) = 0$, meaning the residual is orthogonal to the columns of $A$.
        <br><b>Convexity & Uniqueness:</b>
        <ul>
            <li>The Hessian $\nabla^2 f(\mathbf{x}) = A^\top A$ is always PSD ($\succeq 0$), ensuring convexity.</li>
            <li>If $\mathcal{N}(A) = \{0\}$ (Full Column Rank), $A^\top A$ is PD ($\succ 0$), ensuring <b>strict convexity</b> and a <b>unique global minimum</b>.</li>
        </ul></p>
      </div>




<h3>P0.6 — Matrix Calculus Practice</h3>
      <p>Compute the gradient with respect to $X \in \mathbb{R}^{n \times n}$ for the following functions:</p>
      <ol type="a">
        <li>$f(X) = \mathrm{tr}(A X B)$, where $A, B$ are constant matrices.</li>
        <li>$f(X) = \mathrm{tr}(X^\top X)$.</li>
        <li>$f(X) = a^\top X \mathbf{b}$, where $a, \mathbf{b}$ are constant vectors.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>Use the cyclic property: $\mathrm{tr}(AXB) = \mathrm{tr}(BA X)$.
          The gradient of $\mathrm{tr}(M X)$ is $M^\top$. Here $M = BA$.
          Thus $\nabla_X f(X) = (BA)^\top = A^\top B^\top$.
          </li>
          <li>$f(X) = \|X\|_F^2 = \sum_{ij} X_{ij}^2$.
          $\frac{\partial f}{\partial X_{ij}} = 2 X_{ij}$.
          Thus $\nabla_X f(X) = 2X$.
          </li>
          <li>$a^\top X \mathbf{b} = \mathrm{tr}(a^\top X \mathbf{b}) = \mathrm{tr}(\mathbf{b} a^\top X)$.
          Using the rule from (a) with $M = \mathbf{b} a^\top$:
          $\nabla_X f(X) = (\mathbf{b} a^\top)^\top = (a^\top)^\top \mathbf{b}^\top = a \mathbf{b}^\top$.
          (This is an outer product matrix).
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Matrix Calculus Toolkit:</b>
        <br><b>Definition:</b> The gradient $\nabla_X f(X)$ is the matrix $G$ such that the first-order approximation is $f(X+H) \approx f(X) + \langle G, H \rangle = f(X) + \mathrm{tr}(G^\top H)$.
        <br><b>Key Formulas:</b>
        <ul>
            <li>$\nabla_X \mathrm{tr}(A X) = A^\top$.</li>
            <li>$\nabla_X \mathrm{tr}(X^\top A X) = (A + A^\top)X$. (For symmetric $A$, $2AX$).</li>
            <li>$\nabla_X \|X\|_F^2 = 2X$.</li>
        </ul>
        <br><b>Strategy:</b> Perturb $X \to X + H$, expand terms linear in $H$, and rearrange trace terms to match the form $\mathrm{tr}(G^\top H)$.</p>
      </div>




<h3>P0.7 — Hessian of a Cubic</h3>
      <p>Let $f: \mathbb{R}^2 \to \mathbb{R}$ be defined by $f(\mathbf{x}) = x_1^3 + x_2^3 + 2x_1 x_2$.</p>
      <ol type="a">
        <li>Compute the gradient $\nabla f(\mathbf{x})$.</li>
        <li>Compute the Hessian matrix $\nabla^2 f(\mathbf{x})$.</li>
        <li>For which $\mathbf{x}$ is the Hessian Positive Semidefinite? (This identifies the region where the function is locally convex).</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>Gradient:
          $$ \frac{\partial f}{\partial x_1} = 3x_1^2 + 2x_2, \quad \frac{\partial f}{\partial x_2} = 3x_2^2 + 2x_1 $$
          $\nabla f(\mathbf{x}) = \begin{bmatrix} 3x_1^2 + 2x_2 \\ 3x_2^2 + 2x_1 \end{bmatrix}$.
          </li>
          <li>Hessian:
          $$ \nabla^2 f(\mathbf{x}) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} \end{bmatrix} = \begin{bmatrix} 6x_1 & 2 \\ 2 & 6x_2 \end{bmatrix} $$
          </li>
          <li>PSD Condition:
          A $2 \times 2$ matrix is PSD iff trace $\ge 0$ and determinant $\ge 0$.
          Trace: $6x_1 + 6x_2 \ge 0 \implies x_1 + x_2 \ge 0$.
          Determinant: $36x_1 x_2 - 4 \ge 0 \implies 9x_1 x_2 \ge 1$.
          The condition $x_1 x_2 \ge 1/9$ implies $x_1, x_2$ have the same sign.
          If both negative, $x_1 + x_2 < 0$, violating the trace condition.
          Thus, we need $x_1 > 0, x_2 > 0$ and $x_1 x_2 \ge 1/9$. This region (hyperbola in the first quadrant) is where the function is locally convex.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Convexity Condition:</b> A twice-differentiable function is convex on a domain iff its Hessian matrix is Positive Semidefinite ($\nabla^2 f(\mathbf{x}) \succeq 0$) everywhere in that domain.
        <br><b>Sylvester's Criterion (for $A \succ 0$):</b> A symmetric matrix is Positive Definite iff all <b>leading principal minors</b> (determinants of top-left $k \times k$ submatrices) are positive.
        <br><b>2x2 PSD Test:</b> $\begin{bmatrix} a & \mathbf{b} \\ \mathbf{b} & c \end{bmatrix} \succeq 0 \iff a \ge 0, c \ge 0, ac - \mathbf{b}^2 \ge 0$. (Trace $\ge 0$ and Det $\ge 0$).</p>
      </div>




<h3>P0.8 — Testing Positive Semidefiniteness</h3>
      <p>Determine whether the following matrices are PSD, PD, Indefinite, or Negative Definite/Semidefinite.</p>
      <ol type="a">
        <li>$A = \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$</li>
        <li>$B = \begin{bmatrix} 1 & 2 \\ 2 & 1 \end{bmatrix}$</li>
        <li>$C = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 3 \end{bmatrix}$</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Positive Definite.</b> Using Sylvester's Criterion (Leading Principal Minors): $D_1 = 2 > 0$, $D_2 = \det(A) = 4 - 1 = 3 > 0$. Since all leading principal minors are positive, $A \succ 0$. Alternatively, eigenvalues are $\lambda = 1, 3$.</li>
          <li><b>Indefinite.</b> The determinant is $\det(B) = 1 - 4 = -3 < 0$. Since the product of eigenvalues is negative, they must have opposite signs ($\lambda = 3, -1$). Thus, $B$ is indefinite.</li>
          <li><b>Positive Semidefinite.</b> This is a diagonal matrix with entries $1, 0, 3$, which are the eigenvalues. Since all $\lambda_i \ge 0$ and one is zero, $C \succeq 0$ but is not positive definite.</li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Characterizing Definiteness:</b>
        <ul>
            <li><b>Positive Definite (PD):</b> All $\lambda_i > 0$. Strictly convex bowl.</li>
            <li><b>Positive Semidefinite (PSD):</b> All $\lambda_i \ge 0$. Convex bowl (possibly flat).</li>
            <li><b>Indefinite:</b> Some $\lambda_i > 0$, some $\lambda_i < 0$. Saddle point geometry.</li>
            <li><b>Negative Definite (ND):</b> All $\lambda_i < 0$. Concave cap.</li>
        </ul>
        <br><b>Practical Checks:</b>
        <ul>
            <li>Check determinant (product of eigenvalues) and trace (sum of eigenvalues).</li>
            <li>If $\det(A) < 0$, it must have at least one negative eigenvalue (assuming $n$ even? No, if $\det < 0$ product is negative, so odd number of negative eigenvals. If $n=2$, exactly one neg).</li>
        </ul></p>
      </div>




<h3>P0.9 — Schur Complement Application</h3>
      <p>Use the Schur Complement condition (assuming the top-left pivot is positive) to determine the range of $\mathbf{x}$ for which the matrix $M(\mathbf{x})$ is Positive Semidefinite:</p>
      $$ M(\mathbf{x}) = \begin{bmatrix} \mathbf{x} & 1 \\ 1 & \mathbf{x} \end{bmatrix} $$
      <p>Verify your answer by computing the eigenvalues directly.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Schur Complement Method:</b>
        For $M \succeq 0$, we need the top-left block $A=\mathbf{x} > 0$ and the Schur complement $S = D - C A^{-1} B \ge 0$.
        Here $A=\mathbf{x}, B=1, C=1, D=\mathbf{x}$.
        $S = \mathbf{x} - 1(1/\mathbf{x})(1) = \mathbf{x} - 1/\mathbf{x}$.
        We need $\mathbf{x} > 0$ and $\mathbf{x} - 1/\mathbf{x} \ge 0$.
        $\mathbf{x} - 1/\mathbf{x} \ge 0 \implies \mathbf{x}^2 \ge 1$ (since $\mathbf{x}>0$). Thus $\mathbf{x} \ge 1$.
        So the range is $\mathbf{x} \ge 1$.
        </p>
        <p><b>Eigenvalue Verification:</b>
        Characteristic eq: $(\mathbf{x}-\lambda)^2 - 1 = 0 \implies \mathbf{x}-\lambda = \pm 1 \implies \lambda = \mathbf{x} \pm 1$.
        For PSD, we need $\lambda_{\min} = \mathbf{x}-1 \ge 0$ and $\lambda_{\max} = \mathbf{x}+1 \ge 0$.
        $\mathbf{x}-1 \ge 0 \implies \mathbf{x} \ge 1$. This automatically satisfies $\mathbf{x}+1 \ge 2 \ge 0$.
        Matches.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Schur Complement Lemma:</b> A block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$ (with $A \succ 0$) is PSD if and only if the Schur complement $S = C - B^\top A^{-1} B$ is PSD.
        <br><b>LMI (Linear Matrix Inequality):</b> A constraint of the form $F(\mathbf{x}) \succeq 0$ where $F$ is affine in $\mathbf{x}$.
        <br><b>Trick:</b> Use Schur complements to convert nonlinear constraints (like $\mathbf{x}^2 \le \mathbf{y}$ or quadratic-over-linear forms) into equivalent Linear Matrix Inequalities. This allows them to be solved using Semidefinite Programming (SDP).</p>
      </div>




<h3>P0.10 — Projection onto a Line</h3>
      <p>Let $a = (1, 1, 1)^\top$. Find the projection of $\mathbf{b} = (1, 0, 0)^\top$ onto the line spanned by $a$. Verify that the residual $\mathbf{b} - \mathbf{p}$ is orthogonal to $a$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Projection formula: $\mathbf{p} = \frac{a^\top \mathbf{b}}{a^\top a} a$.
        $a^\top \mathbf{b} = 1 \cdot 1 + 1 \cdot 0 + 1 \cdot 0 = 1$.
        $a^\top a = 1^2 + 1^2 + 1^2 = 3$.
        $\mathbf{p} = \frac{1}{3} (1, 1, 1)^\top = (1/3, 1/3, 1/3)^\top$.
        <br>Residual $\mathbf{r} = \mathbf{b} - \mathbf{p} = (1-1/3, 0-1/3, 0-1/3)^\top = (2/3, -1/3, -1/3)^\top$.
        <br>Check orthogonality: $a^\top \mathbf{r} = 1(2/3) + 1(-1/3) + 1(-1/3) = 0$. Verified.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Vector Projection:</b> The projection of a vector $\mathbf{b}$ onto the line spanned by a non-zero vector $a$ is given by:
        $$ \Pi_a(\mathbf{b}) = \frac{\langle a, \mathbf{b} \rangle}{\|a\|^2} a $$
        <br><b>Geometric Intuition:</b> This scales $a$ by the "shadow" of $\mathbf{b}$ onto $a$.
        <br><b>Projection Matrix:</b> $P = \frac{a a^\top}{a^\top a}$ is a rank-1 orthogonal projector.</p>
      </div>




<h3>P0.11 — Projection onto a Hyperplane</h3>
      <p>Find the projection of the point $\mathbf{y} = (3, 3)^\top$ onto the line (hyperplane in 2D) defined by $x_1 + x_2 = 2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The line is $a^\top \mathbf{x} = \mathbf{b}$ with $a=(1, 1)^\top, \mathbf{b}=2$.
        The formula for projection onto a hyperplane is $\mathbf{p} = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a$.
        $a^\top \mathbf{y} = 3+3=6$.
        $\|a\|^2 = 2$.
        $\mathbf{p} = (3, 3)^\top - \frac{6 - 2}{2} (1, 1)^\top = (3, 3)^\top - 2(1, 1)^\top = (3, 3)^\top - (2, 2)^\top = (1, 1)^\top$.
        <br>Check: $1+1=2$. Point is on the line. Residual $(2, 2)$ is parallel to normal $(1, 1)$. Correct.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hyperplane Definition:</b> A hyperplane is an affine set defined by a single linear equality constraint: $\mathcal{H} = \{\mathbf{x} \mid a^\top \mathbf{x} = \mathbf{b}\}$.
        <br><b>Projection Formula:</b> To project a point $\mathbf{y}$ onto $\mathcal{H}$, you move in the direction of the normal vector $a$ (the most direct path) by exactly the amount needed to satisfy the equation:
        $$ \Pi_{\mathcal{H}}(\mathbf{y}) = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a $$
        <br><b>Signed Distance:</b> The quantity $\frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|}$ is the signed distance from $\mathbf{y}$ to the hyperplane.</p>
      </div>




<h3>P0.12 — Orthogonal Complements</h3>
      <p>Let $S$ be a subspace of $\mathbb{R}^n$. The orthogonal complement is $S^\perp = \{\mathbf{y} \mid \mathbf{y}^\top \mathbf{x} = 0 \ \forall \mathbf{x} \in S\}$.</p>
      <ol type="a">
        <li>Prove that $S^\perp$ is a subspace.</li>
        <li>Prove that $S \cap S^\perp = \{0\}$.</li>
        <li>If $S = \text{span}((1, 0, 0)^\top, (0, 1, 0)^\top)$, calculate $S^\perp$.</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Subspace:</b> Let $y_1, y_2 \in S^\perp$. Then $y_1^\top \mathbf{x} = 0$ and $y_2^\top \mathbf{x} = 0$.
          For any linear combination $\alpha y_1 + \beta y_2$, we have $(\alpha y_1 + \beta y_2)^\top \mathbf{x} = \alpha(y_1^\top \mathbf{x}) + \beta(y_2^\top \mathbf{x}) = 0$.
          Thus closed under linear combinations. Contains 0 since $0^\top \mathbf{x} = 0$.
          </li>
          <li><b>Intersection:</b> Let $\mathbf{x} \in S \cap S^\perp$. Since $\mathbf{x} \in S$ and $\mathbf{x} \in S^\perp$, it must be orthogonal to itself: $\mathbf{x}^\top \mathbf{x} = 0$.
          $\|x\|^2 = 0 \implies \mathbf{x} = 0$. Thus the intersection contains only the zero vector.
          </li>
          <li><b>Calculation:</b> $S$ is the $xy$-plane ($\mathbf{z}=0$). $S^\perp$ is the set of vectors orthogonal to $(1,0,0)$ and $(0,1,0)$.
          $\mathbf{y} \cdot e_1 = y_1 = 0$. $\mathbf{y} \cdot e_2 = y_2 = 0$.
          Thus $\mathbf{y} = (0, 0, y_3)^\top$. $S^\perp$ is the $\mathbf{z}$-axis (span of $e_3$).
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Definition:</b> The orthogonal complement $S^\perp$ contains all vectors orthogonal to every vector in the subspace $S$.
        <br><b>Fundamental Properties:</b>
        <ul>
            <li><b>Decomposition:</b> $\mathbb{R}^n = S \oplus S^\perp$. Any vector $\mathbf{x}$ can be uniquely written as $\mathbf{x} = x_S + x_{\perp}$, with $x_S \in S$ and $x_{\perp} \in S^\perp$.</li>
            <li><b>Dimension:</b> $\dim(S) + \dim(S^\perp) = n$.</li>
            <li><b>Duality:</b> $(S^\perp)^\perp = S$.</li>
        </ul>
        <br><b>Connection to Matrices:</b> $\mathcal{R}(A)^\perp = \mathcal{N}(A^\top)$. (Fundamental Theorem of Linear Algebra).</p>
      </div>




<h3>P0.13 — Frobenius Norm Submultiplicativity</h3>
      <p>We proved $\|AB\|_F \le \|A\|_F \|B\|_F$ in the lecture using Cauchy-Schwarz.
      <br>Re-derive this result by writing $\|X\|_F^2 = \mathrm{tr}(X^\top X)$ and using the property $\mathrm{tr}(M) = \sum \lambda_i(M)$ along with the fact that $\lambda_{\max}(A^\top A) \le \mathrm{tr}(A^\top A)$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          We want to bound $\|AB\|_F^2 = \mathrm{tr}(B^\top A^\top A B)$.
          Using the cyclic property: $\mathrm{tr}(B^\top (A^\top A) B) = \mathrm{tr}((A^\top A) (B B^\top))$.
        </div>
        <div class="proof-step">
          <strong>Step 2: Trace Inequality.</strong>
          Let $P = A^\top A$ and $Q = B B^\top$. Both are symmetric positive semidefinite (PSD).
          We claim $\mathrm{tr}(PQ) \le \lambda_{\max}(P) \mathrm{tr}(Q)$.
          <br><i>Proof:</i> Since $Q$ is symmetric, it has an eigendecomposition $Q = U \Lambda U^\top$ with $\lambda_i \ge 0$.
          $$ \mathrm{tr}(PQ) = \mathrm{tr}(P U \Lambda U^\top) = \mathrm{tr}(U^\top P U \Lambda) $$
          Let $M = U^\top P U$. The diagonal entry $M_{ii} = u_i^\top P u_i$.
          Since $u_i$ is a unit vector (column of orthogonal matrix $U$), the Rayleigh quotient implies:
          $$ \lambda_{\min}(P) \le u_i^\top P u_i \le \lambda_{\max}(P) $$
          Thus $M_{ii} \le \lambda_{\max}(P)$.
          $$ \mathrm{tr}(M \Lambda) = \sum_i M_{ii} \lambda_i \le \sum_i \lambda_{\max}(P) \lambda_i = \lambda_{\max}(P) \sum_i \lambda_i = \lambda_{\max}(P) \mathrm{tr}(Q) $$
        </div>
        <div class="proof-step">
          Applying this: $\|AB\|_F^2 \le \lambda_{\max}(A^\top A) \mathrm{tr}(B B^\top) = \|A\|_2^2 \|B\|_F^2$.
          Since the spectral norm $\|A\|_2$ satisfies $\|A\|_2 \le \|A\|_F$, we have $\|A\|_2^2 \le \|A\|_F^2$.
          <br>Thus $\|AB\|_F^2 \le \|A\|_F^2 \|B\|_F^2$. Taking square roots gives the result.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Frobenius Norm:</b> $\|A\|_F = \sqrt{\sum_{i,j} A_{ij}^2} = \sqrt{\mathrm{tr}(A^\top A)}$. It measures the total "energy" of the matrix entries.
        <br><b>Algebraic Property:</b> The Frobenius norm is <b>submultiplicative</b> (or consistent): $\|AB\|_F \le \|A\|_F \|B\|_F$.
        <br><b>Comparison:</b> $\|A\|_2 \le \|A\|_F \le \sqrt{rank(A)} \|A\|_2$. The spectral norm is "tighter".</p>
      </div>




<h3>P0.14 — Spectral Norm Properties</h3>
      <p>Let $\|A\|_2$ denote the spectral norm (max singular value).</p>
      <ol type="a">
        <li>Show that $\|A\|_2 = 0$ if and only if $A=0$.</li>
        <li>Show that $\|Q A\|_2 = \|A\|_2$ for any orthogonal matrix $Q$. (Orthogonal invariance).</li>
      </ol>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>$\|A\|_2 = \sigma_{\max}(A)$. Singular values are non-negative. The max is 0 iff all singular values are 0. If $\Sigma=0$ in SVD $A=U\Sigma V^\top$, then $A=0$. Conversely if $A=0$, the maximum stretch is 0.</li>
          <li>$\|QA\|_2 = \sup_{\|\mathbf{x}\|=1} \|QA\mathbf{x}\|_2$. Since $Q$ is orthogonal, it preserves Euclidean norms: $\|QAx\|_2 = \|Ax\|_2$.
          Thus $\sup_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|_2 = \|A\|_2$. The spectral norm is invariant under left (and right) orthogonal multiplication.</li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Definition:</b> The spectral norm (or operator norm) is the maximum amplification of a vector's length: $\|A\|_2 = \sup_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2$.
        <br><b>Calculation:</b> $\|A\|_2 = \sigma_{\max}(A) = \sqrt{\lambda_{\max}(A^\top A)}$.
        <br><b>Invariance:</b> The spectral norm is <b>orthogonally invariant</b>. $\|Q A Z\|_2 = \|A\|_2$ for any orthogonal matrices $Q, Z$. It depends only on the singular values (intrinsic geometry).</p>
      </div>




<h3>P0.15 — Isometries</h3>
      <p>An isometry is a linear map $f(\mathbf{x}) = Q\mathbf{x}$ that preserves distances: $\|Qx - Qy\|_2 = \|x - y\|_2$ for all $\mathbf{x}, \mathbf{y}$.
      <br>Show that this condition implies $Q^\top Q = I$. Thus, isometries are represented by orthogonal matrices.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Step 1: Norm Preservation.</strong>
          Setting $\mathbf{y}=0$, linearity implies $Q(0)=0$, so $\|Qx\|_2 = \|x\|_2$ for all $\mathbf{x}$.
          Squaring both sides:
          $$ \mathbf{x}^\top Q^\top Q \mathbf{x} = \mathbf{x}^\top I \mathbf{x} \implies \mathbf{x}^\top (Q^\top Q - I) \mathbf{x} = 0 $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Polarization Identity.</strong>
          Let $M = Q^\top Q - I$. We have $\mathbf{x}^\top M \mathbf{x} = 0$ for all $\mathbf{x}$. Since $M$ is symmetric, we can recover the values of the bilinear form $\mathbf{x}^\top M \mathbf{y}$ using the polarization identity:
          $$ \mathbf{x}^\top M \mathbf{y} = \frac{1}{4} \left( (\mathbf{x}+\mathbf{y})^\top M (\mathbf{x}+\mathbf{y}) - (\mathbf{x}-\mathbf{y})^\top M (\mathbf{x}-\mathbf{y}) \right) $$
          Since the quadratic form is zero for any vector (including $\mathbf{x}+\mathbf{y}$ and $\mathbf{x}-\mathbf{y}$), the right-hand side is zero.
          Thus, $\mathbf{x}^\top M \mathbf{y} = 0$ for all $\mathbf{x}, \mathbf{y}$.
        </div>
        <div class="proof-step">
          <strong>Step 3: Conclusion.</strong>
          Choosing $\mathbf{x} = e_i$ and $\mathbf{y} = e_j$ yields $e_i^\top M e_j = M_{ij} = 0$.
          Since all entries are zero, $M = 0$, so $Q^\top Q = I$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Isometry Definition:</b> A linear map $Q$ is an isometry if $\|Qx\|_2 = \|x\|_2$ for all $\mathbf{x}$.
        <br><b>Polarization Identity:</b> Inner products can be recovered solely from norm evaluations:
        $$ \langle \mathbf{x}, \mathbf{y} \rangle = \frac{1}{4} (\|\mathbf{x}+\mathbf{y}\|^2 - \|\mathbf{x}-\mathbf{y}\|^2) $$
        <br><b>Conclusion:</b> Preserving lengths is equivalent to preserving angles (inner products). Thus, isometries in Euclidean space are exactly the orthogonal matrices ($Q^\top Q = I$).</p>
      </div>




<h3>P0.16 — Loewner Order Transitivity</h3>
      <p>The Loewner order is defined as $X \succeq Y \iff X - Y \succeq 0$. Prove that this order is transitive: if $X \succeq Y$ and $Y \succeq Z$, then $X \succeq Z$. Use the variational definition of PSD matrices.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $\mathbf{v} \in \mathbb{R}^n$ be any vector.
        $X \succeq Y \implies \mathbf{v}^\top (X-Y) \mathbf{v} \ge 0 \implies \mathbf{v}^\top X \mathbf{v} \ge \mathbf{v}^\top Y \mathbf{v}$.
        $Y \succeq Z \implies \mathbf{v}^\top (Y-Z) \mathbf{v} \ge 0 \implies \mathbf{v}^\top Y \mathbf{v} \ge \mathbf{v}^\top Z \mathbf{v}$.
        combining inequalities: $\mathbf{v}^\top X \mathbf{v} \ge \mathbf{v}^\top Y \mathbf{v} \ge \mathbf{v}^\top Z \mathbf{v}$.
        Thus $\mathbf{v}^\top (X-Z) \mathbf{v} \ge 0$.
        Since this holds for all $\mathbf{v}$, $X - Z \succeq 0$, so $X \succeq Z$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Loewner Order ($\succeq$):</b> A partial ordering on the set of symmetric matrices defined by $X \succeq Y \iff X - Y \succeq 0$ (i.e., $X-Y$ is PSD).
        <br><b>Key Properties:</b>
        <ul>
            <li><b>Transitivity:</b> $X \succeq Y$ and $Y \succeq Z \implies X \succeq Z$.</li>
            <li><b>Additivity:</b> $X \succeq Y \implies X+Z \succeq Y+Z$.</li>
            <li><b>Congruence:</b> $X \succeq Y \implies T^\top X T \succeq T^\top Y T$ for any $T$.</li>
            <li><b>Inversion:</b> If $X, Y \succ 0$, then $X \succeq Y \implies Y^{-1} \succeq X^{-1}$ (order reversal).</li>
        </ul></p>
      </div>




<h3>P0.17 — Weighted Inner Product</h3>
      <p>Let $A \in \mathbb{S}^n_{++}$ be a symmetric positive definite matrix.
      <br>(a) Prove that $\langle \mathbf{x}, \mathbf{y} \rangle_A = \mathbf{x}^\top A \mathbf{y}$ satisfies all axioms of an inner product.
      <br>(b) Write down the Cauchy-Schwarz inequality for this inner product.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li><b>Symmetry:</b> $\langle \mathbf{x}, \mathbf{y} \rangle_A = \mathbf{x}^\top A \mathbf{y} = (\mathbf{x}^\top A \mathbf{y})^\top = \mathbf{y}^\top A^\top \mathbf{x} = \mathbf{y}^\top A \mathbf{x} = \langle \mathbf{y}, \mathbf{x} \rangle_A$ (since $A=A^\top$).
          <br><b>Linearity:</b> Linear in first arg (matrix multiplication distributes).
          <br><b>Positive Definiteness:</b> $\langle \mathbf{x}, \mathbf{x} \rangle_A = \mathbf{x}^\top A \mathbf{x}$. Since $A \succ 0$, this is $>0$ for all $\mathbf{x} \neq 0$.
          </li>
          <li><b>Cauchy-Schwarz:</b> $|\langle \mathbf{x}, \mathbf{y} \rangle_A| \le \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle_A} \sqrt{\langle \mathbf{y}, \mathbf{y} \rangle_A}$.
          Explicitly: $|\mathbf{x}^\top A \mathbf{y}| \le \sqrt{\mathbf{x}^\top A \mathbf{x}} \sqrt{\mathbf{y}^\top A \mathbf{y}}$.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Weighted Inner Product:</b> Given a Positive Definite matrix $P \succ 0$, the function $\langle \mathbf{x}, \mathbf{y} \rangle_P = \mathbf{x}^\top P \mathbf{y}$ defines a valid inner product.
        <br><b>Mahalanobis Distance:</b> The induced norm $\|x\|_P = \sqrt{\mathbf{x}^\top P \mathbf{x}}$ measures distance accounting for correlations/scales defined by $P$.
        <br><b>Geometric Meaning:</b> The unit ball $\{x \mid \mathbf{x}^\top P \mathbf{x} \le 1\}$ is an ellipsoid centered at the origin.</p>
      </div>




<h3>P0.18 — Characterization of Projectors</h3>
      <p>A matrix $P$ is an orthogonal projector if and only if it is idempotent ($P^2=P$) and symmetric ($P^\top=P$).
      <br>(a) Prove that if $P$ is an orthogonal projector onto a subspace $S$, it satisfies these conditions.
      <br>(b) Prove the converse.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <ol type="a">
          <li>Let $P$ be the orthogonal projector onto $S$. Let $\mathbf{x} = \mathbf{u} + \mathbf{v}$ where $\mathbf{u} \in S, \mathbf{v} \in S^\perp$. Then $P\mathbf{x} = \mathbf{u}$.
          <br><b>Idempotent:</b> $P^2 \mathbf{x} = P(P\mathbf{x}) = P\mathbf{u}$. Since $\mathbf{u} \in S$, $P\mathbf{u} = \mathbf{u}$. Thus $P^2 \mathbf{x} = \mathbf{u} = P\mathbf{x}$. So $P^2 = P$.
          <br><b>Symmetric:</b> Check $\langle P\mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{x}, P\mathbf{y} \rangle$.
          Decompose $\mathbf{x}=x_S+x_\perp, \mathbf{y}=y_S+y_\perp$.
          $\langle P\mathbf{x}, \mathbf{y} \rangle = \langle x_S, y_S+y_\perp \rangle = \langle x_S, y_S \rangle$ (since $S \perp S^\perp$).
          $\langle \mathbf{x}, P\mathbf{y} \rangle = \langle x_S+x_\perp, y_S \rangle = \langle x_S, y_S \rangle$.
          They are equal, so $P$ is symmetric.
          </li>
          <li>Let $P=P^\top=P^2$. Define $S = \mathcal{R}(P)$.
          For any $\mathbf{x}$, $\mathbf{x} = P\mathbf{x} + (I-P)\mathbf{x}$.
          $P\mathbf{x} \in S$. $(I-P)\mathbf{x}$ is in $S^\perp$? Check orthogonality:
          $(P\mathbf{x})^\top (I-P)\mathbf{x} = \mathbf{x}^\top P^\top (I-P) \mathbf{x} = \mathbf{x}^\top (P - P^2) \mathbf{x}$.
          Since $P^2=P$, this is 0.
          Thus $\mathbf{x}$ is decomposed into a component in $S$ and a component orthogonal to $S$. $P$ maps $\mathbf{x}$ to the component in $S$. This is the definition of an orthogonal projector.
          </li>
        </ol>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Projector Theorem:</b> A matrix $P$ represents an orthogonal projection onto some subspace if and only if:
        <ul>
            <li><b>$P^2 = P$:</b> It is a projection (idempotent). Applying it twice is the same as applying it once.</li>
            <li><b>$P^\top = P$:</b> It is orthogonal (symmetric). The residual is orthogonal to the range.</li>
        </ul>
        <br><b>Oblique Projectors:</b> If $P^2=P$ but $P \neq P^\top$, it is an <i>oblique</i> projection (projects along a non-orthogonal angle).</p>
      </div>




<h3>P0.19 — PSD Cone in 2D</h3>
      <p>Consider the space of $2 \times 2$ symmetric matrices, which has dimension 3.
      <br>Write down the explicit inequalities defining the PSD cone $S \succeq 0$ in terms of the matrix entries $\mathbf{x}, \mathbf{y}, \mathbf{z}$ (where $S = \begin{bmatrix} \mathbf{x} & \mathbf{y} \\ \mathbf{y} & \mathbf{z} \end{bmatrix}$). Relate this to the trace and determinant conditions.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>A $2 \times 2$ matrix is PSD if and only if:
        1. Trace $\ge 0$: $\mathbf{x} + \mathbf{z} \ge 0$.
        2. Determinant $\ge 0$: $xz - \mathbf{y}^2 \ge 0$.
        <br>Note that $xz \ge \mathbf{y}^2 \ge 0$ implies $\mathbf{x}$ and $\mathbf{z}$ have the same sign.
        Since their sum is non-negative, both must be non-negative: $\mathbf{x} \ge 0, \mathbf{z} \ge 0$.
        <br>Thus the conditions are:
        $$ \mathbf{x} \ge 0, \quad \mathbf{z} \ge 0, \quad xz \ge \mathbf{y}^2 $$
        Geometrically, this is a cone in $\mathbb{R}^3$. The boundary $xz=\mathbf{y}^2$ is a rotated quadratic cone surface.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The PSD Cone $\mathbb{S}^n_+$:</b> The set of all symmetric positive semidefinite matrices forms a convex cone in $\mathbb{S}^n$.
        <br><b>Geometry (2x2):</b> In the 3D space of entries $(\mathbf{x}, \mathbf{y}, \mathbf{z})$, the PSD cone is defined by $\mathbf{x} \ge 0, \mathbf{z} \ge 0, xz \ge \mathbf{y}^2$. This looks like an "ice cream cone" with a non-circular cross-section.
        <br><b>Boundary:</b> The boundary of the cone consists of singular PSD matrices (rank deficient, $\det(X)=0$).
        <br><b>Interior:</b> The interior consists of Positive Definite (PD) matrices ($\det(X)>0, \mathbf{x}>0$).</p>
      </div>




<h3>P0.20 — General Quadratic Minimization</h3>
      <p>Solve the unconstrained minimization problem for a general quadratic function:
      $$ \min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}) := \frac{1}{2} \mathbf{x}^\top H \mathbf{x} + \mathbf{g}^\top \mathbf{x} + c $$
      where $H \in \mathbb{S}_{++}^n$ (Positive Definite).</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>By Lemma A2 (Quadratic Minimization), the minimizer is obtained by setting the gradient to zero.
        $$ \nabla f(\mathbf{x}) = H\mathbf{x} + \mathbf{g} = 0 \implies \mathbf{x}^\star = -H^{-1}\mathbf{g} $$
        Substituting this back into the objective to find the optimal value:
        $$ f(\mathbf{x}^\star) = \frac{1}{2} (-H^{-1}\mathbf{g})^\top H (-H^{-1}\mathbf{g}) + \mathbf{g}^\top (-H^{-1}\mathbf{g}) + c $$
        $$ = \frac{1}{2} \mathbf{g}^\top H^{-1} H H^{-1} \mathbf{g} - \mathbf{g}^\top H^{-1} \mathbf{g} + c $$
        $$ = \frac{1}{2} \mathbf{g}^\top H^{-1} \mathbf{g} - \mathbf{g}^\top H^{-1} \mathbf{g} + c = c - \frac{1}{2} \mathbf{g}^\top H^{-1} \mathbf{g} $$
        This formula is the foundation for almost all dual derivations (where we minimize the Lagrangian w.r.t $\mathbf{x}$).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Quadratic Forms:</b> Every convex quadratic problem reduces to solving a linear system ($H\mathbf{x} = -\mathbf{g}$).
        <br><b>Completing the Square:</b> The minimum value is always "constant minus quadratic form of the gradient inverse".</p>
      </div>
    

</section>



<!-- SECTION 12: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>12. Readings & Resources</h2>
      <ul>
        <li>
          <b>Boyd & Vandenberghe, Convex Optimization:</b>
          <ul>
            <li>Appendix A: Mathematical Background (Norms, Analysis, Functions)</li>
            <li>Appendix C: Numerical Linear Algebra (Operations, Factorizations)</li>
          </ul>
        </li>
        <li>
          <b>Gilbert Strang, Introduction to Linear Algebra:</b>
          <ul>
            <li>Chapter 2: Vector Spaces</li>
            <li>Chapter 3: Orthogonality</li>
            <li>Chapter 6: Eigenvalues and Eigenvectors</li>
          </ul>
        </li>
        <li>
          <b>Golub & Van Loan, Matrix Computations:</b>
          <ul>
            <li>Chapter 2: Matrix Multiplication (for numerical aspects)</li>
            <li>Chapter 5: Orthogonalization and Least Squares</li>
          </ul>
        </li>
      </ul>
    </section>


    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initNormGeometryVisualizer } from './widgets/js/norm-geometry-visualizer.js';
    initNormGeometryVisualizer('widget-norm-geometry');
  </script>
  <script type="module">
    import { initOrthogonality } from './widgets/js/orthogonality.js';
    initOrthogonality('widget-orthogonality');
  </script>
  <script type="module">
    import { initRankNullspace } from './widgets/js/rank-nullspace.js';
    initRankNullspace('widget-rank-nullspace');
  </script>
  <script type="module">
    import { initMatrixGeometry } from './widgets/js/matrix-geometry.js';
    initMatrixGeometry('widget-matrix-geometry');
  </script>
  <script type="module">
    import { initHessianLandscapeVisualizer } from './widgets/js/hessian-landscape-visualizer.js';
    initHessianLandscapeVisualizer('widget-hessian-landscape');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script type="module">
    import { initLeastSquaresVisualizer } from './widgets/js/least-squares-visualizer.js';
    initLeastSquaresVisualizer('widget-least-squares');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
