<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>14. Algorithms II: Equality-Constrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../13-unconstrained-minimization/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../15-interior-point-methods/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>14. Algorithms II: Equality-Constrained Minimization</h1>
      <div class="lecture-meta">
        <span>Date: 2026-02-03</span>
        <span>Duration: 90 min</span>
        <span>Tags: algorithms, constrained</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> Newton's method with equality constraints, KKT systems, and elimination techniques.</p>
        <p><strong>Prerequisites:</strong> <a href="../13-unconstrained-minimization/index.html">Lecture 13</a></p>
      </div>
    </header>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul style="line-height: 1.8;">
        <li>Formulate and solve the Newton step for equality constrained problems using KKT systems.</li>
        <li>Distinguish between feasible-start and infeasible-start Newton methods.</li>
        <li>Apply the elimination method (null-space method) and relate it to the KKT system.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>1. Equality Constrained Minimization</h3>
      <p>We solve $\min f(x)$ subject to $Ax = b$. The optimality conditions (KKT) are $Ax^* = b$ and $\nabla f(x^*) + A^\top \nu^* = 0$. This is a system of nonlinear equations (if $f$ is not quadratic) or linear equations (if $f$ is quadratic).</p>

      <h3>2. The KKT System</h3>
      <p>The Newton step $\Delta x_{nt}$ and dual step $\nu$ are found by solving the linearized KKT conditions:
      $$ \begin{bmatrix} \nabla^2 f(x) & A^\top \\ A & 0 \end{bmatrix} \begin{bmatrix} \Delta x_{nt} \\ w \end{bmatrix} = -\begin{bmatrix} \nabla f(x) \\ 0 \end{bmatrix} $$
      Here we assume a feasible start ($Ax=b$). The KKT matrix is nonsingular if $\nabla^2 f(x)$ is positive definite on the null space of $A$ and $A$ has full rank.</p>

      <h3>3. Elimination Method</h3>
      <p>We can eliminate the constraints by parameterizing the feasible set as $\{x : Ax=b\} = \{ Fz + \hat{x} : z \in \mathbb{R}^{n-p} \}$, where $F$ is a matrix whose columns span the null space of $A$, and $\hat{x}$ is a particular solution. This reduces the problem to unconstrained minimization of $\phi(z) = f(Fz + \hat{x})$. The reduced Hessian $\nabla^2 \phi(z) = F^\top \nabla^2 f(x) F$ relates the KKT system to the unconstrained Newton step.</p>

      <h3>4. Newton's Method with Equality Constraints</h3>
      <p>Just like the unconstrained case, we iterate $x_{k+1} = x_k + t \Delta x_{nt}$ with backtracking line search. The Newton decrement is $\lambda(x) = (\Delta x_{nt}^\top \nabla^2 f(x) \Delta x_{nt})^{1/2}$. Convergence is quadratic near the solution.</p>

      <h3>5. Infeasible Start Newton Method</h3>
      <p>If $x_0$ is not feasible ($Ax_0 \ne b$), we solve a modified KKT system:
      $$ \begin{bmatrix} \nabla^2 f(x) & A^\top \\ A & 0 \end{bmatrix} \begin{bmatrix} \Delta x \\ w \end{bmatrix} = -\begin{bmatrix} \nabla f(x) \\ Ax - b \end{bmatrix} $$
      This updates both primal and dual variables to reduce the residual $r(x, \nu) = (\nabla f(x) + A^\top \nu, Ax - b)$. It finds a feasible point and optimum simultaneously.</p>

      <h3>6. Solving KKT Systems</h3>
      <p>Solving the KKT system is the main computational cost. Approaches include:
      <ul>
          <li><strong>LDL Factorization:</strong> Factor the indefinite KKT matrix directly.</li>
          <li><strong>Elimination (Null-Space):</strong> Requires finding a basis $F$ (e.g., via QR or SVD), reducing to a smaller dense system.</li>
          <li><strong>Schur Complement:</strong> If $\nabla^2 f$ is easily invertible (e.g., diagonal), eliminate $\Delta x$ to solve for $w$ first.</li>
      </ul></p>
    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Null-Space Visualizer</h3>
        <p>Visualize the null-space of the equality constraint matrix.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Projected Gradient Descent</h3>
        <p>Visualize the trajectory of projected gradient descent.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 (if exists) -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Feasible vs. Interior-Point Methods</h3>
        <p>Compares the paths taken by a feasible descent method and an interior-point method.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

<!-- SECTION 10: APPENDIX -->
<section class="section-card" id="section-appendix">
  <h2>Appendix: Detailed Derivations</h2>

  <h3>A.1 Solvability of KKT System</h3>
  <div class="proof-box">
    <h4>Proof of Nonsingularity</h4>
    <p>Let $M = \begin{bmatrix} P & A^\top \\ A & 0 \end{bmatrix}$. Assume $M \begin{bmatrix} x \\ y \end{bmatrix} = 0$.
    <br>1. $Px + A^\top y = 0$ and $Ax = 0$.
    <br>2. Multiply first eq by $x^\top$: $x^\top P x + x^\top A^\top y = 0$. Since $Ax=0$, $x^\top P x = 0$.
    <br>3. Since $P \succ 0$ on $\mathcal{N}(A)$, $x$ must be 0.
    <br>4. Then $A^\top y = 0$. Since $A$ has full rank, $y=0$.
    <br>Thus $M$ is nonsingular.</p>
  </div>

  <h3>A.2 Newton Step as Weighted Least Squares</h3>
  <div class="proof-box">
    <h4>Least Squares Interpretation</h4>
    <p>If $H = L L^\top$, then the Newton step minimizes $\|\nabla f + L^\top L v\|_{L^{-1}}$.
    <br>Equivalently, it solves $\min \|L v + L^{-\top} g\|_2^2$ subject to $Av = 0$.
    <br>This connects the Newton step to a constrained least squares problem.</p>
  </div>
</section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 10 — Equality Constrained Minimization</li>
        <li><strong>Course slides:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">PDF</a></li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <div class="example-box">
        <h4>Example 14.1: KKT System for QP</h4>
        <p><strong>Problem:</strong> Solve $\min \frac{1}{2}x^\top P x + q^\top x$ subject to $Ax = b$ analytically.</p>
        <p><strong>Solution:</strong></p>
        <p>KKT conditions:
        1. $P x + q + A^\top \nu = 0$
        2. $Ax = b$
        From (1): $x = -P^{-1}(q + A^\top \nu)$.
        Substitute into (2): $A(-P^{-1}(q + A^\top \nu)) = b$.
        $-AP^{-1}q - AP^{-1}A^\top \nu = b$.
        $\nu = -(AP^{-1}A^\top)^{-1}(b + AP^{-1}q)$.
        Then substitute $\nu$ back to find $x$. This is the method of Lagrange multipliers.</p>
      </div>

      <div class="example-box">
        <h4>Example 14.2: Null-Space Basis Construction</h4>
        <p><strong>Problem:</strong> Construct a null-space matrix $F$ for $A = [1, 1, 1]$.</p>
        <p><strong>Solution:</strong></p>
        <p>We need $F \in \mathbb{R}^{3 \times 2}$ such that $AF = 0$ and $F$ has rank 2.
        One choice: $F = \begin{bmatrix} 1 & 0 \\ -1 & 1 \\ 0 & -1 \end{bmatrix}$.
        Check: $[1, 1, 1] \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix} = 0$, $[1, 1, 1] \begin{bmatrix} 0 \\ 1 \\ -1 \end{bmatrix} = 0$.
        Reduced Hessian $\nabla^2 \phi(z) = F^\top \nabla^2 f(x) F$.
        Allows solving unconstrained problem in $\mathbb{R}^2$.</p>
      </div>

      <div class="example-box">
        <h4>Example 14.3: Projected Gradient vs Newton</h4>
        <p><strong>Problem:</strong> Compare projected gradient descent with Newton for minimizing $\|x\|^2$ s.t. $Ax=b$.</p>
        <p><strong>Solution:</strong></p>
        <p>Objective is quadratic with Hessian $2I$.
        Newton's method converges in <b>one step</b> for quadratic problems.
        Projected gradient descent: $x_{k+1} = \Pi(x_k - t \nabla f(x_k))$.
        For $f(x) = \|x\|^2$, $\nabla f = 2x$. With optimal step, it also converges fast, but depends on conditioning of $A$.
        Newton solves the KKT system directly, which is equivalent to the analytic solution here.</p>
      </div>

      <div class="example-box">
        <h4>Example 14.4: Augmented Lagrangian</h4>
        <p><strong>Problem:</strong> Define the augmented Lagrangian and explain why it helps.</p>
        <p><strong>Solution:</strong></p>
        <p>$L_\rho(x, \nu) = f(x) + \nu^\top(Ax-b) + \frac{\rho}{2}\|Ax-b\|^2$.
        The quadratic penalty term convexifies the problem locally near the solution even if $f$ is not strictly convex.
        It allows finding the solution by alternating minimization of $x$ and update of $\nu$.
        Unlike pure penalty ($\nu=0$), $\rho$ doesn't need to go to infinity, avoiding ill-conditioning.</p>
      </div>

      <div class="example-box">
        <h4>Example 14.5: Infeasible Start Newton Residual</h4>
        <p><strong>Problem:</strong> How does the residual decrease in infeasible start Newton method?</p>
        <p><strong>Solution:</strong></p>
        <p>Residual $r(x, \nu) = (r_{dual}, r_{pri}) = (\nabla f + A^\top \nu, Ax - b)$.
        Newton step solves linearization $r(y) + Dr(y)\Delta y = 0$.
        With step size $t$, new residual is roughly $(1-t)r(y)$.
        Thus the residual norm decreases linearly (or better) to zero.</p>
      </div>

      <div class="example-box">
        <h4>Example 14.6: Analytic Center with Equality</h4>
        <p><strong>Problem:</strong> Find the analytic center of $\{x : x \ge 0, \mathbf{1}^\top x = 1\}$.</p>
        <p><strong>Solution:</strong></p>
        <p>Minimize $-\sum \log x_i$ subject to $\sum x_i = 1$.
        KKT: $-1/x_i + \nu = 0 \implies x_i = 1/\nu$.
        Sum constraint: $\sum (1/\nu) = n/\nu = 1 \implies \nu = n$.
        So $x_i = 1/n$. Center is uniform distribution.</p>
      </div>

      <div class="example-box">
        <h4>Example 14.7: Schur Complement Solver</h4>
        <p><strong>Problem:</strong> How to solve KKT system if $H = \nabla^2 f$ is diagonal?</p>
        <p><strong>Solution:</strong></p>
        <p>System: $\begin{bmatrix} D & A^\top \\ A & 0 \end{bmatrix} \begin{bmatrix} \Delta x \\ w \end{bmatrix} = \begin{bmatrix} -g \\ 0 \end{bmatrix}$.
        1. $D \Delta x + A^\top w = -g \implies \Delta x = -D^{-1}(g + A^\top w)$.
        2. $A \Delta x = 0 \implies -A D^{-1} g - A D^{-1} A^\top w = 0$.
        3. Solve $(A D^{-1} A^\top) w = -A D^{-1} g$ for $w$.
        4. Compute $\Delta x$ from step 1.
        Requires solving system size $p \times p$ instead of $(n+p) \times (n+p)$. Efficient if $p \ll n$.</p>
      </div>

      <div class="example-box">
        <h4>Example 14.8: Complexity Analysis</h4>
        <p><strong>Problem:</strong> Compare elimination vs KKT for dense $A \in \mathbb{R}^{p \times n}$.</p>
        <p><strong>Solution:</strong></p>
        <p>1. <strong>Elimination:</strong> Form $H_{red} = F^\top H F$ ($O(n^2(n-p))$), factor $H_{red}$ ($O((n-p)^3)$).
        2. <strong>KKT LDL:</strong> Factor $(n+p) \times (n+p)$ matrix ($O((n+p)^3)$).
        If $p$ is small ($p \ll n$), Elimination dominates with $O(n^3)$.
        Actually, KKT approach via block elimination (Schur complement) is $O(np^2)$ if diagonal H, or $O(n^3)$ if dense H.
        Often KKT is preferred for sparsity.</p>
      </div>
    </section>

    <!-- Exercises -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2><i data-feather="edit-3"></i> Exercises</h2>

      <div class="problem">
        <h3>P14.1 — Invertibility of KKT Matrix</h3>
        <p>Prove that $\begin{bmatrix} P & A^\top \\ A & 0 \end{bmatrix}$ is nonsingular if $P \succ 0$ and $A$ has full row rank.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            Suppose $\begin{bmatrix} P & A^\top \\ A & 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = 0$.
            $Px + A^\top y = 0$ and $Ax = 0$.
          </div>
          <div class="proof-step">
            Multiply first eq by $x^\top$: $x^\top P x + x^\top A^\top y = 0$.
            Since $Ax=0$, $x^\top A^\top = 0$, so $x^\top P x = 0$.
            Since $P \succ 0$, this implies $x=0$.
          </div>
          <div class="proof-step">
            Then $A^\top y = 0$. Since $A$ has full row rank, $A^\top$ has full column rank, so $y=0$.
            Thus only trivial solution exists $\implies$ Nonsingular.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P14.2 — Newton Step as Weighted Projection</h3>
        <p>Show that the Newton step $\Delta x_{nt}$ minimizes the quadratic approximation of $f$ constrained to the linearized feasible set.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            Problem: $\min \nabla f^\top v + \frac{1}{2}v^\top \nabla^2 f v$ s.t. $A(x+v) = b$.
            Since $Ax=b$ (feasible start), constraint is $Av=0$.
          </div>
          <div class="proof-step">
            Lagrangian: $L(v, w) = \nabla f^\top v + \frac{1}{2}v^\top H v + w^\top Av$.
            Optimality: $H v + \nabla f + A^\top w = 0$ and $Av = 0$.
          </div>
          <div class="proof-step">
            This is exactly the KKT system for the Newton step.
            So $\Delta x_{nt}$ is the solution to this constrained QP.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P14.3 — Projection Operator Derivation</h3>
        <p>Derive the projection of $y$ onto $\{x : Ax=b\}$: $\Pi(y) = y - A^\top(AA^\top)^{-1}(Ay-b)$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            $\min \frac{1}{2}\|x-y\|^2$ s.t. $Ax=b$.
            KKT: $(x-y) + A^\top \nu = 0 \implies x = y - A^\top \nu$.
          </div>
          <div class="proof-step">
            $A(y - A^\top \nu) = b \implies Ay - AA^\top \nu = b$.
            $AA^\top \nu = Ay - b \implies \nu = (AA^\top)^{-1}(Ay - b)$.
          </div>
          <div class="proof-step">
            Substitute $\nu$ back: $x = y - A^\top(AA^\top)^{-1}(Ay-b)$.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P14.4 — Penalty Method Convergence</h3>
        <p>Explain why the condition number of the Hessian in the quadratic penalty method diverges as $\rho \to \infty$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            $f_\rho(x) = f(x) + \frac{\rho}{2}\|Ax-b\|^2$.
            $\nabla^2 f_\rho(x) = \nabla^2 f(x) + \rho A^\top A$.
          </div>
          <div class="proof-step">
            Eigenvalues of $A^\top A$ are non-negative.
            Eigenvalues corresponding to the range of $A^\top$ scale with $\rho$.
            Eigenvalues in the null space of $A$ depend on $\nabla^2 f$ and don't scale with $\rho$.
          </div>
          <div class="proof-step">
            Ratio $\lambda_{\max}/\lambda_{\min} \propto \rho$. Ill-conditioning grows linearly with $\rho$.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P14.5 — Null-Space Operation Count</h3>
        <p>Confirm that solving the reduced Newton system requires $O((n-p)^3)$ operations.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            Reduced system is $H_{red} \Delta z = -g_{red}$ where $H_{red} \in \mathbb{R}^{(n-p) \times (n-p)}$.
          </div>
          <div class="proof-step">
            Cholesky factorization of $H_{red}$ takes $(1/3)(n-p)^3$ flops.
            Back-substitution takes $O((n-p)^2)$.
          </div>
          <div class="proof-step">
            Dominant term is the factorization, cubic in the dimension of the null space.
          </div>
        </div>
      </div>
    </section>
  </main></div>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="../../static/lib/pyodide/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
        import { initNullSpaceVisualizer } from './widgets/js/null-space-visualizer.js';
        initNullSpaceVisualizer('widget-1');
  </script>
  <script type="module">
        import { initProjectedGradientDescent } from './widgets/js/projected-gd.js';
        initProjectedGradientDescent('widget-2');
  </script>
  <script type="module">
    import { initFeasibleVsInterior } from './widgets/js/feasible-vs-interior.js';
    initFeasibleVsInterior('widget-3');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>

  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
