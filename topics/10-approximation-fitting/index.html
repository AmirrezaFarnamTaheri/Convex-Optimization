<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>10. Applications I: Approximation & Fitting — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <!-- Header with navigation -->
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../09-duality/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../11-statistical-estimation/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>10. Applications I: Approximation & Fitting</h1>
      <div class="lecture-meta">
        <span>Date: 2026-01-06</span>
        <span>Duration: 90 min</span>
        <span>Tags: applications, fitting</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> This lecture applies convex optimization to approximation and fitting problems. We explore the geometry of least squares, the impact of regularization (Ridge, LASSO) on solution properties, and robust regression techniques. We also introduce sparse signal recovery and total variation denoising.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00</a> (Norms), <a href="../07-convex-problems-standard/index.html">Lecture 07</a> (Standard Forms), <a href="../09-duality/index.html">Lecture 09</a> (Duality).</p>
      </div>
    </header>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul style="line-height: 1.8;">
        <li>Formulate and solve least squares problems.</li>
        <li>Apply regularization (Ridge, LASSO) to prevent overfitting.</li>
        <li>Formulate and solve robust regression problems.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>1. Least Squares and Projections</h3>
      <p>The method of least squares minimizes the Euclidean norm of the residual, $\|Ax - b\|_2^2$. Geometrically, this projects the vector $b$ orthogonally onto the range of $A$. The analytic solution is given by the normal equations $A^\top A x = A^\top b$, which are typically solved via Cholesky or QR factorization.</p>

      <figure style="margin: 16px 0; text-align: center;">
        <img src="../../static/assets/topics/10-approximation-fitting/least-squares-fit.gif" alt="Animation of a least squares fit" style="max-width: 500px; height: auto; border-radius: 8px;" />
        <figcaption style="font-size: 13px; color: var(--muted); margin-top: 8px;">
          Least squares finds the line that minimizes the sum of squared vertical distances to the data points.
        </figcaption>
      </figure>

      <h3>2. Regularization and Trade-offs</h3>
      <p>To combat overfitting, we introduce regularization, adding a penalty term $\lambda \phi(x)$ to the objective. This creates a trade-off between fitting error and model complexity. <b>Ridge regression</b> uses the $\ell_2$ norm squared, shrinking coefficients evenly. <b>LASSO</b> uses the $\ell_1$ norm, promoting sparsity by driving small coefficients to exactly zero.</p>

      <h3>3. Robust Fitting</h3>
      <p>Standard least squares is vulnerable to outliers due to the quadratic growth of its penalty. <b>Robust regression</b> employs penalties with slower growth, such as the $\ell_1$ norm (absolute deviation) or the <b>Huber loss</b>, which transitions from quadratic to linear for large errors, balancing efficiency with robustness.</p>

      <h3>4. Sparse Recovery and Signal Processing</h3>
      <p>In signal processing, we often seek sparse solutions. <b>Compressed Sensing</b> theory shows that minimizing the $\ell_1$ norm can recover the true sparse signal from limited measurements, solving the combinatorial $\ell_0$ problem via convex relaxation. Similarly, <b>Total Variation (TV) Denoising</b> uses the $\ell_1$ norm of the gradient to remove noise from images while preserving sharp edges.</p>

      <h3>5. Matrix Approximation</h3>
      <p>We extend approximation to matrices, seeking low-rank structures. The <b>Singular Value Decomposition (SVD)</b> provides the optimal low-rank approximation in the Frobenius norm. For <b>matrix completion</b> with missing entries, the <b>nuclear norm</b> (sum of singular values) serves as a convex relaxation for the rank function, enabling recovery of low-rank matrices from partial data.</p>
    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively. Try tweaking parameters to build intuition.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Least Squares Regularization</h3>
        <p>Explore the effect of L1 and L2 regularization on the solution of a least squares problem.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Robust Regression</h3>
        <p>Compare the solution of a least squares problem with the solution of a robust regression problem in the presence of outliers.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Sparse Recovery Demo</h3>
        <p>Demonstrates how L1 regularization can recover a sparse signal from a limited number of measurements.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 4 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Matrix Completion Visualizer</h3>
        <p>Users can hide entries of a low-rank matrix (e.g., an image) and watch an algorithm recover the missing values.</p>
        <div id="widget-4" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 5 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Regularization Theory Tool</h3>
        <p>Visualizes the geometry of different regularizers (L1, L2, Elastic Net).</p>
        <div id="widget-5" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

<!-- SECTION 7: APPENDIX -->
<section class="section-card" id="section-appendix">
  <h2>Appendix: Detailed Derivations</h2>

  <h3>A.1 The LASSO Path</h3>
  <div class="proof-box">
    <h4>Soft-Thresholding Derivation</h4>
    <p>Problem: $\min \frac{1}{2}(x-b)^2 + \lambda |x|$. Subgradient condition: $0 \in x-b + \lambda \partial |x|$.
    <br>If $x > 0$, $x-b+\lambda = 0 \implies x = b-\lambda$ (requires $b > \lambda$).
    <br>If $x < 0$, $x-b-\lambda = 0 \implies x = b+\lambda$ (requires $b < -\lambda$).
    <br>If $x = 0$, $|b| \le \lambda$.
    <br>Result: $x^* = \text{sign}(b)\max(|b|-\lambda, 0)$.</p>
  </div>

  <h3>A.2 Robust Regression as QP</h3>
  <div class="proof-box">
    <h4>Huber Loss Reformulation</h4>
    <p>Minimize $\sum \phi_{hub}(r_i)$.
    <br>Variational form: $\phi_{hub}(u) = \min_v (u-v)^2 + 2M|v|$.
    <br>Problem becomes: $\min_{x,v} \sum (r_i(x)-v_i)^2 + 2M|v_i|$.
    <br>This is a convex QP with constraints or absolute value reformulation.</p>
  </div>

  <h3>A.3 Eckart-Young-Mirsky Theorem</h3>
  <div class="proof-box">
    <h4>Low-Rank Approximation</h4>
    <p>To minimize $\|A-X\|_F$ s.t. rank$(X) \le k$:
    <br>Let $A = U \Sigma V^\top$. Construct $X = \sum_{i=1}^k \sigma_i u_i v_i^\top$.
    <br>Error is $\sum_{i=k+1}^r \sigma_i u_i v_i^\top$.
    <br>Frobenius norm squared is $\sum_{i=k+1}^r \sigma_i^2$.
    <br>Any other rank-$k$ matrix has larger error.</p>
  </div>
</section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 6 — Approximation and Fitting</li>
        <li><strong>Supplementary:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank">Boyd & Vandenberghe Full Text (PDF)</a></li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <div class="example-box">
      <h4>Example 10.1: Ridge Regression (Tikhonov Regularization)</h4>
      <p><strong>Problem:</strong> Derive the closed-form solution for Ridge regression:
      $$ \min_x \|Ax-b\|_2^2 + \lambda\|x\|_2^2 $$
      Explain how $\lambda > 0$ affects the existence of a solution and the condition number.</p>

      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
            <strong>Step 1: Reformulate as Least Squares.</strong>
            We can rewrite the objective as a single squared Euclidean norm:
            $$ \|Ax-b\|_2^2 + \|\sqrt{\lambda} x\|_2^2 = \left\| \begin{bmatrix} A \\ \sqrt{\lambda} I \end{bmatrix} x - \begin{bmatrix} b \\ 0 \end{bmatrix} \right\|_2^2 $$
            Let $\tilde{A} = \begin{bmatrix} A \\ \sqrt{\lambda} I \end{bmatrix}$ and $\tilde{b} = \begin{bmatrix} b \\ 0 \end{bmatrix}$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Normal Equations.</strong>
            The solution satisfies $\tilde{A}^\top \tilde{A} x = \tilde{A}^\top \tilde{b}$.
            $$ \tilde{A}^\top \tilde{A} = \begin{bmatrix} A^\top & \sqrt{\lambda} I \end{bmatrix} \begin{bmatrix} A \\ \sqrt{\lambda} I \end{bmatrix} = A^\top A + \lambda I $$
            $$ \tilde{A}^\top \tilde{b} = \begin{bmatrix} A^\top & \sqrt{\lambda} I \end{bmatrix} \begin{bmatrix} b \\ 0 \end{bmatrix} = A^\top b $$
            Thus, $(A^\top A + \lambda I) x = A^\top b$.
        </div>
        <div class="proof-step">
            <strong>Step 3: Solution and Stability.</strong>
            Since $A^\top A \succeq 0$ and $\lambda > 0$, the matrix $A^\top A + \lambda I$ has eigenvalues $\sigma_i^2 + \lambda \ge \lambda > 0$. It is always invertible.
            $$ x_{\text{ridge}} = (A^\top A + \lambda I)^{-1} A^\top b $$
            <b>Conditioning:</b> The condition number of the problem matrix is $\kappa(\tilde{A}) = \sqrt{\frac{\sigma_{\max}^2 + \lambda}{\sigma_{\min}^2 + \lambda}}$.
            If $A$ is rank deficient ($\sigma_{\min}=0$), the original $\kappa(A) = \infty$. With regularization, $\kappa \approx \sigma_{\max} / \sqrt{\lambda}$, which is finite. This makes the numerical solution stable.
        </div>
      </div>
      </div>

      <div class="example-box">
      <h4>Example 10.2: The LASSO Path</h4>
      <p>The simplified LASSO problem $\min_x \frac{1}{2}\|x-b\|_2^2 + \lambda \|x\|_1$ decomposes into 1D problems solved by the <b>Soft Thresholding</b> operator. This operator sets small coefficients (where $|b_i| \le \lambda$) to zero and shrinks large coefficients by $\lambda$. This mechanism explains why LASSO produces sparse solutions. For the full subgradient derivation, see <a href="#section-appendix">Appendix A.1</a>.</p>
      </div>

      <div class="example-box">
      <h4>Example 10.3: Weighted Least Squares (BLUE)</h4>
      <p><strong>Problem:</strong> Suppose measurements $y_i = a_i^\top x + v_i$ have independent noise with variance $\sigma_i^2$. We want the Best Linear Unbiased Estimator (BLUE). This corresponds to minimizing the $\chi^2$ statistic:
      $$ \min_x \sum_{i=1}^m \frac{(a_i^\top x - b_i)^2}{\sigma_i^2} $$
      Formulate this in matrix notation and find the solution.</p>

      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
            <strong>Step 1: Matrix Formulation.</strong>
            Let $W = \text{diag}(1/\sigma_1^2, \dots, 1/\sigma_m^2)$ be the inverse covariance matrix.
            The objective is $(Ax-b)^\top W (Ax-b) = \|W^{1/2}(Ax-b)\|_2^2$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Gradient.</strong>
            $\nabla f(x) = 2 A^\top W (Ax - b)$.
            Setting to zero gives the normal equations:
            $$ (A^\top W A) x = A^\top W b $$
        </div>
        <div class="proof-step">
            <strong>Step 3: Solution.</strong>
            $$ x^* = (A^\top W A)^{-1} A^\top W b $$
            <b>Interpretation:</b> Measurements with high variance (large $\sigma_i$) get small weight ($1/\sigma_i^2$) in the optimization. The estimate is pulled towards high-precision data points.
        </div>
      </div>
      </div>

      <div class="example-box">
      <h4>Example 10.4: Robust Regression with Huber Loss</h4>
      <p>The Huber loss combines the best of both worlds: quadratic for small errors (differentiable) and linear for large errors (robust). The robust regression problem $\min_x \sum \phi_{hub}(a_i^\top x - b_i)$ can be reformulated as a Quadratic Program by splitting the residual into an "inlier" part (squared) and an "outlier" part (absolute value). For the derivation, see <a href="#section-appendix">Appendix A.2</a>.</p>
      </div>

      <div class="example-box">
      <h4>Example 10.5: Sparse Signal Recovery via Basis Pursuit</h4>
      <p><strong>Problem:</strong> A 100-dimensional signal $x_{true}$ with only 10 nonzero entries is measured via 30 random Gaussian measurements $y = Ax_{true}$. We wish to recover $x_{true}$.
      <br>Since $30 < 100$, the system $Ax=y$ is underdetermined. The sparsest solution ($\min \|x\|_0$) is NP-hard to find.
      <br>Formulate the <b>Basis Pursuit</b> relaxation and explain why it works.</p>

      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
            <strong>Step 1: Convex Relaxation.</strong>
            We replace the non-convex $\ell_0$ "norm" (count of non-zeros) with the $\ell_1$ norm (sum of magnitudes), which is the tightest convex relaxation (convex envelope over the unit ball).
            $$ \min_x \|x\|_1 \quad \text{subject to} \quad Ax = y $$
        </div>
        <div class="proof-step">
            <strong>Step 2: LP Formulation.</strong>
            This is equivalent to the Linear Program:
            $$
            \begin{aligned}
            \min_{x, t} \quad & \mathbf{1}^\top t \\
            \text{subject to} \quad & -t \le x \le t \\
            & Ax = y
            \end{aligned}
            $$
        </div>
        <div class="proof-step">
            <strong>Step 3: Exact Recovery (Theory).</strong>
            Under conditions like the <b>Restricted Isometry Property (RIP)</b> or Incoherence, the solution to the $\ell_1$ problem is exactly $x_{true}$ with high probability, even though the number of measurements ($m=30$) is much less than the dimension ($n=100$).
            <br><i>Intuition:</i> The $\ell_1$ ball is "pointy". A random subspace (defined by $Ax=y$) is statistically likely to touch the $\ell_1$ ball at a vertex (a sparse vector) rather than a face.
        </div>
      </div>
      </div>

      <div class="example-box">
      <h4>Example 10.6: Total Variation (TV) Denoising</h4>
      <p><strong>Problem:</strong> We observe a noisy signal $y = x_{true} + \nu$, where $x_{true}$ is piecewise constant (like a step function). We want to recover $x$ by minimizing the Total Variation. Formulate this as a convex problem.</p>

      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
            <strong>Step 1: Total Variation Definition.</strong>
            The Total Variation of a 1D signal is the sum of absolute differences between adjacent values:
            $$ \text{TV}(x) = \sum_{i=1}^{n-1} |x_{i+1} - x_i| = \|Dx\|_1 $$
            where $D$ is the finite difference matrix ($D_{i,i}=-1, D_{i,i+1}=1$).
        </div>
        <div class="proof-step">
            <strong>Step 2: Optimization Problem.</strong>
            We balance data fidelity (Least Squares) with the TV prior:
            $$ \min_x \frac{1}{2}\|x - y\|_2^2 + \lambda \|Dx\|_1 $$
        </div>
        <div class="proof-step">
            <strong>Step 3: QP Formulation.</strong>
            This is a quadratic program. Introduce $u$ for the absolute differences:
            $$
            \begin{aligned}
            \min_{x, u} \quad & \frac{1}{2}x^\top x - y^\top x + \lambda \mathbf{1}^\top u \\
            \text{subject to} \quad & -u \le Dx \le u
            \end{aligned}
            $$
            Solving this yields a piecewise constant solution that preserves sharp edges, unlike Ridge regularization ($\|Dx\|_2^2$) which would smooth them out.
        </div>
      </div>
      </div>

      <div class="example-box">
      <h4>Example 10.7: Low-Rank Matrix Approximation</h4>
      <p>The problem of finding the best rank-$k$ approximation to a matrix $A$ in the Frobenius norm is solved by the <b>truncated Singular Value Decomposition (SVD)</b>. This result, known as the Eckart-Young-Mirsky theorem, states that one should keep the largest $k$ singular values and zero out the rest. For the proof details, see <a href="#section-appendix">Appendix A.3</a>.</p>
      </div>

      <div class="example-box">
      <h4>Example 10.8: Polynomial Fitting with Regularization</h4>
      <p><strong>Problem:</strong> Fit a polynomial $p(t) = \sum_{j=0}^d x_j t^j$ to data points $(t_i, y_i)$. Discuss the conditioning of the problem and the effect of regularization.</p>

      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
            <strong>Step 1: Vandermonde Matrix.</strong>
            The linear system is $Ax \approx y$, where $A$ is the Vandermonde matrix:
            $$ A_{ij} = t_i^{j-1} $$
        </div>
        <div class="proof-step">
            <strong>Step 2: Ill-Conditioning.</strong>
            Vandermonde matrices are notoriously ill-conditioned. The columns $1, t, t^2, \dots$ become nearly parallel for large $t$ or $d$.
            Solving $A^\top A x = A^\top y$ directly can lead to huge numerical errors and "wobbly" polynomials (overfitting).
        </div>
        <div class="proof-step">
            <strong>Step 3: Ridge Regularization.</strong>
            Minimizing $\|Ax - y\|_2^2 + \lambda \|x\|_2^2$ stabilizes the inversion:
            $$ x = (A^\top A + \lambda I)^{-1} A^\top y $$
            Adding $\lambda I$ to the eigenvalues of $A^\top A$ prevents division by near-zero values, smoothing the resulting polynomial and reducing variance.
        </div>
      </div>
      </div>
    </section>

    <!-- Exercises -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2><i data-feather="edit-3"></i> Exercises</h2>

      <div class="problem">
        <h3>P10.1 — Ridge Regression Uniqueness</h3>
        <p>Show that the solution to $\min_x \|Ax-b\|_2^2 + \lambda\|x\|_2^2$ is unique for any $\lambda > 0$, even if $A$ is rank-deficient.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>The objective is $f(x) = x^\top (A^\top A + \lambda I) x - 2(A^\top b)^\top x + b^\top b$.
          The Hessian is $\nabla^2 f(x) = 2(A^\top A + \lambda I)$.
          Since $A^\top A \succeq 0$ and $\lambda > 0$, the eigenvalues of the Hessian are $\sigma_i^2 + \lambda \ge \lambda > 0$.
          Thus the Hessian is positive definite, implying strict convexity and a unique global minimizer.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P10.2 — Equivalence of Penalized and Constrained Forms</h3>
        <p>Prove that for every $\lambda > 0$, there exists a $t \ge 0$ such that the solution to $\min_x \|Ax-b\|_2^2 + \lambda \|x\|_1$ is also a solution to $\min_x \|Ax-b\|_2^2$ subject to $\|x\|_1 \le t$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>This follows from Lagrange Duality. Let $x^*$ be the solution to the penalized problem.
          Set $t = \|x^*\|_1$.
          The penalized problem is the Lagrangian relaxation of the constrained problem with multiplier $\lambda$.
          Since the problem is convex and Slater's condition holds (for $t>0$), strong duality applies.
          By the KKT conditions, if $\lambda$ is the optimal dual variable corresponding to the constraint $\|x\|_1 \le t$, then $x^*$ is optimal for the primal.
          Thus, the regularization path traces the curve of optimal solutions for the constrained problem.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P10.3 — Weighted Least Squares and MLE</h3>
        <p>Derive the solution to $\min_x (Ax-b)^\top W (Ax-b)$ for diagonal $W \succ 0$. Show that this corresponds to Maximum Likelihood Estimation (MLE) for $y = Ax + \nu$ where $\nu_i \sim \mathcal{N}(0, \sigma_i^2)$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <div class="proof-step">
            <strong>Part 1: Solution.</strong> Gradient is $2A^\top W (Ax - b) = 0 \implies A^\top W A x = A^\top W b \implies x = (A^\top W A)^{-1} A^\top W b$.
          </div>
          <div class="proof-step">
            <strong>Part 2: MLE.</strong> Likelihood $L(x) = \prod \frac{1}{\sqrt{2\pi}\sigma_i} \exp\left(-\frac{(y_i - a_i^\top x)^2}{2\sigma_i^2}\right)$.
            Log-likelihood: $\ell(x) = -\frac{1}{2} \sum \frac{(y_i - a_i^\top x)^2}{\sigma_i^2} + C$.
            Maximizing $\ell(x)$ is equivalent to minimizing $\sum \frac{1}{\sigma_i^2} (y_i - a_i^\top x)^2$.
            This matches the weighted least squares objective with $W_{ii} = 1/\sigma_i^2$.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P10.4 — Subdifferential of $\ell_1$ Norm</h3>
        <p>Derive the subdifferential $\partial \|x\|_1$. Use it to state the optimality condition for Basis Pursuit Denoising: $\min \frac{1}{2}\|Ax-y\|_2^2 + \lambda \|x\|_1$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>$\|x\|_1 = \sum |x_i|$. The subdifferential separates: $\partial \|x\|_1 = \times_i \partial |x_i|$.
          $$ \partial |u| = \begin{cases} \{1\} & u > 0 \\ \{-1\} & u < 0 \\ [-1, 1] & u = 0 \end{cases} $$
          <b>Optimality Condition:</b> $0 \in A^\top (Ax - y) + \lambda \partial \|x\|_1$.
          Let $r = y - Ax$ (residual). Then $A^\top r \in \lambda \partial \|x\|_1$.
          Component-wise: $|(A^\top r)_i| \le \lambda$ for all $i$. If $x_i \ne 0$, then $(A^\top r)_i = \lambda \text{sign}(x_i)$.
          This means the residual is highly correlated with the active features.</p>
        </div>
      </div>

      <div class="problem">
        <h3>P10.5 — Total Variation vs. Ridge</h3>
        <p>Explain why TV denoising ($\lambda \|Dx\|_1$) preserves edges while Ridge ($\lambda \|Dx\|_2^2$) blurs them.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Consider a step function $x$. The derivative $Dx$ is sparse (mostly zeros, large spike at the jump).
          <br><b>Ridge ($\ell_2$):</b> Penalizes the square of the jump. It prefers many small jumps to one large jump (e.g., $1^2+1^2=2 < 2^2=4$). This smooths out the step into a ramp (blur).
          <br><b>TV ($\ell_1$):</b> Penalizes the absolute sum. It is indifferent between one big jump and many small ones ($1+1=2$). However, combined with the data fidelity term, it prefers the single jump because it fits the step data better without incurring extra penalty cost. This allows sharp edges to persist.</p>
        </div>
      </div>
    </section>
  </main></div>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="../../static/lib/pyodide/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initLeastSquaresPlayground } from './widgets/js/least-squares-regularization.js';
    initLeastSquaresPlayground('widget-1');
  </script>
  <script type="module">
    import { initRobustRegression } from './widgets/js/robust-regression.js';
    initRobustRegression('widget-2');
  </script>
  <script type="module">
    import { initSparseRecoveryDemo } from './widgets/js/sparse-recovery.js';
    initSparseRecoveryDemo('widget-3');
  </script>
  <script type="module">
    import { initMatrixCompletionVisualizer } from './widgets/js/matrix-completion.js';
    initMatrixCompletionVisualizer('widget-4');
  </script>
  <script type="module">
    import { initRegularizationTheoryTool } from './widgets/js/regularization-theory.js';
    initRegularizationTheoryTool('widget-5');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>

  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
