<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/lecture-styles.css" />
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture bridges the gap between basic linear algebra and the robust numerical tools used in modern optimization. We move beyond the Normal Equations to explore numerically stable factorizations like QR and the Singular Value Decomposition (SVD), which reveal the geometry of linear maps in their purest form. We also introduce the Moore-Penrose pseudoinverse for solving ill-posed or rank-deficient problems and define the condition number—the fundamental metric for analyzing the stability of algorithms and the convergence speed of gradient descent.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm and matrix completion in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The condition number is the key parameter determining the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a> and motivates the use of Newton's Method.</p>
      </div>
    </header>

        <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Master Geometry:</b> Construct orthonormal bases via Gram-Schmidt and understand isometries.</li>
        <li><b>Factorize Matrices:</b> Compute and interpret QR and SVD factorizations as fundamental geometric decompositions.</li>
        <li><b>Analyze Operators:</b> Use the Spectral Theorem for symmetric matrices and Jordan form for general operators.</li>
        <li><b>Measure Matrices:</b> Apply induced norms and scalar invariants (determinant/trace) to characterize linear maps.</li>
        <li><b>Solve Robustly:</b> Use SVD and pseudoinverses to solve rank-deficient least squares and perform PCA.</li>
      </ul>
    </section>


    <!-- SECTION 1: GEOMETRY & GRAM-SCHMIDT -->
    <section class="section-card" id="section-geometry">
      <h2>1. Geometry of Inner Product Spaces</h2>
      <p>In L00, we introduced inner products. Now we explore the rigorous geometry they enable: orthonormal bases, which convert geometry into simple algebra.</p>

      <h3>1.1 Orthonormal Bases: Coordinates that Don't Distort</h3>
      <p>An <b>orthonormal basis</b> $\{q_1, \dots, q_n\}$ satisfies $\langle q_i, q_j \rangle = \delta_{ij}$.
      <br><b>Expansion Formula:</b> For any $v$,
      $$ v = \sum_{i=1}^n \langle q_i, v \rangle q_i $$
      Coefficients are simply inner products. No linear system needs to be solved.
      <br><b>Parseval's Identity:</b> $\|v\|^2 = \sum |\langle q_i, v \rangle|^2$. Geometry is preserved.</p>

      <h3>1.2 The Gram-Schmidt Process</h3>
      <p>Gram-Schmidt is the constructive proof that every finite-dimensional subspace admits an orthonormal basis.
      <br><b>The Construction:</b> Given independent $v_1, \dots, v_k$:
      1. $q_1 = v_1 / \|v_1\|$.
      2. For $j=2 \dots k$:
         $$ u_j = v_j - \sum_{i=1}^{j-1} \langle q_i, v_j \rangle q_i $$
         $$ q_j = u_j / \|u_j\| $$
      This ensures $\operatorname{span}(q_1, \dots, q_j) = \operatorname{span}(v_1, \dots, v_j)$ at every step.</p>
    </section>

    <!-- SECTION 2: ORTHOGONAL MATRICES & QR -->
    <section class="section-card" id="section-qr">
      <h2>2. Orthogonal Matrices and QR Decomposition</h2>

      <h3>2.1 Orthogonal Matrices (Isometries)</h3>
      <p>A matrix $Q$ is orthogonal if $Q^\top Q = I$.
      <br><b>Properties:</b>
      <ul>
        <li>Preserves norms: $\|Qx\| = \|x\|$.</li>
        <li>Preserves angles: $\langle Qx, Qy \rangle = \langle x, y \rangle$.</li>
        <li>Inverse is transpose: $Q^{-1} = Q^\top$.</li>
      </ul>
      This makes orthogonal matrices the ideal choice for numerical algorithms—they do not amplify errors.</p>

      <h3>2.2 QR Decomposition</h3>
      <p>Every matrix $A \in \mathbb{R}^{m \times n}$ ($m \ge n$) can be factored as:
      $$ A = Q R $$
      where $Q$ has orthonormal columns and $R$ is upper triangular.
      <br><b>Geometry:</b> $Q$ captures the subspace (geometry), $R$ captures the coefficients (coordinates).
      <br><b>Least Squares:</b> To solve $\min \|Ax-b\|$, we solve $Rx = Q^\top b$. This avoids the condition number squaring of the normal equations.</p>
    </section>

    <!-- SECTION 3: SCALAR INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>3. Scalar Invariants: Determinant and Trace</h2>

      <h3>3.1 Determinant: The Volume Form</h3>
      <p>The determinant is the unique alternating multilinear map normalized to 1 on the identity.
      <br><b>Geometry:</b> $|\det(A)|$ is the volume scaling factor of the linear map.
      <br><b>Properties:</b>
      <ul>
        <li>$\det(AB) = \det(A)\det(B)$.</li>
        <li>$\det(A) \ne 0 \iff A$ is invertible.</li>
      </ul>
      </p>

      <h3>3.2 Trace: Infinitesimal Volume Change</h3>
      <p>The trace is the unique linear invariant satisfying $\mathrm{tr}(AB)=\mathrm{tr}(BA)$.
      <br><b>Properties:</b>
      <ul>
        <li>$\mathrm{tr}(A) = \sum \lambda_i$ (sum of eigenvalues).</li>
        <li>Linearization: $\det(I + \epsilon A) \approx 1 + \epsilon \mathrm{tr}(A)$.</li>
      </ul>
      </p>
    </section>

    <!-- SECTION 4: SPECTRAL THEORY -->
    <section class="section-card" id="section-spectral">
      <h2>4. The Spectral Theory</h2>

      <h3>4.1 Eigenvalues and Eigenvectors</h3>
      <p>An eigenvector $v$ satisfies $Av = \lambda v$. It represents an invariant direction where the map acts as simple scaling.
      <br><b>Characteristic Polynomial:</b> $p_A(\lambda) = \det(A - \lambda I)$. Eigenvalues are its roots.</p>

      <h3>4.2 Diagonalization</h3>
      <p>$A$ is diagonalizable if it has a full set of linearly independent eigenvectors.
      $$ A = P D P^{-1} $$
      This is possible if and only if for every eigenvalue, the geometric multiplicity ($\dim \ker(A-\lambda I)$) equals the algebraic multiplicity.</p>

      <h3>4.3 Jordan Canonical Form</h3>
      <p>If diagonalization fails (defective matrix), the closest form is the Jordan form:
      $$ J = \begin{bmatrix} \lambda & 1 & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & \lambda \end{bmatrix} $$
      The "1"s on the superdiagonal represent the failure to diagonalize (nilpotent drift).</p>
    </section>

    <!-- SECTION 5: SPECTRAL THEOREM & SVD -->
    <section class="section-card" id="section-svd-spectral">
      <h2>5. The Spectral Theorem and SVD</h2>

      <h3>5.1 The Spectral Theorem (Symmetric Matrices)</h3>
      <p>If $A$ is symmetric ($A=A^\top$), then:
      1. All eigenvalues are real.
      2. Eigenvectors of distinct eigenvalues are orthogonal.
      3. $A$ is orthogonally diagonalizable: $A = Q \Lambda Q^\top$.
      This is the "perfect" case where geometry and algebra align.</p>

      <h3>5.2 Singular Value Decomposition (SVD)</h3>
      <p>For <i>any</i> matrix $A$ (not necessarily square or symmetric), SVD reveals the intrinsic geometry.
      $$ A = U \Sigma V^\top $$
      <ul>
        <li>$V$: Input rotation (right singular vectors).</li>
        <li>$\Sigma$: Independent scaling (singular values $\sigma_i$).</li>
        <li>$U$: Output rotation (left singular vectors).</li>
      </ul>
      <b>Interpretation:</b> Every linear map maps the unit sphere to a hyperellipse. The semi-axes are $\sigma_i u_i$.</p>

      <h3>5.3 Best Low-Rank Approximation</h3>
      <p>The truncated SVD gives the optimal rank-$k$ approximation to $A$ (in spectral or Frobenius norm):
      $$ A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top $$
      This underpins PCA and compression.</p>
    </section>

<!-- SECTION 5: INDUCED NORMS -->
    <section class="section-card" id="section-induced-norms">
      <h2>6. Induced Matrix Norms</h2>
      <p>We defined vector norms in Lecture 00. Now we ask: how "big" is a matrix $A$? The <b>induced norm</b> measures the maximum factor by which $A$ can stretch a vector.</p>
      $$ \|A\|_p = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|_p}{\|\mathbf{x}\|_p} = \max_{\|\mathbf{x}\|_p = 1} \|A\mathbf{x}\|_p $$

      <h3>5.1 The $\ell_1$ Operator Norm (Max Column Sum)</h3>
      <p>$$ \|A\|_1 = \max_{j} \sum_{i=1}^m |a_{ij}| $$</p>
      <div class="proof-box">
        <h4>Derivation</h4>
        <p>Let $\mathbf{x}$ be a unit vector in $\ell_1$. Then $A\mathbf{x} = \sum x_j a_j$ (linear combination of columns).
        $$ \|A\mathbf{x}\|_1 = \|\sum_j x_j a_j\|_1 \le \sum_j |x_j| \|a_j\|_1 \le (\max_j \|a_j\|_1) \sum_j |x_j| = \max_j \|a_j\|_1 $$
        Equality is achieved by choosing $\mathbf{x} = e_k$ where $k$ is the index of the column with the largest norm.</p>
      </div>

      <h3>5.2 The $\ell_\infty$ Operator Norm (Max Row Sum)</h3>
      <p>$$ \|A\|_\infty = \max_{i} \sum_{j=1}^n |a_{ij}| $$</p>
      <p><i>Intuition:</i> To maximize the $\infty$-norm of $A\mathbf{x}$, we want to maximize a single component $|(A\mathbf{x})_i| = |\sum_j a_{ij} x_j|$. With constraints $|x_j| \le 1$, we set $x_j = \text{sign}(a_{ij})$ to make all terms positive and maximal.</p>

      <h3>5.3 The $\ell_2$ Operator Norm (Spectral Norm)</h3>
      <p>$$ \|A\|_2 = \max_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2 = \sqrt{\lambda_{\max}(A^\top A)} = \sigma_{\max}(A) $$</p>
      <p>This is the square root of the maximum eigenvalue of $A^\top A$, which is the largest <b>singular value</b> of $A$. It represents the maximum stretch of the matrix in the Euclidean sense. This norm plays a crucial role in convergence analysis, as it dictates the worst-case amplification of errors.</p>

      <h3>5.4 Convexity of Induced Norms (Deep Dive)</h3>
      <p>Why are induced norms convex functions of the matrix $A$? We can prove this using the property that the pointwise supremum of linear functions is convex.</p>
      <div class="proof-box">
        <h4>Proof: Convexity via Dual Norms</h4>
        <div class="proof-step">
          <strong>Step 1: Definition.</strong> The induced norm is defined as:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\mathbf{v} \ne 0} \frac{\|X\mathbf{v}\|_a}{\|\mathbf{v}\|_\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \|X\mathbf{v}\|_a $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Dual Norm Representation.</strong> Recall that any norm $\|z\|_a$ can be expressed as a supremum over its dual norm ball: $\|z\|_a = \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top \mathbf{z}$.
          Substituting this into the definition:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \left( \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top (X\mathbf{v}) \right) $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Linear Structure.</strong>
          $$ \|X\|_{a,\mathbf{b}} = \sup \{ \mathbf{u}^\top X \mathbf{v} \mid \|\mathbf{u}\|_{a^*} \le 1, \|\mathbf{v}\|_\mathbf{b} = 1 \} $$
          Notice that for fixed vectors $\mathbf{u}$ and $\mathbf{v}$, the function $f_{\mathbf{u},\mathbf{v}}(X) = \mathbf{u}^\top X \mathbf{v}$ is <b>linear</b> in the entries of $X$.
          Specifically, $\mathbf{u}^\top X \mathbf{v} = \mathrm{tr}(\mathbf{u}^\top X \mathbf{v}) = \mathrm{tr}(\mathbf{v} \mathbf{u}^\top X) = \langle uv^\top, X \rangle$.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong>
          The induced norm $\|X\|_{a,b}$ is the <b>pointwise supremum</b> of a family of linear functions. Since the supremum of any collection of convex functions is convex, the induced norm is a convex function of $X$.
        </div>
      </div>
    </section>


    <!-- SECTION 6: APPLICATIONS -->
    <section class="section-card" id="section-applications">
      <h2>7. Applications: PCA, Least Squares, and Stability</h2>

      <h3>7.1 Principal Component Analysis (PCA)</h3>
      <p>PCA is simply SVD applied to a centered data matrix $X$.
      <br>If $X = U \Sigma V^\top$, the principal directions are the columns of $V$, and the variances are $\sigma_i^2$. This geometric view unifies statistics and linear algebra.</p>

      <h3>7.2 Robust Least Squares and Pseudoinverse</h3>
      <p>When $A$ is rank-deficient, the normal equations fail. The SVD provides the <b>pseudoinverse</b>:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+$ inverts non-zero singular values. The solution $x = A^+ b$ is the unique minimum-norm solution to the least squares problem.</p>

      <h3>7.3 Condition Number and Regularization</h3>
      <p>The condition number $\kappa(A) = \sigma_{\max}/\sigma_{\min}$ measures sensitivity to error.
      <br><b>Tikhonov Regularization:</b> Solving $\min \|Ax-b\|^2 + \lambda \|x\|^2$ is equivalent to filtering singular values: $\sigma_i \to \frac{\sigma_i}{\sigma_i^2+\lambda}$. This stabilizes the inversion of small singular values.</p>
    </section>

<!-- SECTION 9: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>8. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>
    </section>

    <!-- SECTION 10: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 9. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.12 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Upper bound ($\le$):</strong> By Hölder's inequality, $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$.
          The definition of the dual norm is $\|y\|_* = \sup_{\|\mathbf{x}\|_p \le 1} \mathbf{x}^\top \mathbf{y}$.
          From Hölder, if $\|x\|_p \le 1$, then $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{y}\|_q$. Thus $\|y\|_* \le \|\mathbf{y}\|_q$.
        </div>
        <div class="proof-step">
          <strong>Lower bound ($\ge$):</strong> We need to construct a vector $\mathbf{x}$ that "aligns" with $\mathbf{y}$ to maximize the inner product.
          We want $x_i$ to have the same sign as $y_i$, and magnitudes such that equality holds in Hölder's.
          Choose $x_i = \mathrm{sign}(y_i) |y_i|^{q-1}$.
          <br>Check inner product: $\mathbf{x}^\top \mathbf{y} = \sum \mathrm{sign}(y_i)|y_i|^{q-1} y_i = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          <br>Check norm of $\mathbf{x}$: $\|x\|_p^p = \sum |x_i|^p = \sum |y_i|^{p(q-1)}$.
          Since $(p-1)q = p \implies p(q-1) = q$: $\|x\|_p^p = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          So $\|x\|_p = \|y\|_q^{q/p}$.
          <br>Now normalize to get a unit vector $\tilde{\mathbf{x}} = \mathbf{x} / \|\mathbf{x}\|_p = \mathbf{x} / \|\mathbf{y}\|_q^{q/p}$.
          $$ \tilde{\mathbf{x}}^\top \mathbf{y} = \frac{\|\mathbf{y}\|_q^q}{\|\mathbf{y}\|_q^{q/p}} = \|\mathbf{y}\|_q^{q - q/p} = \|\mathbf{y}\|_q^1 = \|\mathbf{y}\|_q $$
          Since we found a unit vector achieving this value, $\|y\|_* \ge \|\mathbf{y}\|_q$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Dual Norm:</b> Defined by $\|y\|_* = \sup \{ \mathbf{y}^\top \mathbf{x} \mid \|\mathbf{x}\| \le 1 \}$.
        <br><b>$\ell_p$ Dual:</b> The dual of the $\ell_p$ norm ($1 < p < \infty$) is the $\ell_q$ norm, where $1/p + 1/q = 1$.
        <br><b>Hölder's Inequality:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$. This is the generalized Cauchy-Schwarz inequality.
        <br><b>Alignment Condition:</b> Equality holds if $|x_i|^p$ is proportional to $|y_i|^q$ (and signs match). This is crucial for constructing worst-case examples in robustness analysis.</p>
      </div>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The Frobenius norm corresponds to the standard Euclidean norm of the vectorized matrix: $\|A\|_F = \|\mathrm{vec}(A)\|_2$.
        The inner product of matrices is $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.
        This is exactly the dot product of the vectorized forms: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        By the standard Cauchy-Schwarz inequality for vectors:
        $$ |\mathrm{vec}(A)^\top \mathrm{vec}(B)| \le \|\mathrm{vec}(A)\|_2 \|\mathrm{vec}(B)\|_2 $$
        Substituting back the matrix notation yields $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The $\mathrm{vec}$ Operator:</b> Stacks columns of a matrix into a single vector in $\mathbb{R}^{mn}$.
        <br><b>Isometry:</b> The mapping $A \mapsto \mathrm{vec}(A)$ is an isometry between $(\mathbb{R}^{m \times n}, \|\cdot\|_F)$ and $(\mathbb{R}^{mn}, \|\cdot\|_2)$.
        <br><b>Inner Product Consistency:</b> $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        <br><b>Result:</b> Cauchy-Schwarz for vectors immediately implies $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.</p>
      </div>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>1. Compute $A^\top A$ and its eigenvalues:</strong>
          $A^\top A = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$.
          Characteristic eq: $(1-\lambda)^2 - 1 = 0 \implies \lambda^2 - 2\lambda = 0 \implies \lambda_1=2, \lambda_2=0$.
          Singular values: $\sigma_1 = \sqrt{2}, \sigma_2 = 0$.
        </div>
        <div class="proof-step">
          <strong>2. Compute $V$ (eigenvectors of $A^\top A$):</strong>
          For $\lambda_1=2$: $\begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \mathbf{v} = 0 \implies v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
          For $\lambda_2=0$: $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
          $V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>3. Compute $U$:</strong>
          $u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
          Since $m=2$, we need a second vector $u_2$ orthogonal to $u_1$. $u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
          $U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>Result:</strong> $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \left(\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\right)^\top$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hand-Calculation Strategy for SVD:</b>
        <ol>
            <li>Compute the Gram matrix $G = A^\top A$.</li>
            <li>Find eigenvalues $\lambda_i$ of $G$. Singular values are $\sigma_i = \sqrt{\lambda_i}$.</li>
            <li>Find normalized eigenvectors $v_i$ of $G$ to form $V$.</li>
            <li>Compute $u_i = \frac{1}{\sigma_i} A v_i$ for non-zero $\sigma_i$.</li>
            <li>If $m > r$, complete $U$ using Gram-Schmidt or inspection (orthogonality).</li>
        </ol>
        <br><b>Check:</b> Verify $AV = U\Sigma$.</p>
      </div>

      <h3>P1.4 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, \mathbf{b} = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>(a) Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        $A^\top A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.
        $A^\top \mathbf{b} = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Solve $\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \mathbf{x} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Multiply by inverse $\frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$:
        $\mathbf{x} = \frac{1}{3} \begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 \\ 5 \end{bmatrix}$.</p>
        <p><b>(b) QR Factorization:</b>
        Gram-Schmidt on columns $a_1, a_2$.
        $u_1 = a_1 = (1, 0, 1)^\top$. $q_1 = (1/\sqrt{2}, 0, 1/\sqrt{2})^\top$.
        $v_2 = a_2 - \langle a_2, q_1 \rangle q_1 = (1, 1, 0)^\top - (1/\sqrt{2}) (1/\sqrt{2}, 0, 1/\sqrt{2})^\top = (1, 1, 0)^\top - (1/2, 0, 1/2)^\top = (1/2, 1, -1/2)^\top$.
        Normalize $v_2$: $\|v_2\|^2 = 1/4 + 1 + 1/4 = 1.5 = 3/2$. $\|v_2\| = \sqrt{3/2}$.
        $q_2 = \sqrt{2/3} (1/2, 1, -1/2)^\top$.
        Solve $R\mathbf{x} = Q^\top \mathbf{b}$.
        $R = \begin{bmatrix} \sqrt{2} & 1/\sqrt{2} \\ 0 & \sqrt{3/2} \end{bmatrix}$. $Q^\top \mathbf{b} = \begin{bmatrix} 3/\sqrt{2} \\ 4\sqrt{2/3}-1/\sqrt{6} \end{bmatrix}$.
        Back substitution yields same $\mathbf{x}$. QR avoids forming $A^\top A$ (better condition number).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Comparing Least Squares Algorithms:</b>
        <ul>
            <li><b>Normal Equations:</b>
                <ul><li>Cost: $O(n^2 m)$ (forming $A^\top A$) + $O(n^3)$ (Cholesky).</li>
                <li>Stability: Unstable. Dependence on $\kappa(A)^2$.</li></ul>
            </li>
            <li><b>QR Factorization:</b>
                <ul><li>Cost: $2 \times$ Normal Equations (roughly).</li>
                <li>Stability: Stable. Dependence on $\kappa(A)$. Standard choice.</li></ul>
            </li>
            <li><b>SVD:</b> Most expensive, but most robust (handles rank deficiency gracefully).</li>
        </ul></p>
      </div>

      <h3>P1.5 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $A = U \Sigma V^\top$. Then $A^+ = V \Sigma^+ U^\top$.
        $P = AA^+ = (U \Sigma V^\top)(V \Sigma^+ U^\top) = U (\Sigma \Sigma^+) U^\top$.
        $\Sigma \Sigma^+$ is a diagonal matrix with 1s for non-zero singular values and 0s otherwise.
        Let $U_r$ be the first $r$ columns of $U$. Then $P = U_r U_r^\top$.
        <br><b>Idempotent:</b> $P^2 = (U_r U_r^\top)(U_r U_r^\top) = U_r (U_r^\top U_r) U_r^\top = U_r I U_r^\top = P$.
        <br><b>Symmetric:</b> $P^\top = (U_r U_r^\top)^\top = (U_r^\top)^\top U_r^\top = U_r U_r^\top = P$.
        <br>Since the columns of $U_r$ form a basis for $\mathcal{R}(A)$, $P$ is the orthogonal projector onto $\mathcal{R}(A)$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Fundamental Projectors via SVD:</b>
        <ul>
            <li><b>Column Space Projector:</b> $P_{\mathcal{R}(A)} = A A^+ = U_r U_r^\top$.</li>
            <li><b>Row Space Projector:</b> $P_{\mathcal{R}(A^\top)} = A^+ A = V_r V_r^\top$.</li>
        </ul>
        <br><b>Intuition:</b> $A^+$ inverts the action on the range and kills the orthogonal complement. $AA^+$ maps range $\to$ range (identity) and orthogonal complement $\to$ 0.</p>
      </div>

      <h3>P1.6 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>We want to find the point $\mathbf{p} = A\mathbf{x}$ in the column space closest to $\mathbf{b}$.
        Problem: $\min_\mathbf{x} \|A\mathbf{x} - \mathbf{b}\|_2^2$.
        Gradient: $2A^\top (A\mathbf{x} - \mathbf{b}) = 0$.
        Normal eqs: $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        Since cols are independent, $A^\top A$ is invertible.
        Solution: $\mathbf{x}^* = (A^\top A)^{-1} A^\top \mathbf{b}$.
        Projection: $\mathbf{p} = A \mathbf{x}^* = A(A^\top A)^{-1} A^\top \mathbf{b}$.
        The matrix mapping $\mathbf{b}$ to $\mathbf{p}$ is $P = A(A^\top A)^{-1} A^\top$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The "Hat" Matrix:</b> $P = A(A^\top A)^{-1} A^\top$.
        <br><b>Requirements:</b> Valid only when $A$ has full column rank (so $A^\top A$ is invertible).
        <br><b>General Case:</b> If $A$ is rank-deficient, use the pseudoinverse form $P = AA^+$.
        <br><b>Properties:</b> $P^2=P$ and $P^\top=P$. The residual is $(I-P)\mathbf{b}$, which projects onto $\mathcal{N}(A^\top)$.</p>
      </div>

      <h3>P1.7 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Normal $a = (1, 1, 1)^\top$. Point $\mathbf{y} = (1, 2, 3)^\top$. Target $\mathbf{b}=3$.
        $\mathbf{p} = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a$.
        $a^\top \mathbf{y} = 1+2+3 = 6$. $\|a\|^2 = 3$.
        $\mathbf{p} = (1, 2, 3)^\top - \frac{6-3}{3} (1, 1, 1)^\top = (1, 2, 3)^\top - (1, 1, 1)^\top = (0, 1, 2)^\top$.
        Check: $0+1+2=3$. Correct.
        Distance: $\|y-p\| = \|(1, 1, 1)\| = \sqrt{3}$.
        Formula: $\frac{|a^\top \mathbf{y} - \mathbf{b}|}{\|a\|} = \frac{|6-3|}{\sqrt{3}} = \frac{3}{\sqrt{3}} = \sqrt{3}$. Matches.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Constraint Projection:</b> The projection of $\mathbf{y}$ onto the affine set $\{x \mid a^\top \mathbf{x} = \mathbf{b}\}$ is:
        $$ \Pi(\mathbf{y}) = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a $$
        <b>Optimization View:</b> This is the solution to $\min_\mathbf{x} \frac{1}{2}\|\mathbf{x}-\mathbf{y}\|^2$ s.t. $a^\top \mathbf{x} = \mathbf{b}$.
        <br><b>Lagrange Multiplier:</b> The term $\frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2}$ corresponds to the Lagrange multiplier $\nu$ associated with the constraint.</p>
      </div>

      <h3>P1.8 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The matrix is diagonal, so its singular values are the absolute values of diagonal entries: $1$ and $\epsilon$.
        Assuming $\epsilon < 1$: $\sigma_{\max} = 1, \sigma_{\min} = \epsilon$.
        $\kappa(A) = 1/\epsilon$.
        As $\epsilon \to 0$, $\kappa(A) \to \infty$. The matrix becomes ill-conditioned (nearly singular).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Condition Number $\kappa(A)$:</b> The ratio $\sigma_{\max}/\sigma_{\min}$ (for square invertible matrices).
        <br><b>Interpretation:</b> The aspect ratio of the hyperellipse image of the unit sphere.
        <br><b>Impact:</b>
        <ul>
            <li><b>Sensitivity:</b> Relative error in $\mathbf{x}$ $\le \kappa(A) \times$ Relative error in $\mathbf{b}$.</li>
            <li><b>Optimization:</b> Controls convergence rate of gradient descent. High $\kappa$ $\implies$ "Zig-zagging".</li>
        </ul></p>
      </div>

      <h3>P1.9 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = xy^\top$ where $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ are non-zero vectors. Show that $A^+ = \frac{A^\top}{\|\mathbf{x}\|_2^2 \|\mathbf{y}\|_2^2}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>SVD of rank-1 matrix: $A = \sigma \mathbf{u} \mathbf{v}^\top$.
        $\|A\|_F = \|x\|\|y\| = \sigma$.
        $\mathbf{u} = \mathbf{x}/\|\mathbf{x}\|$, $\mathbf{v} = \mathbf{y}/\|\mathbf{y}\|$.
        Check: $A = (\|\mathbf{x}\|\|\mathbf{y}\|) \frac{\mathbf{x}}{\|\mathbf{x}\|} \frac{\mathbf{y}^\top}{\|\mathbf{y}\|} = xy^\top$. Correct.
        Pseudoinverse: $A^+ = \frac{1}{\sigma} \mathbf{v} \mathbf{u}^\top = \frac{1}{\|\mathbf{x}\|\|\mathbf{y}\|} \frac{\mathbf{y}}{\|\mathbf{y}\|} \frac{\mathbf{x}^\top}{\|\mathbf{x}\|} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.
        Note that $A^\top = (xy^\top)^\top = yx^\top$.
        Thus $A^+ = \frac{A^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-1 SVD:</b> A matrix $A = xy^\top$ has exactly one non-zero singular value $\sigma_1 = \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$.
        <br><b>Singular Vectors:</b> $u_1 = \mathbf{x}/\|\mathbf{x}\|$, $v_1 = \mathbf{y}/\|\mathbf{y}\|$.
        <br><b>Pseudoinverse Formula:</b> For rank-1 matrices, $A^+ = \frac{A^\top}{\|A\|_F^2} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>

      <h3>P1.10 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$\|A\|_2^2 = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|^2}{\|\mathbf{x}\|^2} = \sup_{\mathbf{x} \ne 0} \frac{\mathbf{x}^\top A^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$.
        The term on the right is the Rayleigh quotient for the symmetric matrix $M = A^\top A$.
        By the variational characterization of eigenvalues, the maximum value of the Rayleigh quotient is $\lambda_{\max}(M)$.
        Thus $\|A\|_2^2 = \lambda_{\max}(A^\top A)$, so $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Variational Definition of Eigenvalues:</b> For symmetric $M$, $\lambda_{\max}(M) = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top M \mathbf{x}$.
        <br><b>Spectral Norm Connection:</b> $\|A\|_2^2 = \max_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|^2 = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top (A^\top A) \mathbf{x} = \lambda_{\max}(A^\top A)$.
        <br><b>Implication:</b> The spectral norm is the square root of the largest eigenvalue of the Gram matrix.</p>
      </div>

      <h3>P1.11 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Frobenius:</b> $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
        $\|UAV^\top\|_F^2 = \mathrm{tr}(V A^\top U^\top U A V^\top) = \mathrm{tr}(V A^\top A V^\top) = \mathrm{tr}(V^\top V A^\top A) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.
        <br><b>Spectral:</b> $\|UAV^\top\|_2 = \sup_{\|\mathbf{x}\|=1} \|UAV^\top \mathbf{x}\|$.
        Since $U$ is orthogonal, $\|Uy\| = \|y\|$. So we maximize $\|A (V^\top \mathbf{x})\|$.
        Since $V$ is orthogonal, as $\mathbf{x}$ ranges over the unit sphere, $\mathbf{y} = V^\top \mathbf{x}$ also ranges over the unit sphere.
        Thus $\sup_{\|\mathbf{x}\|=1} \|A V^\top \mathbf{x}\| = \sup_{\|\mathbf{y}\|=1} \|A\mathbf{y}\| = \|A\|_2$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Invariance:</b> A matrix norm is orthogonally invariant if $\|UAV^\top\| = \|A\|$ for all orthogonal $U, V$.
        <br><b>Schatten p-norms:</b> Defined as the $\ell_p$ norm of the singular values vector $\sigma(A)$.
        <ul>
            <li>$p=\infty$: Spectral Norm.</li>
            <li>$p=2$: Frobenius Norm.</li>
            <li>$p=1$: Nuclear Norm (Trace Norm).</li>
        </ul>
        <br>These are the "natural" norms for matrix analysis.</p>
      </div>

      <h3>P1.12 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Group:</b>
        1. Closure: $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.
        2. Inverse: $Q^{-1} = Q^\top$, which is orthogonal since $(Q^\top)^\top Q^\top = Q Q^\top = I$.
        3. Identity: $I \in O(n)$.
        <br><b>Compactness:</b>
        1. Bounded: The columns are unit vectors, so $\|Q\|_F = \sqrt{n}$. The set is bounded.
        2. Closed: The condition $Q^\top Q = I$ is a continuous equality constraint (level set of a continuous function). Thus the set is closed.
        By Heine-Borel, closed and bounded in finite dimensions implies compact.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The Orthogonal Group $O(n)$:</b> The set of all $n \times n$ orthogonal matrices.
        <br><b>Compactness Proof:</b>
        <ul>
            <li><b>Boundedness:</b> $\|Q\|_F = \sqrt{n}$, so $O(n)$ is contained in a ball of radius $\sqrt{n}$.</li>
            <li><b>Closedness:</b> It is the solution set of continuous polynomial equations $Q^\top Q - I = 0$.</li>
        </ul>
        <br><b>Optimization Consequence:</b> Any continuous function (like a norm or trace) maximized over orthogonal matrices <i>attains</i> its maximum. This is often used to prove existence of SVD.</p>
      </div>
</section>

    <!-- SECTION 11: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>10. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
