[
  {
    "filename": "linear_map_grid.gif",
    "description": "Animation showing a linear map as structure-preserving deformation. Start with a square grid (pure structure). The grid smoothly deforms under a linear map. Straight lines remain straight. Parallel lines remain parallel. The origin never moves. Locks in: a linear map is not about coordinates or formulas—it is exactly a transformation that preserves linear structure. Everything later (eigenvectors, rank, SVD) is a refinement of this picture."
  },
  {
    "filename": "four_fundamental_subspaces.gif",
    "description": "Animation of the four fundamental subspaces under one map (in one view). Domain panel: Row(A) direction vs Null(A) direction. Codomain panel: Col(A) direction vs LeftNull(A) direction. Two different inputs x₁, x₂ differ only in their Null(A) component, yet their outputs overlap: Ax₁ = Ax₂. That's the kernel/row-space split in action—the kernel component is 'invisible' to the transformation, while only the row-space component contributes to the output."
  },
  {
    "filename": "kernel_image_rank_unified.gif",
    "description": "Unified animation of kernel, image, and rank. Left panel (domain): the moving input vector v explores all directions; the dashed line is the kernel direction (inputs that collapse to zero). Right panel (codomain): the output Av always lands on the dashed image line (all possible outputs live in a 1D subspace). When v aligns with the kernel line, the output snaps to (near) zero. The caption reminds: dim(ℝ²) = rank(A) + nullity(A) = 1 + 1. Visualizes the rank-nullity theorem in action."
  },
  {
    "filename": "basis_coordinates.gif",
    "description": "Animation demonstrating basis and coordinates—representation is unique. Black dotted grid: standard coordinates (reference only). Red skewed grid: a new basis {b₁, b₂}. Blue vector v: fixed geometric object. The coefficients (c₁, c₂) update live so that v = c₁b₁ + c₂b₂. Locks in: a basis is a coordinate system, coordinates depend on the basis, and uniqueness of coordinates is the defining feature of a basis. This animation is the conceptual bridge between linear independence, matrices as representations, and change of basis."
  },
  {
    "filename": "dimension_degrees_of_freedom.gif",
    "description": "Animation showing dimension as degrees of freedom. Two panels: Left - motion constrained to a line (1 degree of freedom), one parameter controls everything. Right - motion fills the plane (2 degrees of freedom), two independent parameters are required. Locks in: dimension = number of independent parameters needed to describe all vectors. This conceptual backbone makes rank, nullity, rank–nullity theorem, and 'how many constraints really matter?' feel inevitable."
  },
  {
    "filename": "determinant_area_orientation.gif",
    "description": "Animation showing determinant as signed area scaling. The unit square becomes a parallelogram. |det(A)| is the area scale factor; the sign indicates orientation preservation or flip. When det(A) ≈ 0, the parallelogram collapses (singular map)."
  },
  {
    "filename": "determinant_column_dependence.gif",
    "description": "Animation showing determinant goes to 0 when columns become dependent. As the two columns become nearly collinear, the parallelogram flattens and the determinant shrinks to ~0. This is the clean geometric reason: det(A) = 0 ⟺ columns are linearly dependent ⟺ A is not invertible. The animation makes the collapse from 'full volume' to 'flat area' viscerally clear."
  },
  {
    "filename": "diagonalization_eigenbasis.gif",
    "description": "Animation showing diagonalization: in the eigenbasis the map is just axis scaling. Left: in standard coordinates, a circle becomes an ellipse (matrix action looks complicated). Right: in eigen-coordinates, the same transformation becomes pure independent scaling along axes (matrix is diagonal). This reveals the core insight of diagonalization: every diagonalizable linear map is 'secretly' just independent stretches along special directions."
  },
  {
    "filename": "trace_sum_eigenvalues_constant.gif",
    "description": "Animation showing trace as sum of eigenvalues with constant trace while eigenvalues trade off. A circle is mapped to an ellipse while λ₁ increases and λ₂ decreases, yet tr(A) = λ₁ + λ₂ stays fixed. This nails the intuition: trace is a 'total scaling budget' across invariant directions. The geometric interpretation makes trace invariance under similarity transformations visually obvious."
  },
  {
    "filename": "layer2_21_dot_product_projection.gif",
    "description": "Animation showing dot product as projection (shadow length). Black arrow: fixed direction u. Blue arrow: moving vector x. Red dashed segment + red dot: projection of x onto u. The number displayed is x^T u = ||u|| × (signed length of the shadow of x on u). This animation eliminates the conceptual confusion that 'dot product is coordinate multiplication' or 'dot product is mysterious algebra'. After this visualization, Cauchy–Schwarz becomes inevitable, not magical."
  },
  {
    "filename": "layer2_23_orthogonality_projection.gif",
    "description": "Animation of orthogonality and projection decomposition. Black arrow: fixed u. Blue arrow: v. Red arrow: proj_u(v). Green arrow: residual v − proj_u(v). The displayed number ⟨residual, u⟩ ≈ 0 locks in the fundamental identity v = proj_u(v) + orthogonal_residual. This is the core geometric fact behind least squares, normal equations, Gram–Schmidt, and all orthogonal decompositions. If this animation is understood, least squares later feels obvious."
  },
  {
    "filename": "orthonormal_coordinates.gif",
    "description": "Animation showing orthonormal basis coordinates as dot products. Demonstrates v = (q1^T v)q1 + (q2^T v)q2. When basis is orthonormal, coefficients are literally the inner products with basis vectors."
  },
  {
    "filename": "least_squares_projection.gif",
    "description": "Animation of least squares as projection onto Col(A). Shows b moving, its projection p = proj_{Col(A)}(b), and Ax* = p. The residual length ||b - Ax*|| updates live, visualizing the minimization of squared error."
  },
  {
    "filename": "least_squares_via_qr.gif",
    "description": "Animation of least squares via QR decomposition. Shows stable computation: y = q^T b, then x* = y/r, and Ax* is the same projection result. Demonstrates the numerical stability advantage of QR over normal equations."
  },
  {
    "filename": "supporting_hyperplanes_l1_linf_three_panels.gif",
    "description": "Three-panel animation of supporting hyperplanes sliding over ℓ₁/ℓ∞ balls. Panel 1 (diamond): supporting line is u^T x = sup_{||x||₁≤1} u^T x = ||u||_∞. Panel 2 (square): supporting line is u^T x = sup_{||x||_∞≤1} u^T x = ||u||₁. Panel 3 overlays both: you literally see two different 'shadow lengths' for the same direction u, depending on whether the ball is diamond or square. This is the geometric proof of dual norm duality: (ℓ₁)* = ℓ_∞ and (ℓ_∞)* = ℓ₁."
  },
  {
    "filename": "schur_complement_block.png",
    "description": "A visualization of the Schur complement process as a transformation of matrix structure. Left: A large square matrix M partitioned into blocks A (top-left, red), B (top-right, blue), C (bottom-left, blue), and D (bottom-right, green). The determinant of the whole block matrix is represented as the volume of a parallelpiped formed by its column vectors. Center: An 'Elimination Matrix' E (block lower triangular) acting on M. Right: The result ME which is block upper triangular. The bottom-right block is still D, but the top-left block has transformed into the Schur complement S = A - BD^{-1}C. The determinant is now clearly det(S) * det(D), visualized as the product of the 'volumes' of the diagonal blocks."
  },
  {
    "filename": "determinant-volume-scaling.png",
    "description": "A 2D determinant-as-area visualization. Left: the unit square spanned by standard basis vectors e1 and e2 with area 1. Right: the image of the square under a linear map A, forming a parallelogram spanned by Ae1 and Ae2. The parallelogram area is labeled |det(A)|, and an orientation arrow indicates that the sign of det(A) captures whether orientation is preserved or flipped."
  },
  {
    "filename": "operator-norm-geometry.png",
    "description": "A geometric visualization of the spectral/operator norm ||A||2 as maximum stretch. Left: a unit circle in R^2. Right: its image under A is an ellipse. The longest semi-axis (corresponding to the largest singular value σ_max) is highlighted, illustrating ||A||2 = max_{||x||=1} ||Ax||."
  },
  {
    "filename": "l1-vs-linf-duality.png",
    "description": "A 2D visualization of ℓ1/ℓ∞ duality via inner products. The ℓ1 unit ball is drawn as a diamond. A vector y is shown, and level sets of x^T y are shifted until they touch the diamond at a vertex. The maximizing x selects the coordinate of y with largest magnitude, illustrating sup_{||x||1≤1} x^T y = ||y||∞."
  },
  {
    "filename": "least-squares-orthogonal-projection.png",
    "description": "A 3D geometry picture of least squares as orthogonal projection. A plane through the origin represents the column space R(A). A target vector b lies off the plane. The projection p = Ax* lies on the plane, and the residual r = b − p is drawn as a perpendicular (right angle) to the plane, visualizing the normal-equation condition A^T r = 0."
  },
  {
    "filename": "projection-onto-affine-set.png",
    "description": "A visualization of projection onto an affine set (a translated subspace). The affine constraint set is shown as a plane not passing through the origin. A point y is projected perpendicularly onto the plane. The diagram highlights the translation trick: subtract a particular solution x0 to reduce to projecting (y − x0) onto the parallel linear subspace, then shift back by x0."
  },
  {
    "filename": "gram-schmidt-orthogonalization.png",
    "description": "A step-by-step geometric illustration of Gram–Schmidt orthogonalization. Starting from linearly independent vectors (v1, v2, v3), the construction subtracts successive projections to produce orthogonal vectors (u1, u2, u3), then normalizes to obtain an orthonormal basis (q1, q2, q3)."
  }
]
