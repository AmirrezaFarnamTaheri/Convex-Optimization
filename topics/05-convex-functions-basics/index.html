<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>05. Convex Functions: Basics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../04-convex-sets-cones/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../06-convex-functions-advanced/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>05. Convex Functions: Basics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: convex-functions, jensen, hessian, epigraph</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture introduces the core concept of convex functions, which are the building blocks of convex optimization. We explore the definition via Jensen's inequality, the epigraph characterization, and first- and second-order conditions for differentiable functions. We also build a toolkit of operations that preserve convexity, enabling the construction and recognition of complex convex functions. Advanced topics such as strong convexity, smoothness, and subgradients are covered in <a href="../06-convex-functions-advanced/index.html">Lecture 06</a>.</p>
        <p><strong>Prerequisites:</strong> <a href="../03-convex-sets-geometry/index.html">Lecture 03: Convex Sets Geometry</a> (convex sets, lines, segments).</p>
        <p><strong>Forward Connections:</strong> These concepts are foundational for defining convex problems in <a href="../07-convex-problems-standard/index.html">Lecture 07</a> and for duality theory in <a href="../09-duality/index.html">Lecture 09</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Define Convex Functions:</b> Apply Jensen's inequality and the epigraph definition to test for convexity.</li>
        <li><b>Verify Convexity:</b> Use first-order (tangent) and second-order (Hessian) conditions to prove convexity for differentiable functions.</li>
        <li><b>Apply Calculus Rules:</b> Construct complex convex functions from simple ones using sums, compositions, and pointwise maxima.</li>
        <li><b>Recognize Key Examples:</b> Identify standard convex functions like norms, exponentials, and log-sum-exp.</li>
      </ul>
    </section>

    <!-- TRIFECTA NARRATIVE -->
    <section class="section-card">
      <h2><i data-feather="check-circle"></i> The Completion of the Trifecta</h2>

      <div class="insight">
        <h4>From Sets to Cones to Functions: The Convexity Trifecta</h4>
        <p>This lecture completes a three-lecture arc through the <b>mathematical foundations</b> of convex optimization:</p>

        <table style="width: 100%; border-collapse: collapse; margin-top: 1rem; margin-bottom: 1rem;">
          <thead>
            <tr style="background: var(--surface-2); border-bottom: 2px solid var(--border);">
              <th style="padding: 12px; text-align: left;">Lecture</th>
              <th style="padding: 12px; text-align: left;">Topic</th>
              <th style="padding: 12px; text-align: left;">Key Question</th>
              <th style="padding: 12px; text-align: left;">Tools</th>
            </tr>
          </thead>
          <tbody>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b><a href="../03-convex-sets-geometry/index.html">L03</a></b></td>
              <td style="padding: 12px;">Convex Sets</td>
              <td style="padding: 12px;"><i>What are convex sets?</i></td>
              <td style="padding: 12px;">Geometry, separation theorems, projections</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b><a href="../04-convex-sets-cones/index.html">L04</a></b></td>
              <td style="padding: 12px;">Cones & Operations</td>
              <td style="padding: 12px;"><i>What operations preserve convexity?</i></td>
              <td style="padding: 12px;">Dual cones, generalized inequalities, duality theory</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border); background: var(--surface-2);">
              <td style="padding: 12px;"><b>L05</b> (here)</td>
              <td style="padding: 12px;">Convex Functions</td>
              <td style="padding: 12px;"><i>What functions have convex epigraphs?</i></td>
              <td style="padding: 12px;">Jensen's inequality, gradients, Hessians, composition rules</td>
            </tr>
          </tbody>
        </table>

        <p><b>The Arc:</b> <b>Sets</b> (geometric objects) → <b>Cones</b> (algebraic structure) → <b>Functions</b> (analytical properties) → <b>Problems</b> (optimization)</p>

        <p><b>The Bridge:</b> Every convex function $f$ defines a convex set—its <b>epigraph</b> epi $f = \{(x, t) : f(x) \le t\}$. This is the bridge from <b>functions</b> (what this lecture studies) to <b>sets</b> (what Lectures 03-04 studied). When we minimize $f(x)$, we're really finding the lowest point in epi $f$.</p>
      </div>

      <h3>Why Convex Functions Matter for Optimization</h3>
      <p>Convex functions are <b>the objectives and constraints</b> in convex optimization problems. Here's why they're special:</p>

      <ol>
        <li><b>Local Minimum = Global Minimum:</b> Unlike nonconvex functions (which may have many local minima), a convex function has the property that <i>any local minimum is a global minimum</i>. This makes optimization tractable—gradient descent won't get stuck!</li>

        <li><b>Efficient Verification Tools:</b> This lecture gives you three ways to verify convexity:
          <ul style="margin-top: 0.5rem;">
            <li><b>Definition</b> (§1): Check Jensen's inequality directly (works for any function)</li>
            <li><b>First-order condition</b> (§3): Check that the gradient defines a global underestimator (requires differentiability)</li>
            <li><b>Second-order condition</b> (§4): Check that the Hessian is PSD (requires twice-differentiability, easiest in practice!)</li>
          </ul>
          You'll learn when to use each method in §7.0.
        </li>

        <li><b>Composition Rules</b> (§5): You can <i>build</i> complex convex functions from simple ones using operations that preserve convexity (sums, compositions, pointwise maxima). This is the foundation of <b>Disciplined Convex Programming (DCP)</b> in tools like CVXPY.</li>

        <li><b>Foundation for Standard Problem Classes:</b> The convex functions you'll see in this lecture (norms, quadratic forms, log-sum-exp, etc.) are the building blocks for:
          <ul style="margin-top: 0.5rem;">
            <li><b>Linear Programs (LP):</b> Minimize/maximize a linear function</li>
            <li><b>Quadratic Programs (QP):</b> Minimize a quadratic function</li>
            <li><b>Geometric Programs (GP):</b> Minimize posynomials (after log transform)</li>
            <li><b>Conic Programs (SOCP, SDP):</b> Constraints defined by norms and matrix inequalities</li>
          </ul>
          You'll see these in <a href="../07-convex-problems-standard/index.html">Lecture 07</a> (standard forms) and <a href="../08-convex-problems-conic/index.html">Lecture 08</a> (conic forms).
        </li>
      </ol>

      <div class="insight">
        <h4>What's Next: From Theory to Practice</h4>
        <p>After this lecture, you'll have completed the <b>theoretical core</b> of convex optimization:</p>
        <ul>
          <li>✓ You understand <b>convex sets</b> (geometry)</li>
          <li>✓ You understand <b>cones and dual cones</b> (algebra and duality)</li>
          <li>✓ You understand <b>convex functions</b> (analysis and calculus)</li>
        </ul>
        <p>In <a href="../06-convex-functions-advanced/index.html">Lecture 06</a>, we'll study advanced function properties (subdifferentials, conjugates, strong convexity). Then in <a href="../07-convex-problems-standard/index.html">Lectures 07-09</a>, we'll apply everything to <b>solve real optimization problems</b>—LP, QP, GP, SOCP, SDP, and duality theory.</p>
        <p><b>The Payoff:</b> With the foundations from L03-05, you'll be able to recognize convex problems in the wild, reformulate them into standard forms, and solve them efficiently with off-the-shelf solvers. That's the power of convex optimization!</p>
      </div>
    </section>

    <article>
      <section class="section-card" id="section-1">
        <h2>1. Definition and Basic Properties</h2>
        <h3>1.1 What kind of "function" are we using, really?</h3>

        <p>In convex optimization we almost never work with a plain function $f:\mathbb{R}^n\to\mathbb{R}$. We work with an <b>extended-real-valued function</b>:</p>
        $$ f:\mathbb{R}^n\to \mathbb{R}\cup\{+\infty\} $$
        <p>That "$+\infty$" is not cosmetic. It’s a <i>modeling device</i> that lets you fold constraints into the objective.</p>

        <h4>Effective domain (a.k.a. where the function is finite / legal)</h4>
        $$ \operatorname{dom} f := \{x\in\mathbb{R}^n : f(x) < +\infty\} $$
        <p>Interpretation:</p>
        <ul>
            <li>If $x\notin\operatorname{dom} f$, the model declares $x$ <b>illegal</b> by setting $f(x)=+\infty$.</li>
            <li>Minimizing $f$ automatically avoids illegal points because any finite value beats $+\infty$.</li>
        </ul>

        <h4>The key modeling trick: Indicator Functions</h4>
        <p>Given a constraint set $C\subseteq\mathbb{R}^n$, define its <b>indicator</b>:</p>
        $$ \delta_C(x)= \begin{cases} 0,& x\in C\\ +\infty,& x\notin C. \end{cases} $$
        <p>Then the constrained problem $\min_{x\in C} g(x)$ is exactly the unconstrained extended-real problem $\min_x \; g(x)+\delta_C(x)$. This is one of the deepest "why convex analysis is a language" facts: constraints become terms in the objective.</p>

        <h4>"Proper" Functions</h4>
        <p>A convex analysis convention: we usually want $f$ to be <b>proper</b>, meaning:</p>
        <ol>
            <li>$f(x)>-\infty$ for all $x$ (no $-\infty$ values), and</li>
            <li>$\operatorname{dom} f\neq \emptyset$ (not identically $+\infty$).</li>
        </ol>
        <p>Otherwise optimization becomes degenerate: $-\infty$ destroys minimization, and identically $+\infty$ means no feasible point exists.</p>

        <h3>1.2 Convexity: The Core Definition</h3>

        <p>A function $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ is <a href="#" class="definition-link" data-term="convex function">convex</a> if:</p>
        <ol>
            <li>its effective domain $\mathrm{dom}\, f$ is a <b>convex set</b>, and</li>
            <li>for every $x,y\in\operatorname{dom} f$ and every $\theta\in[0,1]$:
                $$ \boxed{ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) } \tag{1} $$
            </li>
        </ol>
        <p>Read it as a machine: pick two legal points $x,y$, mix them into $z=\theta x+(1-\theta)y$, then "cost at the mix" is at most "mix of the costs". This is the "no surprises when averaging" rule.</p>

        <div class="insight">
          <h4>Why "Domain Convexity" is Not a Nitpick</h4>
          <p>A classic trap is: "I only checked the inequality wherever it’s defined, so it must be convex." That’s wrong, because if the domain is weird, you may never be able to test the inequality on interior mixtures.</p>
          <p><b>Example (Fake Convexity Check):</b> Let $f(x) = 0$ if $x \in \{0, 1\}$ and $+\infty$ otherwise. $\operatorname{dom} f=\{0,1\}$ (not convex).
          <br>If someone naively says "pick $x,y\in\{0,1\}$, done," they ignore the segment between them. For $x=0, y=1, \theta=0.5$:
          <br>LHS: $f(0.5) = +\infty$. RHS: $0.5 f(0) + 0.5 f(1) = 0$.
          <br>The inequality fails. <b>Moral:</b> convexity is fundamentally a statement about <b>behavior under mixing</b>, and mixing leaves the domain only if the domain is nonconvex.</p>
        </div>

        <h3>1.3 Strict Convexity and Uniqueness</h3>
        <p>$f$ is <b>strictly convex</b> if for all distinct $x \neq y$ in $\operatorname{dom} f$ and all $\theta \in (0,1)$:</p>
        $$ f(\theta x + (1-\theta)y) < \theta f(x) + (1-\theta)f(y) $$

        <div class="theorem-box">
            <h4>Theorem: Uniqueness of Minimizers</h4>
            <p>If $f$ is strictly convex on a convex set $C$, then the minimizer of $f$ over $C$, if it exists, is <b>unique</b>.</p>
            <p><b>Proof:</b> Suppose $x_1 \ne x_2$ are both global minimizers with value $m$. Since $C$ is convex, the midpoint $\bar{x} = 0.5(x_1+x_2)$ is in $C$. Strict convexity gives:
            $$ f(\bar{x}) < 0.5 f(x_1) + 0.5 f(x_2) = 0.5 m + 0.5 m = m $$
            Contradiction. Thus, no two distinct minimizers can exist.</p>
        </div>

        <h3>1.4 The "Safety Theorem": Local Min is Global Min</h3>
        <div class="theorem-box">
            <h4>Theorem</h4>
            <p>If $f$ is convex on a convex domain, then every local minimizer is a global minimizer.</p>
        </div>
        <div class="proof-box">
            <h4>Proof (Fully Explicit)</h4>
            <p>Let $x^\star$ be a local minimizer. By definition, there exists $r>0$ such that $f(x^\star) \le f(z)$ for all $z \in \operatorname{dom}f \cap B(x^\star,r)$.
            <br>Now take an arbitrary $x \in \operatorname{dom}f$. If $x=x^\star$, trivial. Assume $x \ne x^\star$.
            <br>Because $\operatorname{dom}f$ is convex, the segment connects them. For small $\theta > 0$, define $x_\theta = (1-\theta)x^\star + \theta x$.
            <br>Choose $\theta$ small enough so $x_\theta \in B(x^\star,r)$. By local minimality: $f(x^\star) \le f(x_\theta)$.
            <br>By convexity: $f(x_\theta) \le (1-\theta)f(x^\star) + \theta f(x)$.
            <br>Combine: $f(x^\star) \le (1-\theta)f(x^\star) + \theta f(x) \implies \theta f(x^\star) \le \theta f(x)$.
            <br>Divide by $\theta > 0$: $f(x^\star) \le f(x)$.
            <br>Since $x$ was arbitrary, $x^\star$ is a global minimizer.</p>
        </div>

        <div class="proof-box">
          <h4>Proof: Norms are Convex (The Sublinear Template)</h4>
          <p>Norms are a special case of <b>sublinear functions</b>—functions that are positively homogeneous and subadditive. Any sublinear function is convex.</p>
          <div class="proof-step">
            <strong>Step 1: Convex Combination.</strong> Let $x, y \in \mathbb{R}^n$ and $\theta \in [0, 1]$.
            $$ f(\theta x + (1-\theta)y) = \|\theta x + (1-\theta)y\| $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Subadditivity (Triangle Inequality).</strong> Using $\|a+b\| \le \|a\| + \|b\|$:
            $$ \|\theta x + (1-\theta)y\| \le \|\theta x\| + \|(1-\theta)y\| $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Positive Homogeneity.</strong> Since $\theta \ge 0$ and $1-\theta \ge 0$:
            $$ \|\theta x\| = \theta \|x\| \quad \text{and} \quad \|(1-\theta)y\| = (1-\theta)\|y\| $$
            Substituting back:
            $$ \|\theta x + (1-\theta)y\| \le \theta \|x\| + (1-\theta)\|y\| = \theta f(x) + (1-\theta)f(y) $$
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> All norms (and all sublinear functions like support functions) are convex.
          </div>
        </div>

        <h3>1.5 Jensen's Inequality: The General Form</h3>
        <p>The basic definition involving two points generalizes to any number of points and even to integrals (expectations).</p>

        <div class="insight">
          <h4>Historical Note</h4>
          <p>Johan Jensen (1906) proved this inequality for continuous functions. It has since become the cornerstone of information theory (Shannon 1948), economics (Atkinson 1970), and machine learning (Variational Inference/ELBO). It is arguably the most useful inequality in analysis.</p>
        </div>

        <h4>Finite Form</h4>
        <p>If $f$ is convex, $x_1, \dots, x_k \in \mathrm{dom}\, f$, and $\theta_1, \dots, \theta_k \ge 0$ with $\sum \theta_i = 1$, then:</p>
        $$
        f\left(\sum_{i=1}^k \theta_i x_i\right) \le \sum_{i=1}^k \theta_i f(x_i)
        $$
        <div class="proof-box">
          <h4>Proof of Finite Jensen by Induction</h4>
          <div class="proof-step">
            <strong>Base Case ($k=2$):</strong> The inequality $f(\theta_1 x_1 + \theta_2 x_2) \le \theta_1 f(x_1) + \theta_2 f(x_2)$ holds by the definition of convexity.
          </div>
          <div class="proof-step">
            <strong>Inductive Step:</strong> Assume the inequality holds for any convex combination of $k$ points. Consider a combination of $k+1$ points: $x = \sum_{i=1}^{k+1} \theta_i x_i$ with $\sum \theta_i = 1$ and $\theta_i > 0$.
            <br>We can "peel off" the last point $x_{k+1}$ and treat the rest as a sub-combination.
            Let $\tilde{\theta} = \sum_{i=1}^{k} \theta_i = 1 - \theta_{k+1}$. Since $\theta_{k+1} < 1$, $\tilde{\theta} > 0$.
            $$ x = \theta_{k+1} x_{k+1} + \tilde{\theta} \sum_{i=1}^{k} \frac{\theta_i}{\tilde{\theta}} x_i = \theta_{k+1} x_{k+1} + \tilde{\theta} \bar{x} $$
            where $\bar{x} = \sum_{i=1}^{k} (\theta_i/\tilde{\theta}) x_i$ is a valid convex combination of $k$ points (weights sum to 1).
          </div>
          <div class="proof-step">
            <strong>Apply Convexity Twice:</strong>
            <br>1. Apply the 2-point definition to $x_{k+1}$ and $\bar{x}$:
            $$ f(x) = f(\theta_{k+1} x_{k+1} + \tilde{\theta} \bar{x}) \le \theta_{k+1} f(x_{k+1}) + \tilde{\theta} f(\bar{x}) $$
            2. Apply the inductive hypothesis to $f(\bar{x})$:
            $$ f(\bar{x}) \le \sum_{i=1}^{k} \frac{\theta_i}{\tilde{\theta}} f(x_i) $$
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> Substituting back:
            $$ f(x) \le \theta_{k+1} f(x_{k+1}) + \tilde{\theta} \left(\sum_{i=1}^{k} \frac{\theta_i}{\tilde{\theta}} f(x_i)\right) = \theta_{k+1} f(x_{k+1}) + \sum_{i=1}^{k} \theta_i f(x_i) = \sum_{i=1}^{k+1} \theta_i f(x_i) $$
          </div>
        </div>

        <h4>Expectation Form</h4>
        <p>If $Z$ is a random variable in $\mathrm{dom}\, f \subseteq \mathbb{R}^n$ and $f$ is convex, then:</p>
        $$
        \boxed{ f(\mathbb{E}[Z]) \le \mathbb{E}[f(Z)] }
        $$
        <p><b>Caveat:</b> We require $Z \in \mathrm{dom} f$ almost surely, $\mathbb{E}[Z]$ finite, and $\mathbb{E}[f(Z)]$ well-defined.</p>

        <div class="proof-box">
          <h4>Proof via Supporting Hyperplane</h4>
          <p>This proof connects Jensen's inequality directly to the geometry of convex sets (see <a href="../04-convex-sets-cones/index.html#section-2">Lecture 04: Separation Theorems</a>).
          <br><b>Fact:</b> If $f$ is convex, its epigraph is a convex set. By the <b>Supporting Hyperplane Theorem</b>, at any boundary point $(\mu, f(\mu))$, there exists a supporting hyperplane. This hyperplane defines a subgradient $g \in \partial f(\mu)$ such that:
          $$ f(x) \ge f(\mu) + g^\top(x - \mu) \quad \forall x $$
          <br><b>Step 1:</b> Let $\mu = \mathbb{E}[Z]$. Apply the subgradient inequality at $\mu$:
          $$ f(Z) \ge f(\mu) + g^\top(Z - \mu) $$
          <b>Step 2:</b> Take expectation (linear operator):
          $$ \mathbb{E}[f(Z)] \ge f(\mu) + g^\top(\mathbb{E}[Z] - \mu) $$
          Since $\mathbb{E}[Z] = \mu$, the linear term vanishes.
          $$ \mathbb{E}[f(Z)] \ge f(\mathbb{E}[Z]) $$
          This proof confirms Jensen is conceptually "supporting hyperplanes + linearity of expectation".</p>
        </div>

        <div class="insight">
          <h4>Jensen as "Variance is Nonnegative"</h4>
          <p>For $f(x) = x^2$, Jensen's inequality gives $(\mathbb{E}[X])^2 \le \mathbb{E}[X^2]$.
          <br>Rearranging: $\mathbb{E}[X^2] - (\mathbb{E}[X])^2 \ge 0 \implies \mathrm{Var}(X) \ge 0$.
          <br>For vector $f(x) = \|x\|_2^2$, it gives $\|\mathbb{E}[X]\|_2^2 \le \mathbb{E}[\|X\|_2^2]$. Convexity penalizes dispersion.</p>
        </div>

        <h4>Equality Conditions</h4>
        <ul>
            <li>If $f$ is <b>strictly convex</b>, equality holds if and only if $Z$ is constant almost surely ($Z = \mathbb{E}[Z]$).</li>
            <li>If $f$ is affine (linear), equality always holds.</li>
        </ul>

        <h3>1.6 Applications of Jensen's Inequality</h3>
        <p>Jensen's inequality is a powerful engine for generating algebraic inequalities.</p>

        <div class="example">
          <h4>1. Arithmetic-Geometric Mean (AM-GM)</h4>
          <p>We prove the generalized weighted AM-GM inequality using the concavity of the logarithm.</p>
          <div class="proof-step">
            <strong>Step 1: Setup.</strong> Let $f(x) = \log x$. Since $f''(x) = -1/x^2 < 0$, $f$ is concave.
            Jensen's inequality for concave functions reverses the direction: $f(\mathbb{E}[X]) \ge \mathbb{E}[f(X)]$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Apply Jensen.</strong>
            Let $\theta \in [0, 1]$.
            $$ \log(\theta a + (1-\theta)b) \ge \theta \log a + (1-\theta) \log b $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Exponentiate.</strong>
            $$ \log(\theta a + (1-\theta)b) \ge \log(a^\theta b^{1-\theta}) \implies \theta a + (1-\theta)b \ge a^\theta b^{1-\theta} $$
            The weighted arithmetic mean is at least the weighted geometric mean.
          </div>
        </div>

        <div class="example">
          <h4>2. From AM-GM to Hölder's Inequality</h4>
          <p>We build up to Hölder's inequality in two steps.</p>

          <p><b>Step A: Young's Inequality.</b> Let $p, q > 1$ with $1/p + 1/q = 1$. For $a, b \ge 0$:
          $$ ab \le \frac{a^p}{p} + \frac{b^q}{q} $$
          <i>Proof:</i> Apply weighted AM-GM with $\theta=1/p$, $u=a^p$, $v=b^q$:
          $$ \frac{1}{p} u + \frac{1}{q} v \ge u^{1/p} v^{1/q} = a b $$
          </p>

          <p><b>Step B: Hölder's Inequality.</b> For vectors $x, y$:
          $$ \sum |x_i y_i| \le \|x\|_p \|y\|_q $$
          <i>Proof:</i>
          <ol>
            <li>Normalize: $\hat{x} = x/\|x\|_p, \hat{y} = y/\|y\|_q$. Note $\sum |\hat{x}_i|^p = 1$.</li>
            <li>Apply Young's term-wise: $|\hat{x}_i \hat{y}_i| \le \frac{|\hat{x}_i|^p}{p} + \frac{|\hat{y}_i|^q}{q}$.</li>
            <li>Sum: $\sum |\hat{x}_i \hat{y}_i| \le \frac{1}{p}(1) + \frac{1}{q}(1) = 1$.</li>
            <li>Scale back: $\frac{1}{\|x\|_p \|y\|_q} \sum |x_i y_i| \le 1$.</li>
          </ol>
          </p>
        </div>
        <div class="insight">
          <h4>Example 3: The Variance Conjecture (A Cautionary Tale)</h4>
          <p>Jensen's inequality says $\mathbb{E} f(\mathbf{x}_0 + \mathbf{v}) \ge f(\mathbf{x}_0)$ for zero-mean noise $\mathbf{v}$.
          <br>Does larger variance imply a larger expected value?
          <br><b>Conjecture:</b> If $\mathrm{var}(\mathbf{v}) > \mathrm{var}(\mathbf{w})$, then $\mathbb{E} f(\mathbf{x}_0+\mathbf{v}) > \mathbb{E} f(\mathbf{x}_0+\mathbf{w})$?
          <br><b>False!</b> Consider $f(x)=e^x$.
          <ul>
              <li>Let $v = \pm 1$ with prob 0.5. $\text{Var}(v)=1$. $\mathbb{E} e^v \approx 1.54$.</li>
              <li>Let $w = \pm 4$ with prob 0.02, else 0. $\text{Var}(w) = 0.64$. $\mathbb{E} e^w \approx 2.05$.</li>
          </ul>
          Here $\mathbf{v}$ has larger variance but smaller expectation. Convex functions are sensitive to tails, not just variance.
          <br><b>However:</b> If $\mathbf{w} = c \mathbf{v}$ is a scaled version of $\mathbf{v}$ (with $|c| < 1$), then the monotonicity holds.</p>
        </div>

        <h3>1.7 Integral Characterization of Convexity</h3>
        <p>A powerful alternative definition of convexity involves integrals. This mirrors the "average value" property of harmonic functions but with an inequality. We define the integral along the chord:</p>
        $$ I_f(x,y) := \int_0^1 f(tx + (1-t)y) dt $$

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>A continuous function $f$ is convex <b>if and only if</b> for all $x, y \in \mathrm{dom}\, f$:
          $$ \int_0^1 f(tx + (1-t)y) dt \le \frac{f(x) + f(y)}{2} $$
          </p>
        </div>

        <div class="proof-box">
          <h4>Proof Breakdown</h4>

          <div class="proof-step">
            <strong>Step 1: The "Little Red Formula" (Affine Case).</strong>
            <p>Let $\ell(z) = a^\top z + b$ be an affine function. We evaluate its integral along the segment $z(t) = tx + (1-t)y$ for $t \in [0, 1]$.
            <br>By linearity:
            $$ \ell(z(t)) = t(a^\top x + b) + (1-t)(a^\top y + b) = t \ell(x) + (1-t) \ell(y) $$
            Now integrate from $0$ to $1$:
            $$ \int_0^1 \ell(z(t)) dt = \ell(x) \underbrace{\int_0^1 t dt}_{1/2} + \ell(y) \underbrace{\int_0^1 (1-t) dt}_{1/2} = \frac{\ell(x) + \ell(y)}{2} $$
            This establishes a crucial baseline: <b>for an affine function, the average value along the segment equals the average of the endpoint values.</b> Geometrically, this is the area of a trapezoid.</p>
          </div>

          <div class="proof-step">
            <strong>Step 2: Convex $\Rightarrow$ Integral Inequality.</strong>
            <p>If $f$ is convex, the function lies below the chord:
            $$ f(tx + (1-t)y) \le t f(x) + (1-t)f(y) $$
            Integrating this inequality with respect to $t$:
            $$ \int_0^1 f(z(t)) dt \le f(x) \int_0^1 t dt + f(y) \int_0^1 (1-t) dt = \frac{f(x)+f(y)}{2} $$
            This direction is simply "integrating the convexity definition". It can be viewed as <b>Jensen's inequality applied to the uniform distribution</b> on the chord.</p>
          </div>

          <div class="proof-step">
            <strong>Step 3: Integral Inequality $\Rightarrow$ Convex.</strong>
            <p>This direction uses a clever trick: <b>subtracting a supporting line</b> to measure the "bump" above it.
            <br>Assume the integral inequality holds for all $x, y$. Suppose for contradiction that $f$ is <b>not</b> convex. Then there exists a point $z_0$ on some segment where $f(z_0)$ lies strictly above the chord.
            <br>Let $\ell(u)$ be the <b>chord</b> connecting $(x, f(x))$ and $(y, f(y))$.
            <br>Define the difference function $g(u) = f(u) - \ell(u)$.
            <br>Key properties of $g$:
            <ul>
                <li>$g(x) = f(x) - \ell(x) = 0$ and $g(y) = 0$.</li>
                <li>At the violating point $z_0$, $g(z_0) > 0$.</li>
            </ul>
            Now, apply the integral inequality to $g$. Since the integral condition is linear, and equality holds for $\ell$:
            $$ \int_0^1 g(z(t)) dt = \int_0^1 f(z(t)) dt - \int_0^1 \ell(z(t)) dt \le \frac{f(x)+f(y)}{2} - \frac{\ell(x)+\ell(y)}{2} = \frac{g(x)+g(y)}{2} = 0 $$
            So the integral of $g$ must be non-positive.
            <br>However, since $g(z_0) > 0$ and we typically assume continuity, there is a small interval around $z_0$ where $g$ is positive. Thus $\int g > 0$.
            <br>The inequality effectively says the "average bump above the supporting line" must be zero (since endpoints are zero), which forces the function to never rise above the chord.
            <br>This contradiction ($\int g > 0$ vs $\int g \le 0$) proves $f$ must be convex.</p>
          </div>
        </div>

        <h3>1.8 Concave Functions: The Dual Perspective</h3>

        <div class="definition-box">
          <h4>Definition: Concave Function</h4>
          <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <b>concave</b> if its negation $-f$ is convex. Equivalently, $f$ is concave if for all $\mathbf{x}, \mathbf{y} \in \mathrm{dom}\, f$ and $\theta \in [0,1]$:
          $$ f(\theta \mathbf{x} + (1-\theta)\mathbf{y}) \ge \theta f(\mathbf{x}) + (1-\theta)f(\mathbf{y}) $$
          The inequality is reversed compared to convexity: the function value at a convex combination is <i>at least</i> as large as the convex combination of function values. Geometrically, <b>the chord lies below the graph</b> (the graph "bulges upward").</p>
        </div>

        <h4>Geometric Intuition: Flipping the Picture</h4>
        <p>While convex functions have "bowl-shaped" graphs where every chord lies above the function, concave functions have "dome-shaped" or "hill-like" graphs where every chord lies below. If you flip a convex function upside down (negate it), you get a concave function.</p>

        <h4>Examples of Concave Functions</h4>
        <ul>
          <li><b>Logarithm:</b> $f(x) = \log x$ on $\mathbb{R}_{++}$ is strictly concave. The second derivative $f''(x) = -1/x^2 < 0$ confirms concavity via the second-order condition.</li>
          <li><b>Negative Quadratic:</b> $f(x) = -x^\top Q x$ where $Q \succeq 0$ (positive semidefinite). This is the negation of a convex quadratic.</li>
          <li><b>Geometric Mean:</b> $f(\mathbf{x}) = \left(\prod_{i=1}^n x_i\right)^{1/n}$ on $\mathbb{R}_{++}^n$ is concave (proven via the Hessian in Section 4).</li>
          <li><b>Entropy:</b> $f(\mathbf{x}) = -\sum_{i=1}^n x_i \log x_i$ on the probability simplex is concave, which is fundamental in information theory and statistical mechanics.</li>
          <li><b>Minimum Function:</b> $f(\mathbf{x}) = \min\{x_1, x_2, \dots, x_n\}$ is concave (the pointwise minimum of concave functions is concave).</li>
        </ul>

        <h4>Connection to Maximization Problems</h4>
        <p>Concave functions are the natural objective functions for <b>maximization</b> problems. Just as minimizing a convex function is tractable (any local minimum is global), <b>maximizing a concave function is equally tractable</b> (any local maximum is global). In fact, the problems are equivalent:
        $$ \max_{\mathbf{x} \in C} f(\mathbf{x}) \quad \text{(concave $f$)} \iff \min_{\mathbf{x} \in C} -f(\mathbf{x}) \quad \text{(convex $-f$)} $$
        This duality means that all the theory and algorithms for convex minimization immediately apply to concave maximization by simply negating the objective.</p>

        <div class="insight">
          <h4>Why Economics Loves Concave Functions</h4>
          <p>In microeconomics, utility functions are typically assumed to be concave, reflecting the principle of <b>diminishing marginal utility</b>: the more you have of a good, the less additional satisfaction each extra unit provides. Similarly, production functions are often concave, reflecting <b>diminishing returns to scale</b>. Maximizing concave utility subject to a budget constraint (a convex set) is a canonical problem in consumer theory, and it's guaranteed to have a unique global maximum under mild regularity conditions.</p>
        </div>

        <h4>Key Characterizations (Duals of Convex Results)</h4>
        <p>All the characterizations of convexity have "flipped" analogs for concavity:</p>
        <ul>
          <li><b>First-Order Condition (Supergradient):</b> A differentiable $f$ is concave if and only if the first-order Taylor approximation is a <i>global overestimate</i>:
          $$ f(\mathbf{y}) \le f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) \quad \forall \mathbf{x}, \mathbf{y} $$
          The tangent hyperplane lies <i>above</i> the graph everywhere.</li>
          <li><b>Second-Order Condition:</b> A twice-differentiable $f$ is concave if and only if its Hessian is <b>negative semidefinite</b> everywhere: $\nabla^2 f(\mathbf{x}) \preceq 0$ for all $\mathbf{x} \in \mathrm{dom}\, f$.</li>
          <li><b>Epigraph Characterization:</b> While the epigraph $\{(x,t) : f(x) \le t\}$ characterizes convex functions, the <b>hypograph</b> $\{(\mathbf{x}, t) : f(\mathbf{x}) \ge t\}$ (the region <i>below</i> the graph) is convex if and only if $f$ is concave.</li>
        </ul>

        <h4>Preservation Under Operations</h4>
        <p>The rules for preserving concavity mirror those for convexity with sign flips:</p>
        <ul>
          <li><b>Sums:</b> The sum of concave functions is concave.</li>
          <li><b>Positive Scaling:</b> If $f$ is concave and $\alpha \ge 0$, then $\alpha f$ is concave.</li>
          <li><b>Pointwise Minimum:</b> If $f_1, \dots, f_m$ are concave, then $f(\mathbf{x}) = \min\{f_1(\mathbf{x}), \dots, f_m(\mathbf{x})\}$ is concave.</li>
          <li><b>Composition:</b> Concave functions composed with affine maps remain concave. However, composition with nonlinear transformations requires careful analysis of monotonicity, just as with convex functions.</li>
        </ul>

        <div class="theorem-box">
          <h4>Theorem: Equivalence Principle</h4>
          <p>Every statement about convex functions has a corresponding statement about concave functions obtained by:
          <ol>
            <li>Replacing $f$ with $-f$.</li>
            <li>Flipping all inequalities ($\le$ becomes $\ge$, etc.).</li>
            <li>Replacing "minimum" with "maximum" and "infimum" with "supremum".</li>
            <li>Replacing "epigraph" with "hypograph".</li>
          </ol>
          This <b>duality</b> between convexity and concavity is a fundamental symmetry in optimization theory.</p>
        </div>

        <h3>1.9 Restriction to a Line (The Conceptual Core)</h3>
        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>$f$ is convex if and only if for all $x \in \mathrm{dom}\, f$ and $v \in \mathbb{R}^n$, the function $g(t) = f(x + tv)$ is convex on its domain.</p>
        </div>
        <div class="proof-box">
          <h4>Proof</h4>
          <p>This theorem allows us to reduce $n$-dimensional convexity to 1-dimensional convexity.</p>
          <div class="proof-step">
            <strong>($\Rightarrow$) Convex $\implies$ Restriction is Convex.</strong>
            Assume $f$ is convex. Fix $x, v$. Let $t_1, t_2$ be scalars and $\theta \in [0, 1]$.
            <br>We compute $g$ at the convex combination of points:
            $$ g(\theta t_1 + (1-\theta)t_2) = f(x + (\theta t_1 + (1-\theta)t_2)v) $$
            Rearrange the argument as a convex combination of vectors:
            $$ = f(\theta(x + t_1 v) + (1-\theta)(x + t_2 v)) $$
            By convexity of $f$:
            $$ \le \theta f(x + t_1 v) + (1-\theta)f(x + t_2 v) = \theta g(t_1) + (1-\theta)g(t_2) $$
            Thus $g$ satisfies the definition of a convex function.
          </div>
          <div class="proof-step">
            <strong>($\Leftarrow$) Restriction is Convex $\implies$ Convex.</strong>
            Assume $g(t) = f(x+tv)$ is convex for all lines.
            Take any two points $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$.
            We want to evaluate $f$ at $z = \theta x + (1-\theta)y$.
            <br>Consider the specific line passing through $\mathbf{x}$ and $\mathbf{y}$. Let $\mathbf{v} = \mathbf{y} - \mathbf{x}$.
            Define the restriction $g(t) = f(x + tv)$.
            Note that:
            <ul>
                <li>$g(0) = f(x)$</li>
                <li>$g(1) = f(x + (y-x)) = f(y)$</li>
                <li>$g(1-\theta) = f(x + (1-\theta)(y-x)) = f(\theta x + (1-\theta)y) = f(z)$</li>
            </ul>
            Since $g$ is convex, we apply the definition with points $t_1=0, t_2=1$ and weight $1-\theta$:
            $$ g((1-(1-\theta))\cdot 0 + (1-\theta)\cdot 1) \le (1-(1-\theta))g(0) + (1-\theta)g(1) $$
            Simplifying the weights:
            $$ g(1-\theta) \le \theta g(0) + (1-\theta)g(1) $$
            Substituting back the function values:
            $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) $$
            Thus $f$ is convex.
          </div>
        </div>
        <p><b>Why this matters:</b> Convexity is a statement about segments. Segments are 1-D objects. Thus, "n-dimensional convexity" is secretly just "1-D convexity along every possible direction".</p>
        <div class="warning-box">
            <h4>Trap: Coordinate-wise Convexity</h4>
            <p>It is <b>NOT</b> sufficient to check convexity along coordinate axes (varying one $x_i$ at a time).
            <br><b>Counterexample:</b> $f(x,y) = xy$.
            <br>Fix $y$: $x \mapsto xy$ is linear (convex). Fix $x$: $y \mapsto xy$ is linear (convex).
            <br>But along the line $x=t, y=-t$, we get $g(t) = -t^2$, which is strictly concave. Thus $xy$ is not convex.</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Inspector: Understanding Convexity</h3>
          <p><b>Explore the Equivalences:</b> Convexity can be defined in multiple ways. This unified tool lets you toggle between different perspectives to see how they relate:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Jensen (Definition):</b> The chord between any two points lies above the graph.</li>
            <li><b>Epigraph (Set Theory):</b> The region above the graph is a convex set.</li>
            <li><b>Tangent (First-Order):</b> The tangent line is always a global underestimator (for differentiable functions).</li>
            <li><b>Quadratic Bound:</b> A quadratic bowl sits below the function, pushing it up. (See <a href="../06-convex-functions-advanced/index.html">Lecture 06</a> for strong convexity.)</li>
          </ul>
          <p><i>Note:</i> Select different functions to see how non-convex functions violate these conditions!</p>
          <div id="widget-convex-function-inspector" style="width: 100%; height: 500px; position: relative; max-width: 900px; margin: 0 auto; border-radius: 8px;"></div>
        </div>
      </section>


<section class="section-card" id="section-2">
        <h2>2. Epigraph Characterization</h2>

        <h3>2.1 The Epigraph: Function as a Set</h3>
        <p>The <a href="#" class="definition-link">epigraph</a> of a function $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ is the set of points lying on or above the graph:</p>
        $$
        \mathrm{epi}\, f = \{(x, t) \in \mathbb{R}^{n+1} \mid x \in \mathrm{dom}\, f, \ f(x) \le t\}
        $$
        <p><b>Orientation:</b> "Epi" means "above". $\mathrm{epi}(f)$ is the set <i>above</i> the graph. (For concave functions, we use the <i>hypograph</i>, the set below).</p>

        <div class="insight">
          <h4>The Epigraph Encodes the Function Completely</h4>
          <p>The epigraph is not merely a certificate of convexity; it is a set-theoretic representation of the function itself. We can recover $f$ from $\mathrm{epi}\, f$ via:</p>
          $$ f(x) = \inf \{ t \mid (x, t) \in \mathrm{epi}\, f \} $$
          <ul>
              <li>If the set of $t$'s is empty, the infimum is $+\infty$ (correct for $x \notin \mathrm{dom} f$).</li>
              <li>If the set is unbounded below, the infimum is $-\infty$.</li>
          </ul>
          <p>This equivalence allows us to translate "minimizing $f(x)$" into "finding the lowest vertical coordinate in a set".</p>
        </div>

        <h3>2.2 Theorem: Convexity $\iff$ Convex Epigraph</h3>
        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>A function $f$ is convex <b>if and only if</b> its epigraph $\mathrm{epi}\, f$ is a convex set in $\mathbb{R}^{n+1}$.</p>
        </div>

        <div class="proof-box">
          <h4>Proof (The "Mix" Logic)</h4>
          <div class="proof-step">
            <strong>($\Rightarrow$) $f$ convex $\implies$ $\mathrm{epi}\, f$ convex.</strong>
            <br>Assume $f$ is convex. Take two points $(x, t)$ and $(y, s)$ in $\mathrm{epi}\, f$.
            <br>By definition: $f(x) \le t$ and $f(y) \le s$.
          </div>

          <div class="proof-step">
            <strong>Step 1:</strong> Consider the convex combination $(\theta x + (1-\theta)y, \theta t + (1-\theta)s)$ for $\theta \in [0, 1]$.
          </div>

          <div class="proof-step">
            <strong>Step 2:</strong> By convexity of $f$:
            $$
            f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) \le \theta t + (1-\theta)s
            $$
          </div>

          <div class="proof-step">
            <strong>Step 3:</strong> Therefore, $(\theta x + (1-\theta)y, \theta t + (1-\theta)s) \in \mathrm{epi}\, f$, proving $\mathrm{epi}\, f$ is convex.
          </div>

          <div class="proof-step">
            <strong>($\Leftarrow$) Convex epigraph implies convex function:</strong> Suppose $\mathrm{epi}\, f$ is convex. Take any $x, y \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$.
          </div>

          <div class="proof-step">
            <strong>Step 1:</strong> The points $(x, f(x))$ and $(y, f(y))$ lie in $\mathrm{epi}\, f$ (on the boundary, in fact).
          </div>

          <div class="proof-step">
            <strong>Step 2:</strong> By convexity of $\mathrm{epi}\, f$, the point $(\theta x + (1-\theta)y, \theta f(x) + (1-\theta)f(y))$ also lies in $\mathrm{epi}\, f$.
          </div>

          <div class="proof-step">
            <strong>Step 3:</strong> By definition of epigraph, this means:
            $$
            f(\theta x + (1-\theta)y) \le \text{vertical coordinate} = \theta f(x) + (1-\theta)f(y)
            $$
            proving $f$ is convex.
          </div>
        </div>

        <h3>2.3 Sublevel Sets (The Bridge to Quasiconvexity)</h3>
        <p>The $\alpha$-sublevel set is $S_\alpha = \{x \in \mathrm{dom} f \mid f(x) \le \alpha\}$.</p>
        <div class="theorem-box">
            <h4>Theorem</h4>
            <p>If $f$ is convex, then every sublevel set $S_\alpha$ is a convex set.</p>
            <p><i>Proof:</i> Take $x, y \in S_\alpha$. Then $f(x) \le \alpha$ and $f(y) \le \alpha$.
            <br>For the mixture $z = \theta x + (1-\theta)y$:
            $$ f(z) \le \theta f(x) + (1-\theta)f(y) \le \theta \alpha + (1-\theta)\alpha = \alpha $$
            So $z \in S_\alpha$.</p>
        </div>
        <p><b>Note:</b> The converse is false. $f(x) = \log x$ has convex sublevel sets (intervals) but is concave. Functions with convex sublevel sets are called <b>quasiconvex</b>.</p>

        <h3>2.4 Convex Hull of a Function (Example 3.30)</h3>
        <p>Similar to how we can define the convex hull of a set, we can define the <a href="#" class="definition-link" data-term="convex envelope">convex hull (or convex envelope)</a> of a function $f: \mathbb{R}^n \to \mathbb{R}$ via its epigraph.</p>
        $$ g(x) = \inf \{ t \mid (x, t) \in \text{conv epi } f \} $$
        <p><b>Geometric Intuition:</b> Imagine "shrink-wrapping" the graph of $f$ from below. The function $g$ is the <b>greatest convex underestimator</b> of $f$. This concept is crucial in relaxation techniques for non-convex optimization (e.g., replacing a non-convex constraint with its convex hull).</p>

        <div class="proof-box">
          <h4>Explicit Formula and Proof of Maximality</h4>
          <p>We prove that $g$ is the largest convex function satisfying $g(\mathbf{x}) \le f(\mathbf{x})$ for all $\mathbf{x}$.
          <br><b>Explicit Formula:</b> The condition $(x,t) \in \text{conv epi } f$ means $(x,t)$ is a convex combination of points in the epigraph of $f$. This leads to the variational formula:
          $$ g(x) = \inf \left\{ \sum_{i=1}^{n+1} \theta_i f(x_i) \ \middle| \ \sum \theta_i x_i = x, \ \sum \theta_i = 1, \ \theta_i \ge 0 \right\} $$
          </p>
          <div class="proof-step">
            <strong>Step 1: $g$ is an underestimator.</strong>
            For any $\mathbf{x}$, we can choose the trivial combination $k=1, x_1=x, \theta_1=1$. Then the objective is $1 \cdot f(x) = f(x)$.
            Since $g(x)$ is the infimum over all such combinations, $g(\mathbf{x}) \le f(\mathbf{x})$.
          </div>
          <div class="proof-step">
            <strong>Step 2: $g$ is convex.</strong>
            The epigraph of $g$ is exactly $\text{conv epi } f$ (technically, its closure for lower semi-continuity). Since the convex hull of any set is a convex set, $\mathrm{epi}\, g$ is convex. A function with a convex epigraph is a convex function.
          </div>
          <div class="proof-step">
            <strong>Step 3: Maximality.</strong>
            Let $h$ be any convex function such that $h(\mathbf{x}) \le f(\mathbf{x})$ for all $\mathbf{x}$.
            Take any candidate convex combination $x = \sum \theta_i x_i$. By Jensen's inequality for $h$:
            $$ h(x) = h\left(\sum \theta_i x_i\right) \le \sum \theta_i h(x_i) $$
            Since $h \le f$ everywhere, $h(x_i) \le f(x_i)$, so:
            $$ h(x) \le \sum \theta_i f(x_i) $$
            This holds for <i>every</i> valid representation of $x$. Thus, it holds for the infimum:
            $$ h(x) \le \inf \sum \theta_i f(x_i) = g(x) $$
            Thus, $g$ dominates every other convex underestimator.
          </div>
        </div>

        <div class="proof-box">
          <h4>Example: Homogeneous Envelope (Exercise 3.31)</h4>
          <p>Given a convex function $f$ (with $f(0)=0$), its <b>homogeneous envelope</b> is defined as:
          $$ g(x) = \inf_{\alpha > 0} \frac{f(\alpha x)}{\alpha} $$
          This function is the "tightest" homogeneous function that stays below $f$.</p>

          <div class="proof-step">
            <strong>1. Homogeneity ($g(tx) = t g(x)$ for $t > 0$).</strong>
            $$ g(tx) = \inf_{\alpha > 0} \frac{f(\alpha t x)}{\alpha} $$
            Let $\beta = \alpha t \implies \alpha = \beta/t$. As $\alpha$ traverses $(0, \infty)$, so does $\beta$.
            $$ g(tx) = \inf_{\beta > 0} \frac{f(\beta x)}{\beta/t} = t \inf_{\beta > 0} \frac{f(\beta x)}{\beta} = t g(x) $$
          </div>

          <div class="proof-step">
            <strong>2. Largest Homogeneous Underestimator.</strong>
            <ul>
              <li><b>Underestimator:</b> Choosing $\alpha=1$ in the infimum gives $g(x) \le f(x)/1 = f(x)$.</li>
              <li><b>Largest:</b> Let $h$ be any homogeneous function with $h \le f$. Then $h(\alpha x) \le f(\alpha x)$.
              Using homogeneity: $\alpha h(x) \le f(\alpha x) \implies h(x) \le f(\alpha x)/\alpha$.
              Since this holds for all $\alpha > 0$, $h(x) \le \inf_{\alpha} \frac{f(\alpha x)}{\alpha} = g(x)$.</li>
            </ul>
          </div>

          <div class="proof-step">
            <strong>3. Convexity via Joint Convexity.</strong>
            This is the subtle part. We use a direct Jensen-style proof with a specific scaling choice.
            <br>We want to show $g(z) \le \theta g(x) + (1-\theta) g(y)$ where $z = \theta x + (1-\theta)y$.
            <br>For any $\epsilon > 0$, pick $\alpha_1, \alpha_2$ such that $\frac{f(\alpha_1 x)}{\alpha_1} \le g(x) + \epsilon$ and $\frac{f(\alpha_2 y)}{\alpha_2} \le g(y) + \epsilon$.
            <br>Choose the "harmonic" scaling $\alpha$ such that $\frac{1}{\alpha} = \frac{\theta}{\alpha_1} + \frac{1-\theta}{\alpha_2}$.
            <br>Let $\lambda = \frac{\theta \alpha}{\alpha_1}$. Then $1-\lambda = \frac{(1-\theta)\alpha}{\alpha_2}$. Note $\lambda \in [0,1]$.
            <br>Then $\alpha z = \lambda (\alpha_1 x) + (1-\lambda)(\alpha_2 y)$.
            <br>By convexity of $f$: $f(\alpha z) \le \lambda f(\alpha_1 x) + (1-\lambda) f(\alpha_2 y)$.
            <br>Divide by $\alpha$:
            $$ \frac{f(\alpha z)}{\alpha} \le \frac{\lambda}{\alpha} f(\alpha_1 x) + \frac{1-\lambda}{\alpha} f(\alpha_2 y) = \theta \frac{f(\alpha_1 x)}{\alpha_1} + (1-\theta) \frac{f(\alpha_2 y)}{\alpha_2} $$
            $$ \le \theta(g(x)+\epsilon) + (1-\theta)(g(y)+\epsilon) $$
            Since $g(z) \le \frac{f(\alpha z)}{\alpha}$, letting $\epsilon \to 0$ gives $g(z) \le \theta g(x) + (1-\theta)g(y)$.
          </div>
        </div>

      </section>


<section class="section-card" id="section-3">
        <h2>3. Standard Convex Function Library</h2>
        <p>Recognizing these atoms instantly is the key to efficient modeling. The meta-rule is: <b>know the domain</b> and <b>know the cone</b>.</p>

        <table class="data-table" style="width: 100%; border-collapse: collapse; margin-top: 1rem;">
          <thead>
            <tr style="background: var(--surface-2); border-bottom: 2px solid var(--border);">
              <th style="padding: 12px; text-align: left;">Function</th>
              <th style="padding: 12px; text-align: left;">Domain</th>
              <th style="padding: 12px; text-align: left;">Epigraph (Modeling)</th>
            </tr>
          </thead>
          <tbody>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>Affine</b> $a^\top x + b$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Halfspace (Linear Constraint)</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>Quadratic</b> $\frac{1}{2}x^\top Q x + c^\top x$ ($Q \succeq 0$)</td>
              <td>$\mathbb{R}^n$</td>
              <td>Rotated SOC ($x^\top x \le t$) or LMI</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>$\ell_2$ Norm</b> $\|x\|_2$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Second-Order Cone (SOC): $(t, x) \in \mathcal{Q}$</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>$\ell_1$ / $\ell_\infty$</b></td>
              <td>$\mathbb{R}^n$</td>
              <td>Linear Constraints (LP)</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>Log-Sum-Exp</b> $\log(\sum e^{x_i})$</td>
              <td>$\mathbb{R}^n$</td>
              <td>Exponential Cone</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>Neg-Log</b> $-\log x$</td>
              <td>$\mathbb{R}_{++}$</td>
              <td>Exponential Cone</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>Entropy</b> $x \log x$</td>
              <td>$\mathbb{R}_{++}$</td>
              <td>Exponential Cone</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border);">
              <td style="padding: 12px;"><b>Quad-over-Lin</b> $x^2/y$</td>
              <td>$y > 0$</td>
              <td>Rotated SOC: $x^2 \le ty$</td>
            </tr>
          </tbody>
        </table>

        <h3>3.1 Deep Dive: Norms (The Archetype)</h3>
        <p>Norms are the "size" functions of linear space. They are the single most important class of convex functions because they bridge geometry (balls) and algebra (inequalities).</p>

        <h4>The Mental Model: What structure are norms capturing?</h4>
        <div class="example-box">
            <p>A norm measures size in a way compatible with scaling and addition:</p>
            <ul>
                <li><b>Scaling sanity $\Rightarrow$ Homogeneity:</b> $\|\alpha x\| = |\alpha|\|x\|$. Scaling the vector scales its size.</li>
                <li><b>Addition sanity $\Rightarrow$ Triangle Inequality:</b> $\|x+y\| \le \|x\| + \|y\|$. No shortcuts via intermediate points. This is the engine of convexity.</li>
                <li><b>Non-degeneracy $\Rightarrow$ Definiteness:</b> $\|x\|=0 \iff x=0$. No invisible non-zero vectors.</li>
            </ul>
        </div>

        <h4>Tiny Lemmas (Reusable Templates)</h4>
        <div class="proof-box">
            <p><b>Lemma 1: Symmetry.</b> $\|x\| = \|-x\|$. (Follows from homogeneity with $\alpha=-1$).</p>
            <p><b>Lemma 2: Reverse Triangle Inequality.</b> $\big| \|x\| - \|y\| \big| \le \|x-y\|$.
            <br><i>Interpretation:</i> The norm function is 1-Lipschitz. It cannot change faster than the distance.</p>
            <p><b>Lemma 3: Two-point Jensen.</b> $\|\theta x + (1-\theta)y\| \le \theta\|x\| + (1-\theta)\|y\|$.
            <br><i>Proof:</i> Triangle inequality + Homogeneity. This proves all norms are convex.</p>
        </div>

        <h4>The Core Theorem: Norms are Convex</h4>
        <p>Every norm satisfies the convexity inequality. This is not a coincidence; it is a direct consequence of the axioms.</p>
        <div class="proof-box">
            <h4>Proof: Triangle + Homogeneity = Convexity</h4>
            <p>We show $f(x) = \|x\|$ satisfies $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$ for $\theta \in [0,1]$.</p>
            <div class="proof-step">
                <strong>Step 1: Triangle Inequality (Subadditivity).</strong>
                $$ \|\theta x + (1-\theta)y\| \le \|\theta x\| + \|(1-\theta)y\| $$
            </div>
            <div class="proof-step">
                <strong>Step 2: Homogeneity.</strong>
                Since $\theta \ge 0$ and $1-\theta \ge 0$, we have $\|\alpha z\| = \alpha \|z\|$.
                $$ \|\theta x\| = \theta \|x\| \quad \text{and} \quad \|(1-\theta)y\| = (1-\theta)\|y\| $$
            </div>
            <div class="proof-step">
                <strong>Conclusion:</strong>
                $$ \|\theta x + (1-\theta)y\| \le \theta \|x\| + (1-\theta)\|y\| $$
            </div>
        </div>

        <h4>Structural Viewpoint: Sublinear Functions</h4>
        <p>The proof above works for any function that is:</p>
        <ol>
            <li><b>Positively Homogeneous:</b> $f(\alpha x) = \alpha f(x)$ for $\alpha \ge 0$.</li>
            <li><b>Subadditive:</b> $f(x+y) \le f(x) + f(y)$.</li>
        </ol>
        <p>Such functions are called <b>sublinear</b>.
        <br><b>Fact:</b> Every sublinear function is convex.
        <br><b>Examples:</b> Norms, support functions $S_C(x) = \sup_{y \in C} y^\top x$, and maximum of linear functions.</p>

        <h4>Examples with Proof Content</h4>
        <div class="example-box">
            <ul>
                <li><b>$\ell_1$ Norm:</b> $\|x\|_1 = \sum |x_i|$. Convexity follows from $|a+b| \le |a|+|b|$.
                <br><i>Engineering Hook:</i> $\ell_1$ minimization promotes sparsity (corners of the unit ball).</li>
                <li><b>$\ell_\infty$ Norm:</b> $\|x\|_\infty = \max |x_i|$. Convexity follows from $\max(a+b) \le \max(a) + \max(b)$.
                <br><i>Engineering Hook:</i> Represents "worst-case" or box constraints.</li>
                <li><b>Euclidean ($\ell_2$) Norm:</b> Convexity follows from Cauchy-Schwarz ($x^\top y \le \|x\|\|y\|$).
                <br><i>Engineering Hook:</i> Rotational invariance, smooth dependence.</li>
            </ul>
            <p><b>Failure Mode:</b> Forgetting that $\|x\|_2^2$ is convex but $\|x\|_2 \ge t$ is <b>non-convex</b> (exterior of ball). Norms are convex, so sublevel sets are convex bodies; superlevel sets are holey.</p>
        </div>

        <h3>3.2 Other Standard Atoms</h3>

        <div class="example-box">
            <h4>2. Log-Sum-Exp ($\text{lse}(x) = \log \sum e^{x_i}$)</h4>
            <p><b>Proof:</b> Hessian is $\text{diag}(p) - pp^\top$ (covariance matrix), hence PSD.</p>
            <p><b>Engineering Hook:</b> Used as a smooth approximation of $\max(x)$.
            <br>$\text{lse}(x) \le t$ is representable using $n$ exponential cones.</p>
            <p><b>Approximation Bounds:</b> For any $x$, we have:
            $$ \max_i x_i \le \text{lse}(x) \le \max_i x_i + \log n $$
            <i>Proof:</i> Let $M = \max_i x_i$. Factor out $e^M$:
            $$ \log\left(\sum e^{x_i}\right) = \log\left(e^M \sum e^{x_i - M}\right) = M + \log\left(\sum e^{x_i - M}\right) $$
            Since $x_i - M \le 0$, each term $e^{x_i-M} \le 1$. The sum is at least 1 (the max term is $e^0=1$) and at most $n$.
            Thus $0 \le \log(\sum \dots) \le \log n$. Adding $M$ gives the result.</p>
            <p><b>Failure Mode:</b> Confusing with $\log(\sum x_i)$ (concave) or $\sum \log x_i$ (concave).</p>
        </div>

        <div class="example-box">
            <h4>3. Quadratic-over-Linear ($x^\top x / y$)</h4>
            <p><b>Domain:</b> $y > 0$ (Crucial!). Not defined at $y=0$ (unless 0/0=0 convention) and non-convex for $y < 0$.</p>
            <p><b>Engineering Hook:</b>
            $$ x^\top x / y \le t \iff x^\top x \le ty, \ y > 0, \ t \ge 0 $$
            This is a <b>Rotated Second-Order Cone</b> constraint $x^\top x \le 2uv$ (with $u=y/2, v=t$).</p>
        </div>

        <div class="example-box">
            <h4>4. Power Functions ($x^p$)</h4>
            <p><b>Domain:</b> $x > 0$.</p>
            <ul>
                <li>$p \ge 1$: Convex.</li>
                <li>$0 \le p \le 1$: Concave.</li>
                <li>$p \le 0$: Convex (e.g., $1/x$).</li>
            </ul>
            <p><b>Failure Mode:</b> Applying $x^3$ on all of $\mathbb{R}$. It is only convex on $\mathbb{R}_+$. On $\mathbb{R}$, $x^3$ changes curvature at 0.</p>
        </div>

        <div class="warning-box">
            <h4>Common Failure Modes Across the Library</h4>
            <ol>
                <li><b>Domain Amnesia:</b> Using $-\log(1 - x^\top x)$ without enforcing $\|x\| < 1$. The solver might step out of domain and crash (or return garbage) if implicit constraints aren't made explicit.</li>
                <li><b>Constraint Direction:</b> $\max(x_i) \le t$ is convex (intersection of halfspaces). $\max(x_i) \ge t$ is non-convex (union of halfspaces). Always check if your constraint carves out a convex set.</li>
            </ol>
        </div>
      </section>


<section class="section-card" id="section-4">

        <h2>4. Operations Preserving Convexity</h2>

        <p>One of the most powerful aspects of convex analysis is that complex convex functions can be built from simpler ones using operations that preserve convexity. This enables sophisticated modeling without sacrificing tractability.</p>

        <h3>4.0 The Meta-Rule: How to Think About Preservation</h3>
        <p>Convexity preservation always stems from one of two geometric principles:</p>
        <ol>
            <li><b>Epigraph Geometry:</b> If an operation transforms epigraphs via intersection, affine mapping, projection, or Minkowski sum, convexity is preserved.</li>
            <li><b>Line Restriction:</b> If the function restricted to every line is convex (1D convexity), the original function is convex.</li>
        </ol>
        <p>Every rule below can be justified by one of these two principles. Nothing is magic.</p>

        <h3>4.1 Nonnegative Weighted Sum</h3>
        <p><b>Rule:</b> The sum of convex functions (with non-negative weights) is convex.</p>

        <div class="example-box">
          <h4>Example: Sum of Norms</h4>
          <p>Let $f(x) = 3x^2 + 5|x|$.
          <br>Since $f_1(x) = x^2$ is convex and $f_2(x) = |x|$ is convex, and weights $3, 5 \ge 0$, the sum is convex.</p>
        </div>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f_1, \ldots, f_m$ are convex functions and $w_1, \ldots, w_m \ge 0$, then:</p>
          $$
          f(x) = \sum_{i=1}^m w_i f_i(x)
          $$
          <p>is convex. This extends to infinite sums and integrals (provided they converge).</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <p>For any $x, y$ and $\theta \in [0, 1]$:</p>
          $$
          \begin{aligned}
          f(\theta x + (1-\theta)y) &= \sum_{i=1}^m w_i f_i(\theta x + (1-\theta)y) \\
          &\le \sum_{i=1}^m w_i \big( \theta f_i(x) + (1-\theta)f_i(y) \big) \quad \text{(convexity of } f_i, \ w_i \ge 0\text{)} \\
          &= \theta \sum_{i=1}^m w_i f_i(x) + (1-\theta) \sum_{i=1}^m w_i f_i(y) \\
          &= \theta f(x) + (1-\theta) f(y)
          \end{aligned}
          $$
        </div>

        <h3>4.2 Pointwise Maximum of Convex Functions</h3>
        <p><b>Rule:</b> The pointwise maximum of any number of convex functions is convex.</p>

        <div class="example-box">
          <h4>Example: Max of Power and Absolute Value</h4>
          <p>Let $f(x) = \max\{x^2, |x|\}$.
          <br>Since $f_1(x) = x^2$ and $f_2(x) = |x|$ are both convex, their pointwise maximum is convex.
          <br><i>Visual Check:</i> Graph both functions. The region "above both" (epigraph) is the intersection of two convex regions (bowls), which is still a convex region.</p>
        </div>

        <p>Let $f_1, \dots, f_m: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ be convex functions. Define their <b>pointwise maximum</b>:</p>
        $$
        f(x) = \max_{i=1,\dots,m} f_i(x)
        $$

        <div class="theorem-box">
          <h4>Claim</h4>
          <p>The pointwise maximum function $f$ is convex.</p>
        </div>

        <div class="insight">
          <h4>Intuition</h4>
          <p>Take several convex curves; at each $x$ keep the one that lies highest. The upper envelope still bends "upwards"; you never get a concave dip by taking a max of convex curves. Geometrically, the epigraph is the intersection of epigraphs.</p>
        </div>

        <div class="proof-box">
          <h4>Algebraic Proof</h4>
          <div class="proof-step">
            <strong>Step 1: Definition.</strong>
            Let $x, y \in \mathbb{R}^n$ and $0 \le \theta \le 1$. We need to show $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y)$.
          </div>
          <div class="proof-step">
            <strong>Step 2: Convexity of components.</strong>
            For each fixed $i$, convexity of $f_i$ gives:
            $$ f_i(\theta x + (1-\theta)y) \le \theta f_i(x) + (1-\theta) f_i(y) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Max over both sides.</strong>
            $$ \max_i f_i(\theta x + (1-\theta)y) \le \max_i \big( \theta f_i(x) + (1-\theta) f_i(y) \big) $$
          </div>
          <div class="proof-step">
            <strong>Step 4: Inequality for max.</strong>
            Use the inequality $\max_i (\theta a_i + (1-\theta)b_i) \le \theta \max_i a_i + (1-\theta)\max_i b_i$.
            $$ \max_i \big( \theta f_i(x) + (1-\theta) f_i(y) \big) \le \theta \max_i f_i(x) + (1-\theta)\max_i f_i(y) = \theta f(x) + (1-\theta) f(y) $$
          </div>
          <div class="proof-step">
            <strong>Conclusion:</strong> $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y)$.
          </div>
        </div>

        <div class="proof-box">
          <h4>Geometric Proof (Epigraph)</h4>
          <p>The epigraph of the max is the intersection of the epigraphs:
          $$ \mathrm{epi}\, f = \bigcap_{i=1}^m \mathrm{epi}\, f_i $$
          Since a point $(x,t)$ satisfies $t \ge \max_i f_i(x)$ if and only if $t \ge f_i(x)$ for all $i$.
          The intersection of convex sets is convex, so $\mathrm{epi}\, f$ is convex.</p>
        </div>

        <h4>Examples of Pointwise Maxima</h4>
        <div class="example">
          <h4>Example 1: Piecewise-Linear (Max of Affine Functions)</h4>
          <p>Let $\ell_i(x) = a_i^\top x + b_i$ for $i=1,\dots,m$. Define:</p>
          $$ f(x) = \max_{1\le i\le m} (a_i^\top x + b_i) $$
          <p>Each $\ell_i$ is convex, so $f$ is convex. Geometrically, this is a polyhedral convex function (its epigraph is a polyhedron).
          <br><b>Application:</b> This function arises in minimizing the $\ell_\infty$ norm residual $\min \|Ax-b\|_\infty$ (which is $\max_i |a_i^\top x - b_i|$).</p>
        </div>

        <div class="example">
          <h4>Example 2: The $\ell_p$-type Mean ($p < 1$)</h4>
          <p>Consider the function $f(x) = \left(\sum x_i^p\right)^{1/p}$ for $0 < p < 1$ on $\mathbb{R}^n_{++}$.
          <br>This function is <b>concave</b>.
          <br><i>Note:</i> For $p \ge 1$, this is the $\ell_p$ norm, which is convex. The switch in curvature for $p < 1$ is a subtle but important distinction. The proof involves checking the Hessian or using a variation of Minkowski's inequality.</p>
        </div>

        <div class="example">
          <h4>Example 3: Sum of the $r$ Largest Components (Ky Fan Norm)</h4>
          <p>For $x \in \mathbb{R}^n$, let $x_{[1]} \ge x_{[2]} \ge \dots \ge x_{[n]}$ denote components sorted in descending order. Define:
          $$ f(x) = \sum_{k=1}^r x_{[k]} $$
          This function is convex and coordinate-wise nondecreasing.</p>

          <div class="proof-box">
            <h4>Detailed Proof: Representation as Max of Linear Functionals</h4>
            <p>Consider the convex set of weights:
            $$ C_r = \{ \lambda \in \mathbb{R}^n \mid 0 \le \lambda_i \le 1, \sum_{i=1}^n \lambda_i = r \} $$
            We claim that $f(x) = \max_{\lambda \in C_r} \sum_{i=1}^n \lambda_i x_i$.
            <br><b>Step 1: Variational Argument.</b> Take any feasible $\lambda \in C_r$. Suppose we have indices $j, l$ such that $x_j > x_l$ but $\lambda_j < 1$ and $\lambda_l > 0$.
            <br>We can shift a small weight $\delta > 0$ from $l$ to $j$: $\lambda'_j = \lambda_j + \delta$, $\lambda'_l = \lambda_l - \delta$.
            <br>The change in the objective is $\delta x_j - \delta x_l = \delta(x_j - x_l) > 0$.
            <br>Thus, to maximize the sum, we must shift mass from smaller components to larger components until we hit the constraints $\lambda_i \in [0, 1]$.
            <br><b>Step 2: Optimal Solution.</b> The process terminates when $\lambda_i = 1$ for the $r$ largest components and $0$ for the rest.
            $$ \max_{\lambda \in C_r} \lambda^\top x = \sum_{i=1}^r x_{[i]} = f(x) $$
            <b>Conclusion:</b> $f(x)$ is the pointwise maximum of the family of linear functions $\{g_\lambda(x) = \lambda^\top x \mid \lambda \in C_r\}$. Since the pointwise maximum of convex (linear) functions is convex, $f$ is convex.
            <br><b>Monotonicity:</b> Increasing any coordinate $x_j$ increases the sum $\lambda^\top x$ (since $\lambda \ge 0$), thus increasing the maximum. So $f$ is nondecreasing.</p>
          </div>
        </div>

        <h3>4.3 Pointwise Supremum over an Index Set</h3>
        <p>The "max of finitely many" generalizes to the supremum over an arbitrary index set $\mathcal{A}$.</p>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>Let $\mathcal{A}$ be any index set. For each $y \in \mathcal{A}$, let $f(\cdot, y)$ be convex in $\mathbf{x}$. Define:</p>
          $$ g(\mathbf{x}) = \sup_{y \in \mathcal{A}} f(\mathbf{x}, y) $$
          <p>Then $g(\mathbf{x})$ is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Proof</h4>
          <p>Let $x_1, x_2 \in \mathbb{R}^n$ and $\theta \in [0, 1]$.
          $$ g(\theta x_1 + (1-\theta)x_2) = \sup_y f(\theta x_1 + (1-\theta)x_2, y) $$
          $$ \le \sup_y \big( \theta f(x_1, y) + (1-\theta) f(x_2, y) \big) $$
          $$ \le \theta \sup_y f(x_1, y) + (1-\theta) \sup_y f(x_2, y) = \theta g(x_1) + (1-\theta) g(x_2) $$
          Geometrically, $\mathrm{epi}\, g = \bigcap_{y \in \mathcal{A}} \mathrm{epi}\, f(\cdot, y)$, which is an intersection of convex sets.</p>
        </div>

        <h4>Examples of Pointwise Supremum</h4>
        <div class="example">
          <h4>1. Support Function</h4>
          <p>For any set $C \subset \mathbb{R}^n$, the support function $S_C(\mathbf{x}) = \sup_{\mathbf{y} \in C} \mathbf{y}^\top \mathbf{x}$ is the pointwise supremum of linear functions $\mathbf{x} \mapsto \mathbf{y}^\top \mathbf{x}$ indexed by $\mathbf{y} \in C$. Thus $S_C$ is always convex, regardless of whether $C$ is convex.</p>
        </div>

        <div class="example">
          <h4>2. Distance to Farthest Point</h4>
          <p>Let $C \subset \mathbb{R}^n$. The function $f(\mathbf{x}) = \sup_{\mathbf{y} \in C} \|\mathbf{x} - \mathbf{y}\|$ (distance to the farthest point in $C$) is convex, because for each fixed $\mathbf{y}$, $\mathbf{x} \mapsto \|\mathbf{x} - \mathbf{y}\|$ is convex.</p>
        </div>

        <div class="example">
          <h4>3. Maximum Eigenvalue</h4>
          <p>For $X \in \mathbb{S}^n$, the maximum eigenvalue $\lambda_{\max}(X)$ is convex. We use its variational representation:</p>
          $$ f(X) = \lambda_{\max}(X) = \sup_{\|y\|_2=1} y^\top X y $$
          <p>For any fixed unit vector $y$, define $g_y(X) = y^\top X y$. Note that:
          $$ g_y(X) = \mathrm{tr}(y^\top X y) = \mathrm{tr}(X y y^\top) = \langle X, yy^\top \rangle $$
          This shows $g_y(X)$ is a <b>linear function</b> of the matrix $X$.
          <br>Since $f(X) = \sup_{\|y\|=1} g_y(X)$ is the pointwise supremum of a family of convex (linear) functions, $f(X)$ is convex.</p>
        </div>

        <div class="example">
          <h4>4. Induced Matrix Norms</h4>
          <p>Let $\|X\|_{a,b} = \sup_{\|v\|_b=1} \|Xv\|_a$ be the operator norm induced by vector norms $\|\cdot\|_a$ and $\|\cdot\|_b$.
          <br>Using the definition of the dual norm $\|\cdot\|_{a^*}$, we can write $\|z\|_a = \sup_{\|u\|_{a^*} \le 1} u^\top z$.
          $$ \|X\|_{a,b} = \sup_{\|v\|_b=1} \left( \sup_{\|u\|_{a^*} \le 1} u^\top X v \right) = \sup_{\|v\|_b=1, \ \|u\|_{a^*} \le 1} u^\top X v $$
          For fixed $u, v$, the function $X \mapsto u^\top X v = \langle X, uv^\top \rangle$ is linear in $X$.
          Thus, the induced norm is a supremum of linear functions, hence convex.</p>
        </div>

        <h3>4.4 Partial Minimization</h3>
        <p>We now look at minimizing over some coordinates: $g(x) = \inf_{y \in C} f(x, y)$.</p>

        <div class="theorem-box">
          <h4>Theorem</h4>
          <p>If $f(x, y)$ is <b>jointly convex</b> in $(x, y)$ and $C$ is a convex set, then:</p>
          $$ g(x) = \inf_{y \in C} f(x, y) $$
          <p>is convex (provided $g(x) > -\infty$).</p>
        </div>

        <div class="insight">
          <h4>Geometric Picture</h4>
          <p>The epigraph of $g$ is the <b>projection</b> of the epigraph of $f$ onto the $(x, t)$ space (projecting out $y$). Since the projection of a convex set is convex, $g$ is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Algebraic Proof</h4>
          <p>Let $x_1, x_2$ and $\theta \in [0, 1]$. We want $g(x_\theta) \le \theta g(x_1) + (1-\theta)g(x_2)$.
          <br>Write the infimum redundantly: $g(x_\theta) = \inf_{y_1, y_2 \in C} f(x_\theta, \theta y_1 + (1-\theta)y_2)$.
          <br>By joint convexity: $f(x_\theta, \theta y_1 + (1-\theta)y_2) \le \theta f(x_1, y_1) + (1-\theta) f(x_2, y_2)$.
          <br>Taking infimum over $y_1, y_2$:
          $$ g(x_\theta) \le \theta \inf_{y_1} f(x_1, y_1) + (1-\theta) \inf_{y_2} f(x_2, y_2) = \theta g(x_1) + (1-\theta) g(x_2) $$
          </p>
        </div>

        <h4>Examples of Partial Minimization</h4>
        <div class="example">
          <h4>1. Distance to a Convex Set</h4>
          <p>For a convex set $S$, $d(x, S) = \inf_{y \in S} \|x - y\|$.
          <br>Here $f(x, y) = \|x - y\|$ is convex in $(x, y)$ and $S$ is convex. Thus $d(x, S)$ is convex.</p>
        </div>

        <div class="example">
          <h4>2. Schur Complement via Quadratic Minimization</h4>
          <p>Let $f(x, y) = x^\top A x + 2x^\top B y + y^\top C y$ with $C \succ 0$. We want to find $g(x) = \inf_y f(x, y)$.
          <br>We "complete the square" in $y$. Let $u = y + C^{-1}B^\top x$. Then $y = u - C^{-1}B^\top x$.
          $$ y^\top C y = (u - C^{-1}B^\top x)^\top C (u - C^{-1}B^\top x) = u^\top C u - 2x^\top B u + x^\top B C^{-1} B^\top x $$
          $$ 2x^\top B y = 2x^\top B(u - C^{-1}B^\top x) = 2x^\top B u - 2x^\top B C^{-1} B^\top x $$
          Summing terms, the cross terms involving $u$ cancel:
          $$ f(x, y) = x^\top A x + u^\top C u - x^\top B C^{-1} B^\top x = u^\top C u + x^\top (A - B C^{-1} B^\top) x $$
          Since $C \succ 0$, $u^\top C u \ge 0$ is minimized at $u=0$ (i.e., $y = -C^{-1}B^\top x$).
          $$ g(x) = x^\top (A - B C^{-1} B^\top) x $$
          Since partial minimization preserves convexity, if the original matrix is PSD, the Schur complement is PSD.</p>
        </div>

        <div class="example">
          <h4>3. Infimum over an Affine Constraint (Example 3.17)</h4>
          <p>Let $f(x) = \inf \{ h(y) \mid Ay = x \}$, where $h$ is a convex function.
          <br>This can be viewed as partial minimization of the function $F(x, y) = h(y) + \delta_{\{Ay=x\}}(x,y)$.
          <br>The set $\{(x, y) \mid Ay=x\}$ is an affine subspace, so its indicator function is convex.
          <br>Thus $F$ is convex (sum of convex functions), and $f$ is convex (partial minimization of $F$).</p>
          <p><b>Alternative Proof (Epigraph Projection):</b>
          <br>This proof relies on the fact that the epigraph of $g(x) = \inf \{ h(y) \mid Ay=x \}$ is a linear projection of a higher-dimensional convex set.</p>
          <div class="proof-box">
            <h4>Detailed Epigraph Construction</h4>
            <div class="proof-step">
              <strong>Step 1: Characterize the Epigraph.</strong>
              $(x,t) \in \text{epi } g \iff g(x) \le t$.
              By definition of infimum, this means there exists some $y$ such that $Ay=x$ and $h(y) \le t$.
            </div>
            <div class="proof-step">
              <strong>Step 2: Lift to Higher Dimension.</strong>
              Consider the set $C = \{ (x,y,t) \mid Ay=x, \ h(y) \le t \}$.
              This set is the intersection of:
              <ul>
                <li>The cylinder $\{(x,y,t) \mid (y,t) \in \text{epi } h\}$, which is convex because $h$ is convex.</li>
                <li>The affine subspace $\{(x,y,t) \mid Ay=x\}$, which is convex.</li>
              </ul>
              Thus, $C$ is a convex set in $(x,y,t)$-space.
            </div>
            <div class="proof-step">
              <strong>Step 3: Project.</strong>
              The condition from Step 1 says that $(x,t) \in \text{epi } g$ if and only if there exists $y$ such that $(x,y,t) \in C$.
              This means $\text{epi } g$ is exactly the projection of the convex set $C$ onto the $(x,t)$ coordinates.
              Since linear projections preserve convexity, $\text{epi } g$ is convex.
            </div>
          </div>
        </div>

        <div class="proof-box">
          <h4>Alternative Proof: Distance to Convex Set (Epsilon-Delta)</h4>
          <p>Let $S$ be convex and $d(x) = \inf_{y \in S} \|x-y\|$. We show $d(\theta x_1 + (1-\theta)x_2) \le \theta d(x_1) + (1-\theta)d(x_2)$.
          <br>Let $\varepsilon > 0$. By definition of infimum, there exist $y_1, y_2 \in S$ such that:
          $$ \|x_1 - y_1\| \le d(x_1) + \varepsilon, \quad \|x_2 - y_2\| \le d(x_2) + \varepsilon $$
          Let $y_\theta = \theta y_1 + (1-\theta)y_2$. Since $S$ is convex, $y_\theta \in S$.
          $$ d(x_\theta) \le \|x_\theta - y_\theta\| = \|(\theta x_1 + (1-\theta)x_2) - (\theta y_1 + (1-\theta)y_2)\| $$
          $$ = \|\theta(x_1 - y_1) + (1-\theta)(x_2 - y_2)\| $$
          By the triangle inequality and homogeneity:
          $$ \le \theta \|x_1 - y_1\| + (1-\theta)\|x_2 - y_2\| $$
          $$ \le \theta(d(x_1) + \varepsilon) + (1-\theta)(d(x_2) + \varepsilon) = \theta d(x_1) + (1-\theta)d(x_2) + \varepsilon $$
          Since $\varepsilon$ is arbitrary, the result follows.</p>
        </div>

        <h3>4.5 Infimal Convolution</h3>
        <p>A sophisticated operation related to partial minimization is <b>infimal convolution</b>. Given two convex functions $f, g: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$, their infimal convolution is defined as:</p>
        $$ (f \square g)(x) = \inf_{y \in \mathbb{R}^n} \big( f(y) + g(x-y) \big) $$
        <p>This operation is convex.
        <br><i>Proof:</i> Consider the function $F(x, y) = f(y) + g(x-y)$. Since $y \mapsto f(y)$ is convex, and $(x, y) \mapsto x-y$ is affine, $g(x-y)$ is jointly convex in $(x,y)$. The sum $F(x,y)$ is thus jointly convex. Minimizing $F$ over $y$ yields $f \square g$, which is convex by the partial minimization rule.</p>
        <p>This operation appears in regularization (e.g., Moreau envelope) and smoothing.</p>

        <h4>The Matrix-Fractional Function</h4>
      <p>The function $f(x, Y) = x^\top Y^{-1} x$ is defined for $x \in \mathbb{R}^n$ and $Y \in \mathbb{S}^n_{++}$. It is <b>jointly convex</b> in $x$ and $Y$.</p>

      <h4>Derivation: Epigraph via Schur Complement</h4>
      <p>The epigraph condition is $t \ge x^\top Y^{-1} x$ with $Y \succ 0$.
      <br>Using the <a href="../00-linear-algebra-basics/index.html#section-5">Schur Complement Lemma</a>, this is equivalent to the Linear Matrix Inequality (LMI):
      $$ M = \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0 $$
      Since the PSD cone is convex, and $M$ depends linearly on $(x, Y, t)$, the epigraph is convex.</p>

      <div class="proof-box">
        <h4>Check via Quadratic Forms</h4>
        <p>Let's verify $M \succeq 0 \iff t \ge x^\top Y^{-1} x$.
        <br>Take an arbitrary vector $z = [v^\top, \alpha]^\top$. $M \succeq 0$ means $z^\top M z \ge 0$ for all $z$.
        $$ z^\top M z = v^\top Y v + 2\alpha x^\top v + \alpha^2 t $$
        View this as a quadratic in $\alpha$: $q(\alpha) = t\alpha^2 + 2(x^\top v)\alpha + (v^\top Y v)$.
        <br>For $q(\alpha) \ge 0$ for all $\alpha$, we need $t \ge 0$ and discriminant $\le 0$:
        $$ 4(x^\top v)^2 - 4t(v^\top Y v) \le 0 \implies (x^\top v)^2 \le t (v^\top Y v) $$
        Let $w = Y^{1/2}v$. Then $v^\top Y v = \|w\|^2$ and $x^\top v = x^\top Y^{-1/2} w$.
        $$ (x^\top Y^{-1/2} w)^2 \le t \|w\|^2 $$
        By Cauchy-Schwarz, the maximum of the LHS over $\|w\|=1$ is $\|Y^{-1/2}x\|^2 = x^\top Y^{-1} x$.
        <br>Thus, the condition holds for all $w$ (and hence all $v$) if and only if $t \ge x^\top Y^{-1} x$.</p>
      </div>

      <div class="insight">
        <h4>Triangle of Equivalence</h4>
        <p>The matrix fractional function connects three concepts via the Schur Complement:</p>
        $$
        \boxed{
        t\ge x^\top Y^{-1}x
        \iff
        \begin{bmatrix}
        Y & x\\
        x^\top & t
        \end{bmatrix}\succeq 0
        \iff
        tY - xx^\top \succeq 0
        }
        $$
        <ul>
          <li><b>Scalar:</b> Quadratic-over-linear inequality.</li>
          <li><b>Block Matrix:</b> LMI form, useful for SDPs.</li>
          <li><b>Rank-1 Update:</b> $tY \succeq xx^\top$. Derived by setting $\alpha=1$ in the quadratic form argument above: $(x^\top v)^2 \le t v^\top Y v \iff v^\top (tY - xx^\top) v \ge 0$.</li>
        </ul>
        <p><b>Determinant Identity:</b> We can derive this using a specific block elimination matrix. Let $M = \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix}$. Define the elimination matrix:
        $$ R = \begin{bmatrix} I & -Y^{-1}x \\ 0 & 1 \end{bmatrix} $$
        Compute the product $MR$:
        <ul>
            <li>Top-left: $Y \cdot I + x \cdot 0 = Y$.</li>
            <li>Top-right: $Y(-Y^{-1}x) + x \cdot 1 = -x + x = 0$.</li>
            <li>Bottom-left: $x^\top \cdot I + t \cdot 0 = x^\top$.</li>
            <li>Bottom-right: $x^\top(-Y^{-1}x) + t \cdot 1 = t - x^\top Y^{-1}x$.</li>
        </ul>
        So $MR = \begin{bmatrix} Y & 0 \\ x^\top & t - x^\top Y^{-1}x \end{bmatrix}$. This is block lower-triangular.
        <br>Thus $\det(MR) = \det(Y)(t - x^\top Y^{-1}x)$. Since $\det(R)=1$, we have:
        $$ \boxed{ \det \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} = (\det Y)(t - x^\top Y^{-1}x) } $$
        </p>
      </div>

      <div class="insight">
        <h4>Alternative View: Variational Form (Detailed)</h4>
        <p>We can also express $f(x, Y) = x^\top Y^{-1} x$ as a supremum of affine functions, giving a direct proof of joint convexity without block matrices.
        <br><b>Identity:</b> $x^\top Y^{-1}x = \sup_{z \in \mathbb{R}^n} (2z^\top x - z^\top Y z)$.
        <br><b>Derivation:</b> Fix $x$ and $Y \succ 0$. Consider the function $g(z) = 2z^\top x - z^\top Y z$.
        <br>This is a strictly concave quadratic in $z$. We find the maximum by setting the gradient to zero:
        $$ \nabla_z g(z) = 2x - 2Yz = 0 \implies Yz = x \implies z^* = Y^{-1}x $$
        Plug this optimal $z^*$ back into the expression:
        $$ g(z^*) = 2(Y^{-1}x)^\top x - (Y^{-1}x)^\top Y (Y^{-1}x) = 2x^\top Y^{-1} x - x^\top Y^{-1} Y Y^{-1} x = x^\top Y^{-1} x $$
        <b>Convexity Argument:</b>
        For each fixed $z$, the function $h_z(x, Y) = 2z^\top x - \mathrm{tr}(zz^\top Y)$ is <b>linear</b> in the joint variable $(x, Y)$.
        Since $f(x, Y) = \sup_z h_z(x, Y)$ is the pointwise supremum of a family of convex (linear) functions, $f$ is jointly convex.</p>
      </div>

      </section>


<section class="section-card" id="section-5">
        <h2>5. Composition and Transformation Rules</h2>
        <p>Building complex convex functions from simpler ones through composition and transformations. These rules form the "algebra" of convexity.</p>

        <h3>5.1 Composition with Increasing/Decreasing Functions</h3>
        <p>Recognizing convexity is often about parsing a function into a composition of atomic convex functions. We use a set of recursive rules.</p>

        <div class="theorem-box">
          <h4>1. Affine Composition</h4>
          <p>If $f: \mathbb{R}^k \to \mathbb{R}$ is convex, and $x \mapsto Ax + b$ is an affine map, then the composition $g(x) = f(Ax + b)$ is convex.</p>

          <div class="example-box">
            <h4>Example: Least Squares</h4>
            <p>Let $f(z) = \|z\|_2^2$ (convex). Let $z = Ax - b$ (affine).
            <br>Then $g(x) = \|Ax - b\|_2^2$ is convex.</p>
          </div>

          <p><b>Proof:</b>
          $$
          \begin{aligned}
          g(\theta x + (1-\theta)y) &= f(A(\theta x + (1-\theta)y) + b) \\
          &= f(\theta(Ax+b) + (1-\theta)(Ay+b)) \\
          &\le \theta f(Ax+b) + (1-\theta)f(Ay+b) \\
          &= \theta g(x) + (1-\theta)g(y)
          \end{aligned}
          $$
          </p>
          <p><b>Examples:</b></p>
          <ul>
            <li><b>Log-Barrier:</b> $f(x) = -\sum \log(b_i - a_i^\top x)$. Inner: affine $b-a^\top x$. Outer: $-\log(\cdot)$ convex. Sum of convex is convex.</li>
            <li><b>Norm of Affine:</b> $\|Ax - b\|$ is convex.</li>
          </ul>
        </div>

        <div class="theorem-box">
          <h4>2. Scalar Composition Rule ($f=h\circ g$) in Depth</h4>
          <p>Let $g: \mathbb{R}^n \to \mathbb{R}$ and $h: \mathbb{R} \to \mathbb{R}$. Then $f(x) = h(g(x))$ is convex if:</p>

          <div class="example-box">
            <h4>Example: Exponential of a Convex Function</h4>
            <p>Let $f(x) = e^{x^2}$.
            <br>Inner: $g(x) = x^2$ is convex.
            <br>Outer: $h(u) = e^u$ is convex and <b>non-decreasing</b>.
            <br>Result: $f$ is convex (Rule 1).</p>
          </div>

          <div class="example-box">
            <h4>Example: Reciprocal of a Concave Function</h4>
            <p>Let $f(x) = \frac{1}{1-x^2}$ on $(-1, 1)$.
            <br>Inner: $g(x) = 1-x^2$ is concave.
            <br>Outer: $h(u) = 1/u$ is convex and <b>non-increasing</b> (for $u > 0$).
            <br>Result: $f$ is convex (Rule 2).</p>
          </div>

          <ol>
            <li>$g$ is <b>convex</b>, and $h$ is <b>convex</b> and <b>non-decreasing</b>.</li>
            <li>$g$ is <b>concave</b>, and $h$ is <b>convex</b> and <b>non-increasing</b>.</li>
            <li>$g$ is <b>affine</b>, and $h$ is <b>convex</b> (monotonicity not required).</li>
          </ol>

          <div class="intuition-box">
            <h4>Calculus Intuition (Chain Rule)</h4>
            <p>For smooth functions on $\mathbb{R}$, we can derive these rules from the second derivative:
            $$ f'(x) = h'(g(x)) g'(x) $$
            $$ f''(x) = h''(g(x)) [g'(x)]^2 + h'(g(x)) g''(x) $$
            We need $f''(x) \ge 0$.
            <ul>
                <li>The first term $h'' (g')^2$ is always non-negative if $h$ is convex ($h'' \ge 0$).</li>
                <li>The second term $h' g''$ sign depends on the product.
                    <ul>
                        <li>If $h$ is increasing ($h' \ge 0$) and $g$ is convex ($g'' \ge 0$), the product is $\ge 0$.</li>
                        <li>If $h$ is decreasing ($h' \le 0$) and $g$ is concave ($g'' \le 0$), the product is $(-) \cdot (-) = (+)$.</li>
                    </ul>
                </li>
            </ul>
            This 1D intuition perfectly maps to the general vector rules.</p>
          </div>

          <div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin: 20px 0;">
            <figure style="flex: 1; min-width: 300px; text-align: center;">
              <img src="assets/convex_composition_case1.png"
                   alt="Composition Case 1: Convex inner + Convex Increasing outer"
                   style="max-width: 500px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 1:</i> Case 1. The convex inner function "curves up". The increasing outer function preserves this order, and its own convexity amplifies the curvature.</figcaption>
            </figure>
            <figure style="flex: 1; min-width: 300px; text-align: center;">
              <img src="assets/convex_composition_case2.png"
                   alt="Composition Case 2: Concave inner + Convex Decreasing outer"
                   style="max-width: 500px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
              <figcaption><i>Figure 2:</i> Case 2. The concave inner function "curves down". The decreasing outer function flips this to "curve up", and its own convexity reinforces the result.</figcaption>
            </figure>
          </div>

          <div class="proof-box">
            <h4>Proof: Scalar Composition Rules</h4>
            <p>We prove the two most critical cases rigorously.</p>

            <h5>Case 1: Outer Convex & Non-decreasing, Inner Convex</h5>
            <p>Assume $g$ is convex, and $h$ is convex and <b>non-decreasing</b> on the range of $g$.</p>
            <div class="proof-step">
              <strong>Step 1: Inner Convexity.</strong>
              Take arbitrary $x, y \in \mathbb{R}^n$ and $\theta \in [0,1]$. By convexity of $g$:
              $$ g(\theta x + (1-\theta) y) \le \theta g(x) + (1-\theta) g(y) $$
            </div>
            <div class="proof-step">
              <strong>Step 2: Apply Monotonicity.</strong>
              Apply $h$ to both sides. Since $h$ is <b>non-decreasing</b>, the inequality direction is maintained:
              $$ h\big(g(\theta x + (1-\theta) y)\big) \le h\big(\theta g(x) + (1-\theta) g(y)\big) $$
            </div>
            <div class="proof-step">
              <strong>Step 3: Apply Outer Convexity.</strong>
              Now use the convexity of $h$ on the argument $\theta g(x) + (1-\theta) g(y)$:
              $$ h\big(\theta g(x) + (1-\theta) g(y)\big) \le \theta h(g(x)) + (1-\theta) h(g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Combine:</strong>
              $$ f(\theta x + (1-\theta)y) = h(g(\theta x + (1-\theta)y)) \le \theta f(x) + (1-\theta) f(y) $$
              Thus $f$ is convex.
            </div>

            <hr style="border-top: 1px dashed var(--border); margin: 16px 0;">

            <h5>Case 2: Outer Convex & Non-increasing, Inner Concave</h5>
            <p>Assume $g$ is concave, and $h$ is convex and <b>non-increasing</b>.</p>
            <div class="proof-step">
              <strong>Step 1: Inner Concavity.</strong>
              $$ g(\theta x + (1-\theta)y) \ge \theta g(x) + (1-\theta) g(y) $$
            </div>
            <div class="proof-step">
              <strong>Step 2: Apply Monotonicity (Flip).</strong>
              Because $h$ is <b>non-increasing</b>, applying it flips the inequality:
              $$ h\big(g(\theta x + (1-\theta)y)\big) \le h\big( \theta g(x) + (1-\theta)g(y)\big) $$
            </div>
            <div class="proof-step">
              <strong>Step 3: Apply Outer Convexity.</strong>
              $$ h(\theta g(x) + (1-\theta)g(y)) \le \theta h(g(x)) + (1-\theta)h(g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Conclusion:</strong>
              $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta) f(y) $$
              Thus $f$ is convex.
            </div>
          </div>
        </div>

        <div class="example">
          <h4>Example: Log-Log-Sum-Exp (Step-by-Step Analysis)</h4>
          <p>Consider the function $f(x) = -\log(-\log(\sum_{i=1}^m e^{a_i^\top x + b_i}))$. We verify convexity by decomposing it into layers:</p>

          <div class="proof-box">
            <div class="proof-step">
              <strong>Layer 1: The Inner Sum (Affine + Exp).</strong>
              Let $z_i(x) = e^{a_i^\top x + b_i}$.
              Since $a_i^\top x + b_i$ is affine and $e^u$ is convex, $z_i(x)$ is convex.
              The sum $S(x) = \sum z_i(x)$ is convex (sum of convex functions).
            </div>

            <div class="proof-step">
              <strong>Layer 2: First Log (Log-Sum-Exp).</strong>
              Let $g(x) = \log S(x) = \log(\sum e^{a_i^\top x + b_i})$.
              This is the composition of the convex log-sum-exp function with affine maps. Thus $g(x)$ is <b>convex</b>.
              (Note: domain requires the sum < 1 for the next log to be defined, so $g(x) < 0$).
            </div>

            <div class="proof-step">
              <strong>Layer 3: Outer Log (Composition Rule).</strong>
              Now $f(x) = h(g(x))$ where $h(u) = -\log(-u)$ for $u < 0$.
              Check properties of $h(u)$:
              <ul>
                <li><b>Convexity:</b> $h'(u) = -1/u$, $h''(u) = 1/u^2 > 0$. So $h$ is convex.</li>
                <li><b>Monotonicity:</b> $h'(u) = -1/u$. Since $u < 0$, $-1/u > 0$. So $h$ is <b>non-decreasing</b>.</li>
              </ul>
            </div>

            <div class="proof-step">
              <strong>Conclusion:</strong>
              We have $f(x) = h(g(x))$ where $g$ is convex and $h$ is convex and non-decreasing.
              By the scalar composition rule, <b>$f(x)$ is convex</b>.
            </div>
          </div>
        </div>

        <div class="theorem-box">
          <h4>3. Vector Composition Rule ($f=h\circ g$) in Depth</h4>
          <p>Let $g: \mathbb{R}^n \to \mathbb{R}^k$ and $h: \mathbb{R}^k \to \mathbb{R}$. Then $f(x) = h(g_1(x), \dots, g_k(x))$ is convex if $h$ is convex and for each $i \in \{1, \dots, k\}$:</p>
          <ul>
            <li>$g_i$ is <b>convex</b> and $h$ is <b>non-decreasing</b> in the $i$-th argument.</li>
            <li>$g_i$ is <b>concave</b> and $h$ is <b>non-increasing</b> in the $i$-th argument.</li>
            <li>$g_i$ is <b>affine</b> (monotonicity in $i$-th argument not required).</li>
          </ul>

          <div class="proof-box">
            <h4>Proof (Convex + Non-decreasing Case)</h4>
            <p>We prove the case where each $g_i$ is convex and $h$ is convex and non-decreasing in each coordinate.</p>
            <div class="proof-step">
              <strong>Step 1: Vector Jensen Inequality.</strong>
              Since each $g_i$ is convex, for $\theta \in [0,1]$:
              $$ g(\theta x + (1-\theta)y) \le \theta g(x) + (1-\theta)g(y) \quad \text{(componentwise)} $$
            </div>
            <div class="proof-step">
              <strong>Step 2: Monotonicity.</strong>
              Since $h$ is non-decreasing in every coordinate, applying it preserves the inequality:
              $$ h(g(\theta x + (1-\theta)y)) \le h(\theta g(x) + (1-\theta)g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Step 3: Convexity of Outer Function.</strong>
              By convexity of $h$:
              $$ h(\theta g(x) + (1-\theta)g(y)) \le \theta h(g(x)) + (1-\theta) h(g(y)) $$
            </div>
            <div class="proof-step">
              <strong>Conclusion.</strong> Combining these:
              $$ f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) $$
            </div>
          </div>
        </div>

        <div class="example">
          <h4>Example 1: Sum of Logs (Concave)</h4>
          <p>Let $u(x) = \sum_{i=1}^m \log g_i(x)$ where each $g_i$ is concave and positive.</p>
          <ul>
            <li>Inner map: $g(x) = (g_1(x), \dots, g_m(x))$ is concave.</li>
            <li>Outer map: $h(y) = \sum \log y_i$. Since $\log$ is concave and increasing, $h$ is concave and increasing.</li>
            <li>Result: $u$ is concave.</li>
          </ul>
        </div>

        <div class="example">
          <h4>Example 2: Log-Sum-Exp of Convex Functions (Detailed Check)</h4>
          <p>Let $f(x) = \log\left(\sum_{i=1}^m \exp(g_i(x))\right)$ where each $g_i$ is convex. We verify the vector composition rule conditions:</p>
          <ol>
            <li><b>Convexity of Outer Function $h$:</b> $h(z) = \log(\sum e^{z_i})$ is convex (Log-Sum-Exp).
            <br><i>Proof:</i> Using Hölder's inequality on $h(\theta z + (1-\theta)w) \le \theta h(z) + (1-\theta)h(w)$.</li>
            <li><b>Monotonicity of Outer Function:</b> $h$ is non-decreasing in each argument.
            <br><i>Proof:</i> $\frac{\partial h}{\partial z_j} = \frac{e^{z_j}}{\sum e^{z_k}} > 0$. Increasing any $z_j$ increases the sum, thus increasing the log-sum.</li>
            <li><b>Convexity of Inner Functions:</b> Each $g_i(x)$ is convex (given).</li>
          </ol>
          <p><b>Conclusion:</b> Since $h$ is convex and non-decreasing, and $g_i$ are convex, the composition $f(x) = h(g_1(x), \dots, g_m(x))$ is convex.</p>
        </div>

        <div class="example">
          <h4>Example 3: $p$-Means ($h(z) = (\sum z_i^p)^{1/p}$)</h4>
          <p>Consider $f(x) = (\sum_{i=1}^k g_i(x)^p)^{1/p}$ where $g_i(x) \ge 0$. We rely on the convexity/concavity and monotonicity of the outer function $h(z) = (\sum z_i^p)^{1/p}$.</p>
          <ul>
            <li><b>Case $p \ge 1$ (Convex):</b>
            <br><i>Convexity:</i> By the Minkowski inequality (triangle inequality for $p$-norm), $h$ is convex.
            <br><i>Monotonicity:</i> $\frac{\partial h}{\partial z_j} \propto z_j^{p-1} \ge 0$ for $z \ge 0$.
            <br><b>Result:</b> Since $h$ is convex and non-decreasing, if $g_i$ are convex and non-negative, $f$ is convex.</li>

            <li><b>Case $0 < p \le 1$ (Concave):</b>
            <br><i>Concavity:</i> By the Reverse Minkowski inequality (or Hessian analysis), $h$ is concave on $\mathbb{R}_{++}^k$.
            <br><i>Monotonicity:</i> $\frac{\partial h}{\partial z_j} \propto z_j^{p-1} > 0$ for $z > 0$.
            <br><b>Result:</b> Since $h$ is concave and non-decreasing, if $g_i$ are concave and positive, $f$ is concave.</li>
          </ul>
        </div>

        <div class="example">
          <h4>Example 4: Geometric Mean ($h(z) = (\prod z_i)^{1/k}$)</h4>
          <p>Consider $f(x) = (\prod_{i=1}^k g_i(x))^{1/k}$ where $g_i(x) > 0$.
          <br>The outer function $h(z) = (\prod z_i)^{1/k}$ is <b>concave</b> and <b>non-decreasing</b> on $\mathbb{R}_{++}^k$.
          <br><b>Proof check:</b>
          <ul>
              <li><b>Monotonicity:</b> $\frac{\partial h}{\partial z_j} = \frac{1}{k} \left(\prod_{i \ne j} z_i\right)^{1/k} z_j^{1/k-1} > 0$.</li>
              <li><b>Concavity:</b> $\nabla^2 h \preceq 0$ (see Section 4 for deep dive). Alternatively, $h$ is log-concave and 1-homogeneous.</li>
          </ul>
          <b>Result:</b> If $g_i$ are concave and positive, then the geometric mean $f(x)$ is concave.</p>
        </div>

        <h3>5.2 Perspective Function (The Modeling Engine)</h3>
        <p>The perspective transformation is more than a convexity property; it is a <b>modeling engine</b>. It allows you to remove denominators (fractions) by "absorbing" them into new variables, transforming fractional expressions into convex ones in a higher dimension.</p>

        <div class="theorem-box">
          <h4>Definition and Theorem</h4>
          <p>If $f: \mathbb{R}^n \to \mathbb{R}$ is convex, then its <b>perspective</b> $g: \mathbb{R}^{n+1} \to \mathbb{R}$ defined by:</p>
          $$
          g(x, t) = t f(x/t), \quad t > 0
          $$
          <p>is convex on $\mathrm{dom}\, g = \{(x, t) \mid x/t \in \mathrm{dom}\, f, \ t > 0\}$.</p>
          <p><b>Geometric Intuition (Cone over Epigraph):</b></p>
          <p>The epigraph of the perspective function is the <b>conic hull</b> of the epigraph of $f$ (lifted to $t=1$).
          <br>Specifically, $(x, t, s) \in \mathrm{epi}(\tilde{f}) \iff t > 0 \text{ and } (x/t, s/t) \in \mathrm{epi}(f)$.
          <br>Since the conic hull of a convex set is convex, the perspective is convex.</p>
        </div>

        <div class="proof-box">
          <h4>Direct Proof of Convexity (The $\lambda$ trick)</h4>
          <p>We show $g(x,t) = t f(x/t)$ is convex by direct definition.</p>
          <div class="proof-step">
            <strong>Step 1: Setup.</strong>
            Take any $(x_1, t_1), (x_2, t_2)$ with $t_1, t_2 > 0$, and $\theta \in [0, 1]$.
            Define $(x, t) = \theta(x_1, t_1) + (1-\theta)(x_2, t_2)$.
            We must show $g(x, t) \le \theta g(x_1, t_1) + (1-\theta)g(x_2, t_2)$.
          </div>

          <div class="proof-step">
            <strong>Step 2: Express $x/t$ as convex combination.</strong>
            This is the key step. We need to write $x/t$ as a mixture of $x_1/t_1$ and $x_2/t_2$.
            $$ \frac{x}{t} = \frac{\theta x_1 + (1-\theta)x_2}{\theta t_1 + (1-\theta)t_2} $$
            Define weights $\lambda_1 = \frac{\theta t_1}{t}$ and $\lambda_2 = \frac{(1-\theta)t_2}{t}$.
            Note that $\lambda_1, \lambda_2 \ge 0$ and $\lambda_1 + \lambda_2 = \frac{\theta t_1 + (1-\theta)t_2}{t} = 1$.
            We can rewrite the fraction as:
            $$ \frac{x}{t} = \lambda_1 \frac{x_1}{t_1} + \lambda_2 \frac{x_2}{t_2} $$
          </div>

          <div class="proof-step">
            <strong>Step 3: Apply Convexity of $f$.</strong>
            $$ f\left(\frac{x}{t}\right) \le \lambda_1 f\left(\frac{x_1}{t_1}\right) + \lambda_2 f\left(\frac{x_2}{t_2}\right) $$
          </div>

          <div class="proof-step">
            <strong>Step 4: Multiply by $t$.</strong>
            $$ g(x, t) = t f(x/t) \le t \lambda_1 f(x_1/t_1) + t \lambda_2 f(x_2/t_2) $$
            Substitute back $t \lambda_1 = \theta t_1$ and $t \lambda_2 = (1-\theta)t_2$:
            $$ g(x, t) \le \theta t_1 f(x_1/t_1) + (1-\theta) t_2 f(x_2/t_2) = \theta g(x_1, t_1) + (1-\theta) g(x_2, t_2) $$
            Done: perspective is convex.
          </div>
        </div>

        <div class="proof-box">
          <h4>Alternative Proof: Epigraph Mapping</h4>
          <p>We can prove convexity by relating the epigraph of $g$ to the epigraph of $f$ via a linear map.
          <br>Define the mapping $\Phi: \{(x,t,s) \mid t>0\} \to \mathbb{R}^{n+1}$ by $\Phi(x,t,s) = (x/t, s/t)$.
          <br>The epigraph of $g$ is defined by $g(x,t) \le s \iff t f(x/t) \le s \iff f(x/t) \le s/t$.
          <br>Thus $(x,t,s) \in \text{epi } g \iff t > 0 \text{ and } (x/t, s/t) \in \text{epi } f$.
          <br>We can express this explicitly as:
          $$ \mathrm{epi}\, g = \Phi^{-1}(\mathrm{epi}\, f) \cap \{(x, t, s) \mid t > 0\} $$
          where $\Phi$ is the linear map defined on homogeneous coordinates. Since the inverse image of a convex set under a linear map is convex, and the intersection with a halfspace ($t>0$) is convex, $\mathrm{epi}\, g$ is convex.
          <br>More simply: $\text{epi } g$ is the conic hull of $\text{epi } f$ (embedded at $t=1$). The conic hull of a convex set is convex.</p>
        </div>

        <h4>The Modeling Engine: Removing Denominators</h4>
        <p>The perspective transform allows us to solve problems involving ratios with positive denominators by <b>linearizing</b> the denominator into a variable $t$.</p>

        <div class="example">
          <h4>Template: Linear-Fractional Programming</h4>
          <p>Minimize $\frac{a^\top x + b}{c^\top x + d}$ subject to $c^\top x + d > 0$ and convex constraints $x \in C$.
          <br><b>Substitution:</b> Let $t = \frac{1}{c^\top x + d}$ and $y = tx$.
          <br><b>Constraints:</b>
          <ul>
              <li>$t > 0$.</li>
              <li>Normalization: $c^\top y + dt = 1$ (since $1/t = c^\top(y/t) + d$).</li>
              <li>$y/t \in C \iff \tilde{I}_C(y, t) \le 0$ (Perspective of indicator). If $C=\{Ax \le b\}$, this becomes $Ay \le bt$.</li>
          </ul>
          <b>Objective:</b> $t(a^\top (y/t) + b) = a^\top y + bt$.
          <br><b>Result:</b> An LP (or convex problem) in $(y,t)$. No bisection needed.</p>
        </div>

        <div class="example">
          <h4>Template: Norm-Ratio Minimization</h4>
          <p>Minimize $\frac{\|Ax - b\|_2}{c^\top x + d}$.
          <br><b>Substitution:</b> $t = \frac{1}{c^\top x + d}$, $y = tx$.
          <br><b>Objective:</b> $t \|A(y/t) - b\|_2 = \|Ay - bt\|_2$.
          <br><b>Result:</b> $\min \|Ay - bt\|_2$ s.t. $c^\top y + dt = 1$, $t > 0$.
          <br>This transforms a complex ratio into a standard SOCP problem (minimizing a norm).</p>
        </div>

        <div class="example">
          <h4>Example: Relative Entropy</h4>
          <p>Let $f(u) = -\log u$, which is convex. Its perspective is:
          $$ g(x, t) = t f(x/t) = -t \log(x/t) = t \log t - t \log x $$
          This is related to the relative entropy function $x \log(x/y)$, which is jointly convex.</p>
        </div>

        <div class="example">
          <h4>Example: Power-over-Linear (Deconstruction)</h4>
          <p>The function $f(x, t) = \frac{\|x\|_p^p}{t^{p-1}}$ (where $p > 1, t > 0$) is convex. Here is how to recognize it:</p>
          <div class="proof-box">
            <div class="proof-step">
              <strong>Step 1: Identify homogeneity.</strong>
              If we scale $(x,t)$ by $\alpha$, we get $\frac{\|\alpha x\|^p}{(\alpha t)^{p-1}} = \frac{\alpha^p \|x\|^p}{\alpha^{p-1} t^{p-1}} = \alpha f(x,t)$.
              Since $f$ is 1-homogeneous, it is likely a perspective function.
            </div>
            <div class="proof-step">
              <strong>Step 2: Factor out $t$ (The Perspective Trick).</strong>
              Rewrite the expression to isolate $x/t$:
              $$ f(x,t) = \frac{\|x\|_p^p}{t^{p-1}} = t \cdot \frac{\|x\|_p^p}{t^p} = t \left\| \frac{x}{t} \right\|_p^p $$
            </div>
            <div class="proof-step">
              <strong>Step 3: Identify the base function.</strong>
              This matches the form $t g(x/t)$ where $g(u) = \|u\|_p^p$.
              Since $p \ge 1$, $g(u)$ is convex (power of a norm).
            </div>
            <div class="proof-step">
              <strong>Conclusion:</strong> $f$ is the perspective of a convex function, so it is convex.
            </div>
          </div>
        </div>

        <div class="example">
          <h4>Example: Quadratic-over-Linear</h4>
          <p>The function $f(x, y) = \frac{x^2}{y}$ for $y > 0$ is convex (Perspective of $x^2$).
          <br>More generally, $f(x) = \frac{\|Ax+b\|_2^2}{c^\top x + d}$ is convex (Affine composition of perspective).</p>
        </div>

        <h3>5.3 Power Functions and Homogeneity</h3>

        <p>Power functions are a rich source of examples and inequalities.</p>

        <h4>Convexity of $x^p$</h4>
        <p>The function $f(x) = x^p$ on $\mathbb{R}_{++}$ is:</p>
        <ul>
            <li><b>Convex</b> for $p \ge 1$ or $p \le 0$ ($f''(x) = p(p-1)x^{p-2} \ge 0$).</li>
            <li><b>Concave</b> for $0 \le p \le 1$ ($f''(x) \le 0$).</li>
        </ul>

        <h4>Homogeneity and Inequalities</h4>
        <p>For $p \ge 1$, $f(x) = x^p$ is convex. Using the tangent line inequality at $x=1$ ($f(x) \ge f(1) + f'(1)(x-1)$):</p>
        $$ x^p \ge 1 + p(x-1) $$
        <p>This can be used to prove that for $x, y \ge 0$ and $p \ge 1$:</p>
        $$ \boxed{ x^p + y^p \ge x + y \quad \text{for } x,y \in [0,1] } $$
        <p>Also, due to homogeneity ($(\lambda x)^p = \lambda^p x^p$), we can scale inequalities. For example, the function $f(x, y) = \frac{x^2}{y}$ arises from the homogeneity of quadratics.</p>

        <div class="proof-box">
          <h4>Theorem: Log-Concavity + Homogeneity $\implies$ Concavity</h4>
          <p>Let $f: \mathbb{R}^n_+ \to \mathbb{R}_+$ be a function that is:
          <ol>
            <li><b>Log-Concave:</b> $f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta}$.</li>
            <li><b>Positively Homogeneous (degree 1):</b> $f(\alpha x) = \alpha f(x)$ for $\alpha > 0$.</li>
          </ol>
          Then $f$ is <b>concave</b>: $f(\theta x + (1-\theta)y) \ge \theta f(x) + (1-\theta) f(y)$.</p>
          <div class="proof-step">
            <strong>Proof Idea:</strong> Normalize to the level set.
            Let $x, y$ be points such that $f(x) = \alpha, f(y) = \beta$.
            Define normalized vectors $\tilde{x} = x/\alpha, \tilde{y} = y/\beta$, so $f(\tilde{x})=f(\tilde{y})=1$.
            Let $z = \theta x + (1-\theta)y = \theta \alpha \tilde{x} + (1-\theta) \beta \tilde{y}$.
            Let $\gamma = \theta \alpha + (1-\theta)\beta$. Rewrite $z = \gamma (\lambda \tilde{x} + (1-\lambda)\tilde{y})$ where $\lambda = \theta \alpha / \gamma$.
            By homogeneity: $f(z) = \gamma f(\lambda \tilde{x} + (1-\lambda)\tilde{y})$.
            By log-concavity: $f(\lambda \tilde{x} + (1-\lambda)\tilde{y}) \ge f(\tilde{x})^\lambda f(\tilde{y})^{1-\lambda} = 1^\lambda 1^{1-\lambda} = 1$.
            Thus $f(z) \ge \gamma \cdot 1 = \theta f(x) + (1-\theta)f(y)$.
          </div>
          <div class="proof-step">
            <strong>Application: Geometric Mean.</strong>
            $g(x) = (\prod x_i)^{1/n}$.
            1. Homogeneous? $g(\alpha x) = (\alpha^n \prod x_i)^{1/n} = \alpha g(x)$. Yes.
            2. Log-concave? $\log g(x) = \frac{1}{n}\sum \log x_i$ is a sum of concave functions. Yes.
            Therefore, $g(x)$ is concave. This offers a conceptual alternative to the Hessian proof.
          </div>
        </div>

        <h3>5.4 The Minkowski Functional</h3>
        <p>The Minkowski functional generalizes the notion of a norm using a convex set. Let $C \subset \mathbb{R}^n$ be a convex set containing the origin. We define for each $x$:</p>
        $$
        M_C(x) = \inf \{ t > 0 \mid t^{-1} x \in C \}
        $$

        <div class="insight">
          <h4>Domain & Interpretation</h4>
          <p>Consider the ray $R_x = \{s x \mid s \ge 0\}$. The value $M_C(x)$ is determined by where this ray exits the set $C$.
          <br>Specifically, the set of scaling factors is $\{t > 0 \mid t^{-1} x \in C\} = \{1/s \mid s > 0, s x \in C\}$.
          <br>Thus, $M_C(x)$ is finite if and only if the ray intersects $C$ at some positive multiple.
          <br>If $x \in C$, then $t=1$ is feasible, so $M_C(x) \le 1$.
          <br>Geometrically, $M_C(x)$ represents the "inverse distance" to the boundary along the direction $x$. $M_C(x) = 1$ means $x$ is on the "unit sphere" of the geometry defined by $C$.</p>
        </div>

        <div class="proof-box">
          <h4>Properties: Homogeneity and Convexity</h4>

          <div class="proof-step">
            <strong>1. Positive Homogeneity.</strong>
            Let $\alpha > 0$. We compute $M_C(\alpha x) = \inf \{ t > 0 \mid t^{-1} \alpha x \in C \}$.
            <br>Let $s = t/\alpha$ (so $t = \alpha s$). The condition becomes $s^{-1} x \in C$.
            $$ M_C(\alpha x) = \inf \{ \alpha s \mid s > 0, s^{-1} x \in C \} = \alpha \inf \{ s > 0 \mid s^{-1} x \in C \} = \alpha M_C(x) $$
            For $\alpha=0$, since $0 \in C$, $M_C(0)=0$. Thus homogeneity holds for all $\alpha \ge 0$.
          </div>

          <div class="proof-step">
            <strong>2. Convexity.</strong>
            We show $M_C(\theta x + (1-\theta)y) \le \theta M_C(x) + (1-\theta) M_C(y)$.
            <br>Fix $\varepsilon > 0$. Since $M_C(x)$ is an infimum, there exists $a > 0$ such that $a^{-1}x \in C$ and $a \le M_C(x) + \varepsilon$.
            <br>Similarly, there exists $b > 0$ such that $b^{-1}y \in C$ and $b \le M_C(y) + \varepsilon$.
            <br>By convexity of $C$, the combination $\theta a^{-1}x + (1-\theta)b^{-1}y$ is not necessarily in $C$ directly, but we form a convex combination of points in $C$.
            <br>Consider $z = \theta x + (1-\theta)y$. Let $t = \theta a + (1-\theta)b$.
            We check the point $t^{-1}z$:
            $$ \frac{z}{t} = \frac{\theta x + (1-\theta)y}{\theta a + (1-\theta)b} = \frac{\theta a (a^{-1}x) + (1-\theta)b (b^{-1}y)}{\theta a + (1-\theta)b} $$
            This is a convex combination (weights $\lambda_1 = \theta a/t, \lambda_2 = (1-\theta)b/t$ sum to 1).
            Since $a^{-1}x \in C$ and $b^{-1}y \in C$, and $C$ is convex, $t^{-1}z \in C$.
            <br>Therefore, $M_C(z) \le t = \theta a + (1-\theta)b \le \theta M_C(x) + (1-\theta)M_C(y) + \varepsilon$.
            <br>Letting $\varepsilon \to 0$ proves convexity.
          </div>
        </div>

        <div class="theorem-box">
          <h4>When is $M_C$ a Norm?</h4>
          <p>We require three conditions on $C$: <b>Closed, Bounded, Symmetric</b> ($x \in C \implies -x \in C$), and <b>$0 \in \text{int } C$</b>.</p>
          <div class="proof-step">
            <strong>1. Positivity ($M_C(x) \ge 0$).</strong> Obvious from definition ($t>0$).
          </div>
          <div class="proof-step">
            <strong>2. Symmetry ($M_C(-x) = M_C(x)$).</strong>
            $$ M_C(-x) = \inf \{ t > 0 \mid t^{-1}(-x) \in C \} = \inf \{ t > 0 \mid -(t^{-1}x) \in C \} $$
            Since $C$ is symmetric, $u \in C \iff -u \in C$. Thus the condition is equivalent to $t^{-1}x \in C$, so $M_C(-x)=M_C(x)$.
          </div>
          <div class="proof-step">
            <strong>3. Non-degeneracy (Strict Positivity).</strong>
            We must show $M_C(x)=0 \iff x=0$.
            Suppose $M_C(x)=0$ for some $x \ne 0$.
            By definition, this means for every $\varepsilon > 0$, there exists $t \in (0, \varepsilon)$ such that $t^{-1}x \in C$.
            As $t \to 0^+$, the norm $\|t^{-1}x\| = t^{-1}\|x\| \to \infty$.
            This implies $C$ contains points of arbitrarily large norm, which contradicts the assumption that $C$ is <b>bounded</b>.
            Thus, if $C$ is bounded, $M_C(x)=0$ implies $x=0$.
          </div>
        </div>

      </section>


<section class="section-card" id="section-6">
        <h2>6. First-Order Conditions (Tangent Line Property)</h2>

        <h3>6.1 The First-Order Characterization</h3>
        <div class="theorem-box">
          <h4>Theorem (First-Order Condition for Convexity)</h4>
          <p>Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable. Then $f$ is convex <b>if and only if</b> $\mathrm{dom}\, f$ is convex and for all $x, y \in \mathrm{dom}\, f$:</p>
          $$
          f(y) \ge f(x) + \nabla f(x)^\top (y - x)
          $$
          <p>This states that the <b>tangent line (or tangent hyperplane) at any point is a global underestimator</b> of the function.</p>
        </div>

        <div class="proof-box">
          <h4>Proof: Tangent Underestimator</h4>

          <div class="proof-step">
            <strong>Part 1: Convex $\Rightarrow$ Inequality (via Line Restriction)</strong>
            <p>We prove that if $f$ is convex, the tangent underestimates it.
            <br>Fix $x, y \in \operatorname{dom} f$. Let $\phi(t) = f(x + t(y-x))$. Since $f$ is convex, $\phi$ is convex on $[0,1]$.
            <br>For a convex function $\phi$, secant slopes are non-decreasing. Thus, the slope at $t=0$ is a lower bound for any secant:
            $$ \phi'(0) \le \frac{\phi(t) - \phi(0)}{t} \quad \forall t > 0 $$
            Multiplying by $t$: $\phi(t) \ge \phi(0) + \phi'(0)t$.
            <br>Chain Rule: $\phi'(0) = \nabla f(x)^\top (y-x)$.
            <br>Setting $t=1$: $f(y) \ge f(x) + \nabla f(x)^\top (y-x)$.</p>
          </div>

          <div class="proof-step">
            <strong>Part 2: Inequality $\Rightarrow$ Convex</strong>
            <p>Assume the inequality holds for all $\mathbf{x}, \mathbf{y}$. Let $\mathbf{z} = \theta \mathbf{x} + (1-\theta)\mathbf{y}$ for $\theta \in [0, 1]$.
            <br>Apply the inequality at $\mathbf{z}$ targeting $\mathbf{x}$:
            $$ f(\mathbf{x}) \ge f(\mathbf{z}) + \nabla f(\mathbf{z})^\top (\mathbf{x}-\mathbf{z}) $$
            Apply the inequality at $\mathbf{z}$ targeting $\mathbf{y}$:
            $$ f(\mathbf{y}) \ge f(\mathbf{z}) + \nabla f(\mathbf{z})^\top (\mathbf{y}-\mathbf{z}) $$
            Multiply the first by $\theta$ and the second by $(1-\theta)$ and sum:
            $$ \theta f(x) + (1-\theta)f(y) \ge f(z) + \nabla f(z)^\top (\theta(x-z) + (1-\theta)(y-z)) $$
            Since $\theta(x-z) + (1-\theta)(y-z) = 0$, the gradient term vanishes.
            $$ \theta f(x) + (1-\theta)f(y) \ge f(z) = f(\theta x + (1-\theta)y) $$
            Thus $f$ is convex.</p>
          </div>
        </div>

        <div class="insight">
          <h4>Key Takeaway</h4>
          <p>For differentiable functions, convexity is equivalent to: <b>the first-order Taylor approximation always underestimates the function</b>. This is an extremely useful characterization for analysis and algorithm design.</p>
        </div>

        <div class="insight">
          <h4>Bregman Divergence</h4>
          <p>The difference between the function and its first-order approximation is called the <b>Bregman Divergence</b>:
          $$ D_f(y, x) := f(y) - f(x) - \nabla f(x)^\top (y - x) $$
          The first-order condition is equivalent to saying $D_f(y, x) \ge 0$ for all $x, y$. This non-negative "distance" measures how much the function curves up from its tangent plane and is central to analyzing mirror descent algorithms.</p>
        </div>

        <div class="proof-box">
          <h4>First-Order Optimality Condition</h4>
          <p>The first-order characterization immediately gives a powerful <b>optimality condition</b>.</p>
          <div class="proof-step">
            <strong>Theorem:</strong> Let $f$ be convex and differentiable. Then $x^*$ is a <b>global minimizer</b> of $f$ if and only if:
            $$ \nabla f(x^*) = 0 $$
          </div>
          <div class="proof-step">
            <strong>Proof ($\Leftarrow$):</strong> Suppose $\nabla f(x^*) = 0$. By the first-order condition, for all $y$:
            $$ f(y) \ge f(x^*) + \nabla f(x^*)^\top (y - x^*) = f(x^*) + 0 = f(x^*) $$
            Thus $x^*$ is a global minimizer.
          </div>
          <div class="proof-step">
            <strong>Proof ($\Rightarrow$):</strong> Suppose $x^*$ is a global minimizer but $\nabla f(x^*) \neq 0$. Then moving in direction $-\nabla f(x^*)$ would decrease $f$ (by differentiability), contradicting minimality.
          </div>
          <div class="proof-step">
            <strong>For constrained problems:</strong> When minimizing over a convex set $C$, the condition becomes:
            $$ \nabla f(x^*)^\top (y - x^*) \ge 0 \quad \forall y \in C $$
            This is the variational inequality characterization of optimality.
          </div>
        </div>

        <h3>6.2 Monotonicity of the Gradient</h3>
        <p>Another powerful first-order characterization involves the change in gradients. A differentiable function $f$ is convex <b>if and only if</b> its gradient is a <a href="#" class="definition-link">monotone operator</a>:</p>
        $$
        (\nabla f(x) - \nabla f(y))^\top (x - y) \ge 0 \quad \forall x, y \in \mathrm{dom}\, f
        $$
        <p><b>Geometric Intuition:</b> As you move from $\mathbf{y}$ to $\mathbf{x}$ (direction $\mathbf{x}-\mathbf{y}$), the gradient changes in a way that aligns with that movement. In 1D, this simply means $f'(x)$ is non-decreasing ($f'(x) \ge f'(y)$ whenever $x > y$).</p>

        <div class="proof-box">
            <h4>Proof: Convexity $\iff$ Monotone Gradient</h4>
            <div class="proof-step">
                <strong>($\Rightarrow$) Convex $\implies$ Monotone.</strong>
                <br>Apply the first-order condition twice:
                $$ f(y) \ge f(x) + \nabla f(x)^\top (y-x) $$
                $$ f(x) \ge f(y) + \nabla f(y)^\top (x-y) $$
                Summing these inequalities:
                $$ f(y) + f(x) \ge f(x) + f(y) + \nabla f(x)^\top (y-x) + \nabla f(y)^\top (x-y) $$
                $$ 0 \ge (\nabla f(x) - \nabla f(y))^\top (y-x) $$
                Multiplying by $-1$ reverses the inequality:
                $$ (\nabla f(x) - \nabla f(y))^\top (x-y) \ge 0 $$
            </div>
            <div class="proof-step">
                <strong>($\Leftarrow$) Monotone $\implies$ Convex.</strong>
                <br>Let $g(t) = f(x + t(y-x))$. Then $g'(t) = \nabla f(x+t(y-x))^\top (y-x)$.
                <br>By the Mean Value Theorem, $f(y) - f(x) = g(1) - g(0) = \int_0^1 g'(t) dt$.
                <br>Using monotonicity, we can show $g'(t) \ge g'(0)$.
                $$ ( \nabla f(z) - \nabla f(x) )^\top (z-x) \ge 0 \implies \nabla f(z)^\top (y-x) \cdot t \ge \nabla f(x)^\top (y-x) \cdot t $$
                Dividing by $t>0$, the directional derivative increases. Thus $f(y) \ge f(x) + \nabla f(x)^\top (y-x)$.
            </div>
        </div>

        <h3>6.3 Consequences and Applications</h3>

        <div class="example">
          <h4>Application 1: Proving a Function is Convex</h4>
          <p>Show that $f(x) = e^x$ is convex on $\mathbb{R}$.</p>
          <p><b>Solution:</b> We verify the first-order condition. For any $x, y \in \mathbb{R}$:</p>
          $$
          f(y) = e^y, \quad f(x) + f'(x)(y - x) = e^x + e^x(y - x) = e^x(1 + y - x)
          $$
          <p>We need to show $e^y \ge e^x(1 + y - x)$ for all $x, y$. Dividing by $e^x > 0$ and letting $t = y - x$:</p>
          $$
          e^t \ge 1 + t
          $$
          <p>This is a well-known inequality (follows from the Taylor series of $e^t$ with all positive terms). Therefore, $f(x) = e^x$ is convex.</p>
        </div>

        <div class="example">
          <h4>Application 2: PSD Quadratic (The Canonical Atom)</h4>
          <p>Let $f(x) = \frac{1}{2} x^\top Q x + c^\top x + r$ with $Q \succeq 0$.
          <br>Gradient: $\nabla f(x) = Qx + c$.
          <br>Tangent underestimator gap (Bregman Divergence):
          $$ D_f(y,x) = f(y) - f(x) - \nabla f(x)^\top (y-x) $$
          Expanding terms:
          $$ = \tfrac{1}{2} y^\top Q y - \tfrac{1}{2} x^\top Q x - (Qx)^\top (y-x) = \tfrac{1}{2} (y-x)^\top Q (y-x) $$
          Since $Q \succeq 0$, this gap is always $\ge 0$. Thus the first-order condition holds exactly with a quadratic margin.</p>
        </div>

        <div class="example">
          <h4>Application 2: Optimality Condition</h4>
          <p>If $f$ is convex and differentiable, then $x^*$ minimizes $f$ over a convex set $C$ if and only if:</p>
          $$
          \nabla f(x^*)^\top (y - x^*) \ge 0 \quad \forall y \in C
          $$
          <p>This is the <b>first-order optimality condition</b>. For unconstrained problems ($C = \mathbb{R}^n$), this reduces to $\nabla f(x^*) = 0$ (the gradient vanishes). This variational inequality is the basis for the <b>stationarity condition</b> in KKT theory (<a href="../09-duality/index.html">Lecture 09</a>).</p>
        </div>

        <div class="warning-box">
            <h4>Common Failure Modes (First-Order)</h4>
            <ol>
                <li><b>Boundary Gradients:</b> Using $\nabla f(x)$ at a boundary point where it might not be well-defined (e.g., barrier functions). Always work in the open interior $\operatorname{dom} f^\circ$.</li>
                <li><b>Nondifferentiable Functions:</b> Applying the tangent condition to functions like $|x|$. You must use <b>subgradients</b> instead ($f(y) \ge f(x) + g^\top(y-x)$).</li>
                <li><b>Local vs Global:</b> Thinking the tangent underestimator is a local property. It is <b>global</b>. If it only holds locally, the function is not convex.</li>
                <li><b>Convex vs Concave:</b> Convex is "above tangent". Concave is "below tangent". Don't flip the inequality!</li>
            </ol>
        </div>

      </section>


<section class="section-card" id="section-7">
        <h2>7. Second-Order Conditions (Hessian Test)</h2>

        <h3>7.1 The Second-Order Characterization</h3>

        <p>For twice-differentiable functions, curvature is measured by the Hessian matrix.</p>

        <div class="theorem-box">
          <h4>Theorem (Second-Order Condition for Convexity)</h4>
          <p>Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is twice differentiable on an open convex set $D$. Then $f$ is convex <b>if and only if</b> for all $x \in D$:</p>
          $$
          \nabla^2 f(x) \succeq 0
          $$
          <p>That is, the <b>Hessian matrix is positive semidefinite</b> (PSD) at every point.
          <br><b>Linear Algebra Connection:</b> This means all eigenvalues of $\nabla^2 f(x)$ are non-negative ($\lambda_i \ge 0$). Geometrically, the function curves "up" (or is flat) in every direction.</p>
        </div>

        <div class="insight">
          <h4>Strict vs. Strong Convexity</h4>
          <p>It is important to distinguish between levels of curvature:</p>
          <ul>
            <li><b>Strict Convexity:</b> $\nabla^2 f(x) \succ 0$ (PD) everywhere is a <i>sufficient</i> condition for strict convexity. It implies the graph is strictly curved up, ensuring at most one minimizer.</li>
            <li><b>Strong Convexity:</b> $\nabla^2 f(x) \succeq mI$ for some $m > 0$. This is a stronger condition, meaning the function is "at least as curved as a quadratic bowl" ($m\|x\|^2/2$). This guarantees existence and uniqueness of a minimizer and faster convergence of algorithms.</li>
          </ul>
        </div>

        <div class="proof-box">
          <h4>Proof: Convexity $\iff$ PSD Hessian (via Line Restriction)</h4>
          <p>We use the scalar chain rule on $\phi(t) = f(x+tv)$. Recall: $f$ is convex iff $\phi''(t) \ge 0$ for all lines.</p>

          <div class="proof-step">
            <strong>Part 1: Convex $\implies$ Hessian PSD.</strong>
            Assume $f$ is convex. Pick any $x \in \mathrm{dom} f$ and any direction $v \in \mathbb{R}^n$.
            Define the 1D function $g(t) = f(x + tv)$.
            Since $f$ is convex, $g$ is convex.
            For a twice-differentiable 1D function, convexity implies $g''(t) \ge 0$.
            By the chain rule:
            $$ g'(t) = \nabla f(x+tv)^\top v \implies g''(t) = v^\top \nabla^2 f(x+tv) v $$
            Evaluating at $t=0$: $g''(0) = v^\top \nabla^2 f(x) v \ge 0$.
            Since $\mathbf{v}$ was arbitrary, this means $\nabla^2 f(\mathbf{x})$ is PSD.
          </div>

          <div class="proof-step">
            <strong>Part 2: Hessian PSD $\implies$ Convex.</strong>
            Assume $\nabla^2 f(\mathbf{x}) \succeq 0$ for all $\mathbf{x}$.
            Consider any two points $\mathbf{x}, \mathbf{y}$ and define $g(t) = f(\mathbf{x} + t(\mathbf{y}-\mathbf{x}))$ for $t \in [0, 1]$.
            The second derivative is $g''(t) = (\mathbf{y}-\mathbf{x})^\top \nabla^2 f(\mathbf{z}) (\mathbf{y}-\mathbf{x})$, where $\mathbf{z}$ is on the line segment.
            By assumption, the Hessian is PSD at $z$, so $g''(t) \ge 0$ for all $t$.
            A function with non-negative second derivative is convex.
            Thus $g(t)$ is convex, which implies:
            $$ g(\theta) \le (1-\theta)g(0) + \theta g(1) $$
            Substituting back:
            $$ f(x + \theta(y-x)) \le (1-\theta)f(x) + \theta f(y) $$
            This is exactly the definition of convexity for $f$.
          </div>
        </div>

        <h3>7.2 Practical Verification</h3>

        <div class="proof-box">
          <h4>Example 1: Quadratic Function $f(x) = \|Ax - b\|_2^2$</h4>
          <p>We derive the Hessian explicitly to show convexity.</p>
          <div class="proof-step">
            <strong>Step 1: Expand.</strong>
            $$ f(x) = (Ax-b)^\top (Ax-b) = x^\top A^\top A x - 2b^\top A x + b^\top b $$
          </div>
          <div class="proof-step">
            <strong>Step 2: Gradient.</strong>
            $$ \nabla f(x) = 2A^\top A x - 2A^\top b = 2A^\top (Ax - b) $$
          </div>
          <div class="proof-step">
            <strong>Step 3: Hessian.</strong>
            $$ \nabla^2 f(x) = 2A^\top A $$
            For any vector $\mathbf{v}$, $\mathbf{v}^\top (2A^\top A) \mathbf{v} = 2(A\mathbf{v})^\top (A\mathbf{v}) = 2\|A\mathbf{v}\|_2^2 \ge 0$.
            Thus the Hessian is PSD everywhere, so least squares is convex.
          </div>
        </div>

        <div class="proof-box">
          <h4>Example 2: Log Barrier of Affine ($-\sum \log(b_i - a_i^\top x)$)</h4>
          <p>Let $f(x) = -\log(b - a^\top x)$ on domain $a^\top x < b$.
          <br>Gradient: $\nabla f(x) = \frac{a}{b-a^\top x}$.
          <br>Hessian: $\nabla^2 f(x) = \frac{aa^\top}{(b-a^\top x)^2}$.
          <br>Check PSD: $v^\top \nabla^2 f(x) v = \frac{(a^\top v)^2}{(b-a^\top x)^2} \ge 0$.
          <br><b>Sum of barriers:</b> If $F(x) = \sum -\log(b_i - a_i^\top x)$, then $\nabla^2 F(x)$ is a sum of rank-1 PSD matrices, hence PSD.</p>
        </div>

        <div class="proof-box">
          <h4>Example 3: Log-Sum-Exp (Full Derivation)</h4>
          <p>Let $f(x) = \log \left(\sum_{k=1}^n e^{x_k}\right)$. This function is a smooth approximation of the maximum function $\max(x)$.
          <br><b>Smooth Max Intuition:</b> $\max(x) \le f(x) \le \max(x) + \log n$. As we scale $x$, $f(x)$ hugs the max function.</p>
          <p>Define $z_k = e^{x_k}$ and $S = \sum_{k=1}^n z_k$. Then $f(x) = \log S$.</p>

          <div class="proof-step">
            <strong>Step 1: Gradient.</strong>
            Using the chain rule: $\frac{\partial f}{\partial x_i} = \frac{1}{S} \frac{\partial S}{\partial x_i} = \frac{1}{S} e^{x_i} = \frac{z_i}{S}$.
            <br>In vector form: $\nabla f(x) = \frac{1}{S} z$, which is the <b>softmax</b> vector (probability distribution).
          </div>

          <div class="proof-step">
            <strong>Step 2: Hessian (Element-wise).</strong>
            We differentiate $\frac{\partial f}{\partial x_i} = \frac{z_i}{S}$ with respect to $x_j$ using the quotient rule:
            $$
            \frac{\partial^2 f}{\partial x_j \partial x_i} = \frac{\partial}{\partial x_j} \left( z_i S^{-1} \right)
            = \frac{\partial z_i}{\partial x_j} S^{-1} + z_i \frac{\partial S^{-1}}{\partial x_j}
            $$
            <ul>
              <li>$\frac{\partial z_i}{\partial x_j} = z_i \delta_{ij}$ (since $x_j$ only affects $z_j$).</li>
              <li>$\frac{\partial S^{-1}}{\partial x_j} = -S^{-2} \frac{\partial S}{\partial x_j} = -S^{-2} z_j$.</li>
            </ul>
            Combining these:
            $$ \frac{\partial^2 f}{\partial x_j \partial x_i} = \frac{z_i \delta_{ij}}{S} - \frac{z_i z_j}{S^2} $$
            In matrix form:
            $$ \nabla^2 f(x) = \frac{1}{S}\mathrm{diag}(z) - \frac{1}{S^2} z z^\top $$
          </div>

          <div class="proof-step">
            <strong>Step 3: PSD Check via Cauchy-Schwarz.</strong>
            For any vector $v \in \mathbb{R}^n$, consider the quadratic form:
            $$
            v^\top \nabla^2 f(x) v = \frac{1}{S} v^\top \mathrm{diag}(z) v - \frac{1}{S^2} v^\top (z z^\top) v
            $$
            $$ = \frac{1}{S} \sum_i z_i v_i^2 - \frac{1}{S^2} \left(\sum_i z_i v_i\right)^2 $$
            Multiply by $S^2$ (which is positive) to clear denominators:
            $$ S^2 (v^\top \nabla^2 f(x) v) = S \sum_i z_i v_i^2 - \left(\sum_i z_i v_i\right)^2 $$
            Recall $S = \sum z_i$. We use Cauchy-Schwarz with vectors $a_i = \sqrt{z_i}v_i$ and $b_i = \sqrt{z_i}$:
            $$ \left(\sum z_i v_i\right)^2 = \left(\sum (\sqrt{z_i}v_i)(\sqrt{z_i})\right)^2 \le \left(\sum z_i v_i^2\right) \left(\sum z_i\right) = \left(\sum z_i v_i^2\right) S $$
            Rearranging gives $S \sum z_i v_i^2 - (\sum z_i v_i)^2 \ge 0$.
            <br>Thus $\mathbf{v}^\top \nabla^2 f(\mathbf{x}) \mathbf{v} \ge 0$ for all $\mathbf{v}$, so the Hessian is PSD.
          </div>
        </div>
        <div class="example">
            <h4>Example 4: Trace of Inverse</h4>
            <p>Let $f(X) = \mathrm{tr}(X^{-1})$ on $\mathbb{S}^n_{++}$. We can establish convexity in two ways:</p>
            <p><strong>Method 1: Spectral Functions</strong><br>
            The function $f(X)$ is a spectral function corresponding to the scalar function $g(x) = 1/x$. Since $g(x)$ is convex on $(0, \infty)$, the corresponding matrix trace function $f(X) = \sum_{i=1}^n g(\lambda_i(X))$ is convex on $\mathbb{S}^n_{++}$.</p>
            <p><strong>Method 2: Second Derivative</strong><br>
            Consider the restriction to a line $X(t) = X + tH$. The second derivative is given by:
            $$ \frac{d^2}{dt^2} f(X+tH) \Big|_{t=0} = 2 \mathrm{tr}(X^{-1} H X^{-1} H X^{-1}) $$
            Let $M = X^{-1/2} H X^{-1/2}$. Substituting $H = X^{1/2} M X^{1/2}$ into the trace expression:
            $$ 2 \mathrm{tr}(X^{-1} (X^{1/2} M X^{1/2}) X^{-1} (X^{1/2} M X^{1/2}) X^{-1}) = 2 \mathrm{tr}(X^{-1/2} M M X^{-1/2}) $$
            Using the cyclic property of the trace, this simplifies to:
            $$ 2 \mathrm{tr}(X^{-1} M^2) $$
            Since $M$ is symmetric, $M^2$ is positive semidefinite. Since $X^{-1}$ is positive definite, the trace of the product of a positive semidefinite matrix and a positive definite matrix is non-negative (proven by observing $\mathrm{tr}(X^{-1} M^2) = \mathrm{tr}(X^{-1/2} M^2 X^{-1/2}) \ge 0$). Thus, the second derivative is non-negative for all $H$, establishing convexity.</p>
        </div>

        <div class="proof-box">
          <h4>Deep Dive: Matrix Cauchy-Schwarz & Hilbert-Schmidt Inequality</h4>
          <p>The Frobenius inner product $\langle A, B \rangle = \mathrm{tr}(A^\top B)$ satisfies the Cauchy-Schwarz inequality:
          $$ (\mathrm{tr}(A^\top B))^2 \le \mathrm{tr}(A^\top A) \mathrm{tr}(B^\top B) = \|A\|_F^2 \|B\|_F^2 $$
          This allows us to prove the submultiplicativity of the Frobenius norm: $\|AB\|_F \le \|A\|_F \|B\|_F$.
          <br><b>Proof:</b>
          $$ \|AB\|_F^2 = \mathrm{tr}((AB)^\top AB) = \mathrm{tr}(B^\top A^\top A B) = \langle A^\top A, B B^\top \rangle $$
          By Matrix Cauchy-Schwarz:
          $$ \langle A^\top A, B B^\top \rangle \le \|A^\top A\|_F \|B B^\top\|_F $$
          Using singular values, $\|A^\top A\|_F^2 = \sum \sigma_i^4 \le (\sum \sigma_i^2)^2 = \|A\|_F^4$. So $\|A^\top A\|_F \le \|A\|_F^2$.
          <br>Combining these:
          $$ \|AB\|_F^2 \le \|A\|_F^2 \|B\|_F^2 \implies \|AB\|_F \le \|A\|_F \|B\|_F $$
          This inequality is useful when bounding Hessian terms involving matrix products.</p>
        </div>

        <div class="example">
            <h4>Example 5: Geometric Mean of Eigenvalues</h4>
            <p>Let $f(X) = (\det X)^{1/n}$ on $\mathbb{S}^n_{++}$.
            <br>This function is <b>concave</b>.
            <br><i>Proof:</i> This follows from the Minkowski Determinant Inequality:
            $$ \det(A+B)^{1/n} \ge \det(A)^{1/n} + \det(B)^{1/n} $$
            Also related to log-concavity of determinant: $\log \det X$ is concave.</p>
        </div>

        <div class="proof-box">
          <h4>Deep Dive: Hessian of $\ell_p$ Quasi-Norm ($0 < p \le 1$)</h4>
          <p>Consider $h(z) = (\sum_{i=1}^n z_i^p)^{1/p}$ on $\mathbb{R}_{++}^n$ with $0 < p \le 1$.
          <br>We verify concavity by showing the Hessian is negative semidefinite.
          <br>Let $S = \sum z_i^p$. Then $h = S^{1/p}$.
          <br><b>Gradient:</b> $\frac{\partial h}{\partial z_j} = \frac{1}{p}S^{1/p-1} (p z_j^{p-1}) = S^{1/p-1} z_j^{p-1}$.
          <br><b>Hessian entries:</b>
          $$ H_{jk} = S^{1/p-2} \left[ (1-p) z_j^{p-1} z_k^{p-1} + (p-1) S z_j^{p-2} \delta_{jk} \right] $$
          <b>Quadratic Form:</b> For any vector $\mathbf{v}$:
          $$ \mathbf{v}^\top H \mathbf{v} = (1-p) S^{1/p-2} \left( (\sum v_i z_i^{p-1})^2 - (\sum z_i^p)(\sum v_i^2 z_i^{p-2}) \right) $$
          Let $a_i = v_i z_i^{(p-2)/2}$ and $b_i = z_i^{p/2}$.
          The term in the parenthesis is $(\sum a_i b_i)^2 - (\sum b_i^2)(\sum a_i^2)$.
          <br>By <b>Cauchy-Schwarz</b>, this is $\le 0$. Since $1-p \ge 0$, we have $v^\top H v \le 0$.
          <br>Thus $h$ is concave.</p>
        </div>

        <div class="proof-box">
          <h4>Deep Dive: Hessian of Geometric Mean</h4>
          <p>Let $g(x) = (\prod x_i)^{1/n}$. We prove concavity directly via the Hessian.
          <br>Using $g(x) = e^{\phi(x)}$ with $\phi(x) = \frac{1}{n}\sum \log x_i$, we derived:
          $$ \nabla^2 g(x) = g(x) \left( u u^\top - \frac{1}{n} \mathrm{diag}(1/x_i^2) \right) $$
          where $u_i = \frac{1}{n x_i}$.
          <br>Testing the quadratic form with vector $\mathbf{v}$:
          $$ \mathbf{v}^\top \nabla^2 g(\mathbf{x}) \mathbf{v} \propto \left(\sum \frac{v_i}{n x_i}\right)^2 - \frac{1}{n} \sum \frac{v_i^2}{x_i^2} $$
          Let $a_i = v_i/x_i$. The condition for NSD becomes:
          $$ (\sum a_i)^2 \le n \sum a_i^2 $$
          This is exactly <b>Cauchy-Schwarz</b> applied to the vector $a$ and the all-ones vector $\mathbf{1}$: $(\mathbf{1}^\top a)^2 \le \|\mathbf{1}\|^2 \|a\|^2$.
          <br>Thus $\nabla^2 g(x) \preceq 0$, so the geometric mean is concave.</p>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Heatmap: Hessian Eigenvalue Analyzer</h3>
          <p><b>Visualize Convexity Through the Hessian:</b> For twice-differentiable functions, convexity is determined by the Hessian being positive semidefinite (PSD). This tool provides a visual test:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Choose a 2D function:</b> Select from various functions (quadratic, Rosenbrock, etc.)</li>
            <li><b>Heatmap display:</b> the color at each point shows the minimum eigenvalue of the Hessian $\nabla^2 f(x)$</li>
            <li><b>Interactive Probe:</b> Hover over the heatmap to see the local quadratic approximation. Observe how the local "bowl" or "saddle" shape relates to the Hessian eigenvalues.</li>
            <li><b>Interpret colors:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li><b>Blue/Green (positive):</b> PSD Hessian—locally convex (bowl)</li>
                <li><b>Red (negative):</b> Not PSD—locally concave or saddle point</li>
              </ul>
            </li>
            <li><b>Global convexity:</b> A function is convex if the entire region is blue/green (no red)</li>
          </ul>
          <p><i>Practical insight:</i> This is exactly how numerical optimization libraries verify convexity in practice—by checking that all eigenvalues of the Hessian are non-negative!</p>
          <div id="widget-hessian-heatmap" style="width: 100%; height: 550px; position: relative; max-width: 900px; margin: 0 auto; border-radius: 8px;"></div>
        </div>

        <div class="warning-box">
            <h4>Common Traps (Second-Order)</h4>
            <ol>
                <li><b>One-Point Check:</b> Checking $\nabla^2 f(x) \succeq 0$ at a specific point (e.g., the optimum) does NOT prove convexity. It must hold for <b>all</b> $x \in \operatorname{dom} f$.</li>
                <li><b>Boundary Issues:</b> Barriers like $-\log x$ blow up at the boundary. You must verify the Hessian on the <b>open</b> domain.</li>
                <li><b>PSD vs PD:</b> $\nabla^2 f(x) \succ 0$ implies strict convexity, but $\nabla^2 f(x) \succeq 0$ allows flat regions (non-unique minimizers).</li>
                <li><b>Symmetry:</b> If your computed Hessian is not symmetric, you made a calculation error.</li>
            </ol>
        </div>
      </section>



<div class="example-box">
          <h4>Example: Log Barrier Hessian</h4>
          <p>Consider $f(x) = -\sum \log(b_i - a_i^\top x)$. The domain is the open polyhedron $\{x \mid Ax < b\}$.</p>
          <p>Gradient: $\nabla f(x) = \sum \frac{a_i}{b_i - a_i^\top x}$.</p>
          <p>Hessian: $\nabla^2 f(x) = \sum \frac{a_i a_i^\top}{(b_i - a_i^\top x)^2}$.</p>
          <p>Since each term $a_i a_i^\top$ is a rank-1 PSD matrix and coefficients are positive, the sum is PSD. Thus $f$ is convex.</p>
        </div>


<section class="section-card" id="section-8">
        <h2>8. Summary and Modeling Patterns</h2>
        <p>A comprehensive summary of convexity-preserving operations, pitfalls to avoid, and standard modeling patterns.</p>

        <h3>8.0 When to Use Which Convexity Verification Method</h3>

        <p>This lecture introduced three characterizations of convexity. Each has different requirements and trade-offs. Here's a practical guide for <b>which tool to use when</b>:</p>

        <div class="insight">
          <h4>The Three Characterizations Compared</h4>
          <table class="data-table" style="width: 100%; border-collapse: collapse; margin-top: 1rem;">
            <thead>
              <tr style="background: var(--surface-2); border-bottom: 2px solid var(--border);">
                <th style="padding: 12px; text-align: left;">Method</th>
                <th style="padding: 12px; text-align: left;">Requirements</th>
                <th style="padding: 12px; text-align: left;">Pros</th>
                <th style="padding: 12px; text-align: left;">Cons</th>
                <th style="padding: 12px; text-align: left;">When to Use</th>
              </tr>
            </thead>
            <tbody>
              <tr style="border-bottom: 1px solid var(--border);">
                <td style="padding: 12px;"><b>Definition</b> (§1.2)<br><span style="font-size: 0.9em;">$f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$</span></td>
                <td style="padding: 12px;">None<br>(works for any function)</td>
                <td style="padding: 12px;">
                  • Always applicable<br>
                  • Direct from first principles<br>
                  • Works for non-smooth functions
                </td>
                <td style="padding: 12px;">
                  • Tedious algebra<br>
                  • No geometric insight<br>
                  • Hard to check all $\theta, x, y$
                </td>
                <td style="padding: 12px;">
                  <b>Use when:</b> Function is not differentiable<br>
                  <b>Example:</b> $f(x) = |x|$, $f(x) = \max\{g_1(x), g_2(x)\}$
                </td>
              </tr>
              <tr style="border-bottom: 1px solid var(--border);">
                <td style="padding: 12px;"><b>First-Order</b> (§6)<br><span style="font-size: 0.9em;">$f(y) \ge f(x) + \nabla f(x)^\top (y-x)$</span></td>
                <td style="padding: 12px;">Differentiable on open convex domain</td>
                <td style="padding: 12px;">
                  • Geometric intuition ("tangent underestimates")<br>
                  • Good for analysis<br>
                  • Connects to optimization (gradient descent)
                </td>
                <td style="padding: 12px;">
                  • Requires differentiability<br>
                  • Still need to verify for all $x, y$<br>
                  • Can be algebraically messy
                </td>
                <td style="padding: 12px;">
                  <b>Use when:</b> Function is differentiable but Hessian is hard to compute<br>
                  <b>Example:</b> Implicitly defined $f$, composite functions
                </td>
              </tr>
              <tr style="border-bottom: 1px solid var(--border);">
                <td style="padding: 12px;"><b>Second-Order</b> (§7)<br><span style="font-size: 0.9em;">$\nabla^2 f(x) \succeq 0$ for all $x$</span></td>
                <td style="padding: 12px;">Twice differentiable on open convex domain</td>
                <td style="padding: 12px;">
                  • <b>Easiest to check in practice</b><br>
                  • Reduces to eigenvalue problem<br>
                  • Clear linear algebra connection
                </td>
                <td style="padding: 12px;">
                  • Requires twice-differentiability<br>
                  • Computing Hessian can be tedious<br>
                  • Eigenvalue check needed for $n > 2$
                </td>
                <td style="padding: 12px;">
                  <b>Use when:</b> Function is twice differentiable<br>
                  <b>Example:</b> $f(x) = x^\top Q x$, $f(x) = e^x$, $f(x) = -\log x$<br>
                  <b>Rule of thumb:</b> <i>Try this first for smooth functions!</i>
                </td>
              </tr>
            </tbody>
          </table>
        </div>

        <h4>Decision Tree: Which Method Should I Use?</h4>
        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px; margin-top: 16px;">
          <ol style="margin-top: 0.5rem; line-height: 1.8;">
            <li><b>Is $f$ twice differentiable?</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>✓ <b>Yes</b> → Use <b>Second-Order Condition</b> (Hessian $\nabla^2 f(x) \succeq 0$). Easiest option!</li>
                <li>✗ <b>No</b> → Go to step 2.</li>
              </ul>
            </li>
            <li><b>Is $f$ differentiable (but not twice)?</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>✓ <b>Yes</b> → Use <b>First-Order Condition</b> (tangent underestimates). Good for analysis.</li>
                <li>✗ <b>No</b> → Go to step 3.</li>
              </ul>
            </li>
            <li><b>Function is non-smooth or complex?</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>→ Try <b>Composition Rules</b> (§5) if $f$ is built from known convex functions.</li>
                <li>→ Otherwise, fall back to <b>Definition</b> (check Jensen's inequality directly).</li>
              </ul>
            </li>
          </ol>
          <p style="margin-top: 1rem; margin-bottom: 0;"><b>Pro Tip:</b> In practice, most textbook functions are twice differentiable, so the Hessian test (second-order) is your workhorse. For real-world ML/optimization, functions are often non-smooth (e.g., $\ell_1$ regularization, hinge loss), so composition rules (§5) become essential.</p>
        </div>

        <h4>Examples: Applying the Decision Tree</h4>
        <div class="example-box" style="margin-top: 16px;">
          <p><b>Example 1:</b> $f(x) = |x|$ on $\mathbb{R}$</p>
          <ul>
            <li><b>Step 1:</b> Is $f$ twice differentiable? No (not even differentiable at $x=0$).</li>
            <li><b>Step 2:</b> Is $f$ differentiable? No.</li>
            <li><b>Step 3:</b> Use <b>definition</b> or recognize $|x| = \max\{x, -x\}$ (pointwise max of convex functions).</li>
          </ul>
          <p><b>Verification via definition:</b> For $x, y \in \mathbb{R}$ and $\theta \in [0,1]$:
          $$ |\theta x + (1-\theta)y| \le |\theta x| + |(1-\theta)y| = \theta|x| + (1-\theta)|y| $$
          by the triangle inequality. ✓ Convex.</p>
        </div>

        <div class="example-box" style="margin-top: 16px;">
          <p><b>Example 2:</b> $f(x) = x^2$ on $\mathbb{R}$</p>
          <ul>
            <li><b>Step 1:</b> Is $f$ twice differentiable? Yes.</li>
            <li><b>Use second-order condition:</b> $\nabla^2 f(x) = 2 > 0$ for all $x$. ✓ Convex (in fact, strictly convex).</li>
          </ul>
          <p><b>Why this is easier:</b> Computing the Hessian took one line. Verifying the definition would require showing $(\theta x + (1-\theta)y)^2 \le \theta x^2 + (1-\theta)y^2$ for all $\theta, x, y$ (doable, but more algebra).</p>
        </div>

        <div class="example-box" style="margin-top: 16px;">
          <p><b>Example 3:</b> $f(x) = e^{x^\top Q x}$ where $Q \succeq 0$ (implicitly defined via exponentiation of quadratic)</p>
          <ul>
            <li><b>Step 1:</b> Is $f$ twice differentiable? Yes.</li>
            <li><b>But:</b> Computing the Hessian of $e^{x^\top Q x}$ is tedious (chain rule + product rule).</li>
            <li><b>Alternative:</b> Use <b>composition rules</b> (§5). Note $f = h \circ g$ where $g(x) = x^\top Q x$ (convex since $Q \succeq 0$) and $h(u) = e^u$ (convex and non-decreasing). By composition rule, $f$ is convex. ✓</li>
          </ul>
          <p><b>Lesson:</b> Even for twice-differentiable functions, composition rules (§5) can be faster than Hessian computation.</p>
        </div>

        <h3>8.1 Pitfalls and Illegal Moves</h3>
        <p>A common pitfall is to assume the <b>minimum</b> of convex functions is convex. It is generally <b>not</b>.</p>
        <div class="insight">
          <h4>Counterexample</h4>
          <p>Let $f_1(x) = (x-1)^2$ and $f_2(x) = (x+1)^2$. Both are convex. Their minimum $g(x) = \min((x-1)^2, (x+1)^2)$ is "W"-shaped. At $x=0$, $g(0)=1$. However, at $x=1$ and $x=-1$, $g(\pm 1)=0$. The convex combination of minimal points gives $0 \cdot \frac{1}{2} + 0 \cdot \frac{1}{2} = 0$, but $g(0)=1 > 0$, violating the convexity inequality.</p>
        </div>

        <div class="warning-box" style="background: #fff5f5; border: 1px solid #fc8181; padding: 16px; border-radius: 8px; margin-top: 16px;">
          <h4 style="margin-top: 0; color: #c53030;">Illegal Moves (Ways to Destroy Convexity)</h4>
          <p>These operations generally result in non-convex functions:</p>
          <ul style="margin-bottom: 0;">
            <li><b>Products:</b> $f(x)g(x)$ (even if both convex, e.g., $x \cdot x^2$ is convex but $x \cdot (-x)$ is not).</li>
            <li><b>Ratios:</b> $f(x)/g(x)$ (unless specifically structured like perspective).</li>
            <li><b>Difference:</b> $f(x) - g(x)$ (DC functions are a broad class, but generally not convex).</li>
            <li><b>Composition with Decreasing Convex:</b> $h(g(x))$ where $h$ is convex but decreasing (e.g., $e^{-x^2}$).</li>
            <li><b>Separate Convexity:</b> Being convex in $x$ and convex in $y$ does <b>not</b> imply joint convexity in $(x,y)$. (Counterexample: $f(x,y) = xy$).</li>
          </ul>
        </div>

        <h3>8.2 Summary: A Complete "Legal Moves Checklist"</h3>
        <p>When you build or analyze a function, follow this "compiler" checklist. If your function passes, it is certified convex.</p>
        <ul>
            <li>Is it an <b>atom</b> from the standard library? (See Section 6).</li>
            <li>Is it a <b>nonnegative sum</b> of convex functions?</li>
            <li>Is it a <b>max/sup</b> of convex functions?</li>
            <li>Is it a convex function of an <b>affine expression</b> (affine precomposition)?</li>
            <li>Is it obtained by <b>partial minimization</b> of a jointly convex function?</li>
            <li>Is it a <b>monotone convex composition</b> (e.g., convex non-decreasing of convex)?</li>
            <li>Is it a <b>perspective</b> of a convex function?</li>
        </ul>
        <p>If <b>yes</b> $\to$ convex.<br>If <b>no</b> $\to$ stop. You likely need a specific proof engine (Hessian, definition, or line restriction).</p>

        <div class="recap-box" style="background: var(--surface-2); border: 1px dashed var(--primary-300); padding: 16px; border-radius: 8px;">
          <h4 style="margin-top: 0;">Quick Recognition Guide</h4>
          <ul style="margin-top: 0.5rem;">
            <li><b>Pointwise Max / Sup:</b> "Upper envelope". Intersection of epigraphs. (e.g., $\lambda_{\max}$, support function).</li>
            <li><b>Partial Minimization:</b> "Projection". Requires joint convexity. (e.g., distance to convex set, Schur complement).</li>
            <li><b>Composition:</b> Check curvature ("bowl vs dome") and monotonicity.</li>
            <li><b>Perspective:</b> Scales the domain and function value together; preserves convexity.</li>
          </ul>
        </div>

        <div class="widget-container" style="margin: 24px 0;">
          <h3 style="margin-top: 0;">Interactive Builder: Operations Preserving Convexity</h3>
          <p><b>Build Complex Convex Functions from Simple Ones:</b> This interactive tool lets you combine convex building blocks using convexity-preserving operations:</p>
          <ul style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            <li><b>Start with basic functions:</b> $x^2$, $|x|$, $e^x$, $-\log(x)$, etc.</li>
            <li><b>Apply operations:</b>
              <ul style="margin-left: 1.5rem; margin-top: 0.25rem;">
                <li>Nonnegative weighted sum: $w_1 f_1 + w_2 f_2$ (sliders for weights)</li>
                <li>Pointwise maximum: $\max\{f_1, f_2, \ldots\}$</li>
                <li>Composition: $h(g(x))$ with appropriate monotonicity</li>
                <li>Affine transformations: $f(Ax + b)$</li>
              </ul>
            </li>
            <li><b>Visualize the result:</b> See the graph of the combined function</li>
            <li><b>Verify convexity:</b> Test via Jensen's inequality and Hessian eigenvalues</li>
            <li><b>Export formula:</b> Get the mathematical expression for the constructed function</li>
          </ul>
          <p><i>Modeling power:</i> Almost every convex function you'll encounter in practice can be built using these operations! This is the foundation of disciplined convex programming (DCP) in tools like CVXPY.</p>
          <div id="widget-operations-preserving" style="width: 100%; height: 600px; position: relative; max-width: 900px; margin: 0 auto; border-radius: 8px;"></div>
        </div>

        <h3>8.3 Standard Epigraph Modeling Patterns</h3>
        <p>This section provides the exact "compiler rules" to translate mathematical expressions into standard LP/SOCP/SDP atoms. These are not heuristics; they are exact algebraic equivalences.</p>

        <div class="example-box">
            <h4>Phase 5.1: The Absolute Value Atom (LP)</h4>
            <p>The absolute value $|u|$ is the simplest non-linear convex function. It represents a "kink". LPs cannot see kinks, but they can see intersections of halfspaces.</p>
            <p><b>The Atomic Equivalence:</b> For any scalar $u$ and $s$:</p>
            $$ \boxed{|u| \le s \quad \Longleftrightarrow \quad -s \le u \le s \quad \Longleftrightarrow \quad \begin{cases} u \le s \\ -u \le s \end{cases}} $$
            <p><b>Why this works:</b> The epigraph of $|u|$ is a "V" shape. This "V" is exactly the intersection of two halfspaces: $s \ge u$ and $s \ge -u$.
            <br><b>Compiler Rule:</b> Whenever you see $| \text{affine}(x) | \le t$, replace it with two linear inequalities.</p>
        </div>

        <div class="example-box">
            <h4>Phase 5.2: The $\ell_1$ and $\ell_\infty$ Norms (LP)</h4>
            <p>These norms are "polyhedral norms" and can be modeled entirely with LPs.</p>

            <p><b>1. $\ell_\infty$ Norm (Max-Absolute):</b></p>
            $$ \|x\|_\infty \le t \iff \max_i |x_i| \le t \iff |x_i| \le t \ \forall i $$
            <p>Using the atom, this becomes $2n$ linear constraints:
            $$ -t \le x_i \le t \quad \forall i $$
            <b>Geometric shape:</b> A hypercube (box). "One slack ($t$) bounds many components."</p>

            <p><b>2. $\ell_1$ Norm (Sum-Absolute):</b></p>
            $$ \|x\|_1 \le t \iff \sum_i |x_i| \le t $$
            <p>We cannot just write $-t \le \sum x_i \le t$ (that's $|\sum x_i|$, not $\sum |x_i|$).
            <br><b>Correct Lifting:</b> Introduce a slack vector $s \in \mathbb{R}^n$ to bound <i>each</i> component's magnitude:
            $$ \begin{aligned} & |x_i| \le s_i \iff -s_i \le x_i \le s_i \quad \forall i \\ & \sum s_i \le t \end{aligned} $$
            <b>Geometric shape:</b> A cross-polytope (diamond). "Many slacks ($s_i$) sum to one bound."</p>
        </div>

        <div class="example-box">
            <h4>Phase 5.3: The Euclidean Norm Atom (SOCP)</h4>
            <p>The Euclidean norm $\|x\|_2$ corresponds to the <b>Second-Order Cone (SOC)</b>. This is not polyhedral.</p>
            <p><b>The Atom:</b> $\|Ax+b\|_2 \le c^\top x + d$ is exactly the constraint:</p>
            $$ (c^\top x + d, \ Ax+b) \in \mathcal{Q}_{m+1} $$
            <p><b>Solver Requirement:</b> The first component (RHS) must be non-negative.
            <br><b>Trap:</b> Do not square it! $\|Ax+b\|^2 \le (c^\top x + d)^2$ creates a quadratic-minus-quadratic term, which is not visibly convex to many solvers and can destroy numerical precision. Use the cone form.</p>
        </div>

        <div class="insight">
          <h4>Deep Dive: "Convex" vs "Representable"</h4>
          <p>Consider $\|Ax+b\| \le c^\top x + d$.
          <br><b>Convexity:</b> The function $g(x) = \|Ax+b\| - (c^\top x + d)$ is convex (sum of convex norm and linear). Thus $g(x) \le 0$ defines a convex set.
          <br><b>Representability:</b> A solver like MOSEK doesn't "know" $g(x)$. It knows the cone $\mathcal{K} = \{(z, t) : \|z\| \le t\}$. To map your problem to $\mathcal{K}$, you must ensure the RHS matches the cone variable $t$, which must be non-negative.
          <br><b>Lesson:</b> You must often add redundant constraints (like $c^\top x + d \ge 0$) to satisfy the solver's API, even if the math implies them.</p>
        </div>
      </section>


<section class="section-card" id="section-9">
        <h2><i data-feather="edit-3"></i> 9. Exercises</h2>

        <div class="problem">
          <h3>P5.1 — Convexity of Basic Functions</h3>
          <p>Determine whether the following functions are convex, concave, or neither. Prove your answer (e.g., using Hessian, composition rules).</p>
          <ol type="a">
            <li>$f(x) = e^{ax}$ on $\mathbb{R}$.</li>
            <li>$f(x) = x^a$ on $\mathbb{R}_{++}$ for $a \in \mathbb{R}$.</li>
            <li>$f(x) = |x|^p$ on $\mathbb{R}$ for $p \ge 1$.</li>
            <li>$f(x) = x \log x$ on $\mathbb{R}_{++}$.</li>
            <li>$u_\alpha(x) = \frac{x^\alpha - 1}{\alpha}$ for $\alpha \in (0,1]$ (CRRA Utility).</li>
            <li>$f(x) = \frac{1}{2}x^\top P x + q^\top x + r$ (Unconstrained Quadratic). Find the minimizer if $P \in \mathbb{S}^n_{++}$.</li>
          </ol>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Curvature Rules:</b> Convexity is fundamentally about "non-negative curvature" ($f'' \ge 0$).</li>
                <li><b>Composition Power:</b> Most complex convex functions are built from simple atoms (exp, norm, power) using composition rules that preserve curvature.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <ul>
              <li><b>(a) Convex:</b> $f''(x) = a^2 e^{ax} \ge 0$.</li>
              <li><b>(b) Depends on $a$:</b> We compute the second derivative: $f'(x) = a x^{a-1}$ and $f''(x) = a(a-1)x^{a-2}$. Since $x > 0$, the sign depends on the factor $a(a-1)$.
                <ul>
                  <li><b>Convex ($f'' \ge 0$):</b> Requires $a(a-1) \ge 0$. This holds when $a \ge 1$ or $a \le 0$.</li>
                  <li><b>Concave ($f'' \le 0$):</b> Requires $a(a-1) \le 0$. This holds when $0 \le a \le 1$.</li>
                </ul>
              </li>
              <li><b>(c) Convex:</b> Composition of convex $g(u)=u^p$ ($p \ge 1$) and convex $h(x)=|x|$. Since $g$ is increasing on $\mathbb{R}_+$, convexity is preserved.</li>
              <li><b>(d) Convex:</b> $f'(x) = 1 + \log x$, $f''(x) = 1/x > 0$. (Negative entropy).</li>
              <li><b>(e) Concave:</b> $u' = x^{\alpha-1} > 0$, $u'' = (\alpha-1)x^{\alpha-2}$. Since $\alpha \le 1$, $u'' \le 0$. Note: As $\alpha \to 0$, this approaches $\log x$.</li>
              <li><b>(f) Quadratic Minimization:</b>
                <ul>
                  <li><b>Convexity:</b> The Hessian is $\nabla^2 f(x) = P$. Since $P \in \mathbb{S}^n_{++}$ (given), $P \succ 0$, so $f$ is strictly convex.</li>
                  <li><b>Minimizer:</b> For a convex function, the global minimum occurs where the gradient is zero.
                  $$ \nabla f(x) = Px + q = 0 \implies Px = -q \implies x^\star = -P^{-1}q $$
                  This is the unique global minimizer.</li>
                </ul>
              </li>
            </ul>
          </div>
        </div>

        <div class="problem">
          <h3>P5.2 — Arithmetic-Geometric Mean Inequality</h3>
          <p>Prove the generalized AM-GM inequality using Jensen's inequality:
          $$ \sum_{i=1}^n \theta_i x_i \ge \prod_{i=1}^n x_i^{\theta_i} $$
          where $x_i > 0, \theta_i \ge 0, \sum \theta_i = 1$.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Jensen as a Generator:</b> Jensen's inequality is not just a definition; it is a machine for generating algebraic inequalities (AM-GM, Hölder, Minkowski).</li>
                <li><b>Choice of Function:</b> The key is choosing the right convex/concave function (here, $\log x$) to lift the arithmetic/geometric operations into the inequality.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              Let $f(u) = -\log u$. Since $f''(u) = 1/u^2 > 0$, $f$ is convex on $\mathbb{R}_{++}$.
            </div>
            <div class="proof-step">
              Apply Jensen's inequality:
              $$ -\log\left(\sum \theta_i x_i\right) \le \sum \theta_i (-\log x_i) = -\sum \log(x_i^{\theta_i}) = -\log\left(\prod x_i^{\theta_i}\right) $$
            </div>
            <div class="proof-step">
              Multiply by $-1$ (flipping inequality) and exponentiate (monotone):
              $$ \sum \theta_i x_i \ge \prod x_i^{\theta_i} $$
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P5.3 — Log-Sum-Exp</h3>
          <p>Show that $f(x) = \log(\sum_{i=1}^n e^{x_i})$ is convex.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Smooth Max:</b> Log-Sum-Exp ($\text{lse}(x)$) is the smooth, convex approximation of $\max_i x_i$.</li>
                <li><b>Key Properties:</b> It is convex, non-decreasing in each argument, and bounded: $\max x_i \le \text{lse}(x) \le \max x_i + \log n$.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <p><b>Method 1: Hessian.</b> (See Lecture derivation). $\nabla^2 f(x) = \text{diag}(p) - pp^\top$ where $p_i = e^{x_i}/\sum e^{x_k}$. By Cauchy-Schwarz, $v^\top \nabla^2 f v = \sum p_i v_i^2 - (\sum p_i v_i)^2 \ge 0$.</p>
            <p><b>Method 2: Hölder's Inequality (Restriction to a Line).</b>
            Consider $g(t) = f(x+tv) = \log(\sum e^{x_i+tv_i})$.
            We need $g$ to be convex. Let $\alpha, \beta$ be points on the line and $\theta \in [0,1]$.
            Let $u_i = e^{x_i + \alpha v_i}$ and $w_i = e^{x_i + \beta v_i}$.
            Then $e^{x_i + (\theta \alpha + (1-\theta)\beta)v_i} = u_i^\theta w_i^{1-\theta}$.
            We need to show $\log(\sum u_i^\theta w_i^{1-\theta}) \le \theta \log(\sum u_i) + (1-\theta) \log(\sum w_i)$.
            Exponentiating both sides: $\sum u_i^\theta w_i^{1-\theta} \le (\sum u_i)^\theta (\sum w_i)^{1-\theta}$.
            This is exactly <b>Hölder's Inequality</b> with exponents $1/\theta$ and $1/(1-\theta)$.</p>
          </div>
        </div>

        <div class="problem">
          <h3>P5.4 — Quadratic-over-Linear</h3>
          <p>Show that $f(x, y) = x^2/y$ is convex for $y > 0$. Generalize to vector case $f(x,y) = \|x\|^2/y$.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Perspective Transform:</b> $f(x,y) = x^2/y$ is the perspective of the parabola $x^2$. Perspective functions $tf(x/t)$ preserve convexity.</li>
                <li><b>Schur Connection:</b> The convexity of $x^2/y$ is the scalar equivalent of the Schur complement condition for matrix positive definiteness.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              <b>Scalar case:</b> The Hessian is:
              $$ \nabla^2 f(x,y) = \begin{bmatrix} 2/y & -2x/y^2 \\ -2x/y^2 & 2x^2/y^3 \end{bmatrix} = \frac{2}{y^3} \begin{bmatrix} y^2 & -xy \\ -xy & x^2 \end{bmatrix} = \frac{2}{y^3} \begin{bmatrix} y \\ -x \end{bmatrix} \begin{bmatrix} y & -x \end{bmatrix} $$
              This is a rank-1 PSD matrix (since $y^3 > 0$).
            </div>
            <div class="proof-step">
              <b>Vector case:</b> $f(x,y)$ is the perspective of the convex function $h(x) = \|x\|^2$.
              $f(x,y) = y h(x/y) = y \|x/y\|^2 = y \frac{\|x\|^2}{y^2} = \frac{\|x\|^2}{y}$.
              Since perspective preserves convexity, $f$ is convex.
            </div>
          </div>
        </div>

        <div class="problem">
          <h3>P5.5 — Distance to a Set</h3>
          <p>Let $C \subseteq \mathbb{R}^n$ be a convex set. Show that $d(x) = \inf_{y \in C} \|x - y\|$ is a convex function.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Partial Minimization:</b> Minimizing a jointly convex function $f(x,y)$ over one variable ($y$) yields a convex function in the other ($x$).</li>
                <li><b>Geometric Projection:</b> The epigraph of the result is the projection of the original epigraph. Projecting a convex set preserves convexity.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <p>This is a partial minimization.
            Let $F(x, y) = \|x - y\| + I_C(y)$, where $I_C(y) = 0$ if $y \in C$ else $\infty$.
            $F$ is jointly convex in $(x, y)$ because $\|x-y\|$ is convex (composition of norm with affine) and $I_C$ is convex (since $C$ is convex).
            Then $d(x) = \inf_y F(x, y)$. Partial minimization of a jointly convex function yields a convex function.</p>
          </div>
        </div>

        <div class="problem">
          <h3>P5.6 — Entropy</h3>
          <p>Show that the negative entropy function $f(x) = \sum_{i=1}^n x_i \log x_i$ is strictly convex on $\mathbb{R}^n_{++}$.</p>

          <div class="recap-box">
            <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
            <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
                <li><b>Separability:</b> Functions of the form $\sum f_i(x_i)$ inherit properties (like strict convexity) directly from their scalar components.</li>
                <li><b>Information Geometry:</b> Negative entropy induces the KL-divergence as its Bregman divergence, central to information theory and interior point methods.</li>
            </ul>
          </div>

          <div class="solution-box">
            <h4>Solution</h4>
            <div class="proof-step">
              The function is separable: $f(x) = \sum g(x_i)$ where $g(u) = u \log u$.
              A sum of strictly convex functions is strictly convex.
            </div>
            <div class="proof-step">
              Check $g(u)$:
              $$ g'(u) = 1 + \log u $$
              $$ g''(u) = 1/u $$
              On $\mathbb{R}_{++}$, $u > 0$, so $g''(u) > 0$. Thus $g$ is strictly convex.
              Consequently, $f$ is strictly convex.
            </div>
          </div>
<div class="problem">
  <h3>P5.7 — Convexity of Log-Sum-Exp via Cauchy-Schwarz</h3>
  <p>Prove that $f(x) = \log(\sum e^{x_i})$ is convex by restricting it to a line and using the Cauchy-Schwarz inequality on the second derivative.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Hessian Structure:</b> The Hessian of $\text{lse}(x)$ is the covariance matrix of the softmax probability distribution. Covariance matrices are always PSD.</li>
        <li><b>Cauchy-Schwarz:</b> The core inequality proving convexity is simply $(\mathbb{E}[X])^2 \le \mathbb{E}[X^2]$, which is Cauchy-Schwarz (or Jensen's).</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Let $g(t) = f(x+tv)$. We need $g''(t) \ge 0$.
      $$ g'(t) = \nabla f(x+tv)^\top v = \frac{\sum v_i e^{x_i+tv_i}}{\sum e^{x_i+tv_i}} $$
      Let $z_i = e^{x_i+tv_i}$ and $S = \sum z_i$. Then $g'(t) = \frac{1}{S} \sum v_i z_i$.
    </div>
    <div class="proof-step">
      Differentiating again:
      $$ g''(t) = \frac{S (\sum v_i^2 z_i) - (\sum v_i z_i)(\sum v_i z_i)}{S^2} $$
    </div>
    <div class="proof-step">
      We need $S \sum v_i^2 z_i \ge (\sum v_i z_i)^2$.
      Define the vectors $a, b \in \mathbb{R}^n$ with components:
      $$ a_i = v_i \sqrt{z_i}, \quad b_i = \sqrt{z_i} $$
      (Note: $z_i = e^{x_i+tv_i} > 0$, so $\sqrt{z_i}$ is real and positive).
    </div>
    <div class="proof-step">
      Calculate the terms for Cauchy-Schwarz:
      <ul>
        <li>$\|a\|_2^2 = \sum a_i^2 = \sum v_i^2 z_i$</li>
        <li>$\|b\|_2^2 = \sum b_i^2 = \sum z_i = S$</li>
        <li>$(a^\top b)^2 = (\sum a_i b_i)^2 = (\sum v_i z_i)^2$</li>
      </ul>
    </div>
    <div class="proof-step">
      By the Cauchy-Schwarz inequality $(a^\top b)^2 \le \|a\|_2^2 \|b\|_2^2$:
      $$ \left(\sum (v_i\sqrt{z_i})(\sqrt{z_i})\right)^2 \le \left(\sum v_i^2 z_i\right) \left(\sum z_i\right) $$
      Substitute $S = \sum z_i$:
      $$ (\sum v_i z_i)^2 \le (\sum v_i^2 z_i) S $$
      This implies $S (\sum v_i^2 z_i) - (\sum v_i z_i)^2 \ge 0$.
      Thus the numerator of $g''(t)$ is non-negative, so $g''(t) \ge 0$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P5.8 — Sensitivity Analysis using Subgradients</h3>
  <p>Let $p^*(u)$ be the optimal value of the perturbed problem $\min f_0(x)$ s.t. $f_i(x) \le u_i, Ax=b$. Prove that the optimal dual variables $\lambda^*$ provide a lower bound on the perturbed value: $p^*(u) \ge p^*(0) - \lambda^{*\top} u$. This implies $\lambda_i^* \approx -\frac{\partial p^*}{\partial u_i}$ (Shadow Prices).</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Weak Duality:</b> $p^*(u) \ge g(\lambda, \nu)$ for any feasible dual variables.</li><li><b>Global Inequality:</b> Convexity gives us a global linear underestimator, not merely a local derivative.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Perturbed Lagrangian.</strong>
    The primal problem is $\min f_0(x)$ s.t. $f_i(x) \le u_i$.
    The Lagrangian is $L(x, \lambda) = f_0(x) + \sum \lambda_i (f_i(x) - u_i) = f_0(x) + \lambda^\top f(x) - \lambda^\top u$.
    (Ignoring equality constraints for brevity, logic is identical).</div>
    <div class="proof-step"><strong>Step 2: Dual Function.</strong>
    $g(\lambda) = \inf_x (f_0(x) + \lambda^\top f(x) - \lambda^\top u) = (\inf_x L_0(x, \lambda)) - \lambda^\top u$.
    Where $L_0$ is the unperturbed Lagrangian. So $g(\lambda) = g_0(\lambda) - \lambda^\top u$.</div>
    <div class="proof-step"><strong>Step 3: Weak Duality.</strong>
    For any feasible $x$ of the perturbed problem and any $\lambda \ge 0$:
    $f_0(x) \ge g(\lambda) = g_0(\lambda) - \lambda^\top u$.
    Taking inf over $x$: $p^*(u) \ge g_0(\lambda) - \lambda^\top u$.</div>
    <div class="proof-step"><strong>Step 4: Use Optimal Dual.</strong>
    Let $\lambda^*$ be the optimal dual for the *unperturbed* problem ($u=0$).
    Then $g_0(\lambda^*) = p^*(0)$ (Strong Duality assumption).
    So:
    $$ p^*(u) \ge p^*(0) - \lambda^{*\top} u $$
    This shows $-\lambda^*$ is a subgradient of $p^*$ at $u=0$.</div>
  </div>
</div>

<div class="problem">
  <h3>P5.9 — Advanced Composition Rules (3.22)</h3>
  <p>Determine the convexity of the following functions and identify the composition rule used. ($x \in \mathbb{R}^n$).</p>
  <ol type="a">
    <li>$f(x) = -\log(-\log \sum_{i=1}^m e^{a_i^\top x + b_i})$ on domain $\sum e^{a_i^\top x + b_i} < 1$.</li>
    <li>$f(x, u, v) = -\sqrt{uv - x^\top x}$ on $u,v > 0, uv > x^\top x$.</li>
    <li>$f(x, u, v) = -\log(uv - x^\top x)$ on the same domain.</li>
    <li>$f(x, t) = -(t^p - \|x\|_p^p)^{1/p}$ on $t \ge \|x\|_p$ ($p>1$).</li>
    <li>$f(x, t) = -\log(t^p - \|x\|_p^p)$ on $t \ge \|x\|_p$ ($p>1$).</li>
  </ol>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Structural Analysis:</b> Don't differentiate blindly! Decompose complex functions into layers ($h(g(x))$).</li>
        <li><b>Core composition patterns (scalar inner):</b> (i) convex + nondecreasing $\circ$ convex $\Rightarrow$ convex; (ii) convex + nonincreasing $\circ$ concave $\Rightarrow$ convex; (iii) concave + nondecreasing $\circ$ concave $\Rightarrow$ concave; (iv) concave + nonincreasing $\circ$ convex $\Rightarrow$ concave. (Affine inner functions are always safe.)</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <p>We analyze domains and composition conditions carefully, tracking inner and outer functions step-by-step.</p>

    <div class="proof-step">
      <strong>(a) Log-Log-Sum-Exp.</strong>
      <p>Domain: $\{x \mid \sum e^{a_i^\top x + b_i} < 1\}$. We show $f(x)$ is convex via a chain of compositions.</p>
      <ul>
        <li><b>Step 1: Inner Sum.</b> Let $s(x) = \sum_{i=1}^m e^{a_i^\top x + b_i}$.
        Since $u_i(x) = a_i^\top x + b_i$ is affine and exponential is convex, $e^{u_i(x)}$ is convex. The sum of convex functions is convex.</li>
        <li><b>Step 2: Log of Sum.</b> Let $t(x) = \log s(x)$.
        The Log-Sum-Exp function is convex. Since $s(x) < 1$ on the domain, $t(x) \in (-\infty, 0)$.</li>
        <li><b>Step 3: Outer Function.</b> Let $\phi(z) = -\log(-z)$ for $z < 0$.
        Derivatives: $\phi'(z) = 1/(-z) > 0$ (Strictly increasing), $\phi''(z) = 1/z^2 > 0$ (Convex).
        So $\phi$ is convex and non-decreasing.</li>
        <li><b>Step 4: Composition.</b> $f(x) = \phi(t(x))$.
        We have a convex outer function $\phi$ which is non-decreasing, composed with a convex inner function $t(x)$.
        Rule: <b>Convex non-decreasing</b> of <b>Convex</b> $\implies$ <b>Convex</b>.</li>
      </ul>
    </div>

    <div class="proof-step">
      <strong>(b) Root-Quadratic.</strong>
      <p>Domain: $u>0, v>0, uv > x^\top x$. Rewrite as $f(x,u,v) = -\sqrt{u(v - x^\top x/u)}$.</p>
      <ul>
        <li><b>Step 1: Decomposition.</b> Let $s(x,u,v) = u$ and $t(x,u,v) = v - \frac{x^\top x}{u}$.
        $s$ is affine (so concave).
        For $t$, recall that $q(x,u) = x^\top x/u$ is convex (quadratic-over-linear). Thus $-q$ is concave, and $t = v - q$ is concave.</li>
        <li><b>Step 2: Outer Function.</b> Let $r(s,t) = -\sqrt{st}$ on $\mathbb{R}_{++}^2$.
        Hessian analysis shows $r$ is convex.
        Monotonicity: $\frac{\partial r}{\partial s} = -\frac{1}{2}\sqrt{t/s} < 0$ and $\frac{\partial r}{\partial t} = -\frac{1}{2}\sqrt{s/t} < 0$.
        So $r$ is non-increasing in both arguments.</li>
        <li><b>Step 3: Multi-argument Composition.</b>
        $f = r(s, t)$. Outer $r$ is convex and non-increasing in both args. Inner functions $s, t$ are concave.
        Rule: <b>Convex non-increasing</b> of <b>Concave</b> $\implies$ <b>Convex</b>.</li>
      </ul>
    </div>

    <div class="proof-step">
      <strong>(c) Log-Quadratic.</strong>
      <p>Using the result from (b):</p>
      <ul>
        <li><b>Step 1: Inner Concavity.</b> Let $w(x,u,v) = \sqrt{uv - x^\top x}$.
        From part (b), we know $-w$ is convex, so $w$ is <b>concave</b> ($w>0$).</li>
        <li><b>Step 2: Scalar Composition.</b>
        $f(x,u,v) = -\log(uv - x^\top x) = -\log(w^2) = -2\log w$.
        Let $\psi(z) = -2\log z$.
        $\psi'(z) = -2/z < 0$ (decreasing) and $\psi''(z) = 2/z^2 > 0$ (convex).
        Composition: $f = \psi(w)$.
        Rule: <b>Convex non-increasing</b> of <b>Concave</b> $\implies$ <b>Convex</b>.</li>
      </ul>
    </div>

    <div class="proof-step">
      <strong>(d) Norm Cone Concavity.</strong>
      <p>Let $s(x,t) = (t^p - \|x\|_p^p)^{1/p}$. We show $s$ is concave. We present two proofs.</p>
      <p><b>Proof 1: Geometric (Hypograph).</b></p>
      <ul>
        <li><b>Step 1: Hypograph.</b> $\text{hypo } s = \{(x,t,s) \mid s \le s(x,t)\}$.
        Since $s \ge 0$, this is equivalent to $s^p \le t^p - \|x\|_p^p \iff \|x\|_p^p + s^p \le t^p$.</li>
        <li><b>Step 2: Norm Cone.</b> The inequality $\|x\|_p^p + |s|^p \le t^p$ is exactly $\|(x,s)\|_p \le t$.
        This defines the $(n+1)$-dimensional $p$-norm cone $K_p = \{(y,t) \mid \|y\|_p \le t\}$.</li>
        <li><b>Step 3: Convexity.</b> Norm cones are convex sets. Therefore, the hypograph of $s$ is a convex set.</li>
      </ul>
      <p><b>Proof 2: Algebraic (Composition).</b></p>
      <ul>
        <li>Factor the expression: $t^p - \|x\|_p^p = t^{p-1} (t - \|x\|_p^p/t^{p-1})$.</li>
        <li>The term $q(x,t) = \|x\|_p^p/t^{p-1}$ is the perspective of the convex function $\|x\|_p^p$, so $q$ is convex.</li>
        <li>Thus $r(x,t) = t - q(x,t)$ is concave and positive on the domain.</li>
        <li>We can write $s(x,t) = (t^{p-1} r(x,t))^{1/p} = t^{(p-1)/p} r(x,t)^{1/p}$.</li>
        <li>This is the composition of the function $\Phi(a,b) = a^{(p-1)/p} b^{1/p}$ with $a=t$ (affine) and $b=r(x,t)$ (concave).</li>
        <li>$\Phi(a,b)$ is a weighted geometric mean, which is concave and non-decreasing in both arguments.</li>
        <li>Therefore, by the composition rule (concave non-decreasing outer $\circ$ concave inner), $s(x,t)$ is concave.</li>
      </ul>
      <p><b>Conclusion:</b> $f = -s$ is <b>Convex</b>.</p>
    </div>

    <div class="proof-step">
      <strong>(e) Log-Norm Cone.</strong>
      <p>$f(x,t) = -\log(t^p - \|x\|_p^p)$.</p>
      <ul>
        <li><b>Step 1: Rewrite.</b> $t^p - \|x\|_p^p = s(x,t)^p$.
        So $f = -\log(s(x,t)^p) = -p \log s(x,t)$.</li>
        <li><b>Step 2: Composition.</b>
        Inner: $s(x,t)$ is <b>concave</b> and positive (from part d).
        Outer: $\psi(z) = -p \log z$.
        $\psi'(z) = -p/z < 0$ (decreasing) and $\psi''(z) = p/z^2 > 0$ (convex).
        Rule: <b>Convex non-increasing</b> of <b>Concave</b> $\implies$ <b>Convex</b>.</li>
      </ul>
    </div>
  </div>
</div>

<div class="problem">
  <h3>P5.10 — Perspective Examples (3.23)</h3>
  <p>Show that the following functions are convex using the perspective transformation.</p>
  <ol type="a">
    <li>$f(x, t) = \|x\|_p^p / t^{p-1}$ for $p > 1, t > 0$.</li>
    <li>$f(x) = \frac{\|Ax+b\|_2^2}{c^\top x + d}$ on $c^\top x + d > 0$.</li>
  </ol>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Pattern Recognition:</b> Look for terms like $\|x\|^p/t^{p-1}$ or $x^2/y$. This "homogenization" suggests a perspective function.</li>
        <li><b>Convexity Preservation:</b> Perspective takes a convex function and creates a convex cone-like structure (related to the conic hull of the epigraph).</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>(a) Power Perspective.</strong>
      <ul>
        <li><b>Step 1: Base Function.</b> Let $\phi(z) = \sum_{i=1}^n |z_i|^p = \|z\|_p^p$.
        For $p \ge 1$, the map $u \mapsto |u|^p$ is convex, so $\phi$ is convex (sum of convex).</li>
        <li><b>Step 2: Perspective.</b> The perspective of $\phi$ is $g(x, t) = t \phi(x/t)$.
        $$ g(x, t) = t \sum \left|\frac{x_i}{t}\right|^p = t \sum \frac{|x_i|^p}{t^p} = \frac{1}{t^{p-1}} \sum |x_i|^p = \frac{\|x\|_p^p}{t^{p-1}} $$</li>
        <li><b>Conclusion:</b> Since the perspective operation preserves convexity, $f=g$ is convex on $t>0$.</li>
      </ul>
    </div>
    <div class="proof-step">
      <strong>(b) Euclidean Perspective Composition.</strong>
      <ul>
        <li><b>Step 1: Base Function.</b> Let $\phi(z) = \|z\|_2^2 = z^\top z$. This is convex (Hessian $2I \succ 0$).</li>
        <li><b>Step 2: Perspective.</b> $g(z, t) = t \phi(z/t) = \frac{\|z\|_2^2}{t}$ is convex for $t > 0$.</li>
        <li><b>Step 3: Affine Substitution.</b> Define the affine map:
        $z(x) = Ax + b$ and $t(x) = c^\top x + d$.
        Then $f(x) = g(z(x), t(x)) = \frac{\|Ax+b\|_2^2}{c^\top x + d}$.</li>
        <li><b>Conclusion:</b> The composition of a convex function $g$ with an affine map is convex.</li>
      </ul>
    </div>
  </div>
</div>

<div class="problem">
  <h3>P5.11 — Hessian of $\log f(x)$ and Log-Concavity</h3>
  <p>Let $f: \mathbb{R}^n \to \mathbb{R}_{++}$ be twice continuously differentiable.
  <br><b>(a)</b> Show that $\nabla^2 \log f(x) = \frac{1}{f(x)}\nabla^2 f(x) - \frac{1}{f(x)^2}\nabla f(x)\nabla f(x)^\top$.
  <br><b>(b)</b> Use this to show that if $f$ is log-concave, then $f$ satisfies the condition $f(x)\nabla^2 f(x) \preceq \nabla f(x)\nabla f(x)^\top$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <p>We provide an <b>ultra-granular derivation</b> from first principles to ensure every step is clear.</p>

    <div class="proof-step">
      <strong>Step 1: Compute the Gradient $\nabla h(x)$.</strong>
      Write $h(x)$ as a composition: $h(x) = \psi(f(x))$ where $\psi(s) = \log s$.
      <br>By the chain rule for a scalar-to-scalar map:
      $$ \frac{\partial h}{\partial x_i} = \psi'(f(x)) \cdot \frac{\partial f}{\partial x_i} = \frac{1}{f(x)} f_i(x) $$
      Stacking these into a vector gives the gradient:
      $$ \nabla h(x) = \frac{1}{f(x)} \nabla f(x) $$
    </div>

    <div class="proof-step">
      <strong>Step 2: Compute the Hessian Entry-by-Entry.</strong>
      We compute the partial derivative of the $i$-th gradient component $h_i(x) = f_i(x) / f(x)$ with respect to $x_j$:
      $$ h_{ij}(x) = \frac{\partial}{\partial x_j} \left( \frac{f_i(x)}{f(x)} \right) $$
      Using the <b>Quotient Rule</b> $(u/v)' = (u'v - uv')/v^2$:
      <ul>
        <li>Numerator part 1: $\frac{\partial f_i}{\partial x_j} f(x) = f_{ij}(x) f(x)$.</li>
        <li>Numerator part 2: $f_i(x) \frac{\partial f}{\partial x_j} = f_i(x) f_j(x)$.</li>
      </ul>
      Substituting back:
      $$ h_{ij} = \frac{f_{ij} f - f_i f_j}{f^2} = \frac{f_{ij}}{f} - \frac{f_i f_j}{f^2} $$
    </div>

    <div class="proof-step">
      <strong>Step 3: Convert to Matrix Form.</strong>
      We recognize the terms in the expression for $h_{ij}$:
      <ul>
        <li>$f_{ij}$ are the entries of the Hessian matrix $\nabla^2 f(x)$.</li>
        <li>$f_i f_j$ are the entries of the outer product matrix $\nabla f(x) \nabla f(x)^\top$.</li>
      </ul>
      Thus, in matrix notation:
      $$ \nabla^2 h(x) = \frac{1}{f(x)}\nabla^2 f(x) - \frac{1}{f(x)^2}\nabla f(x)\nabla f(x)^\top $$
    </div>

    <div class="proof-step">
        <strong>Directional Derivative Intuition.</strong>
        For any direction vector $v \in \mathbb{R}^n$, the quadratic form is:
        $$ v^\top \nabla^2 (\log f) v = \frac{v^\top \nabla^2 f v}{f} - \frac{(v^\top \nabla f)^2}{f^2} $$
        This expresses the curvature of $\log f$ as the "curvature of $f$ divided by $f$" minus the "squared slope divided by $f^2$".
        Since the squared slope term is always non-negative, the log function "subtracts curvature," pushing the function towards concavity.
        (For $f$ to be log-concave, we need this expression $\le 0$, meaning the curvature of $f$ must not exceed its squared slope relative to its value).
    </div>

    <div class="proof-step">
      <strong>(b) Log-Concavity Condition.</strong>
      $f$ is log-concave if and only if $h(x)$ is concave, which means $\nabla^2 h(x) \preceq 0$ (Negative Semidefinite).
      <br>Test with an arbitrary direction vector $v \in \mathbb{R}^n$:
      $$ v^\top \nabla^2 h(x) v \le 0 $$
      Substitute the formula:
      $$ \frac{1}{f} v^\top \nabla^2 f v - \frac{1}{f^2} v^\top (\nabla f \nabla f^\top) v \le 0 $$
      Note that $v^\top (\nabla f \nabla f^\top) v = (v^\top \nabla f)^2$. Multiplying by $f^2$ (which is positive):
      $$ f(x) (v^\top \nabla^2 f(x) v) - (v^\top \nabla f(x))^2 \le 0 $$
      This is equivalent to the matrix inequality:
      $$ f(x) \nabla^2 f(x) \preceq \nabla f(x) \nabla f(x)^\top $$
      <i>Geometric Interpretation:</i> The "curvature" of $f$ ($f \nabla^2 f$) must be bounded above by the "squared slope" ($\nabla f \nabla f^\top$). Logarithms flatten functions; for the result to be concave, the original function can't curve up too fast relative to its slope.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P5.12 — Log-Concavity of Shifted Function</h3>
  <p>Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is log-concave and $f(x) > a \ge 0$. Show that the function $g(x) = f(x) - a$ is log-concave.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <p>We need to show $H(x) = \log(f(x) - a)$ is concave.
    <br>Let $h(x) = \log f(x)$. Since $f$ is log-concave, $h$ is concave.
    <br>We can write $g(x) = e^{h(x)} - a$, so $H(x) = \log(e^{h(x)} - a)$.
    <br>Let $\phi(t) = \log(e^t - a)$. Then $H(x) = \phi(h(x))$.
    <br>This is a composition of a concave function $h(x)$ with $\phi(t)$.
    <br>For the composition to be concave, we need $\phi$ to be <b>concave</b> and <b>non-decreasing</b>.</p>
    <ul>
      <li><b>Monotonicity:</b> $\phi'(t) = \frac{e^t}{e^t - a}$. Since $e^t = f(x) > a$, denominator is positive. $\phi'(t) > 0$. Non-decreasing.</li>
      <li><b>Concavity:</b> $\phi''(t) = \frac{(e^t - a)e^t - e^t(e^t)}{(e^t - a)^2} = \frac{e^{2t} - a e^t - e^{2t}}{(e^t - a)^2} = \frac{-a e^t}{(e^t - a)^2}$.
      Since $a \ge 0$, $\phi''(t) \le 0$. Thus $\phi$ is concave.</li>
    </ul>
    <p><b>Conclusion:</b> Since $\phi$ is concave and increasing, and $h$ is concave, the composition $H(x) = \phi(h(x))$ is concave. Thus $g(x)$ is log-concave.</p>
  </div>
</div>

        </div>

      </section>

      <section class="section-card" id="section-10">
        <h2>10. Recap &amp; What's Next</h2>
        <div class="recap-box">
          <ul style="margin: 0 0 0 20px;">
            <li><b>Definition-first mindset:</b> convexity is Jensen's inequality plus a convex domain.</li>
            <li><b>Geometry:</b> epigraphs convert function questions into set questions (intersections and projections).</li>
            <li><b>Differentiable tests:</b> first-order (supporting hyperplanes) and second-order (Hessian PSD) conditions give fast convexity checks.</li>
            <li><b>Construction rules:</b> sums, pointwise maxima, affine precomposition, and perspectives generate new convex functions reliably.</li>
            <li><b>Strong convexity:</b> adds curvature, giving uniqueness of minimizers and stability guarantees.</li>
          </ul>
        </div>
        <div class="interpretation-box">
          <p style="margin: 0;"><b>Forward look:</b> <a href="../06-convex-functions-advanced/index.html">Lecture 06</a> introduces the convex conjugate (the algebra of supporting hyperplanes) and quasiconvexity/log-concavity. These become the main tools for duality (<a href="../09-duality/index.html">Lecture 09</a>) and for recognizing standard problem classes (<a href="../07-convex-problems-standard/index.html">Lecture 07</a>).</p>
        </div>
      </section>
    </article>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
