<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>11. Applications II: Statistical Estimation & Machine Learning — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="../../static/lib/katex/katex.min.js"></script>
  <script defer src="../../static/lib/katex/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="../../static/lib/katex/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
</head>
<body>
  <a href="#main" class="skip-link">Skip to content</a>
  <header class="site-header sticky" role="banner">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav" role="navigation" aria-label="Lecture Navigation">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../10-approximation-fitting/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../12-geometric-problems/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main id="main" class="lecture-content" role="main">
    <header class="lecture-header section-card">
      <h1>11. Applications II: Statistical Estimation & Machine Learning</h1>
      <div class="lecture-meta">
        <span>Date: 2026-01-13</span>
        <span>Duration: 90 min</span>
        <span>Tags: applications, ml, statistics</span>
      </div>
      <div class="lecture-summary">
        <p><strong>Overview:</strong> MLE, logistic regression, SVM, experiment design—convex optimization at the heart of ML.</p>
        <p><strong>Prerequisites:</strong> <a href="../10-approximation-fitting/index.html">Lecture 10</a></p>
      </div>
    </header>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul style="line-height: 1.8;">
        <li>Formulate maximum likelihood estimation (MLE) as a convex optimization problem, particularly for exponential families.</li>
        <li>Derive the mathematical formulations of logistic regression, support vector machines (SVM), and their duals.</li>
        <li>Apply convex optimization to experiment design (A-opt, D-opt, E-opt).</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>1. Maximum Likelihood Estimation (MLE)</h3>
      <p>MLE seeks parameters $\theta$ that maximize the probability of observed data. For an exponential family with density $p(y|\theta) = \exp(y^\top \theta - \psi(\theta))$, the negative log-likelihood is convex because the log-partition function $\psi(\theta)$ is convex. Minimizing $-\log L(\theta)$ is therefore a convex problem.</p>

      <h3>2. Logistic Regression and GLMs</h3>
      <p><b>Logistic Regression</b> models binary outcomes using the sigmoid function. The negative log-likelihood is a sum of softplus functions, which is convex. This framework extends to <b>Generalized Linear Models (GLMs)</b>, where the canonical link function ensures convexity of the negative log-likelihood for distributions like Poisson and Exponential.</p>

      <h3>3. Support Vector Machines (SVM)</h3>
      <p>SVMs find the hyperplane that separates classes with the maximum margin. The soft-margin SVM minimizes a trade-off between the $\ell_2$ norm of the weights (regularization) and the hinge loss (classification error). This is a convex quadratic program.</p>

      <h3>4. Regularization and MAP Estimation</h3>
      <p>Regularization adds a penalty term $R(\theta)$ to the negative log-likelihood, interpreting it as a prior in Bayesian <b>Maximum A Posteriori (MAP)</b> estimation. $\ell_2$ regularization corresponds to a Gaussian prior (Ridge), while $\ell_1$ corresponds to a Laplacian prior (LASSO), inducing sparsity.</p>

      <h3>5. Experiment Design</h3>
      <p>In experiment design, we choose measurement vectors $v_i$ to minimize the variance of our estimator. This leads to optimizing the information matrix $\sum \lambda_i v_i v_i^\top$. Different scalarizations lead to different designs: <b>D-optimal</b> (determinant) minimizes the confidence ellipsoid volume, while <b>E-optimal</b> (eigenvalue) minimizes the worst-case error. These are convex problems over the probability simplex.</p>
    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Classification Boundary</h3>
        <p>Visualize the decision boundary of a logistic regression or SVM classifier.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">SVM Margin Visualizer</h3>
        <p>Explore the effect of the C parameter on the margin of an SVM.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 (if exists) -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Logistic Regression Solver</h3>
        <p>An interactive solver for logistic regression.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

<!-- SECTION 12: APPENDIX -->
<section class="section-card" id="section-appendix">
  <h2>Appendix: Detailed Derivations</h2>

  <h3>A.1 Logistic Regression Hessian</h3>
  <div class="proof-box">
    <h4>Proof of Convexity</h4>
    <p>Loss: $J(w) = \sum \log(1+e^{z_i}) - y_i z_i$ where $z_i = w^\top x_i$.
    <br>Gradient: $\nabla J = \sum (\sigma(z_i) - y_i) x_i$.
    <br>Hessian: $\nabla^2 J = \sum \sigma'(z_i) x_i x_i^\top = \sum \sigma(z_i)(1-\sigma(z_i)) x_i x_i^\top$.
    <br>Since $\sigma(1-\sigma) > 0$, the Hessian is a sum of rank-1 PSD matrices, so it is PSD.</p>
  </div>

  <h3>A.2 SVM Dual Problem</h3>
  <div class="proof-box">
    <h4>Derivation</h4>
    <p>Lagrangian: $L = \frac{1}{2}\|w\|^2 + C\sum \xi_i - \sum \alpha_i(y_i(w^\top x_i+b)-1+\xi_i) - \sum \mu_i \xi_i$.
    <br>Stationarity ($w$): $w = \sum \alpha_i y_i x_i$.
    <br>Stationarity ($b$): $\sum \alpha_i y_i = 0$.
    <br>Stationarity ($\xi$): $C - \alpha_i - \mu_i = 0 \implies \alpha_i \le C$.
    <br>Substitute back: $\max \sum \alpha_i - \frac{1}{2}\sum \alpha_i \alpha_j y_i y_j x_i^\top x_j$ subject to $0 \le \alpha_i \le C, \sum \alpha_i y_i = 0$.</p>
  </div>

  <h3>A.3 Experiment Design (A-Optimal)</h3>
  <div class="proof-box">
    <h4>SDP Formulation</h4>
    <p>Minimize $\text{tr}((\sum \lambda_i v_i v_i^\top)^{-1})$.
    <br>Using epigraph variables $u_k$: $\min \sum u_k$ s.t. $e_k^\top (\sum \lambda_i v_i v_i^\top)^{-1} e_k \le u_k$.
    <br>By Schur complement, this is equivalent to:
    $$ \begin{bmatrix} \sum \lambda_i v_i v_i^\top & e_k \\ e_k^\top & u_k \end{bmatrix} \succeq 0 $$
    This is an SDP in $\lambda$ and $u$.</p>
  </div>
</section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 7 — Statistical Estimation</li>
        <li><strong>Course slides:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">PDF</a></li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <div class="example-box">
        <h4>Example 11.1: MLE for Gaussian Distribution</h4>
        <p><strong>Problem:</strong> Given $n$ i.i.d. samples $y_1, \ldots, y_n$ from $\mathcal{N}(\mu, \Sigma)$, formulate the MLE problem for estimating $\mu$ and $\Sigma$. Show it is convex in terms of natural parameters.</p>
        <p><strong>Solution:</strong></p>
        <p>The log-likelihood is:
        $$ \ell(\mu, \Sigma) = -\frac{n}{2}\log\det(2\pi\Sigma) - \frac{1}{2}\sum_{i=1}^n (y_i - \mu)^\top \Sigma^{-1} (y_i - \mu) $$
        In terms of $\mu$ and $\Sigma$, this is not jointly concave. However, change variables to the natural parameters of the exponential family: $P = \Sigma^{-1}$ (precision matrix) and $q = \Sigma^{-1}\mu$.
        $$ (y - \mu)^\top \Sigma^{-1} (y - \mu) = y^\top P y - 2 y^\top q + q^\top P^{-1} q $$
        The log-likelihood becomes (ignoring constants):
        $$ \mathcal{L}(P, q) = \frac{n}{2}\log\det(P) - \frac{1}{2}\sum_i (y_i^\top P y_i - 2 y_i^\top q + q^\top P^{-1} q) $$
        This involves the concave term $\log\det(P)$ and the convex term $q^\top P^{-1} q$ (matrix fractional function). We want to maximize $\mathcal{L}$, which means minimizing $-\mathcal{L}$.
        <br><b>Key subtlety:</b> The joint problem over both $\mu$ and $\Sigma$ is not jointly convex in the standard parameterization. However, we can decouple the estimation:
        <br>(1) Optimizing over $\mu$ in closed form gives $\hat{\mu} = \bar{y}$ (the sample mean).
        <br>(2) Substituting $\hat{\mu}$ back, optimizing over $P = \Sigma^{-1}$ gives $\hat{\Sigma} = \frac{1}{n}\sum (y_i-\bar{y})(y_i-\bar{y})^\top$.
        <br>If $\mu$ is known (or fixed at $\hat{\mu}$), the negative log-likelihood in $P$ is $-\frac{n}{2}\log\det(P) + \frac{1}{2}\text{tr}(SP)$ where $S = \frac{1}{n}\sum(y_i-\mu)(y_i-\mu)^\top$. This is convex in $P \succ 0$ (since $-\log\det$ is convex on the PSD cone and the trace term is linear).</p>
      </div>

      <div class="example-box">
        <h4>Example 11.2: Logistic Regression Gradient</h4>
        <p>The gradient of the logistic loss is $\nabla J(w) = \sum (\sigma(w^\top x_i) - y_i) x_i$. This intuitive result says that we update weights based on the correlation between the input $x_i$ and the prediction error $(\hat{y}_i - y_i)$. For the derivation of the Hessian and proof of convexity, see <a href="#section-appendix">Appendix A.1</a>.</p>
      </div>

      <div class="example-box">
        <h4>Example 11.3: Hard-Margin SVM Dual</h4>
        <p>The dual of the Hard-Margin SVM is a Quadratic Program (QP) that maximizes $\sum \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^\top x_j$ subject to $\alpha_i \ge 0$ and $\sum \alpha_i y_i = 0$. This formulation depends only on dot products $x_i^\top x_j$, enabling the "Kernel Trick". The full derivation parallels the Soft-Margin case in <a href="#section-appendix">Appendix A.2</a> (set $C=\infty$).</p>
      </div>

      <div class="example-box">
        <h4>Example 11.4: Soft-Margin SVM as Unconstrained Problem</h4>
        <p><strong>Problem:</strong> Show that soft-margin SVM is equivalent to minimizing hinge loss + L2 regularization.</p>
        <p><strong>Solution:</strong></p>
        <p>Primal: $\min \frac{1}{2}\|w\|^2 + C \sum \xi_i$ s.t. $y_i(w^\top x_i + b) \ge 1 - \xi_i, \xi_i \ge 0$.
        Rewrite constraints: $\xi_i \ge 1 - y_i(w^\top x_i + b)$ and $\xi_i \ge 0$.
        To minimize the objective, we choose the smallest possible $\xi_i$:
        $$ \xi_i = \max(0, 1 - y_i(w^\top x_i + b)) $$
        Substitute into objective:
        $$ \min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i(w^\top x_i + b)) $$
        This is unconstrained minimization of (Regularizer + $C \times$ Hinge Loss).</p>
      </div>

      <div class="example-box">
        <h4>Example 11.5: Poisson Regression</h4>
        <p><strong>Problem:</strong> Derive the loss function for Poisson regression.</p>
        <p><strong>Solution:</strong></p>
        <p>Model: $y \sim \text{Poisson}(\lambda)$ with $\lambda = e^{w^\top x}$.
        Likelihood: $P(y|x;w) = \frac{\lambda^y e^{-\lambda}}{y!}$.
        Log-likelihood: $\ell(w) = y \log \lambda - \lambda - \log(y!) = y(w^\top x) - e^{w^\top x} - \text{const}$.
        Negative Log-Likelihood (to minimize):
        $$ J(w) = \sum_{i=1}^n \left( e^{w^\top x_i} - y_i w^\top x_i \right) $$
        Gradient: $\sum (e^{w^\top x_i} - y_i) x_i$. Hessian: $\sum e^{w^\top x_i} x_i x_i^\top \succeq 0$.
        This is convex.</p>
      </div>

      <div class="example-box">
        <h4>Example 11.6: Softmax Regression Gradient</h4>
        <p><strong>Problem:</strong> Compute gradient for cross-entropy loss with softmax.</p>
        <p><strong>Solution:</strong></p>
        <p>Softmax: $p_k = \frac{e^{z_k}}{\sum_j e^{z_j}}$ where $z_k = w_k^\top x$.
        Cross-entropy loss for one sample (class $y$): $L = -\log p_y = -z_y + \log(\sum_j e^{z_j})$.
        Gradient w.r.t $z_k$:
        $$ \frac{\partial L}{\partial z_k} = -\mathbb{1}(y=k) + \frac{e^{z_k}}{\sum_j e^{z_j}} = p_k - \mathbb{1}(y=k) $$
        Gradient w.r.t $w_k$:
        $$ \nabla_{w_k} L = (p_k - \mathbb{1}(y=k)) x $$
        This leads to the standard update rule.</p>
      </div>

      <div class="example-box">
        <h4>Example 11.7: Regularized Logistic (L1)</h4>
        <p><strong>Problem:</strong> Formulate L1-regularized logistic regression.</p>
        <p><strong>Solution:</strong></p>
        <p>Objective: $f(w) = \sum \log(1+e^{-y_i w^\top x_i}) + \lambda \|w\|_1$.
        This is a composite convex problem: Smooth $g(w)$ + Non-smooth $h(w)$.
        Solved via Proximal Gradient Descent (ISTA):
        1. Gradient step: $w^{k+1/2} = w^k - \eta \nabla g(w^k)$
        2. Prox step: $w^{k+1} = S_{\eta \lambda}(w^{k+1/2})$
        Where $S_\tau(u)$ is soft-thresholding: $\text{sign}(u)\max(|u|-\tau, 0)$.
        This induces sparsity in $w$.</p>
      </div>

      <div class="example-box">
        <h4>Example 11.8: Robust MLE with Huber Loss</h4>
        <p><strong>Problem:</strong> Define the Huber loss and its use in robust regression.</p>
        <p><strong>Solution:</strong></p>
        <p>Huber loss $H_\delta(u)$:
        $$ H_\delta(u) = \begin{cases} \frac{1}{2}u^2 & |u| \le \delta \\ \delta(|u| - \frac{1}{2}\delta) & |u| > \delta \end{cases} $$
        This is convex, differentiable, and grows linearly for large errors (robust to outliers).
        Problem: $\min_w \sum_i H_\delta(y_i - w^\top x_i)$.
        Can be solved via L-BFGS or by converting to a QP (since Huber is the envelope of parabolas or via slack variables).</p>
      </div>
    </section>

    <!-- Exercises -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2><i data-feather="edit-3"></i> Exercises</h2>

      <div class="problem">
        <h3>P11.1 — SVM Dual Derivation (Soft Margin)</h3>
        <p>Derive the dual of the soft-margin SVM: $\min \frac{1}{2}\|w\|^2 + C \sum \xi_i$ s.t. $y_i(w^\top x_i + b) \ge 1-\xi_i, \xi_i \ge 0$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>Lagrangian: $L = \frac{1}{2}\|w\|^2 + C \sum \xi_i - \sum \alpha_i(y_i(w^\top x_i+b)-1+\xi_i) - \sum \mu_i \xi_i$.</p>
          <div class="proof-step">
            <strong>Optimality Conditions:</strong>
            <ol>
              <li>$\nabla_w L = w - \sum \alpha_i y_i x_i = 0 \implies w = \sum \alpha_i y_i x_i$</li>
              <li>$\nabla_b L = -\sum \alpha_i y_i = 0 \implies \sum \alpha_i y_i = 0$</li>
              <li>$\nabla_{\xi_i} L = C - \alpha_i - \mu_i = 0 \implies \alpha_i = C - \mu_i$. Since $\mu_i \ge 0$, we get $\alpha_i \le C$.</li>
            </ol>
          </div>
          <div class="proof-step">
            <strong>Dual Problem:</strong>
            $$ \max_\alpha \sum \alpha_i - \frac{1}{2}\sum_{ij} \alpha_i \alpha_j y_i y_j x_i^\top x_j $$
            Subject to: $0 \le \alpha_i \le C$ and $\sum \alpha_i y_i = 0$.
            The only difference from Hard-Margin is the box constraint $\alpha_i \le C$.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P11.2 — Logistic Hessian Analysis</h3>
        <p>Prove that the Hessian of the logistic loss is strictly positive definite if $X$ has full rank.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>$\nabla^2 J(w) = X^\top D X$ where $D = \text{diag}(p_i(1-p_i))$.</p>
          <div class="proof-step">
            For any $v \ne 0$: $v^\top \nabla^2 J(w) v = v^\top X^\top D X v = (Xv)^\top D (Xv)$.
            Let $u = Xv$. Since $X$ is full rank, $u \ne 0$ for $v \ne 0$.
            The term is $\sum u_i^2 p_i(1-p_i)$.
            Since $0 < p_i < 1$ (for finite weights), $p_i(1-p_i) > 0$.
            Thus the sum is strictly positive. $J(w)$ is strictly convex.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P11.3 — Poisson Regression Hessian</h3>
        <p>Derive the Hessian for Poisson regression $\ell(w) = \sum (e^{w^\top x_i} - y_i w^\top x_i)$ and show convexity.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>$\nabla \ell(w) = \sum (e^{w^\top x_i} - y_i) x_i$.</p>
          <div class="proof-step">
            $\nabla^2 \ell(w) = \sum e^{w^\top x_i} x_i x_i^\top$.
            Let $v$ be any vector. $v^\top \nabla^2 \ell(w) v = \sum e^{w^\top x_i} (x_i^\top v)^2$.
            Since exponential is positive and squared term is non-negative, the sum is $\ge 0$.
            Thus Hessian is PSD $\implies$ Convex.
          </div>
        </div>
      </div>

      <div class="problem">
        <h3>P11.4 — Experiment Design (A-optimal)</h3>
        <p>Formulate the A-optimal experiment design problem $\min \text{tr}((\sum \lambda_i v_i v_i^\top)^{-1})$ as an SDP. See <a href="#section-appendix">Appendix A.3</a> for the Schur Complement derivation.</p>
      </div>

      <div class="problem">
        <h3>P11.5 — L1 Proximal Operator</h3>
        <p>Derive $\text{prox}_{\lambda \|\cdot\|_1}(v) = \text{argmin}_x \frac{1}{2}\|x-v\|^2 + \lambda \|x\|_1$.</p>
        <div class="solution-box">
          <h4>Solution</h4>
          <p>The problem separates by coordinate: $\min_{x_i} \frac{1}{2}(x_i - v_i)^2 + \lambda |x_i|$.</p>
          <div class="proof-step">
            Subgradient optimality: $x_i - v_i + \lambda \partial |x_i| \ni 0$.
            <ul>
              <li>Case 1: $x_i > 0 \implies \partial = \{1\} \implies x_i - v_i + \lambda = 0 \implies x_i = v_i - \lambda$. Valid if $v_i > \lambda$.</li>
              <li>Case 2: $x_i < 0 \implies \partial = \{-1\} \implies x_i - v_i - \lambda = 0 \implies x_i = v_i + \lambda$. Valid if $v_i < -\lambda$.</li>
              <li>Case 3: $x_i = 0 \implies \partial = [-1, 1]$. Need $0 \in v_i - \lambda[-1, 1] \implies |v_i| \le \lambda$.</li>
            </ul>
            Combining: $x_i = \text{sign}(v_i)\max(|v_i|-\lambda, 0)$.
          </div>
        </div>
      </div>
    </section>
  </main></div>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="../../static/lib/pyodide/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initClassificationBoundaryVisualizer } from './widgets/js/classification-boundary.js';
    initClassificationBoundaryVisualizer('widget-1');
  </script>
  <script type="module">
    import { initSVMMargin } from './widgets/js/svm-margin.js';
    initSVMMargin('widget-2');
  </script>
  <script type="module">
    import { initLogisticRegression } from './widgets/js/logistic-regression.js';
    initLogisticRegression('widget-3');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>

  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
