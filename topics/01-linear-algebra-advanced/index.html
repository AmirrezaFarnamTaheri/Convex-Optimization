<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>01. Linear Algebra Advanced — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script src="https://unpkg.com/feather-icons"></script>
  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.128/build/three.module.js"
      }
    }
  </script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../00-linear-algebra-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../02-introduction/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>01. Linear Algebra Advanced</h1>
      <div class="lecture-meta">
        <span>Date: 2025-10-21</span>
        <span>Duration: 90 min</span>
        <span>Tags: prerequisites, linear-algebra, svd, qr, conditioning</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture bridges the gap between basic linear algebra and the robust numerical tools used in modern optimization. We move beyond the Normal Equations to explore numerically stable factorizations like QR and the Singular Value Decomposition (SVD), which reveal the geometry of linear maps in their purest form. We also introduce the Moore-Penrose pseudoinverse for solving ill-posed or rank-deficient problems and define the condition number—the fundamental metric for analyzing the stability of algorithms and the convergence speed of gradient descent.</p>
        <p><strong>Prerequisites:</strong> <a href="../00-linear-algebra-basics/index.html">Lecture 00: Linear Algebra Basics</a> (Subspaces, Norms, Least Squares).</p>
        <p><strong>Forward Connections:</strong> The SVD is central to the analysis of the Nuclear Norm and matrix completion in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>. The condition number is the key parameter determining the convergence rates of First-Order Methods in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a> and motivates the use of Newton's Method.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Factorize Matrices:</b> Compute and interpret the QR decomposition and Singular Value Decomposition (SVD) as fundamental tools for numerical linear algebra.</li>
        <li><b>Solve Robust Least Squares:</b> Apply QR and SVD to solve least squares problems without forming the potentially ill-conditioned matrix $A^\top A$.</li>
        <li><b>Handle Rank Deficiency:</b> Use the Moore-Penrose pseudoinverse to find minimum-norm solutions for rank-deficient or underdetermined systems.</li>
        <li><b>Analyze Stability:</b> Calculate condition numbers and estimate the sensitivity of linear systems to perturbations in data.</li>
        <li><b>Approximate Matrices:</b> Apply the Eckart-Young-Mirsky theorem for optimal low-rank matrix approximations (PCA) using truncated SVD.</li>
      </ul>
    </section>

    <!-- SECTION 1: SCALAR INVARIANTS -->
    <section class="section-card" id="section-invariants">
      <h2>1. Scalar Invariants: Determinant and Trace</h2>
      <p>A scalar invariant is a function of a matrix that does not change under similarity transformations ($A \mapsto P^{-1}AP$). Geometrically, these are properties of the linear operator itself, not its specific matrix representation. There are exactly two fundamental independent polynomial invariants: the Determinant and the Trace.</p>

      <h3>1.1 Determinant: The Unique Volume Form</h3>
      <p>The determinant is often taught as a recursive formula. It is better understood as a characterized object. It is the <b>unique</b> function $\det: \mathbb{F}^{n \times n} \to \mathbb{F}$ satisfying three axioms:</p>
      <ol>
        <li><b>Multilinearity:</b> Linear in each column separately.</li>
        <li><b>Alternating:</b> If two columns are equal, the value is zero. (Implies swapping columns flips the sign).</li>
        <li><b>Normalization:</b> $\det(I) = 1$.</li>
      </ol>
      <p>These axioms force the Leibniz formula: $\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{i=1}^n A_{i, \sigma(i)}$.</p>

      <div class="proof-box">
        <h4>Derivation: Why Axioms Force the Formula</h4>
        <p>This derivation shows that the determinant is not an arbitrary definition, but a necessary consequence of geometry.
        <br>1. <b>Expand columns:</b> Write each column $a_j$ as a linear combination of standard basis vectors $e_i$: $a_j = \sum_i A_{ij} e_i$.
        <br>2. <b>Apply Multilinearity:</b> Expanding $\det(a_1, \dots, a_n)$ involves $n$ sums.
        $$ \det(A) = \sum_{i_1, \dots, i_n} A_{i_1, 1} \dots A_{i_n, n} \det(e_{i_1}, \dots, e_{i_n}) $$
        <br>3. <b>Apply Alternation:</b> If any two indices $i_j, i_k$ are the same, $\det(e_{i_1}, \dots, e_{i_n}) = 0$. Thus, we only sum over <b>permutations</b> $\sigma$ where all indices are distinct.
        <br>4. <b>Apply Normalization:</b> Swapping columns to order the basis vectors $(e_{\sigma(1)}, \dots, e_{\sigma(n)})$ into $(e_1, \dots, e_n)$ requires sign flips.
        $$ \det(e_{\sigma(1)}, \dots, e_{\sigma(n)}) = \mathrm{sgn}(\sigma) \det(I) = \mathrm{sgn}(\sigma) $$
        <br><b>Result:</b> $\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma) \prod_{j=1}^n A_{\sigma(j), j}$.</p>
      </div>

      <h4>Geometric Interpretation: Volume and Orientation</h4>
      <p>For $A \in \mathbb{R}^{n \times n}$, $|\det(A)|$ is the volume of the parallelepiped spanned by the columns of $A$.
      <br><b>Properties forced by geometry:</b>
      <ul>
        <li><b>Invertibility:</b> $\det(A) \neq 0 \iff$ columns are linearly independent $\iff$ volume is non-zero.</li>
        <li><b>Multiplicativity:</b> $\det(AB) = \det(A)\det(B)$. The volume scaling of a composite map is the product of the individual scalings.</li>
        <li><b>Orientation:</b> The sign of $\det(A)$ indicates whether the map preserves or reverses orientation (e.g., reflection has det -1).</li>
      </ul>
      </p>

      <h3>1.2 Trace: The Unique Linear Invariant</h3>
      <p>The trace is the unique linear functional $\mathrm{tr}: \mathbb{F}^{n \times n} \to \mathbb{F}$ (up to scaling) that satisfies the <b>cyclic property</b>:
      $$ \mathrm{tr}(AB) = \mathrm{tr}(BA) $$
      This property implies similarity invariance: $\mathrm{tr}(P^{-1}AP) = \mathrm{tr}(APP^{-1}) = \mathrm{tr}(A)$. Thus, the trace is defined for linear operators, not just matrices.</p>

      <div class="proof-box">
        <h4>Proof: Uniqueness of Trace</h4>
        <p>Why is the sum of diagonal elements the <i>only</i> invariant?
        <br>Let $f: \mathbb{F}^{n \times n} \to \mathbb{F}$ be a linear functional satisfying $f(AB) = f(BA)$ and $f(I) = n$.
        <br>1. <b>Similarity Invariance:</b> $f(P A P^{-1}) = f(P^{-1} P A) = f(A)$.
        <br>2. <b>Reduction to Diagonal:</b> Over $\mathbb{C}$, any matrix is similar to an upper triangular matrix (Schur Form) with eigenvalues on the diagonal. By invariance, $f(A)$ depends only on these diagonal entries $\lambda_i$.
        <br>3. <b>Symmetry:</b> Permutation matrices are orthogonal, so $f$ is invariant under reordering $\lambda_i$. Linearity implies $f(\operatorname{diag}(\lambda_1, \dots, \lambda_n)) = c \sum \lambda_i$.
        <br>4. <b>Normalization:</b> $f(I) = c \sum 1 = cn = n \implies c=1$.
        <br>Thus, $f(A) = \sum \lambda_i = \sum A_{ii} = \mathrm{tr}(A)$.</p>
      </div>

      <h4>Geometric Interpretation: Infinitesimal Volume Change</h4>
      <p>While determinant is global volume scaling, trace is the <b>derivative</b> of volume.
      $$ \det(I + \epsilon A) = 1 + \epsilon \mathrm{tr}(A) + O(\epsilon^2) $$
      This connects linear algebra to calculus:
      <ul>
        <li><b>Divergence:</b> For a vector field $f(x) = Ax$, the divergence is $\nabla \cdot f = \mathrm{tr}(A)$. It measures the rate of flow expansion.</li>
        <li><b>Derivative of Det:</b> $\frac{d}{dt} \det(A(t)) = \det(A(t)) \mathrm{tr}(A(t)^{-1} A'(t))$.</li>
      </ul>
      </p>
    </section>

    <!-- SECTION 4: EIGENVALUES -->
    <section class="section-card" id="section-eigenvalues">
      <h2>2. The Spectral Layer: Eigenvalues and Diagonalization</h2>
      <p>We now ask: <b>Are there directions that a linear map preserves, changing only their length?</b> This is the spectral perspective. It collapses the complexity of matrix multiplication into simple scalar multiplication along specific axes.</p>

      <h3>2.1 Invariant Subspaces and Eigenvalues</h3>
      <p>The concept of eigenvalues begins with <b>invariant subspaces</b>. A subspace $U \subseteq V$ is $T$-invariant if $T(u) \in U$ for all $u \in U$. This allows us to restrict the operator to a smaller domain, simplifying the analysis.</p>
      <ul>
        <li><b>Trivial Invariants:</b> $\{0\}$, $V$, $\text{ker}(T)$, and $\text{im}(T)$ are always invariant.</li>
        <li><b>1D Invariants (Eigenvectors):</b> If a 1-dimensional subspace spanned by $v \neq 0$ is invariant, then $T(v)$ must be a multiple of $v$. This forces the condition $T(v) = \lambda v$.</li>
      </ul>

      <p>A non-zero vector $\mathbf{v}$ is an <b>eigenvector</b> with <b>eigenvalue</b> $\lambda$ if $T(\mathbf{v}) = \lambda \mathbf{v}$. This is a statement about invariant 1D subspaces.
      <br><b>The Logic of Detection:</b>
      $$ T(\mathbf{v}) = \lambda \mathbf{v} \iff (T - \lambda I)\mathbf{v} = 0 \iff \ker(T - \lambda I) \neq \{0\} $$
      Thus, $\lambda$ is an eigenvalue if and only if the operator $T - \lambda I$ is singular (not invertible). This is detected by the characteristic polynomial $p_T(\lambda) = \det(T - \lambda I) = 0$.
      <br><b>Basis Invariance:</b> Since $P(A-\lambda I)P^{-1} = PAP^{-1} - \lambda I$, similar matrices have the same characteristic polynomial and thus the same eigenvalues.</p>

      <h3>2.2 Diagonalization: When do Eigenvectors Form a Basis?</h3>
      <p>An operator $T$ is <b>diagonalizable</b> if there exists a basis of $V$ consisting of eigenvectors. In this basis, the matrix of $T$ is diagonal.
      <br><b>The Criterion:</b> Diagonalization is possible if and only if for every eigenvalue $\lambda$, the geometric multiplicity equals the algebraic multiplicity ($g_\lambda = m_\lambda$).
      <ul>
        <li><b>Algebraic Multiplicity ($m_\lambda$):</b> Multiplicity of $\lambda$ as a root of $p_T(\lambda)$.</li>
        <li><b>Geometric Multiplicity ($g_\lambda$):</b> Dimension of the eigenspace $E_\lambda = \ker(T - \lambda I)$.</li>
      </ul>
      <p><b>Distinct Eigenvalues implies Diagonalizable:</b> If an $n \times n$ matrix has $n$ distinct eigenvalues, the corresponding eigenvectors are linearly independent, forming a basis.</p>

      <h3>2.3 Jordan Canonical Form: When Diagonalization Fails</h3>
      <p>Diagonalization ($A = PDP^{-1}$) is the dream case: the linear map splits into independent scalings along axes. <b>Jordan Canonical Form</b> is what reality gives you when that dream fails. It tells you exactly <i>how</i> it fails, quantifying the "defect" in the matrix structure.</p>

      <h4>1) Why Diagonalization Fails (The Obstruction)</h4>
      <p>Recall the criterion: A matrix $A \in \mathbb{C}^{n \times n}$ is diagonalizable iff the total dimension of its eigenspaces equals $n$. Failure occurs exactly when, for some eigenvalue $\lambda$:</p>
      $$ \underbrace{\dim\ker(A-\lambda I)}_{\text{geometric multiplicity } g_\lambda} < \underbrace{\text{mult}_\lambda}_{\text{algebraic multiplicity } m_\lambda} $$
      <p><b>Interpretation:</b>
      <ul>
        <li>The characteristic polynomial promises $m_\lambda$ "slots" for eigenvalues.</li>
        <li>The eigenspace only supplies $g_\lambda$ genuine invariant directions.</li>
        <li>The missing $m_\lambda - g_\lambda$ directions must exist—but <b>not</b> as eigenvectors. They are hidden in the "generalized" structure.</li>
      </ul>
      </p>

      <h4>2) Generalized Eigenvectors: The Missing Link</h4>
      <p>If $\mathbf{v}$ is an eigenvector, $(A-\lambda I)\mathbf{v} = 0$. But what if there are vectors $\mathbf{w} \neq 0$ such that:
      $$ (A-\lambda I)\mathbf{w} = \mathbf{v} \quad \text{(an eigenvector)} $$
      Then $\mathbf{w}$ is <b>not</b> an eigenvector, but applying $A$ gives:
      $$ A\mathbf{w} = \lambda \mathbf{w} + \mathbf{v} $$
      So $\mathbf{w}$ is <i>almost</i> invariant—it scales by $\lambda$, but suffers a "drift" term aligned with $\mathbf{v}$. This is a <b>generalized eigenvector</b>.</p>

      <h4>3) Generalized Eigenspaces</h4>
      <p>For an eigenvalue $\lambda$, define the <b>generalized eigenspace</b>:
      $$ \mathcal{G}_\lambda := \ker\big((A-\lambda I)^n\big) $$
      <b>Facts:</b>
      <ul>
        <li>$\dim(\mathcal{G}_\lambda) = m_\lambda$. (It recovers the full algebraic multiplicity).</li>
        <li>$\ker(A-\lambda I) \subseteq \mathcal{G}_\lambda$.</li>
        <li>$\mathcal{G}_\lambda$ is invariant under $A$.</li>
      </ul>
      This space contains the missing dimensions.</p>

      <h4>4) The Jordan Block: The Atomic Failure Unit</h4>
      <p>The simplest non-diagonalizable behavior occurs in a <b>Jordan block</b>. A Jordan block $J_k(\lambda)$ of size $k$ is:</p>
      $$ J_k(\lambda) = \begin{bmatrix} \lambda & 1 & 0 & \dots \\ 0 & \lambda & 1 & \dots \\ \vdots & \ddots & \ddots & 1 \\ 0 & \dots & 0 & \lambda \end{bmatrix} $$
      <p><b>Properties:</b>
      <ul>
        <li>Diagonal entries are $\lambda$ (pure scaling).</li>
        <li>Superdiagonal entries are $1$ (coupling/drift).</li>
        <li>Geometric multiplicity is $1$ (only one true eigenvector $e_1$). Algebraic multiplicity is $k$.</li>
      </ul>
      This is the irreducible unit of defect.</p>

      <h4>5) Jordan Chains</h4>
      <p>A basis for the generalized eigenspace is built from <b>Jordan chains</b>:
      $$ (A-\lambda I)\mathbf{v}_1 = 0 \quad (\text{eigenvector}) $$
      $$ (A-\lambda I)\mathbf{v}_2 = \mathbf{v}_1 $$
      $$ \vdots $$
      $$ (A-\lambda I)\mathbf{v}_k = \mathbf{v}_{k-1} $$
      In this basis, the matrix takes the Jordan block form.</p>

      <div class="theorem-box">
        <h4>6) The Jordan Canonical Form Theorem</h4>
        <p>Let $A \in \mathbb{C}^{n \times n}$. There exists an invertible matrix $P$ such that:
        $$ \boxed{ A = P J P^{-1} } $$
        where $J$ is block diagonal:
        $$ J = \operatorname{diag}(J_{k_1}(\lambda_1), \dots, J_{k_r}(\lambda_r)) $$
        <b>Uniqueness:</b> The form is unique up to the ordering of blocks.
        <br><b>Structure:</b>
        <ul>
            <li>The total size of blocks for $\lambda$ equals $m_\lambda$.</li>
            <li>The number of blocks for $\lambda$ equals $g_\lambda$.</li>
        </ul>
        </p>
      </div>

      <h4>7) Decomposition: Scaling + Nilpotent</h4>
      <p>Jordan form decomposes a linear map into $A = S + N$, where:
      <ul>
        <li>$S = P \operatorname{diag}(\lambda_i) P^{-1}$ is <b>diagonalizable</b> (semisimple).</li>
        <li>$N$ is <b>nilpotent</b> ($N^n = 0$).</li>
        <li>$S$ and $N$ commute ($SN = NS$).</li>
      </ul>
      Diagonalizable matrices are exactly those where $N=0$. The nilpotent part $N$ measures the "defect".</p>

      <h4>8) Dynamics: Why Blocks Matter</h4>
      <p>Consider the discrete system $x_{t+1} = A x_t$. For a Jordan block:
      $$ J_k(\lambda)^t \approx \lambda^t \begin{bmatrix} 1 & t & t^2/2 & \dots \\ 0 & 1 & t & \dots \\ \vdots & & \ddots & \\ 0 & \dots & 0 & 1 \end{bmatrix} $$
      Even if $|\lambda| < 1$, the polynomial terms ($t, t^2$) cause <b>transient growth</b> before asymptotic decay. This explains why stability analysis based solely on eigenvalues ($\rho(A) < 1$) can miss short-term instabilities ("humps").</p>

      <h4>9) Invariants Preserved</h4>
      <p>Jordan form preserves all algebraic invariants: eigenvalues, algebraic/geometric multiplicities, and the minimal polynomial. However, it does <b>not</b> preserve geometric properties like orthogonality or singular values. It is an algebraic classification, not a geometric one.</p>

      <div class="insight">
        <h4>10) Real vs. Complex Fields</h4>
        <p>Over $\mathbb{C}$, Jordan form always exists. Over $\mathbb{R}$, if eigenvalues are complex, we get $2 \times 2$ rotation blocks instead of diagonal entries. This is why Spectral Theory (symmetric matrices) is so powerful: it guarantees real eigenvalues and orthogonal diagonalization, avoiding these complications.</p>
      </div>

      <h4>11) Numerical Instability</h4>
      <p>Jordan form is <b>discontinuous</b> with respect to matrix entries. An arbitrarily small perturbation can break a Jordan block into distinct eigenvalues (diagonalizable). Thus, Jordan form is a theoretical tool, not a computational one. For numerics, we use <b>Schur Decomposition</b> (unitary triangularization) or SVD.</p>

      <h4>12) Conceptual Summary</h4>
      <p><b>Diagonalization</b> fails when eigenvectors are insufficient. <b>Jordan form</b> shows that the failure is controlled: each eigenvalue carries a nilpotent "defect" measuring how far the map is from being diagonal.</p>

      <h4>13) The Big Picture</h4>
      <ul>
        <li><b>Spectral Theorem:</b> Perfect geometry (Self-Adjoint).</li>
        <li><b>Jordan Form:</b> Perfect algebra (Similarity classification).</li>
        <li><b>SVD:</b> Perfect geometry for <i>all</i> matrices.</li>
      </ul>

      <h3>2.4 The Spectral Theorem: Perfect Geometry</h3>
      <p><b>Theorem:</b> If $T$ is <b>self-adjoint</b> ($T = T^*$), then:
      <ol>
        <li>All eigenvalues are <b>real</b>.</li>
        <li>Eigenvectors corresponding to distinct eigenvalues are <b>orthogonal</b>.</li>
        <li>$V$ admits an <b>orthonormal basis of eigenvectors</b>.</li>
      </ol>
      This is the "perfect" case: no complex numbers, no defects, and the coordinate system is orthogonal.
      <br><b>Matrix Form:</b> If $A$ is symmetric, $A = Q \Lambda Q^\top$ with $Q$ orthogonal.
      <br><b>Why this matters:</b> This theorem underpins the geometry of ellipsoids ($\mathbf{x}^\top A \mathbf{x} = 1$), the analysis of Hessian curvature, and the convergence of optimization algorithms.</p>

      <h3>2.4 The Rayleigh Quotient</h3>
      <p>For a symmetric matrix $A$, the <b>Rayleigh Quotient</b> is defined as:
      $$ R_A(\mathbf{x}) = \frac{\mathbf{x}^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}} $$
      This scalar measures the "gain" of the matrix in the direction $\mathbf{x}$. Its critical points are the eigenvectors.</p>
      <div class="theorem-box">
        <h4>Variational Characterization of Eigenvalues</h4>
        <p>The eigenvalues of a symmetric matrix $A$ are the stationary values of the Rayleigh Quotient. Specifically:
        $$ \lambda_{\min}(A) = \min_{\mathbf{x} \ne 0} R_A(\mathbf{x}) \quad \text{and} \quad \lambda_{\max}(A) = \max_{\mathbf{x} \ne 0} R_A(\mathbf{x}) $$
        </p>
      </div>
      <p>This connects linear algebra to optimization: finding the largest eigenvalue is equivalent to maximizing a quadratic form over the unit sphere.</p>
    </section>

    <!-- SECTION 5: INDUCED NORMS -->
    <section class="section-card" id="section-induced-norms">
      <h2>3. Induced Matrix Norms</h2>
      <p>We defined vector norms in Lecture 00. Now we ask: how "big" is a matrix $A$? The <b>induced norm</b> measures the maximum factor by which $A$ can stretch a vector.</p>
      $$ \|A\|_p = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|_p}{\|\mathbf{x}\|_p} = \max_{\|\mathbf{x}\|_p = 1} \|A\mathbf{x}\|_p $$

      <h3>3.1 The $\ell_1$ Operator Norm (Max Column Sum)</h3>
      <p>$$ \|A\|_1 = \max_{j} \sum_{i=1}^m |a_{ij}| $$</p>
      <div class="proof-box">
        <h4>Derivation</h4>
        <p>Let $\mathbf{x}$ be a unit vector in $\ell_1$. Then $A\mathbf{x} = \sum x_j a_j$ (linear combination of columns).
        $$ \|A\mathbf{x}\|_1 = \|\sum_j x_j a_j\|_1 \le \sum_j |x_j| \|a_j\|_1 \le (\max_j \|a_j\|_1) \sum_j |x_j| = \max_j \|a_j\|_1 $$
        Equality is achieved by choosing $\mathbf{x} = e_k$ where $k$ is the index of the column with the largest norm.</p>
      </div>

      <h3>3.2 The $\ell_\infty$ Operator Norm (Max Row Sum)</h3>
      <p>$$ \|A\|_\infty = \max_{i} \sum_{j=1}^n |a_{ij}| $$</p>
      <p><i>Intuition:</i> To maximize the $\infty$-norm of $A\mathbf{x}$, we want to maximize a single component $|(A\mathbf{x})_i| = |\sum_j a_{ij} x_j|$. With constraints $|x_j| \le 1$, we set $x_j = \text{sign}(a_{ij})$ to make all terms positive and maximal.</p>

      <h3>3.3 The $\ell_2$ Operator Norm (Spectral Norm)</h3>
      <p>$$ \|A\|_2 = \max_{\|\mathbf{x}\|_2=1} \|A\mathbf{x}\|_2 = \sqrt{\lambda_{\max}(A^\top A)} = \sigma_{\max}(A) $$</p>
      <p>This is the square root of the maximum eigenvalue of $A^\top A$, which is the largest <b>singular value</b> of $A$. It represents the maximum stretch of the matrix in the Euclidean sense. This norm plays a crucial role in convergence analysis, as it dictates the worst-case amplification of errors.</p>

      <h3>3.4 Convexity of Induced Norms</h3>
      <p>Why are induced norms convex functions of the matrix $A$? We can prove this using the property that the pointwise supremum of linear functions is convex.</p>
      <div class="proof-box">
        <h4>Proof: Convexity via Dual Norms</h4>
        <div class="proof-step">
          <strong>Step 1: Definition.</strong> The induced norm is defined as:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\mathbf{v} \ne 0} \frac{\|X\mathbf{v}\|_a}{\|\mathbf{v}\|_\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \|X\mathbf{v}\|_a $$
        </div>
        <div class="proof-step">
          <strong>Step 2: Dual Norm Representation.</strong> Recall that any norm $\|z\|_a$ can be expressed as a supremum over its dual norm ball: $\|z\|_a = \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top \mathbf{z}$.
          Substituting this into the definition:
          $$ \|X\|_{a,\mathbf{b}} = \sup_{\|\mathbf{v}\|_\mathbf{b} = 1} \left( \sup_{\|\mathbf{u}\|_{a^*} \le 1} \mathbf{u}^\top (X\mathbf{v}) \right) $$
        </div>
        <div class="proof-step">
          <strong>Step 3: Linear Structure.</strong>
          $$ \|X\|_{a,\mathbf{b}} = \sup \{ \mathbf{u}^\top X \mathbf{v} \mid \|\mathbf{u}\|_{a^*} \le 1, \|\mathbf{v}\|_\mathbf{b} = 1 \} $$
          Notice that for fixed vectors $\mathbf{u}$ and $\mathbf{v}$, the function $f_{\mathbf{u},\mathbf{v}}(X) = \mathbf{u}^\top X \mathbf{v}$ is <b>linear</b> in the entries of $X$.
          Specifically, $\mathbf{u}^\top X \mathbf{v} = \mathrm{tr}(\mathbf{u}^\top X \mathbf{v}) = \mathrm{tr}(\mathbf{v} \mathbf{u}^\top X) = \langle uv^\top, X \rangle$.
        </div>
        <div class="proof-step">
          <strong>Conclusion:</strong>
          The induced norm $\|X\|_{a,b}$ is the <b>pointwise supremum</b> of a family of linear functions. Since the supremum of any collection of convex functions is convex, the induced norm is a convex function of $X$.
        </div>
      </div>
    </section>

    <!-- SECTION 6: QR DECOMPOSITION -->
    <section class="section-card" id="section-qr">
      <h2>4. The QR Decomposition: Geometry First</h2>

      <p>QR is not just an algorithm. It is the <b>algebraic shadow</b> of the Gram-Schmidt process. It formalizes the fact that every matrix can be split into "perfect geometry" ($Q$) and "coordinates" ($R$).</p>

      <h3>4.1 Definition and Existence</h3>
      <p>Let $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ (Full column rank). There exists a unique factorization:
      $$ \boxed{ A = Q R } $$
      where:
      <ul>
        <li>$Q \in \mathbb{R}^{m \times n}$ has <b>orthonormal columns</b> ($Q^\top Q = I_n$). This represents an isometric embedding.</li>
        <li>$R \in \mathbb{R}^{n \times n}$ is <b>upper triangular</b> with positive diagonal entries. This represents coordinates in the orthonormal basis.</li>
      </ul>
      <p><b>Proof by Geometry (Gram-Schmidt as Projection):</b> Let $a_1, \dots, a_n$ be the columns of $A$. At each step $k$, we seek a vector $q_k$ that captures the "new direction" of $a_k$ orthogonal to the previous subspace $W_{k-1} = \mathrm{span}(a_1, \dots, a_{k-1})$.
      <br>There is exactly one way to do this:
      $$ u_k = a_k - \operatorname{proj}_{W_{k-1}}(a_k) $$
      This residual $u_k$ is orthogonal to $W_{k-1}$ by the projection theorem. Normalizing it gives $q_k = u_k / \|u_k\|$.
      <br>Since $a_k$ is a linear combination of $q_k$ and the previous basis vectors ($q_1, \dots, q_{k-1}$), we have a triangular relation:
      $$ a_k = \sum_{j=1}^k r_{jk} q_j $$
      where $r_{jk} = \langle q_j, a_k \rangle$. This forces the matrix form $A=QR$, where $R$ is upper triangular. The diagonal entries $r_{kk} = \|u_k\|$ are positive, ensuring uniqueness.</p>

      <h3>4.2 Why QR is the "Correct" Way to Solve Least Squares</h3>
      <p>Consider $\min \|A\mathbf{x} - \mathbf{b}\|_2$.
      Using Normal Equations: $A^\top A \mathbf{x} = A^\top \mathbf{b}$. The condition number is $\kappa(A^\top A) = \kappa(A)^2$. This squaring doubles the number of digits of precision lost.
      <br>Using QR:
      $$ \|A\mathbf{x} - \mathbf{b}\|_2 = \|QR\mathbf{x} - \mathbf{b}\|_2 $$
      Since $Q$ preserves norms (on its range), we multiply the residual by $Q^\top$ (projecting onto the geometry):
      $$ R\mathbf{x} = Q^\top \mathbf{b} $$
      This is a triangular system. The condition number is $\kappa(R) = \kappa(A)$.
      <br><b>Conclusion:</b> QR is numerically stable because it separates the geometry ($Q$) from the conditioning ($R$). It never squares the condition number.</p>

      <h3>4.3 Full vs. Thin QR</h3>
      <p>
      <ul>
        <li><b>Thin QR ($Q \in \mathbb{R}^{m \times n}$):</b> Only computes an orthonormal basis for the column space. Sufficient for least squares.</li>
        <li><b>Full QR ($Q \in \mathbb{R}^{m \times m}$):</b> Extends $Q$ to a basis for the entire space $\mathbb{R}^m$. $A = [Q_1 \ Q_2] \begin{bmatrix} R \\ 0 \end{bmatrix}$. $Q_2$ forms a basis for the left nullspace $\mathcal{N}(A^\top)$.</li>
      </ul>
      </p>
    </section>

    <!-- SECTION 7: SVD -->
    <section class="section-card" id="section-svd">
      <h2>5. Singular Value Decomposition (SVD): The True Geometry</h2>
      <p>If the spectral theorem was "perfect geometry under symmetry," and QR was "geometry separated from coordinates," then <b>SVD is the fundamental theorem of linear algebra</b>. It applies to every matrix, square or rectangular, symmetric or not.</p>

      <h3>5.1 The Geometry of SVD: Rotate → Scale → Rotate</h3>
      <p><b>Theorem (SVD):</b> Any matrix $A \in \mathbb{R}^{m \times n}$ can be factored as:
      $$ \boxed{ A = U \Sigma V^\top } $$
      where:
      <ul>
        <li>$U \in \mathbb{R}^{m \times m}$ is orthogonal (Left singular vectors, basis for codomain).</li>
        <li>$V \in \mathbb{R}^{n \times n}$ is orthogonal (Right singular vectors, basis for domain).</li>
        <li>$\Sigma \in \mathbb{R}^{m \times n}$ is diagonal with non-negative entries $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$.</li>
      </ul>
      <p><b>Interpretation:</b>
      $$ x \xrightarrow{V^\top} \text{rotate coordinates} \xrightarrow{\Sigma} \text{independent scaling} \xrightarrow{U} \text{rotate to output basis} $$
      This proves that the image of the unit sphere under <i>any</i> linear map is a hyperellipse.</p>

      <div class="proof-box">
        <h4>Rigorous Derivation via $A^\top A$</h4>
        <p>The existence of the SVD is a direct consequence of the Spectral Theorem applied to the symmetric positive semidefinite matrix $G = A^\top A$. This matrix captures the geometry of the domain metric induced by $A$.</p>
        <div class="proof-step">
            <strong>Step 1: Spectral Decomposition of $G$.</strong>
            Since $G = A^\top A$ is symmetric, there exists an orthonormal basis $\{v_1, \dots, v_n\}$ of eigenvectors with real eigenvalues $\lambda_1 \ge \dots \ge \lambda_n$.
            $$ A^\top A v_i = \lambda_i v_i $$
            Since $\|A v_i\|^2 = v_i^\top A^\top A v_i = \lambda_i \|v_i\|^2 = \lambda_i$, we must have $\lambda_i \ge 0$.
        </div>
        <div class="proof-step">
            <strong>Step 2: Define Singular Values and Right Singular Vectors.</strong>
            Set $\sigma_i = \sqrt{\lambda_i}$. Let $r$ be the number of non-zero singular values (the rank).
            The matrix $V = [v_1 \dots v_n]$ is orthogonal.
        </div>
        <div class="proof-step">
            <strong>Step 3: Construct Left Singular Vectors.</strong>
            For $i \le r$, define $u_i = \frac{1}{\sigma_i} A v_i$.
            Check orthonormality:
            $$ u_i^\top u_j = \frac{1}{\sigma_i \sigma_j} v_i^\top A^\top A v_j = \frac{1}{\sigma_i \sigma_j} v_i^\top (\lambda_j v_j) = \frac{\sigma_j^2}{\sigma_i \sigma_j} \delta_{ij} = \delta_{ij} $$
            For $i > r$, $A v_i = 0$. We complete the set $\{u_1, \dots, u_r\}$ to an orthonormal basis $\{u_1, \dots, u_m\}$ for $\mathbb{R}^m$ (e.g., via Gram-Schmidt).
        </div>
        <div class="proof-step">
            <strong>Step 4: Matrix Factorization.</strong>
            Compute $A V$:
            $$ A v_i = \begin{cases} \sigma_i u_i & i \le r \\ 0 & i > r \end{cases} $$
            This is exactly the column-wise definition of $U \Sigma$. Thus $AV = U \Sigma$, or $A = U \Sigma V^\top$.
        </div>
      </div>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-wiki.png" alt="SVD Decomposition Wiki" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption><i>Figure 5a:</i> Standard SVD visualization (Wikimedia).</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/svd-ellipse.png" alt="SVD Ellipse" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption><i>Figure 5b:</i> Unit circle to ellipse transformation.</figcaption>
        </figure>
      </div>

      <h3>5.2 Rank and the Four Fundamental Subspaces</h3>
      <p>SVD provides the optimal basis for all four fundamental subspaces of the matrix $A$. Let $r$ be the number of non-zero singular values.</p>
      <ul>
        <li><b>Range (Column Space):</b> Spanned by the first $r$ columns of $U$ (Left singular vectors).
        $$ \mathcal{R}(A) = \operatorname{span}\{u_1, \dots, u_r\} $$</li>
        <li><b>Nullspace (Kernel):</b> Spanned by the last $n-r$ columns of $V$ (Right singular vectors corresponding to $\sigma_i=0$).
        $$ \mathcal{N}(A) = \operatorname{span}\{v_{r+1}, \dots, v_n\} $$</li>
        <li><b>Row Space:</b> Spanned by the first $r$ columns of $V$ (Right singular vectors corresponding to $\sigma_i>0$).
        $$ \mathcal{R}(A^\top) = \operatorname{span}\{v_1, \dots, v_r\} $$</li>
        <li><b>Left Nullspace:</b> Spanned by the last $m-r$ columns of $U$.
        $$ \mathcal{N}(A^\top) = \operatorname{span}\{u_{r+1}, \dots, u_m\} $$</li>
      </ul>
      <p>This explicitly diagonalizes the geometry: $A$ maps the row space basis $\{v_1, \dots, v_r\}$ to the column space basis $\{u_1, \dots, u_r\}$ with scaling factors $\sigma_i$, and kills everything else.</p>

      <h3>5.3 Low-Rank Approximation (Eckart-Young-Mirsky)</h3>
      <p>SVD solves the problem of finding the best approximation of a matrix by one of lower rank.
      $$ A_k = \sum_{i=1}^k \sigma_i u_i v_i^\top $$
      This matrix $A_k$ minimizes $\|A - X\|$ in both Spectral and Frobenius norms among all matrices of rank $k$.
      <br><b>Applications:</b> Image compression, PCA, Denoising.</p>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD for Image Compression</h3>
        <p><b>The Power of Low-Rank Approximation:</b> An image can be viewed as a matrix of pixel intensities. By computing its SVD and keeping only the largest singular values, we can reconstruct the image with far less data.</p>
        <div id="widget-svd-approximator" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: SVD Geometry</h3>
        <p>Visualize the SVD decomposition $A = U \Sigma V^\top$ as a sequence of three geometric transformations: rotation ($V^\top$), axis-aligned scaling ($\Sigma$), and rotation ($U$).</p>
        <div id="widget-svd-geometry" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>5.4 The Schur Complement: Block Elimination</h3>
      <p>The <b>Schur Complement</b> arises naturally when solving block linear systems or analyzing positive definiteness of block matrices. It corresponds to Gaussian elimination performed on blocks.</p>
      <p>Consider a symmetric block matrix $M = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}$. If $A$ is invertible, we can eliminate $B^\top$ by multiplying by a block row-operation matrix:
      $$ \begin{bmatrix} I & 0 \\ -B^\top A^{-1} & I \end{bmatrix} \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix} = \begin{bmatrix} A & B \\ 0 & C - B^\top A^{-1} B \end{bmatrix} $$
      The matrix $S = C - B^\top A^{-1} B$ is the Schur complement of $A$ in $M$.</p>

      <div class="theorem-box">
        <h4>Theorem: Schur Complement Condition for PSD</h4>
        <p>If $A \succ 0$, then:
        $$ M \succeq 0 \iff S = C - B^\top A^{-1} B \succeq 0 $$
        This converts a block matrix constraint into a scalar (or smaller matrix) constraint involving inverses. This is the key tool for converting rational inequalities into Linear Matrix Inequalities (LMIs) in convex optimization.</p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive Demo: Schur Complement</h3>
        <p>Visualize the quadratic form of a block matrix. See how the Schur complement condition $s = a - b^2/d \ge 0$ determines positive semidefiniteness. The line $y = -(b/d)x$ corresponds to minimizing the quadratic over $y$ for a fixed $x$, which is the geometric operation underlying the Schur complement.</p>
        <div id="widget-schur-complement" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>5.5 Application: PCA as SVD of Data</h3>
      <p>Principal Component Analysis (PCA) is often presented as a statistical method. In reality, it is <b>pure geometry</b>: finding the best ellipsoidal approximation to a point cloud. We derive it from scratch using SVD, bypassing "covariance" jargon.</p>

      <h4>1) The Question</h4>
      <p>Given data points $\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^d$, PCA asks: <b>Along which orthogonal directions does the data spread the most?</b></p>

      <h4>2) Centering (Non-negotiable)</h4>
      <p>Define the mean $\mu = \frac{1}{N}\sum \mathbf{x}_i$. We work with centered data $\tilde{\mathbf{x}}_i = \mathbf{x}_i - \mu$.
      <br>Stack them into a data matrix $X \in \mathbb{R}^{N \times d}$ (rows are points):
      $$ X = \begin{bmatrix} \tilde{\mathbf{x}}_1^\top \\ \vdots \\ \tilde{\mathbf{x}}_N^\top \end{bmatrix} $$
      </p>

      <h4>3) Variance as a Quadratic Form</h4>
      <p>The variance along a unit direction $\mathbf{v}$ is the average squared projection:
      $$ \text{Var}(\mathbf{v}) = \frac{1}{N} \sum_{i=1}^N (\tilde{\mathbf{x}}_i^\top \mathbf{v})^2 = \frac{1}{N} \|X\mathbf{v}\|_2^2 = \mathbf{v}^\top \left( \frac{1}{N} X^\top X \right) \mathbf{v} $$
      Let $C = \frac{1}{N} X^\top X$ be the empirical covariance matrix. The problem is to maximize $\mathbf{v}^\top C \mathbf{v}$ subject to $\|\mathbf{v}\|=1$. This is exactly the Rayleigh Quotient maximization.</p>

      <h4>4) Enter SVD</h4>
      <p>Take the SVD of the centered data matrix: $\boxed{X = U \Sigma V^\top}$.
      <br>Then the covariance matrix is:
      $$ C = \frac{1}{N} (V \Sigma U^\top)(U \Sigma V^\top) = V \left( \frac{1}{N} \Sigma^2 \right) V^\top $$
      This is the eigendecomposition of $C$.
      </p>

      <h4>5) The Geometric Translation</h4>
      <ul>
        <li><b>Principal Components:</b> The columns of $V$ (Right singular vectors). These are the axes of the ellipse.</li>
        <li><b>Explained Variance:</b> The eigenvalues $\lambda_i = \sigma_i^2 / N$.</li>
        <li><b>Principal Scores:</b> The coordinates of the data in the new basis are given by $XV = U \Sigma$.</li>
      </ul>

      <h4>6) Dimensionality Reduction</h4>
      <p>The truncated SVD $X_k = U_k \Sigma_k V_k^\top$ gives the optimal rank-$k$ approximation of the data in the Frobenius norm. This means PCA minimizes the reconstruction error:
      $$ \sum_{i=1}^N \|\mathbf{x}_i - \text{proj}_k(\mathbf{x}_i)\|^2 $$
      PCA is not just a heuristic; it is the <b>optimal</b> linear compression scheme.</p>
    </section>
    </section>

    <!-- SECTION 8: PSEUDOINVERSE -->
    <section class="section-card" id="section-pseudoinverse">
      <h2>6. The Pseudoinverse and Condition Number</h2>

      <h3>6.1 The Moore-Penrose Pseudoinverse ($A^+$)</h3>
      <p>When $A$ is not square or not invertible, we cannot define $A^{-1}$. However, we can define a generalized inverse $A^+$ that "inverts" the action of $A$ on its range and sends the orthogonal complement to zero.</p>

      <h4>The Moore-Penrose Axioms</h4>
      <p>The pseudoinverse $A^+$ is the unique matrix satisfying these four algebraic conditions:
      <ol>
        <li>$A A^+ A = A$ ( $A A^+$ is a projection on the range).</li>
        <li>$A^+ A A^+ = A^+$ ( $A^+$ acts like an inverse on the range of $A$).</li>
        <li>$(A A^+)^\top = A A^+$ (Symmetry of projector).</li>
        <li>$(A^+ A)^\top = A^+ A$ (Symmetry of projector).</li>
      </ol>
      </p>

      <div class="proof-box">
        <h4>Proof: Uniqueness of the Pseudoinverse</h4>
        <p>Suppose $X$ and $Y$ both satisfy the four Moore-Penrose axioms. We show $X=Y$.
        <br>1. Start with $X = X A X$.
        <br>2. Use $A = A Y A$: $X = X (A Y A) X$.
        <br>3. Group terms: $X = (X A) Y (A X)$.
        <br>4. Use Axiom 3/4 to introduce symmetry: $X A = (X A)^\top = A^\top X^\top$ and $A X = (A X)^\top = X^\top A^\top$. (Wait, this direction is harder).
        <br><b>Alternative Clean Derivation:</b>
        $$ X = X A X = (X A) X = (X A)^\top X = A^\top X^\top X = (A Y A)^\top X^\top X = A^\top Y^\top A^\top X^\top X $$
        $$ = (Y A)^\top (X A)^\top X = Y A X A X = Y A X $$
        Symmetrically, $Y = Y A Y = Y A (X A Y) = Y A X A Y = (Y A X) A Y$.
        From above, $Y A X = X$, so $Y = X A Y$.
        <br>Now apply symmetry again:
        $X = Y A X = Y (A X) = Y (A X)^\top = Y X^\top A^\top = Y X^\top (A Y A)^\top = Y X^\top A^\top Y^\top A^\top$.
        This algebraic manipulation confirms that the geometric definition (via SVD) produces the <i>unique</i> matrix satisfying the algebraic axioms.
        </p>
      </div>

      <p><b>Definition via SVD:</b> If $A = U \Sigma V^\top$, then:
      $$ A^+ = V \Sigma^+ U^\top $$
      where $\Sigma^+$ is the $n \times m$ matrix obtained by transposing $\Sigma$ and inverting the non-zero singular values:
      $$
      \Sigma^+ = \begin{bmatrix}
      1/\sigma_1 & & & 0 & \dots \\
      & \ddots & & \vdots & \\
      & & 1/\sigma_r & 0 & \dots \\
      0 & \dots & 0 & 0 & \dots \\
      \vdots & & \vdots & \vdots & \ddots
      \end{bmatrix}
      $$
      (The non-zero block is diagonal with entries $1/\sigma_i$; all other entries are zero).</p>

      <h4>Properties and Applications</h4>
      <ul>
        <li><b>Minimum Norm Solution:</b> For any system $A\mathbf{x}=\mathbf{b}$ (or the least squares problem), the vector $\mathbf{x}^\star = A^+ \mathbf{b}$ is the solution that has the <b>minimum Euclidean norm</b>.
        <br>
        <div class="proof-box">
          <h4>Proof: $x_{LS} = A^+\mathbf{b}$ minimizes norm</h4>
        <p>The set of all least-squares solutions is given by $S_{LS} = \{\mathbf{x} \mid A^\top A \mathbf{x} = A^\top \mathbf{b}\}$. Since this is an affine set $x_p + \mathcal{N}(A)$, there is a unique element with minimum Euclidean norm. We show $\mathbf{x}^+ = A^+\mathbf{b}$ is that element.</p>

          <div class="proof-step">
          <strong>Step 1: Verification of Solution.</strong>
          First, we check that $\mathbf{x}^+$ actually solves the least squares problem (satisfies Normal Equations).
          Recall $A = U \Sigma V^\top$ and $A^+ = V \Sigma^+ U^\top$.
          $$ A \mathbf{x}^+ = U \Sigma V^\top V \Sigma^+ U^\top \mathbf{b} = U (\Sigma \Sigma^+) U^\top \mathbf{b} $$
          The product $\Sigma \Sigma^+$ is a diagonal projection matrix (1s for indices $1 \dots r$, 0s otherwise). It projects onto the range of $A$.
          Thus $A\mathbf{x}^+$ is the orthogonal projection of $\mathbf{b}$ onto $\mathcal{R}(A)$, which defines the least squares solution.
          </div>

          <div class="proof-step">
          <strong>Step 2: Orthogonality of $\mathbf{x}^+$.</strong>
          We check where $\mathbf{x}^+$ lies.
          $\mathbf{x}^+ = V (\Sigma^+ U^\top \mathbf{b})$. This vector is a linear combination of the columns of $V$ corresponding to non-zero singular values (indices $1 \dots r$).
          These columns span the <b>row space</b> $\mathcal{R}(A^\top)$.
          Thus, $\mathbf{x}^+ \in \mathcal{R}(A^\top)$.
          Recall the fundamental orthogonality: $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.
          </div>

          <div class="proof-step">
          <strong>Step 3: Pythagorean Minimization.</strong>
          Let $\mathbf{x}$ be any solution to the least squares problem. The set of all solutions is the affine set $S = \mathbf{x}^+ + \mathcal{N}(A)$.
          Any solution $\mathbf{x}$ can be written as $\mathbf{x} = \mathbf{x}^+ + \mathbf{z}$, where $\mathbf{z} \in \mathcal{N}(A)$.
          <br>We established in Step 2 that $\mathbf{x}^+ \in \mathcal{R}(A^\top)$.
          The Fundamental Theorem of Linear Algebra states that $\mathcal{R}(A^\top) \perp \mathcal{N}(A)$.
          Therefore, $\mathbf{x}^+ \perp \mathbf{z}$.
          <br>By the Pythagorean Theorem:
          $$ \|\mathbf{x}\|_2^2 = \|\mathbf{x}^+ + \mathbf{z}\|_2^2 = \|\mathbf{x}^+\|_2^2 + \|\mathbf{z}\|_2^2 $$
          Since $\|z\|_2^2 \ge 0$, the minimum possible value for $\|x\|_2^2$ occurs when $\|z\|_2^2 = 0$, i.e., $\mathbf{z}=0$.
          Thus, $\mathbf{x} = \mathbf{x}^+$ is the unique solution with minimum norm.
          </div>
        </div>
        </li>
        <li><b>Projectors:</b> $AA^+$ is the orthogonal projector onto the column space $\mathcal{R}(A)$. $A^+A$ is the orthogonal projector onto the row space $\mathcal{R}(A^\top)$.</li>
      </ul>

      <h3>6.2 Condition Number</h3>
      <p>The condition number $\kappa(A)$ measures the sensitivity of the solution of a linear system to perturbations in the data. Formally, it is defined as:</p>
      $$ \kappa(A) = \|A\|_2 \|A^+\|_2 = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} $$
      <p>where $\sigma_{\min}$ is the smallest <i>non-zero</i> singular value.</p>

      <ul>
        <li><b>$\kappa \approx 1$:</b> Well-conditioned. Errors are not amplified. Orthogonal matrices have $\kappa = 1$.</li>
        <li><b>$\kappa \gg 1$:</b> Ill-conditioned. Small errors in $\mathbf{b}$ can lead to massive errors in $\mathbf{x}$.</li>
      </ul>

      <div class="row" style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; margin: 24px 0; flex-wrap: wrap;">
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/conditioning-geometry.png"
               alt="Well-conditioned versus ill-conditioned error amplification geometry"
               style="width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 6:</i> Condition number as error magnification: the same small uncertainty in $\mathbf{b}$ can map to a small or huge uncertainty in $\mathbf{x}$ depending on $\kappa(A)$.</figcaption>
        </figure>
        <figure style="text-align: center; flex: 1; min-width: 280px;">
          <img src="assets/zig-zag-ill-conditioning.png"
               alt="Gradient descent zig-zagging on elongated contours in an ill-conditioned problem"
               style="width: 100%; height: auto; border-radius: 8px;" />
          <figcaption><i>Figure 7:</i> Ill-conditioning creates elongated level sets; first-order methods like gradient descent can zig-zag across the valley, slowing convergence.</figcaption>
        </figure>
      </div>

      <div class="insight">
        <h4>Impact on Optimization Algorithms</h4>
        <p>The condition number is not just a numerical nuisance; it fundamentally limits the speed of optimization algorithms.
        For a quadratic function $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A \mathbf{x}$, Gradient Descent converges at a rate determined by $\frac{\kappa-1}{\kappa+1}$.
        <ul>
            <li>If $\kappa=1$ (spherical), convergence is instant.</li>
            <li>If $\kappa \gg 1$ (valley), convergence is arbitrarily slow.</li>
        </ul>
        This motivates <b>Newton's Method</b> and preconditioning, which effectively transform the problem to make $\kappa \approx 1$. We will study this in <a href="../13-unconstrained-minimization/index.html">Lecture 13</a>.</p>
      </div>

      <div class="widget-container" style="margin: 24px 0;">
        <h3 style="margin-top: 0;">Interactive: Condition Number & Stability</h3>
        <p><b>Visualize Ill-Conditioning:</b> Compare solving $A\mathbf{x}=\mathbf{b}$ for a well-conditioned matrix versus an ill-conditioned one. In the ill-conditioned case, the "ellipse" of the matrix is very skinny. A small change in the target vector $\mathbf{b}$ (moving the target slightly off the major axis) requires a massive change in $\mathbf{x}$ to compensate.</p>
        <div id="widget-condition-number" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <h3>6.3 Regularization: Controlled Bias for Stability</h3>
      <p>When a matrix is ill-conditioned, the pseudoinverse solution $x_{LS} = \sum \frac{\mathbf{u}_i^\top \mathbf{b}}{\sigma_i} \mathbf{v}_i$ becomes unstable because division by small $\sigma_i$ amplifies noise.
      <br><b>Regularization</b> stabilizes the inversion by filtering out these small singular values. The classic method is <b>Tikhonov Regularization</b> (Ridge Regression):</p>
      $$ \min_{\mathbf{x}} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2 $$
      <p>The solution is $\mathbf{x}_{\text{ridge}} = (A^\top A + \lambda I)^{-1} A^\top \mathbf{b}$.</p>

      <h4>SVD Interpretation: Spectral Filtering</h4>
      <p>In terms of the SVD, Tikhonov regularization replaces the inversion $1/\sigma_i$ with a <b>filter factor</b> $f_i(\lambda)$:
      $$ \mathbf{x}_{\text{ridge}} = \sum_{i=1}^r \underbrace{\left( \frac{\sigma_i^2}{\sigma_i^2 + \lambda} \right)}_{\text{Filter } f_i} \frac{1}{\sigma_i} (\mathbf{u}_i^\top \mathbf{b}) \mathbf{v}_i $$
      Let's analyze the filter factor $f_i(\lambda) = \frac{\sigma_i^2}{\sigma_i^2+\lambda}$:
      <ul>
        <li><b>Signal Regime ($\sigma_i \gg \sqrt{\lambda}$):</b> $f_i \approx 1$. We keep the component (pseudoinverse behavior).</li>
        <li><b>Noise Regime ($\sigma_i \ll \sqrt{\lambda}$):</b> $f_i \approx \sigma_i^2/\lambda \to 0$. We suppress the component.</li>
      </ul>
      This acts as a "smooth low-pass filter" on the spectrum of the matrix, damping out high-frequency noise (small singular values) while preserving the dominant structure.</p>

      <h4>Truncated SVD (Hard Thresholding)</h4>
      <p>Another approach is to simply discard singular values below a threshold. This is the optimal low-rank approximation applied to the inversion process.
      $$ \mathbf{x}_{\text{trunc}} = \sum_{\sigma_i > \epsilon} \frac{1}{\sigma_i} (\mathbf{u}_i^\top \mathbf{b}) \mathbf{v}_i $$
      Both methods trade a small amount of bias (exactness) for a large reduction in variance (stability).</p>

      <div class="insight">
        <h4>The Irreducible Truth</h4>
        <p>You should now be able to see the single thread connecting these topics:
        <br><b>PCA, Least Squares, Pseudoinverses, and Regularization are all consequences of the SVD.</b>
        <br>The SVD exposes the intrinsic geometry of a linear map: which directions survive, which collapse, and which amplify noise. Every stable algorithm in optimization is, explicitly or implicitly, an SVD-aware algorithm.</p>
      </div>
    </section>

    <!-- SECTION 9: REVIEW -->
    <section class="section-card" id="section-review">
      <h2>7. Review & Cheat Sheet</h2>

      <table class="data-table">
        <thead>
          <tr>
            <th>Decomposition</th>
            <th>Formula</th>
            <th>Components</th>
            <th>Primary Use</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>QR</b></td>
            <td>$A = QR$</td>
            <td>$Q$ orthonormal, $R$ upper triangular</td>
            <td>Least squares, Gram-Schmidt</td>
          </tr>
          <tr>
            <td><b>SVD</b></td>
            <td>$A = U \Sigma V^\top$</td>
            <td>$U, V$ orthogonal, $\Sigma$ diagonal</td>
            <td>Rank, Norms, Approximation, PCA</td>
          </tr>
          <tr>
            <td><b>Spectral</b></td>
            <td>$A = Q \Lambda Q^\top$</td>
            <td>$Q$ orthogonal, $\Lambda$ real diagonal</td>
            <td>Symmetric matrices ($A=A^\top$)</td>
          </tr>
        </tbody>
      </table>

      <h3>Key Identities</h3>
      <ul>
        <li>$\|A\|_2 = \sigma_1$</li>
        <li>$\|A\|_F = \sqrt{\sum \sigma_i^2}$</li>
        <li>$A^+ = V \Sigma^+ U^\top$</li>
        <li>$\kappa(A) = \sigma_1 / \sigma_r$</li>
      </ul>

      <h3>The Unified Picture</h3>
      <table class="data-table">
        <thead>
          <tr>
            <th>Concept</th>
            <th>SVD Interpretation ($A=U\Sigma V^\top$)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>PCA</td>
            <td>SVD of centered data matrix</td>
          </tr>
          <tr>
            <td>Variance</td>
            <td>Squared singular values ($\sigma_i^2/N$)</td>
          </tr>
          <tr>
            <td>Least Squares</td>
            <td>Invert $\Sigma$ on range, zero on nullspace</td>
          </tr>
          <tr>
            <td>Ill-conditioning</td>
            <td>Decay of $\sigma_i$ to zero</td>
          </tr>
          <tr>
            <td>Regularization</td>
            <td>Smooth filtering of small $\sigma_i$</td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- SECTION 10: EXERCISES -->
    <section class="section-card" id="section-exercises"><h2><i data-feather="edit-3"></i> 8. Exercises</h2>
      <div class="insight">
        <h4>Recap & Key Concepts</h4>
        <p>These exercises explore the theoretical depths of matrix norms, the geometry of projections, and the algebraic properties of the trace. P1.1 links norms to duality (Lecture 02). P1.12 links SVD to the operator norm.</p>
      </div>

      <h3>P1.1 — Dual of $\ell_p$ Norm</h3>
      <p>Show that for $1 < p < \infty$, the dual norm of $\|\cdot\|_p$ is $\|\cdot\|_q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Use Hölder's Inequality for the upper bound and construct an alignment vector for the lower bound.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>Upper bound ($\le$):</strong> By Hölder's inequality, $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$.
          The definition of the dual norm is $\|y\|_* = \sup_{\|\mathbf{x}\|_p \le 1} \mathbf{x}^\top \mathbf{y}$.
          From Hölder, if $\|x\|_p \le 1$, then $\mathbf{x}^\top \mathbf{y} \le \|\mathbf{y}\|_q$. Thus $\|y\|_* \le \|\mathbf{y}\|_q$.
        </div>
        <div class="proof-step">
          <strong>Lower bound ($\ge$):</strong> We need to construct a vector $\mathbf{x}$ that "aligns" with $\mathbf{y}$ to maximize the inner product.
          We want $x_i$ to have the same sign as $y_i$, and magnitudes such that equality holds in Hölder's.
          Choose $x_i = \mathrm{sign}(y_i) |y_i|^{q-1}$.
          <br>Check inner product: $\mathbf{x}^\top \mathbf{y} = \sum \mathrm{sign}(y_i)|y_i|^{q-1} y_i = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          <br>Check norm of $\mathbf{x}$: $\|x\|_p^p = \sum |x_i|^p = \sum |y_i|^{p(q-1)}$.
          Since $(p-1)q = p \implies p(q-1) = q$: $\|x\|_p^p = \sum |y_i|^q = \|\mathbf{y}\|_q^q$.
          So $\|x\|_p = \|y\|_q^{q/p}$.
          <br>Now normalize to get a unit vector $\tilde{\mathbf{x}} = \mathbf{x} / \|\mathbf{x}\|_p = \mathbf{x} / \|\mathbf{y}\|_q^{q/p}$.
          $$ \tilde{\mathbf{x}}^\top \mathbf{y} = \frac{\|\mathbf{y}\|_q^q}{\|\mathbf{y}\|_q^{q/p}} = \|\mathbf{y}\|_q^{q - q/p} = \|\mathbf{y}\|_q^1 = \|\mathbf{y}\|_q $$
          Since we found a unit vector achieving this value, $\|y\|_* \ge \|\mathbf{y}\|_q$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Dual Norm:</b> Defined by $\|y\|_* = \sup \{ \mathbf{y}^\top \mathbf{x} \mid \|\mathbf{x}\| \le 1 \}$.
        <br><b>$\ell_p$ Dual:</b> The dual of the $\ell_p$ norm ($1 < p < \infty$) is the $\ell_q$ norm, where $1/p + 1/q = 1$.
        <br><b>Hölder's Inequality:</b> $|\mathbf{x}^\top \mathbf{y}| \le \|\mathbf{x}\|_p \|\mathbf{y}\|_q$. This is the generalized Cauchy-Schwarz inequality.
        <br><b>Alignment Condition:</b> Equality holds if $|x_i|^p$ is proportional to $|y_i|^q$ (and signs match). This is crucial for constructing worst-case examples in robustness analysis.</p>
      </div>

      <h3>P1.2 — Frobenius Cauchy–Schwarz</h3>
      <p>Prove that $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$ by viewing matrices in $\mathbb{R}^{m \times n}$ as vectors in $\mathbb{R}^{mn}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The Frobenius norm corresponds to the standard Euclidean norm of the vectorized matrix: $\|A\|_F = \|\mathrm{vec}(A)\|_2$.
        The inner product of matrices is $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i,j} A_{ij} B_{ij}$.
        This is exactly the dot product of the vectorized forms: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        By the standard Cauchy-Schwarz inequality for vectors:
        $$ |\mathrm{vec}(A)^\top \mathrm{vec}(B)| \le \|\mathrm{vec}(A)\|_2 \|\mathrm{vec}(B)\|_2 $$
        Substituting back the matrix notation yields $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The $\mathrm{vec}$ Operator:</b> Stacks columns of a matrix into a single vector in $\mathbb{R}^{mn}$.
        <br><b>Isometry:</b> The mapping $A \mapsto \mathrm{vec}(A)$ is an isometry between $(\mathbb{R}^{m \times n}, \|\cdot\|_F)$ and $(\mathbb{R}^{mn}, \|\cdot\|_2)$.
        <br><b>Inner Product Consistency:</b> $\langle A, B \rangle = \mathrm{tr}(A^\top B) = \mathrm{vec}(A)^\top \mathrm{vec}(B)$.
        <br><b>Result:</b> Cauchy-Schwarz for vectors immediately implies $|\mathrm{tr}(A^\top B)| \le \|A\|_F \|B\|_F$.</p>
      </div>

      <h3>P1.3 — SVD Computation</h3>
      <p>Compute the Singular Value Decomposition $A = U \Sigma V^\top$ by hand for the matrix $A = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}$. Verify that the singular values are the square roots of the eigenvalues of $A^\top A$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <div class="proof-step">
          <strong>1. Compute $A^\top A$ and its eigenvalues:</strong>
          $A^\top A = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$.
          Characteristic eq: $(1-\lambda)^2 - 1 = 0 \implies \lambda^2 - 2\lambda = 0 \implies \lambda_1=2, \lambda_2=0$.
          Singular values: $\sigma_1 = \sqrt{2}, \sigma_2 = 0$.
        </div>
        <div class="proof-step">
          <strong>2. Compute $V$ (eigenvectors of $A^\top A$):</strong>
          For $\lambda_1=2$: $\begin{bmatrix} -1 & 1 \\ 1 & -1 \end{bmatrix} \mathbf{v} = 0 \implies v_1 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$.
          For $\lambda_2=0$: $v_2 = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}$.
          $V = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>3. Compute $U$:</strong>
          $u_1 = \frac{1}{\sigma_1} A v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix} \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} 2 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
          Since $m=2$, we need a second vector $u_2$ orthogonal to $u_1$. $u_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
          $U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$.
        </div>
        <div class="proof-step">
          <strong>Result:</strong> $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{2} & 0 \\ 0 & 0 \end{bmatrix} \left(\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\right)^\top$.
        </div>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Hand-Calculation Strategy for SVD:</b>
        <ol>
            <li>Compute the Gram matrix $G = A^\top A$.</li>
            <li>Find eigenvalues $\lambda_i$ of $G$. Singular values are $\sigma_i = \sqrt{\lambda_i}$.</li>
            <li>Find normalized eigenvectors $v_i$ of $G$ to form $V$.</li>
            <li>Compute $u_i = \frac{1}{\sigma_i} A v_i$ for non-zero $\sigma_i$.</li>
            <li>If $m > r$, complete $U$ using Gram-Schmidt or inspection (orthogonality).</li>
        </ol>
        <br><b>Check:</b> Verify $AV = U\Sigma$.</p>
      </div>

      <h3>P1.4 — Normal Equations vs. QR</h3>
      <p>Solve the system $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}, \mathbf{b} = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}$ using (a) Normal Equations and (b) QR factorization. Compare the intermediate steps.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>(a) Normal Equations:</b> $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        $A^\top A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$.
        $A^\top \mathbf{b} = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Solve $\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix} \mathbf{x} = \begin{bmatrix} 3 \\ 4 \end{bmatrix}$.
        Multiply by inverse $\frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}$:
        $\mathbf{x} = \frac{1}{3} \begin{bmatrix} 6-4 \\ -3+8 \end{bmatrix} = \frac{1}{3} \begin{bmatrix} 2 \\ 5 \end{bmatrix}$.</p>
        <p><b>(b) QR Factorization:</b>
        Gram-Schmidt on columns $a_1, a_2$.
        $u_1 = a_1 = (1, 0, 1)^\top$. $q_1 = (1/\sqrt{2}, 0, 1/\sqrt{2})^\top$.
        $v_2 = a_2 - \langle a_2, q_1 \rangle q_1 = (1, 1, 0)^\top - (1/\sqrt{2}) (1/\sqrt{2}, 0, 1/\sqrt{2})^\top = (1, 1, 0)^\top - (1/2, 0, 1/2)^\top = (1/2, 1, -1/2)^\top$.
        Normalize $v_2$: $\|v_2\|^2 = 1/4 + 1 + 1/4 = 1.5 = 3/2$. $\|v_2\| = \sqrt{3/2}$.
        $q_2 = \sqrt{2/3} (1/2, 1, -1/2)^\top$.
        Solve $R\mathbf{x} = Q^\top \mathbf{b}$.
        $R = \begin{bmatrix} \sqrt{2} & 1/\sqrt{2} \\ 0 & \sqrt{3/2} \end{bmatrix}$. $Q^\top \mathbf{b} = \begin{bmatrix} 3/\sqrt{2} \\ 4\sqrt{2/3}-1/\sqrt{6} \end{bmatrix}$.
        Back substitution yields same $\mathbf{x}$. QR avoids forming $A^\top A$ (better condition number).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Comparing Least Squares Algorithms:</b>
        <ul>
            <li><b>Normal Equations:</b>
                <ul><li>Cost: $O(n^2 m)$ (forming $A^\top A$) + $O(n^3)$ (Cholesky).</li>
                <li>Stability: Unstable. Dependence on $\kappa(A)^2$.</li></ul>
            </li>
            <li><b>QR Factorization:</b>
                <ul><li>Cost: $2 \times$ Normal Equations (roughly).</li>
                <li>Stability: Stable. Dependence on $\kappa(A)$. Standard choice.</li></ul>
            </li>
            <li><b>SVD:</b> Most expensive, but most robust (handles rank deficiency gracefully).</li>
        </ul></p>
      </div>

      <h3>P1.5 — Pseudoinverse Projectors</h3>
      <p>Show that $P = AA^+$ is the orthogonal projector onto $\mathcal{R}(A)$. Verify $P^2=P$ and $P^\top=P$ using SVD.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Let $A = U \Sigma V^\top$. Then $A^+ = V \Sigma^+ U^\top$.
        $P = AA^+ = (U \Sigma V^\top)(V \Sigma^+ U^\top) = U (\Sigma \Sigma^+) U^\top$.
        $\Sigma \Sigma^+$ is a diagonal matrix with 1s for non-zero singular values and 0s otherwise.
        Let $U_r$ be the first $r$ columns of $U$. Then $P = U_r U_r^\top$.
        <br><b>Idempotent:</b> $P^2 = (U_r U_r^\top)(U_r U_r^\top) = U_r (U_r^\top U_r) U_r^\top = U_r I U_r^\top = P$.
        <br><b>Symmetric:</b> $P^\top = (U_r U_r^\top)^\top = (U_r^\top)^\top U_r^\top = U_r U_r^\top = P$.
        <br>Since the columns of $U_r$ form a basis for $\mathcal{R}(A)$, $P$ is the orthogonal projector onto $\mathcal{R}(A)$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Fundamental Projectors via SVD:</b>
        <ul>
            <li><b>Column Space Projector:</b> $P_{\mathcal{R}(A)} = A A^+ = U_r U_r^\top$.</li>
            <li><b>Row Space Projector:</b> $P_{\mathcal{R}(A^\top)} = A^+ A = V_r V_r^\top$.</li>
        </ul>
        <br><b>Intuition:</b> $A^+$ inverts the action on the range and kills the orthogonal complement. $AA^+$ maps range $\to$ range (identity) and orthogonal complement $\to$ 0.</p>
      </div>

      <h3>P1.6 — Projector Formula</h3>
      <p>Let $A$ have linearly independent columns. Derive the projection formula $P = A(A^\top A)^{-1}A^\top$ by minimizing $\|Ax - b\|_2^2$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>We want to find the point $\mathbf{p} = A\mathbf{x}$ in the column space closest to $\mathbf{b}$.
        Problem: $\min_\mathbf{x} \|A\mathbf{x} - \mathbf{b}\|_2^2$.
        Gradient: $2A^\top (A\mathbf{x} - \mathbf{b}) = 0$.
        Normal eqs: $A^\top A \mathbf{x} = A^\top \mathbf{b}$.
        Since cols are independent, $A^\top A$ is invertible.
        Solution: $\mathbf{x}^* = (A^\top A)^{-1} A^\top \mathbf{b}$.
        Projection: $\mathbf{p} = A \mathbf{x}^* = A(A^\top A)^{-1} A^\top \mathbf{b}$.
        The matrix mapping $\mathbf{b}$ to $\mathbf{p}$ is $P = A(A^\top A)^{-1} A^\top$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The "Hat" Matrix:</b> $P = A(A^\top A)^{-1} A^\top$.
        <br><b>Requirements:</b> Valid only when $A$ has full column rank (so $A^\top A$ is invertible).
        <br><b>General Case:</b> If $A$ is rank-deficient, use the pseudoinverse form $P = AA^+$.
        <br><b>Properties:</b> $P^2=P$ and $P^\top=P$. The residual is $(I-P)\mathbf{b}$, which projects onto $\mathcal{N}(A^\top)$.</p>
      </div>

      <h3>P1.7 — Hyperplane Projection</h3>
      <p>Find the projection of $(1, 2, 3)^\top$ onto $x_1 + x_2 + x_3 = 3$. Verify your result using the standard formula for distance to a plane.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>Normal $a = (1, 1, 1)^\top$. Point $\mathbf{y} = (1, 2, 3)^\top$. Target $\mathbf{b}=3$.
        $\mathbf{p} = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a$.
        $a^\top \mathbf{y} = 1+2+3 = 6$. $\|a\|^2 = 3$.
        $\mathbf{p} = (1, 2, 3)^\top - \frac{6-3}{3} (1, 1, 1)^\top = (1, 2, 3)^\top - (1, 1, 1)^\top = (0, 1, 2)^\top$.
        Check: $0+1+2=3$. Correct.
        Distance: $\|y-p\| = \|(1, 1, 1)\| = \sqrt{3}$.
        Formula: $\frac{|a^\top \mathbf{y} - \mathbf{b}|}{\|a\|} = \frac{|6-3|}{\sqrt{3}} = \frac{3}{\sqrt{3}} = \sqrt{3}$. Matches.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Constraint Projection:</b> The projection of $\mathbf{y}$ onto the affine set $\{x \mid a^\top \mathbf{x} = \mathbf{b}\}$ is:
        $$ \Pi(\mathbf{y}) = \mathbf{y} - \frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2} a $$
        <b>Optimization View:</b> This is the solution to $\min_\mathbf{x} \frac{1}{2}\|\mathbf{x}-\mathbf{y}\|^2$ s.t. $a^\top \mathbf{x} = \mathbf{b}$.
        <br><b>Lagrange Multiplier:</b> The term $\frac{a^\top \mathbf{y} - \mathbf{b}}{\|a\|^2}$ corresponds to the Lagrange multiplier $\nu$ associated with the constraint.</p>
      </div>

      <h3>P1.8 — Condition Number Calculation</h3>
      <p>Calculate the condition number $\kappa(A) = \sigma_{\max}(A)/\sigma_{\min}(A)$ for the matrix $A = \begin{bmatrix} 1 & 0 \\ 0 & \epsilon \end{bmatrix}$ where $\epsilon > 0$. What happens as $\epsilon \to 0$?</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>The matrix is diagonal, so its singular values are the absolute values of diagonal entries: $1$ and $\epsilon$.
        Assuming $\epsilon < 1$: $\sigma_{\max} = 1, \sigma_{\min} = \epsilon$.
        $\kappa(A) = 1/\epsilon$.
        As $\epsilon \to 0$, $\kappa(A) \to \infty$. The matrix becomes ill-conditioned (nearly singular).</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Condition Number $\kappa(A)$:</b> The ratio $\sigma_{\max}/\sigma_{\min}$ (for square invertible matrices).
        <br><b>Interpretation:</b> The aspect ratio of the hyperellipse image of the unit sphere.
        <br><b>Impact:</b>
        <ul>
            <li><b>Sensitivity:</b> Relative error in $\mathbf{x}$ $\le \kappa(A) \times$ Relative error in $\mathbf{b}$.</li>
            <li><b>Optimization:</b> Controls convergence rate of gradient descent. High $\kappa$ $\implies$ "Zig-zagging".</li>
        </ul></p>
      </div>

      <h3>P1.9 — Pseudoinverse of Rank-1 Matrix</h3>
      <p>Let $A = xy^\top$ where $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ are non-zero vectors. Show that $A^+ = \frac{A^\top}{\|\mathbf{x}\|_2^2 \|\mathbf{y}\|_2^2}$.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>SVD of rank-1 matrix: $A = \sigma \mathbf{u} \mathbf{v}^\top$.
        $\|A\|_F = \|x\|\|y\| = \sigma$.
        $\mathbf{u} = \mathbf{x}/\|\mathbf{x}\|$, $\mathbf{v} = \mathbf{y}/\|\mathbf{y}\|$.
        Check: $A = (\|\mathbf{x}\|\|\mathbf{y}\|) \frac{\mathbf{x}}{\|\mathbf{x}\|} \frac{\mathbf{y}^\top}{\|\mathbf{y}\|} = xy^\top$. Correct.
        Pseudoinverse: $A^+ = \frac{1}{\sigma} \mathbf{v} \mathbf{u}^\top = \frac{1}{\|\mathbf{x}\|\|\mathbf{y}\|} \frac{\mathbf{y}}{\|\mathbf{y}\|} \frac{\mathbf{x}^\top}{\|\mathbf{x}\|} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.
        Note that $A^\top = (xy^\top)^\top = yx^\top$.
        Thus $A^+ = \frac{A^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Rank-1 SVD:</b> A matrix $A = xy^\top$ has exactly one non-zero singular value $\sigma_1 = \|\mathbf{x}\|_2 \|\mathbf{y}\|_2$.
        <br><b>Singular Vectors:</b> $u_1 = \mathbf{x}/\|\mathbf{x}\|$, $v_1 = \mathbf{y}/\|\mathbf{y}\|$.
        <br><b>Pseudoinverse Formula:</b> For rank-1 matrices, $A^+ = \frac{A^\top}{\|A\|_F^2} = \frac{yx^\top}{\|\mathbf{x}\|^2 \|\mathbf{y}\|^2}$.</p>
      </div>

      <h3>P1.10 — Operator Norm Properties</h3>
      <p>Prove that $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$. Use the Rayleigh Quotient definition of eigenvalues.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p>$\|A\|_2^2 = \sup_{\mathbf{x} \ne 0} \frac{\|A\mathbf{x}\|^2}{\|\mathbf{x}\|^2} = \sup_{\mathbf{x} \ne 0} \frac{\mathbf{x}^\top A^\top A \mathbf{x}}{\mathbf{x}^\top \mathbf{x}}$.
        The term on the right is the Rayleigh quotient for the symmetric matrix $M = A^\top A$.
        By the variational characterization of eigenvalues, the maximum value of the Rayleigh quotient is $\lambda_{\max}(M)$.
        Thus $\|A\|_2^2 = \lambda_{\max}(A^\top A)$, so $\|A\|_2 = \sqrt{\lambda_{\max}(A^\top A)}$.</p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Variational Definition of Eigenvalues:</b> For symmetric $M$, $\lambda_{\max}(M) = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top M \mathbf{x}$.
        <br><b>Spectral Norm Connection:</b> $\|A\|_2^2 = \max_{\|\mathbf{x}\|=1} \|A\mathbf{x}\|^2 = \max_{\|\mathbf{x}\|=1} \mathbf{x}^\top (A^\top A) \mathbf{x} = \lambda_{\max}(A^\top A)$.
        <br><b>Implication:</b> The spectral norm is the square root of the largest eigenvalue of the Gram matrix.</p>
      </div>

      <h3>P1.11 — Orthogonal Invariance</h3>
      <p>Show that $\|U A V^\top\|_F = \|A\|_F$ and $\|U A V^\top\|_2 = \|A\|_2$ for orthogonal $U, V$. This proves that these norms depend only on the singular values.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Frobenius:</b> $\|X\|_F^2 = \mathrm{tr}(X^\top X)$.
        $\|UAV^\top\|_F^2 = \mathrm{tr}(V A^\top U^\top U A V^\top) = \mathrm{tr}(V A^\top A V^\top) = \mathrm{tr}(V^\top V A^\top A) = \mathrm{tr}(A^\top A) = \|A\|_F^2$.
        <br><b>Spectral:</b> $\|UAV^\top\|_2 = \sup_{\|\mathbf{x}\|=1} \|UAV^\top \mathbf{x}\|$.
        Since $U$ is orthogonal, $\|Uy\| = \|y\|$. So we maximize $\|A (V^\top \mathbf{x})\|$.
        Since $V$ is orthogonal, as $\mathbf{x}$ ranges over the unit sphere, $\mathbf{y} = V^\top \mathbf{x}$ also ranges over the unit sphere.
        Thus $\sup_{\|\mathbf{x}\|=1} \|A V^\top \mathbf{x}\| = \sup_{\|\mathbf{y}\|=1} \|A\mathbf{y}\| = \|A\|_2$.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>Orthogonal Invariance:</b> A matrix norm is orthogonally invariant if $\|UAV^\top\| = \|A\|$ for all orthogonal $U, V$.
        <br><b>Schatten p-norms:</b> Defined as the $\ell_p$ norm of the singular values vector $\sigma(A)$.
        <ul>
            <li>$p=\infty$: Spectral Norm.</li>
            <li>$p=2$: Frobenius Norm.</li>
            <li>$p=1$: Nuclear Norm (Trace Norm).</li>
        </ul>
        <br>These are the "natural" norms for matrix analysis.</p>
      </div>

      <h3>P1.12 — The Orthogonal Group</h3>
      <p>Show that the set of orthogonal matrices forms a compact group.</p>
      <div class="solution-box">
        <h4>Solution</h4>
        <p><b>Group:</b>
        1. Closure: $(Q_1 Q_2)^\top (Q_1 Q_2) = Q_2^\top Q_1^\top Q_1 Q_2 = Q_2^\top I Q_2 = I$.
        2. Inverse: $Q^{-1} = Q^\top$, which is orthogonal since $(Q^\top)^\top Q^\top = Q Q^\top = I$.
        3. Identity: $I \in O(n)$.
        <br><b>Compactness:</b>
        1. Bounded: The columns are unit vectors, so $\|Q\|_F = \sqrt{n}$. The set is bounded.
        2. Closed: The condition $Q^\top Q = I$ is a continuous equality constraint (level set of a continuous function). Thus the set is closed.
        By Heine-Borel, closed and bounded in finite dimensions implies compact.
        </p>
      </div>
      <div class="recap-box">
        <h4>Recap</h4>
        <p><b>The Orthogonal Group $O(n)$:</b> The set of all $n \times n$ orthogonal matrices.
        <br><b>Compactness Proof:</b>
        <ul>
            <li><b>Boundedness:</b> $\|Q\|_F = \sqrt{n}$, so $O(n)$ is contained in a ball of radius $\sqrt{n}$.</li>
            <li><b>Closedness:</b> It is the solution set of continuous polynomial equations $Q^\top Q - I = 0$.</li>
        </ul>
        <br><b>Optimization Consequence:</b> Any continuous function (like a norm or trace) maximized over orthogonal matrices <i>attains</i> its maximum. This is often used to prove existence of SVD.</p>
      </div>
</section>

    <!-- SECTION 11: READINGS -->
    <section class="section-card" id="section-readings">
      <h2>9. Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Required:</strong> Boyd & Vandenberghe, Appendix A.</li>
        <li><strong>Recommended:</strong> Trefethen & Bau, <em>Numerical Linear Algebra</em>, Lectures 1–5.</li>
        <li><strong>Advanced References:</strong> Golub & Van Loan, <em>Matrix Computations</em>, Chapter 2.</li>
      </ul>
    </section>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script src="../../static/js/theme-switcher.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initSvdApproximator } from './widgets/js/svd-approximator.js';
    initSvdApproximator('widget-svd-approximator');
  </script>
  <script type="module">
    import { initConditionNumber } from './widgets/js/condition-number.js';
    initConditionNumber('widget-condition-number');
  </script>
  <script type="module">
    import { initAdvancedLaDemos } from './widgets/js/advanced-la-demos.js';
    initAdvancedLaDemos('widget-svd-geometry', 'svd');
    initAdvancedLaDemos('widget-schur-complement', 'schur');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
</body>
</html>
