<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>13. Algorithms I: Unconstrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="section-card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">13. Algorithms I: Unconstrained Minimization</h1>
      <div class="meta">
        Date: 2025-12-16 · Duration: 90 min · Tags: algorithms, unconstrained, strong-convexity, newton-method
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> A rigorous "zero-to-hero" analysis of descent methods for unconstrained convex optimization. Covers gradient descent, steepest descent, and Newton's method with full convergence proofs, including detailed derivations of strong convexity implications and self-concordance.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05</a></p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you will have a deep, step-by-step understanding of:</p>
      <ul style="line-height: 1.8;">
        <li><strong>Convergence Analysis:</strong> The rigorous, mathematical derivations of convergence rates for gradient descent. We will use Taylor's theorem to prove exactly how fast these methods approach the solution under strong convexity and smoothness assumptions.</li>
        <li><strong>Geometry of Optimization:</strong> How the shape of the function (specifically its "condition number" $\kappa$ and the width of its sublevel sets) directly controls the speed of convergence. We will visualize why "skinny" valleys are hard to optimize.</li>
        <li><strong>Steepest Descent & Norms:</strong> The profound connection between the choice of a norm (Euclidean, $\ell_1$, Quadratic) and the resulting optimization algorithm. We will see how "steepest descent" changes depending on how you measure distance.</li>
        <li><strong>Newton's Method:</strong> A deep dive into Newton's method, interpreting it as steepest descent in a changing "Hessian geometry." We will explain its "two-phase" behavior: a linear phase followed by a rapid quadratic phase.</li>
        <li><strong>Practical Implementation:</strong> How to turn these mathematical concepts into working Python code, benchmarking algorithms like Gradient Descent and Newton's Method on real problems such as Analytic Centering.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>0. Notation and the "Model"</h3>
      <p>
        We study the unconstrained optimization problem:
        $$ p^\star = \inf_{x\in \mathrm{dom}\,f} f(x) \tag{9.1} $$
        To analyze algorithms rigorously, we make specific assumptions about the function $f$. These are not just bureaucratic details; they are the mathematical foundation that allows us to prove convergence.
      </p>
      <ul>
        <li><strong>Convexity:</strong> $f$ is convex on $\mathrm{dom}\,f$. This means the domain is a convex set and the function curves "upward" (satisfying Jensen's inequality). Crucially, it implies that <em>any local minimum is a global minimum</em>.</li>
        <li><strong>Twice Continuously Differentiable ($C^2$):</strong> We assume $f \in C^2(\mathrm{dom}\,f)$.
            <ul>
                <li>The <strong>Gradient</strong> $\nabla f(x)$ exists and is continuous. This gives us the "slope" or direction of steepest increase.</li>
                <li>The <strong>Hessian</strong> $\nabla^2 f(x)$ exists and is continuous. This gives us the "curvature" information.</li>
            </ul>
        </li>
        <li><strong>Open Domain:</strong> $\mathrm{dom}\,f$ is an <strong>open set</strong>.
            <br><em>Why this matters:</em> An open set does not include its boundary. For any point $x$ in an open domain, there is a tiny ball around $x$ that is entirely contained within the domain. This ensures that we can always take a step $\epsilon$ in <em>any</em> direction without "falling off the edge" of the feasible set. Without this, standard derivatives (which rely on limits from all directions) would not be well-defined at the boundary.
        </li>
        <li><strong>Solvability:</strong> The problem is <strong>solvable</strong>, meaning the optimal value $p^\star = \min_x f(x)$ is finite and is actually achieved at some point $x^\star$. (Counter-example: $f(x) = e^{-x}$ is bounded below by 0, but never reaches it, so no $x^\star$ exists. Our theory assumes an $x^\star$ exists).</li>
      </ul>
      <p>
        <strong>Important Subtlety on Domains:</strong> If $f$ is a standard function like $x^2$ on $\mathbb{R}^n$, the domain is naturally all of $\mathbb{R}^n$, which is open. However, in convex analysis, we often deal with functions like the logarithmic barrier $f(x) = -\sum \log(x_i)$. This function is only defined when all $x_i > 0$. The domain is the "positive orthant" $\mathbb{R}^n_{++}$, which is a strict open subset of $\mathbb{R}^n$. Our algorithms must stay strictly inside this open set.
      </p>
      <p>
        <strong>The "Container" Set $S$:</strong>
        The analysis repeatedly refers to a set $S$. In this chapter, $S$ is the <strong>initial sublevel set</strong>:
        $$ S=\{x\in\mathrm{dom}\,f \mid f(x)\le f(x^{(0)})\} \tag{9.3} $$
        <strong>Why do we care about $S$?</strong>
        <br>
        Think of $S$ as a "container" or "trap" for our algorithm.
        <ol>
            <li><strong>Descent Property:</strong> Our algorithms are "descent methods," meaning $f(x^{(k+1)}) < f(x^{(k)})$.</li>
            <li><strong>Trapped:</strong> Because the function value keeps dropping, the algorithm can never climb back up to a height greater than where it started ($f(x^{(0)})$). Therefore, the entire sequence of iterates $x^{(0)}, x^{(1)}, \dots$ stays permanently inside $S$.</li>
            <li><strong>Local becomes Global:</strong> This allows us to make assumptions (like strong convexity) only on $S$, rather than on the entire universe $\mathbb{R}^n$. If the function is well-behaved <em>inside the container</em>, the algorithm will converge.</li>
        </ol>
      </p>

      <h3>1. The Core Equivalence (Optimality Conditions)</h3>
      <p>
        The unconstrained problem is:
        $$ \min_x f(x) \tag{9.1} $$
        "Unconstrained" means no explicit constraints (like $Ax=b$) are written, but constraints are implicit in the domain $\mathrm{dom}\,f$. Stepping outside the domain is forbidden (function value is effectively $+\infty$).
      </p>
      <h4>1.1 Why Convexity + Differentiability Makes Optimality Global</h4>
      <p>
        The necessary and sufficient condition for optimality is:
        $$ \nabla f(x^\star)=0 \tag{9.2} $$
        In standard nonconvex calculus, $\nabla f=0$ is merely a <em>stationary point</em> condition—it could be a local minimum, a local maximum, or a saddle point. However, <strong>convexity upgrades "stationary point" to "global minimizer."</strong>
        This relies on the <strong>first-order characterization of convexity</strong>:
        $$ f(y) \ge f(x) + \nabla f(x)^T (y-x) \qquad (\star) $$
        Geometrically, this inequality says that the first-order Taylor approximation (the tangent hyperplane) is a <strong>global underestimator</strong> of the function. The graph of the function lies entirely <em>above</em> its tangent plane at any point $x$.
      </p>
      <p><strong>Detailed Proof of $(\star)$:</strong></p>
      <ul>
          <li><strong>Definition of Convexity:</strong> By definition, for any $t \in [0,1]$, the function segment lies below the chord:
              $$ f(x+t(y-x)) \le (1-t)f(x)+t f(y) $$
          </li>
          <li><strong>Isolate $f(y)$:</strong> Rearrange the inequality to isolate the terms involving $f(y)$:
              $$ f(x+t(y-x)) - f(x) \le t(f(y)-f(x)) $$
          </li>
          <li><strong>Form Difference Quotient:</strong> Divide by $t$ (assuming $t > 0$):
              $$ \frac{f(x+t(y-x))-f(x)}{t} \le f(y)-f(x) $$
          </li>
          <li><strong>Limit $t \to 0$:</strong> Take the limit as $t \downarrow 0$. By definition of the gradient, the LHS converges to the directional derivative $\nabla f(x)^T(y-x)$.
              $$ \nabla f(x)^T(y-x) \le f(y) - f(x) $$
              Rearranging gives the result: $f(y) \ge f(x) + \nabla f(x)^T (y-x)$.
          </li>
      </ul>
      <p><strong>Proof of Equivalence ($\nabla f(x^\star)=0 \iff \text{Global Min}$):</strong></p>
      <ul>
          <li><strong>Sufficiency ($\nabla f(x^\star)=0 \Rightarrow$ Min):</strong>
              Suppose $\nabla f(x^\star) = 0$. We plug this into the convexity inequality $(\star)$. For any feasible point $y$:
              $$ f(y) \ge f(x^\star) + \underbrace{\nabla f(x^\star)^T}_{0} (y-x^\star) = f(x^\star) $$
              Since $f(y) \ge f(x^\star)$ for all $y$, $x^\star$ is a global minimizer.
          </li>
          <li><strong>Necessity (Min $\Rightarrow \nabla f(x^\star)=0$):</strong>
              We prove this by contradiction. Assume $x^\star$ is a local minimizer but $\nabla f(x^\star) \neq 0$.
              <ol>
                  <li><strong>Open Domain:</strong> Since $\mathrm{dom}\,f$ is open, for any direction $v$, we can move a small distance $t$ in that direction without leaving the domain.</li>
                  <li><strong>Choose Descent Direction:</strong> Let $v = -\nabla f(x^\star)$. This is the direction of steepest descent.</li>
                  <li><strong>Single-Variable Function:</strong> Define $\phi(t) = f(x^\star + tv)$. Since $x^\star$ is a minimizer, $\phi(t)$ must have a minimum at $t=0$.</li>
                  <li><strong>First Derivative Test:</strong> Calculus tells us that for $t=0$ to be a minimum, $\phi'(0)$ must be 0 (or $\ge 0$ if constrained, but we are unconstrained).
                      $$ \phi'(0) = \nabla f(x^\star)^T v = \nabla f(x^\star)^T (-\nabla f(x^\star)) = -\|\nabla f(x^\star)\|_2^2 $$
                  </li>
                  <li><strong>Contradiction:</strong> If $\nabla f(x^\star) \neq 0$, then $- \|\nabla f(x^\star)\|_2^2 < 0$. This means $\phi'(0) < 0$.
                      <br>
                      Since the derivative is negative, the function is <em>decreasing</em> at $t=0$. For a very small $t > 0$, we must have $\phi(t) < \phi(0)$, which means $f(x^\star + tv) < f(x^\star)$.
                  </li>
                  <li><strong>Conclusion:</strong> This contradicts the assumption that $x^\star$ is a minimizer. Therefore, $\nabla f(x^\star)$ must be 0.</li>
              </ol>
          </li>
      </ul>

      <h3>2. Motivating Examples</h3>
      <p>Before diving into the proofs, let's look at the canonical problems we are trying to solve. These examples will serve as our testbed for algorithms.</p>
      <h4>2.1 Quadratic Minimization</h4>
      <p>
        The most basic convex optimization problem is the unconstrained quadratic program:
        $$ \min_x \ f(x) = \frac12 x^T P x + q^T x + r,\qquad P\in S^n_+ \tag{9.4} $$
        where $P$ is symmetric positive semidefinite ($P \succeq 0$).
      </p>
      <div style="background: var(--code-bg); padding: 12px; border-left: 4px solid var(--accent); margin-bottom: 16px;">
        <strong>Step-by-Step Derivation of Gradient:</strong>
        <br>
        Let's compute $\nabla (\frac{1}{2} x^T P x)$. Write it in scalar sum notation:
        $$ \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n P_{ij} x_i x_j $$
        Now take the partial derivative with respect to $x_k$:
        $$ \frac{\partial}{\partial x_k} \left( \frac{1}{2} \sum_{i,j} P_{ij} x_i x_j \right) = \frac{1}{2} \sum_{j} P_{kj} x_j + \frac{1}{2} \sum_{i} P_{ik} x_i $$
        Using symmetry ($P_{ij} = P_{ji}$), both sums are equal to the $k$-th row of $P$ dot product with $x$.
        $$ = \sum_{j} P_{kj} x_j = (Px)_k $$
        Thus, the vector gradient is $\nabla f(x) = Px + q$.
      </div>
      <p>
        Setting the gradient to zero gives the linear system:
        $$ Px^\star = -q $$
      </p>
      <ul>
          <li><strong>Case 1: $P \succ 0$ (Positive Definite).</strong> The matrix $P$ is invertible. There is a <strong>unique global minimizer</strong>: $x^\star = -P^{-1}q$. The level sets are ellipsoids, and the function looks like a bowl.</li>
          <li><strong>Case 2: $P \succeq 0$ but Singular.</strong> The matrix $P$ has a non-trivial nullspace.
            <ul>
                <li>If $-q \in \mathrm{range}(P)$, there are infinitely many solutions (a linear subspace of solutions).</li>
                <li>If $-q \notin \mathrm{range}(P)$, the system is inconsistent. The function is unbounded below (decreases to $-\infty$ along directions in the nullspace).</li>
            </ul>
          </li>
      </ul>

      <h4>2.2 Least Squares</h4>
      <p>
        $$ \min_x \|Ax-b\|_2^2 $$
        We can expand the squared norm to reveal it is a quadratic function:
        $$ f(x) = (Ax-b)^T (Ax-b) = x^T A^T A x - 2b^T A x + b^T b $$
        Comparing this to the standard form in 2.1:
        <ul>
            <li>$P = 2A^T A$ (Always symmetric positive semidefinite).</li>
            <li>$q = -2A^T b$.</li>
        </ul>
        The optimality condition $\nabla f(x) = 0$ becomes the famous <strong>Normal Equations</strong>:
        $$ 2A^T A x - 2A^T b = 0 \implies A^T A x = A^T b $$
      </p>

      <h4>2.3 Log-Sum-Exp (Softmax)</h4>
      <p>
        This function approximates the "max" function ($\max_i (a_i^T x + b_i)$) but is smooth and convex:
        $$ f(x)=\log\Big(\sum_{i=1}^m e^{a_i^T x+b_i}\Big) $$
        <strong>Gradient Interpretation:</strong> The gradient is a probability-weighted sum of the vectors $a_i$:
        $$ \nabla f(x) = \sum_{i=1}^m w_i(x) a_i $$
        where the weights $w_i(x)$ are given by the <strong>Softmax</strong> function:
        $$ w_i(x) = \frac{e^{a_i^T x+b_i}}{\sum_j e^{a_j^T x+b_j}} $$
        Note that $w_i(x) > 0$ and $\sum w_i(x) = 1$.
        <br>
        <strong>Meaning:</strong> The gradient is a <strong>convex combination</strong> (weighted average) of the vectors $a_i$. If one term $k$ is significantly larger than the others, $w_k \approx 1$ and the others $\approx 0$, so $\nabla f(x) \approx a_k$ (which is the gradient of the pure $\max$ function).
        <br>
        <strong>Hessian:</strong> The Hessian turns out to be the covariance matrix of the vectors $a_i$ under this distribution. Since covariance matrices are always PSD, $\nabla^2 f(x) \succeq 0$, proving convexity.
      </p>

      <h4>2.4 Analytic Centers (Logarithmic Barriers)</h4>
      <p>
        $$ f(x) = -\sum_{i=1}^m \log(b_i - a_i^T x) $$
        defined on the polyhedron $\mathcal{P} = \{x \mid a_i^T x < b_i \forall i\}$.
        <br>
        <strong>Intuition:</strong> The term $-\log(b_i - a_i^T x)$ approaches $+\infty$ as $a_i^T x \to b_i$. This acts as a <strong>repulsive force field</strong> from the boundary of the feasible set.
        Minimizing $f(x)$ finds a point $x^\star$ that is "maximally inside" the set, balancing the repulsion from all walls. This point is called the <strong>Analytic Center</strong>. This concept is the engine behind <strong>Interior Point Methods</strong>, which solve constrained problems by following a path of analytic centers.
      </p>

      <h3>3. Strong Convexity: The Engine of Convergence</h3>
      <p>
        We assume $f$ is <strong>strongly convex on $S$</strong>. This is a condition on the curvature of the function. Formally, there exists a scalar $m>0$ such that:
        $$ \nabla^2 f(x)\succeq mI,\qquad \forall x\in S. \tag{9.7} $$
        <strong>Meaning:</strong> The eigenvalues of the Hessian are all at least $m$. For every non-zero vector $v$, the curvature along direction $v$ satisfies $v^T\nabla^2 f(x)v \ge m\|v\|_2^2$.
        <br>
        Intuitively, the function is "at least as curved as a quadratic bowl with curvature $m$." It cannot contain any flat regions.
      </p>

      <h4>3.1 From Taylor’s Theorem to the Quadratic Lower Bound</h4>
      <p>
        Let's derive the most important consequence of strong convexity.
        Consider two points $x,y\in S$. Taylor’s theorem with the <strong>Lagrange remainder form</strong> states that there exists a point $z$ on the line segment connecting $x$ and $y$ such that:
        $$ f(y) = f(x) + \nabla f(x)^T(y-x) + \underbrace{\frac{1}{2} (y-x)^T \nabla^2 f(z) (y-x)}_{\text{Remainder Term } R_1(y)} $$
        Because $z \in S$ (due to convexity of $S$), we can apply our strong convexity assumption ($\nabla^2 f(z) \succeq mI$). This allows us to bound the remainder term from below:
        $$ (y-x)^T \nabla^2 f(z) (y-x) \ge m \|y-x\|_2^2 $$
        Substituting this lower bound into the Taylor expansion yields the <strong>Quadratic Lower Bound</strong>:
        $$ f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{m}{2} \|y-x\|_2^2 \tag{9.8} $$
        <strong>Geometric Interpretation:</strong>
        <ul>
            <li>For a standard convex function ($m=0$), the graph sits above the tangent plane: $f(y) \ge f(x) + \nabla f(x)^T(y-x)$.</li>
            <li>For a strongly convex function, the graph sits above a <strong>paraboloid</strong> that touches the graph at $x$. This paraboloid grows quadratically, forcing the function to rise steeply as we move away from the minimum.</li>
        </ul>
      </p>

      <h4>3.2 Bounding Suboptimality and Distance to Optimizer</h4>
      <p>
        The quadratic lower bound allows us to bound how far we are from the solution, both in terms of function value and distance in $\mathbb{R}^n$.
        <br>
        <strong>1. Suboptimality Bound ($f(x) - p^\star$):</strong>
        Fix $x$. The right-hand side of (9.8) is a function of $y$:
        $$ g(y) = f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|^2 $$
        Since $f(y) \ge g(y)$ for all $y$, minimizing both sides implies that $\min_y f(y) \ge \min_y g(y)$, so $p^\star \ge \min_y g(y)$.
        <br>
        To find $\min_y g(y)$, we set its gradient to zero:
        $$ \nabla_y g(y) = \nabla f(x) + m(y-x) = 0 $$
        Solving for the optimal $y$ (let's call it $\tilde{y}$):
        $$ \tilde{y} = x - \frac{1}{m}\nabla f(x) $$
        Plugging $\tilde{y}$ back into the expression for $g(y)$:
        $$ \min_y g(y) = f(x) + \nabla f(x)^T \left(-\frac{1}{m}\nabla f(x)\right) + \frac{m}{2} \left\|-\frac{1}{m}\nabla f(x)\right\|^2 $$
        $$ = f(x) - \frac{1}{m}\|\nabla f(x)\|^2 + \frac{1}{2m}\|\nabla f(x)\|^2 = f(x) - \frac{1}{2m}\|\nabla f(x)\|^2 $$
        Therefore, since $p^\star \ge \min_y g(y)$:
        $$ p^\star \ge f(x) - \frac{1}{2m}\|\nabla f(x)\|_2^2 \implies f(x)-p^\star \le \frac{1}{2m}\|\nabla f(x)\|_2^2 \tag{9.9} $$
        This is incredibly useful. It tells us that if the gradient is small, the function value is close to optimal. We can use $\|\nabla f(x)\|_2 \le \sqrt{2m\epsilon}$ as a rigorous stopping criterion.
      </p>
      <p>
        <strong>2. Distance to Optimizer ($\|x - x^\star\|$):</strong>
        Applying the master inequality (9.8) with $y=x^\star$:
        $$ p^\star \ge f(x)+\nabla f(x)^T(x^\star-x)+\frac{m}{2}\|x^\star-x\|^2 $$
        Since $p^\star \le f(x)$, we have:
        $$ 0 \ge \nabla f(x)^T(x^\star-x)+\frac{m}{2}\|x^\star-x\|^2 $$
        Rearranging:
        $$ -\nabla f(x)^T(x^\star-x) \ge \frac{m}{2}\|x^\star-x\|^2 $$
        Using the Cauchy-Schwarz inequality ($|a^Tb| \le \|a\|\|b\|$):
        $$ \|\nabla f(x)\|_2 \|x^\star-x\|_2 \ge \frac{m}{2}\|x^\star-x\|_2^2 $$
        Dividing both sides by $\|x^\star - x\|_2$ (assuming $x \neq x^\star$):
        $$ \|x-x^\star\|_2 \le \frac{2}{m}\|\nabla f(x)\|_2 \tag{9.11} $$
        This implies the optimizer $x^\star$ is <strong>unique</strong>.
      </p>

      <h3>4. Smoothness: The Guarantee of Descent</h3>
      <p>
        Complementing the lower bound, we need an upper bound on curvature.
        Since $S$ is bounded (which is implied by strong convexity) and closed, and $f$ is twice continuously differentiable, the maximum eigenvalue of the Hessian must be bounded above by some constant $M$ on $S$:
        $$ \nabla^2 f(x)\preceq MI,\qquad \forall x\in S \tag{9.12} $$
        This property is called <strong>M-smoothness</strong> (or Lipschitz continuity of the gradient with constant $M$).
      </p>
      <h4>4.1 Quadratic Upper Bound</h4>
      <p>
        Using Taylor's theorem in the exact same way as before, but using the upper bound on the Hessian:
        $$ f(y) \le f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}\|y-x\|_2^2 \tag{9.13} $$
        <strong>The Sandwich Principle:</strong> Combining (9.8) and (9.13), we see that the function $f(y)$ is <strong>sandwiched</strong> between two quadratic bowls anchored at $x$:
      </p>
      <div style="text-align: center; margin: 16px 0; padding: 12px; background: var(--panel); border-radius: 8px;">
          $$ \underbrace{f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|^2}_{\text{Lower Paraboloid (Blue)}} \le f(y) \le \underbrace{f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}\|y-x\|^2}_{\text{Upper Paraboloid (Red)}} $$
          <p style="font-size: 0.9em; color: #666; margin-top: 8px;">The function lives in the "Goldilocks zone" between these two quadratics.</p>
      </div>
      <p>
        <strong>Implication for Descent:</strong> The upper bound guarantees that if we take a small enough step opposite to the gradient, the function value <em>must</em> decrease.
        Minimizing the quadratic upper bound (RHS of 9.13) with respect to $y$ gives $y = x - \frac{1}{M}\nabla f(x)$. Plugging this specific $y$ into the inequality:
        $$ f\left(x - \frac{1}{M}\nabla f(x)\right) \le f(x) - \frac{1}{M}\|\nabla f(x)\|^2 + \frac{M}{2}\frac{1}{M^2}\|\nabla f(x)\|^2 = f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 $$
        This implies:
        $$ p^\star \le f(x) - \frac{1}{2M}\|\nabla f(x)\|_2^2 \tag{9.14} $$
        So, a single gradient step reduces the function value by at least $\frac{1}{2M}\|\nabla f(x)\|^2$.
      </p>
      <p>
        <strong>Summary of Bounds:</strong>
        $$ \frac{1}{2M}\|\nabla f(x)\|^2 \le f(x)-p^\star \le \frac{1}{2m}\|\nabla f(x)\|^2 $$
      </p>

      <h3>5. Condition Number of Sublevel Sets</h3>
      <p>
        From (9.7) and (9.12), the eigenvalues of the Hessian are bounded: $mI \preceq \nabla^2 f(x) \preceq MI$ for all $x \in S$.
        The ratio $\kappa = M/m$ bounds the condition number of the Hessian matrix at any point:
        $$ \kappa(\nabla^2 f(x))=\frac{\lambda_{\max}(\nabla^2 f(x))}{\lambda_{\min}(\nabla^2 f(x))}\le \frac{M}{m} $$
        We call this upper bound $\kappa$ the condition number of the problem (on $S$).
      </p>
      <h4>5.1 Geometric Meaning: Eccentricity and "Skinniness"</h4>
      <p>
        The condition number $\kappa$ measures the <strong>anisotropy</strong> (direction-dependence) of the function's curvature.
        <ul>
            <li><strong>$\kappa \approx 1$ (Isotropic):</strong> The function is shaped like a nice, round bowl (spherical level sets). The gradient points straight to the minimum. Walking towards the minimum is like walking across a soccer field—easy and direct.</li>
            <li><strong>$\kappa \gg 1$ (Anisotropic):</strong> The function is shaped like a narrow, deep valley (elongated ellipsoidal level sets). The gradient is nearly orthogonal to the direction of the minimum. Walking towards the minimum is like navigating a long, narrow hallway; you bounce off the walls if you aren't perfectly aligned.</li>
        </ul>
        Geometrically, any sublevel set $C_\alpha = \{x : f(x) \le \alpha\}$ contains a ball of radius $r$ and is contained in a ball of radius $R$. The ratio $R^2/r^2$ is bounded by $\kappa = M/m$. Thus, $\sqrt{\kappa}$ represents the maximum "aspect ratio" or eccentricity of the level sets.
      </p>
      <div style="background: var(--code-bg); padding: 16px; border-radius: 8px; border-left: 4px solid var(--accent); margin-bottom: 16px;">
        <strong>Example: Width of an Ellipsoid.</strong> Let $\mathcal{E}=\{x\mid (x-x_0)^T A^{-1}(x-x_0)\le 1\}$ with $A \in S^n_{++}$.
        The "width" of the ellipsoid varies with direction.
        The condition number of the set is exactly the condition number of the matrix $A$:
        $$ \mathrm{cond}(\mathcal{E}) = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)} = \kappa(A) $$
        If $\kappa = 100$, the ellipsoid is 10 times longer in one direction than another ($\sqrt{100}=10$). Gradient descent will "bounce" back and forth across the narrow valley rather than moving down it.
      </div>

      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/kappa_effect_paths.gif" alt="Gradient descent zig-zag worsening with condition number" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;"><strong>The Condition Number Tax:</strong> As $\kappa = M/m$ grows (ellipses become thinner), gradient descent zig-zags significantly, slowing convergence linearly with $\kappa$.</figcaption>
      </figure>

      <h3>6. Descent Methods</h3>
      <p>
        Most unconstrained optimization algorithms follow a simple iterative structure. We produce a sequence of points $x^{(k)}, k=1,\dots$ such that $f(x^{(k)}) \to p^\star$.
        <br>
        <strong>General Algorithm:</strong>
        <pre><code class="language-python">
def descent_method(f, gradient, x0, tol=1e-6):
    x = x0
    while True:
        # 1. Compute Descent Direction
        # For standard gradient descent, this is simply -gradient(x)
        delta_x = compute_direction(x)

        # 2. Check Stopping Criterion
        # Why this formula? Recall bound (9.9): f(x) - p* <= (1/2m) * ||grad||^2
        # In Newton's method, -g.dot(dx) / 2 is exactly the "Newton Decrement" squared.
        # It's a rigorous estimate of the suboptimality gap.
        if -gradient(x).dot(delta_x) / 2 <= tol:
            break

        # 3. Line Search (Step Size)
        # Find a scalar t > 0 such that f(x + t*delta_x) < f(x)
        t = line_search(f, x, delta_x)

        # 4. Update
        x = x + t * delta_x
    return x
        </code></pre>
        The search direction $\Delta x$ must be a <strong>descent direction</strong>: $\nabla f(x)^T \Delta x < 0$. This ensures that for small enough step sizes, the objective function decreases.
      </p>

      <h4>6.1 Exact Line Search: A Rigorous Proof of Linear Convergence</h4>
      <p>
        Suppose we choose step size $t$ to minimize $f$ exactly along the ray (i.e., $t = \text{argmin}_{s \ge 0} f(x + s \Delta x)$). How fast does the error decay?
        <br>
        <strong>Proof Step 1: Guaranteed Decrease from Smoothness.</strong>
        We don't know the exact optimal step $t$, but we know exact line search performs <em>at least as well</em> as any specific step choice.
        Let's test the specific step $t = 1/M$. From the smoothness upper bound (9.13), taking step $t = 1/M$ along the descent direction $d = -\nabla f(x)$ gives:
        $$ f(x - \frac{1}{M}\nabla f(x)) \le f(x) + \nabla f(x)^T(-\frac{1}{M}\nabla f(x)) + \frac{M}{2}\left\|-\frac{1}{M}\nabla f(x)\right\|^2 $$
        $$ = f(x) - \frac{1}{M}\|\nabla f(x)\|^2 + \frac{1}{2M}\|\nabla f(x)\|^2 = f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 $$
        Since the exact line search finds the global minimum along the line, it satisfies:
        $$ f(x_{\text{next}}) \le f(x - \frac{1}{M}\nabla f(x)) \le f(x) - \frac{1}{2M}\|\nabla f(x)\|^2 $$
        <br>
        <strong>Proof Step 2: Relating Gradient to Optimality Gap.</strong>
        We need to relate $\|\nabla f(x)\|^2$ to the error $f(x) - p^\star$.
        Recall the Strong Convexity bound (9.9):
        $$ f(x) - p^\star \le \frac{1}{2m}\|\nabla f(x)\|^2 \implies \|\nabla f(x)\|^2 \ge 2m(f(x) - p^\star) $$
        Substituting this into our inequality from Step 1:
        $$ f(x_{\text{next}}) \le f(x) - \frac{1}{2M} \left( 2m(f(x) - p^\star) \right) = f(x) - \frac{m}{M}(f(x) - p^\star) $$
        <br>
        <strong>Proof Step 3: Subtracting $p^\star$.</strong>
        Subtract $p^\star$ from both sides to see the evolution of the error $\epsilon_k = f(x^{(k)}) - p^\star$:
        $$ f(x_{\text{next}}) - p^\star \le (f(x) - p^\star) - \frac{m}{M}(f(x) - p^\star) $$
        $$ \epsilon_{k+1} \le \left(1 - \frac{m}{M}\right) \epsilon_k $$
        This is <strong>Linear Convergence</strong>. The error contracts by a fixed factor $c = 1 - m/M < 1$ at every step.
        <br>
        <em>Note:</em> The contraction factor $1 - 1/\kappa$ depends directly on the condition number.
        <ul>
            <li>If $\kappa=1$ (perfectly spherical), error becomes 0 in one step.</li>
            <li>If $\kappa=1000$ (ill-conditioned), $c = 0.999$, so we only remove 0.1% of the error per step.</li>
        </ul>
      </p>

      <h4>6.2 Backtracking (Armijo) Line Search</h4>
      <p>
        Exact minimization is usually too expensive. Instead, we use <strong>Backtracking Line Search</strong>. It guarantees a "sufficient decrease" without finding the exact minimum.
        <br>
        <strong>The Algorithm:</strong>
        <pre><code class="language-python">
def backtracking_line_search(f, x, delta_x, gradient, alpha=0.3, beta=0.8):
    """
    alpha: Fraction of decrease we demand (0 < alpha < 0.5)
           Usually 0.01 to 0.3.
    beta:  Shrink factor (0 < beta < 1).
           Usually 0.1 to 0.8.
    """
    t = 1.0  # Start with full step
    current_f = f(x)
    linear_term = alpha * gradient.dot(delta_x)

    # While function is above the "Armijo Line"
    while f(x + t * delta_x) > current_f + t * linear_term:
        t = t * beta  # Reduce step size

    return t
        </code></pre>
        <strong>Geometric Interpretation:</strong>
        The condition $f(x+t\Delta x) \le f(x) + \alpha t \nabla f^T \Delta x$ forces the function value to lie below a linear ray starting at $f(x)$ with slope $\alpha \nabla f^T \Delta x$.
        Since the actual slope of the function at $t=0$ is $\nabla f^T \Delta x$ (which is steeper/more negative than $\alpha \nabla f^T \Delta x$ because $\alpha < 1$), the condition <strong>must</strong> hold for sufficiently small $t$. The loop is guaranteed to terminate.
      </p>
      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/backtracking_armijo.gif" alt="Backtracking line search visualization" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Backtracking reduces step size $t$ until the function value drops below the linear extrapolation (Armijo line).</figcaption>
      </figure>

      <h3>7. Steepest Descent</h3>
      <p>
        Standard gradient descent follows the negative gradient. But why? This is the direction of steepest descent <strong>only if distance is measured in the Euclidean norm ($\ell_2$)</strong>.
        <br>
        <strong>Definition:</strong> The normalized steepest descent direction $\Delta x_{\mathrm{nsd}}$ solves:
        $$ \Delta x_{\mathrm{nsd}} = \text{argmin}_{v} \{ \nabla f(x)^T v \mid \|v\| \le 1 \} $$
        It is the direction that yields the most negative directional derivative per unit of "length."
        <br>
        <strong>Intuition of Dual Norms:</strong>
        We are minimizing a linear function $\nabla f(x)^T v$ over a unit ball defined by a norm $\|\cdot\|$.
        Recall the definition of the <strong>dual norm</strong>:
        $$ \|z\|_* = \sup \{z^T x \mid \|x\| \le 1\} $$
        The maximum value of $z^T x$ over the unit ball is $\|z\|_*$.
        Since we want to <em>minimize</em> $\nabla f(x)^T v$, we want to achieve $-\|\nabla f(x)\|_*$.
      </p>

      <h4>7.1 Unnormalized Step and Cases</h4>
      <p>
        For convenience, we define the <strong>unnormalized</strong> steepest descent step as:
        $$ \Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_* \Delta x_{\mathrm{nsd}} $$
        This scaling ensures that the directional derivative is:
        $$ \nabla f(x)^T \Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_* (\nabla f(x)^T \Delta x_{\mathrm{nsd}}) = \|\nabla f(x)\|_* (-\|\nabla f(x)\|_* ) = -\|\nabla f(x)\|_*^2 $$
      </p>

      <div class="card-grid" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 20px;">

        <!-- Case 1: L2 Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">1. Euclidean Norm ($\ell_2$)</h4>
            <p><strong>Norm:</strong> $\|v\|_2 = \sqrt{v^T v}$. <strong>Dual:</strong> $\|g\|_2$.</p>
            <p><strong>Geometry:</strong> The unit ball is a sphere.</p>
            <p><strong>Direction:</strong> $\Delta x_{\mathrm{sd}} = -\nabla f(x)$.</p>
            <p><strong>Result:</strong> Standard Gradient Descent. The gradient is perpendicular to the tangent plane of the level set.</p>
        </div>

        <!-- Case 2: Quadratic Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">2. Quadratic Norm (Preconditioning)</h4>
            <p><strong>Norm:</strong> $\|v\|_P = \sqrt{v^T P v}$ ($P \in S^n_{++}$). <strong>Dual:</strong> $\|g\|_{P^{-1}}$.</p>
            <p><strong>Geometry:</strong> The unit ball is an ellipsoid $\{v \mid v^T P v \le 1\}$.</p>
            <p><strong>Direction:</strong> $\Delta x_{\mathrm{sd}} = -P^{-1}\nabla f(x)$.</p>
            <p><strong>Result:</strong> Preconditioned Gradient Descent. By choosing $P \approx \nabla^2 f(x)$, we align the "metric" with the function's curvature, turning ellipses into circles.</p>
            <figure style="margin: 12px 0; text-align: center;">
                <img src="assets/preconditioning_comparison.gif" alt="Preconditioned GD vs GD" style="width: 100%; border-radius: 4px;">
            </figure>
        </div>

        <!-- Case 3: L1 Norm -->
        <div style="background: var(--panel); padding: 16px; border-radius: 8px; border: 1px solid var(--border);">
            <h4 style="margin-top: 0;">3. $\ell_1$ Norm (Manhattan)</h4>
            <p><strong>Norm:</strong> $\|v\|_1 = \sum |v_i|$. <strong>Dual:</strong> $\|g\|_\infty = \max_i |g_i|$.</p>
            <p><strong>Geometry:</strong> The unit ball is a diamond (polytope).</p>
            <p><strong>Direction:</strong> Let $i$ be the index where $|(\nabla f)_i|$ is maximal.
            $$ \Delta x_{\mathrm{sd}} = -\text{sign}\left( \frac{\partial f}{\partial x_i} \right) e_i $$
            </p>
            <p><strong>Result:</strong> Coordinate Descent (Greedy). We only update the single variable with the largest partial derivative.</p>
            <figure style="margin: 12px 0; text-align: center;">
                <img src="assets/l1_coordinate_descent_vs_gd.gif" alt="L1 Coordinate Descent vs GD" style="width: 100%; border-radius: 4px;">
            </figure>
        </div>
      </div>

      <h3>8. Newton's Method</h3>
      <p>
        Newton's method is the gold standard for unconstrained minimization. It uses curvature information (the Hessian) to take much smarter steps than gradient descent.
        $$ \Delta x_{\mathrm{nt}} = -\nabla^2 f(x)^{-1} \nabla f(x) $$
      </p>
      <h4>8.1 Three Interpretations</h4>
      <ol>
          <li><strong>Minimizing the Quadratic Model:</strong>
          We approximate $f$ near $x$ by its second-order Taylor expansion:
          $$ \hat{f}(x+v) = f(x) + \nabla f(x)^T v + \frac{1}{2}v^T \nabla^2 f(x) v $$
          To find the best step $v$, we minimize this quadratic. Setting the derivative with respect to $v$ to zero:
          $$ \nabla f(x) + \nabla^2 f(x) v = 0 \implies v = -\nabla^2 f(x)^{-1} \nabla f(x) $$
          <figure style="margin: 12px 0; text-align: center;">
            <img src="assets/newton_quadratic_model.gif" alt="Newton step minimizes local quadratic model" style="width: 100%; max-width: 500px; border-radius: 8px; border: 1px solid #ddd;">
            <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Newton's method jumps directly to the minimum of the local quadratic approximation.</figcaption>
          </figure>
          </li>
          <li><strong>Steepest Descent in Hessian Norm:</strong>
          It is exactly steepest descent with respect to the norm $\|v\|_H = \sqrt{v^T \nabla^2 f(x) v}$.
          This means Newton's method is "Geometry-Adaptive." It resizes the unit ball at every step to match the local curvature of the function.
          </li>
          <li><strong>Affine Invariance:</strong>
          A crucial property. If we change coordinates $x = Ty$ (where $T$ is invertible), the algorithm produces the exact same sequence of points (mapped through $T$).
          <br>
          <strong>Proof of Affine Invariance:</strong>
          Let $g(y) = f(Ty)$ be the function in the new coordinates.
          <br>
          1. <strong>Gradients:</strong> By chain rule, $\nabla g(y) = T^T \nabla f(x)$.
          <br>
          2. <strong>Hessians:</strong> $\nabla^2 g(y) = T^T \nabla^2 f(x) T$.
          <br>
          3. <strong>Newton Step for $g$:</strong>
          $$ \Delta y_{\mathrm{nt}} = -(\nabla^2 g(y))^{-1} \nabla g(y) = -(T^T \nabla^2 f(x) T)^{-1} (T^T \nabla f(x)) $$
          Using $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$:
          $$ = -T^{-1} (\nabla^2 f(x))^{-1} (T^{-T} T^T) \nabla f(x) $$
          $$ = -T^{-1} (\nabla^2 f(x))^{-1} \nabla f(x) $$
          $$ = T^{-1} \Delta x_{\mathrm{nt}} $$
          Since $x = Ty$, a displacement of $\Delta y$ corresponds to a displacement of $T \Delta y = \Delta x$. The steps correspond perfectly!
          <br>
          <em>Contrast with Gradient Descent:</em> The gradient descent step is $\Delta y = -T^T \nabla f(x)$. This involves $T^T$, not $T^{-1}$. It does <strong>not</strong> transform correctly.
          </li>
      </ol>

      <h4>8.2 The Newton Decrement $\lambda(x)$</h4>
      <p>
        The quantity $\lambda(x) = (\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x))^{1/2}$ is called the <strong>Newton Decrement</strong>.
        It plays a dual role:
        <ul>
            <li><strong>Stopping Criterion:</strong> It is an affine-invariant measure of the proximity to optimality. $\frac{1}{2}\lambda(x)^2$ is exactly the difference between $f(x)$ and the minimum of the local quadratic model. Stopping when $\lambda(x)^2/2 \le \epsilon$ is a rigorous standard.</li>
            <li><strong>Directional Derivative:</strong> The slope in the Newton direction is $-\lambda(x)^2$.
            $$ \nabla f(x)^T \Delta x_{\mathrm{nt}} = -\nabla f(x)^T (\nabla^2 f)^{-1} \nabla f(x) = -\lambda(x)^2 $$
            </li>
        </ul>
      </p>

      <h4>8.3 Implementation</h4>
      <pre><code class="language-python">
def newton_step(f, grad, hess, x):
    g = grad(x)
    H = hess(x)

    # Solve linear system H * dx = -g
    # More stable than inv(H)
    delta_x = np.linalg.solve(H, -g)

    # Compute Newton Decrement
    lambda_sq = g.dot(np.linalg.solve(H, g)) # g^T H^-1 g

    return delta_x, lambda_sq
      </code></pre>

      <h3>9. Convergence Analysis & Self-Concordance</h3>
      <p>
        Newton's method has a unique two-phase convergence behavior.
      </p>
      <h4>9.1 The Two Phases</h4>
      <ul>
          <li><strong>Damped Phase (Far from $x^\star$):</strong>
          When gradient is large, we must use a step size $t < 1$ (via backtracking). The function value decreases by a constant amount at each step. This phase is linear, but transient.
          </li>
          <li><strong>Pure Newton Phase (Close to $x^\star$):</strong>
          Once we get close enough (specifically, when $\lambda(x) < 0.68$), the backtracking search will always accept $t=1$.
          Convergence becomes <strong>Quadratic</strong>. The number of correct digits doubles at every iteration.
          $$ \frac{\epsilon_{k+1}}{\epsilon_k^2} \le C \quad \implies \quad \epsilon_{k+1} \approx \epsilon_k^2 $$
          In practice, this means you go from 1 digit of accuracy to 2, then 4, then 8, then 16 (machine precision) in just 4-5 steps.
          </li>
      </ul>

      <h4>9.2 Self-Concordance: Theoretical Rigor</h4>
      <p>
        Classical convergence proofs rely on unknown constants $m, M, L$. <strong>Self-Concordance</strong> is a scale-invariant property that limits how fast the Hessian changes:
        $$ |f'''(x)[h,h,h]| \le 2 (f''(x)[h,h])^{3/2} $$
        Functions like $-\log(x)$ and linear/quadratic functions satisfy this.
        <br>
        <strong>Why it matters:</strong> It allows us to prove complexity bounds that depend <strong>only</strong> on the problem structure (e.g., number of constraints) and not on the data values. It is the theoretical foundation of Interior Point Methods.
      </p>

      <h4>9.3 Quasi-Newton Methods (BFGS)</h4>
      <p>
        If $N$ is large ($> 1000$), computing $\nabla^2 f^{-1}$ ($O(N^3)$) is too slow.
        <strong>Quasi-Newton methods</strong> update an approximation of the Hessian $B \approx \nabla^2 f$ using the change in gradients ($\Delta g = B \Delta x$).
        <br>
        <strong>BFGS Update:</strong> A rank-2 update rule that preserves symmetry and positive definiteness. It converges <strong>superlinearly</strong> (faster than gradient descent, but no quadratic doubling).
      </p>

    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Gradient Descent Visualizer</h3>
        <p>Visualize the trajectory of gradient descent for a 2D quadratic function.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- Widget 4: Norm Steepest Descent -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Steepest Descent Geometry</h3>
        <p>Visualizing how the norm determines the "steepest" direction. L2 steepest opposes the gradient. L1 steepest snaps to an axis (coordinate descent).</p>
        <div id="widget-norm-steepest" style="width: 100%; height: auto; position: relative; text-align: center;"></div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">GD vs. Newton Race</h3>
        <p>Compare the convergence of gradient descent and Newton's method.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;"></div>
      </div>

      <!-- Widget 3 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Convergence Rate Comparison</h3>
        <p>Plots the convergence rates of different first-order methods.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;"></div>
      </div>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Solved Exercises & Example Problems</h2>

      <div class="example-problem">
        <h3>Exercise 9.1: Minimizing a Quadratic Function (Detailed)</h3>
        <p><strong>Problem:</strong> Consider minimizing $f(x)=\frac{1}{2}x^T P x+q^T x+r$ where $P \in S^n$.
        <br>(a) Show that if $P \not\succeq 0$, the problem is unbounded below.
        <br>(b) Suppose $P \succeq 0$ but $Px = -q$ has no solution. Show it is unbounded below.</p>

        <p><strong>Step-by-Step Solution:</strong></p>
        <p><strong>Part (a): $P \not\succeq 0$.</strong>
        <br>1. <strong>Definition:</strong> If $P$ is not positive semidefinite, there exists at least one eigenvector $v$ with a negative eigenvalue $\lambda < 0$. Thus, $v^T P v = \lambda \|v\|^2 < 0$.
        <br>2. <strong>Construct Ray:</strong> Consider the path $x(t) = tv$ for $t > 0$.
        <br>3. <strong>Evaluate Function:</strong>
        $$ f(tv) = \frac{1}{2}t^2 (v^T P v) + t(q^T v) + r $$
        <br>4. <strong>Limit:</strong> As $t \to \infty$, the term $t^2 (v^T P v)$ dominates because it grows quadratically. Since its coefficient is negative, $f(tv) \to -\infty$.
        </p>

        <p><strong>Part (b): $P \succeq 0$ but $q \notin \mathcal{R}(P)$.</strong>
        <br>1. <strong>Linear Algebra Fact:</strong> Since $P$ is symmetric, the nullspace $\mathcal{N}(P)$ is orthogonal to the range $\mathcal{R}(P)$. Any vector $q$ can be decomposed into $q_{\text{range}} + q_{\text{null}}$. If $q \notin \mathcal{R}(P)$, then $q_{\text{null}} \neq 0$.
        <br>2. <strong>Pick Direction:</strong> Let $z = -q_{\text{null}}$. Note that $Pz = 0$ (by definition of nullspace).
        <br>3. <strong>Evaluate Function along $x(t) = tz$:</strong>
        $$ f(tz) = \frac{1}{2}t^2 (z^T P z) + t(q^T z) + r = 0 + t(q^T z) + r $$
        <br>4. <strong>Analyze Slope:</strong> $q^T z = (q_{\text{range}} + q_{\text{null}})^T (-q_{\text{null}}) = -\|q_{\text{null}}\|^2 < 0$.
        <br>5. <strong>Limit:</strong> As $t \to \infty$, $f(tz) \to -\infty$ linearly.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.5: Backtracking Line Search Proof (Rigorous)</h3>
        <p><strong>Problem:</strong> Prove that if $0 < t \le -\frac{\nabla f(x)^T \Delta x}{M\|\Delta x\|_2^2}$, the backtracking condition $f(x+t\Delta x) \le f(x) + \alpha t \nabla f^T \Delta x$ holds for $\alpha \le 0.5$.</p>
        <p><strong>Proof:</strong></p>
        <ol>
            <li><strong>Smoothness Upper Bound:</strong> Start with the M-smoothness bound (9.13) along direction $\Delta x$:
            $$ f(x+t\Delta x) \le f(x) + t\nabla f^T \Delta x + \frac{M}{2}t^2\|\Delta x\|^2 $$
            </li>
            <li><strong>Substitute $t$ bound:</strong> We are given $t \le \frac{-\nabla f^T \Delta x}{M\|\Delta x\|^2}$. Multiply by $\frac{M}{2}t\|\Delta x\|^2$:
            $$ \frac{M}{2}t^2\|\Delta x\|^2 \le \frac{1}{2}t (-\nabla f^T \Delta x) $$
            </li>
            <li><strong>Plug into Upper Bound:</strong>
            $$ f(x+t\Delta x) \le f(x) + t\nabla f^T \Delta x - \frac{1}{2}t \nabla f^T \Delta x = f(x) + \frac{1}{2}t\nabla f^T \Delta x $$
            </li>
            <li><strong>Compare to Armijo:</strong> The condition requires slope $\alpha$. Since we proved it holds for slope $0.5$, and $\nabla f^T \Delta x < 0$ (descent), the condition holds for any $\alpha \le 0.5$ (since a smaller $\alpha$ makes the required drop <em>smaller</em>/easier).
            </li>
        </ol>
      </div>

      <div class="example-problem">
        <h3>Example 13.2: Newton's Method for Logistic Regression</h3>
        <p><strong>Problem:</strong> Derive the Newton step for Logistic Regression explicitly.</p>
        <p><strong>Solution:</strong></p>
        <p><strong>Objective:</strong> $f(w) = \sum_{i=1}^m \log(1 + \exp(w^T x_i)) - y_i w^T x_i$.
        <br>Let $z_i = w^T x_i$. The sigmoid function is $p_i(w) = \sigma(z_i) = \frac{1}{1+e^{-z_i}}$.
        <br><strong>Gradient:</strong>
        $$ \frac{\partial f}{\partial w} = \sum_{i=1}^m (p_i - y_i) x_i = X^T (p - y) $$
        <br><strong>Hessian:</strong>
        $$ \frac{\partial^2 f}{\partial w^2} = \sum_{i=1}^m \frac{\partial p_i}{\partial z_i} x_i x_i^T = \sum_{i=1}^m p_i(1-p_i) x_i x_i^T = X^T D X $$
        where $D$ is diagonal with $D_{ii} = p_i(1-p_i)$.
        <br><strong>Newton Step:</strong>
        $$ \Delta w = -(X^T D X)^{-1} X^T (p - y) $$
        This formula is the basis for the <strong>Iteratively Reweighted Least Squares (IRLS)</strong> algorithm used by standard statistical packages.
        </p>
      </div>

      <div class="example-problem">
        <h3>Example 13.3: Steepest Descent in $\ell_\infty$ Norm</h3>
        <p><strong>Problem:</strong> Explicitly derive the steepest descent direction for the $\ell_\infty$ norm.</p>
        <p><strong>Solution:</strong></p>
        <p>We solve $\min \nabla f^T v$ s.t. $\|v\|_\infty \le 1$.
        The objective is $\sum_{i=1}^n (\nabla f)_i v_i$.
        Since the variables $v_i$ are constrained independently ($|v_i| \le 1$), we can minimize the sum by minimizing each term separately.
        To minimize $c_i v_i$ subject to $v_i \in [-1, 1]$:
        <ul>
            <li>If $c_i > 0$, set $v_i = -1$.</li>
            <li>If $c_i < 0$, set $v_i = +1$.</li>
            <li>If $c_i = 0$, $v_i$ is arbitrary (usually 0).</li>
        </ul>
        Thus, $v_i = -\text{sign}((\nabla f)_i)$.
        <strong>Geometric Insight:</strong> In the $\ell_\infty$ "box" world, the fastest way down is to go to a corner of the hypercube.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.15: Proving Self-Concordance</h3>
        <p><strong>Problem:</strong> Prove $f(x,y) = -\log(y^2 - x^T x)$ is self-concordant.</p>
        <p><strong>Solution Strategy:</strong></p>
        <p>We use the <strong>barrier method</strong> logic.
        1. <strong>Restrict to a line:</strong> $x(t) = \hat{x} + tv$, $y(t) = \hat{y} + tw$.
        2. <strong>Composition with Log:</strong> Use the property that $-\log(-g(t)) - \log t$ is self-concordant if $|g'''| \le 3g''/t$.
        3. <strong>Algebra:</strong> Factor $y^2 - x^T x$ into geometric components.
        For $f(x,y)$, restricting to a line yields a function of the form $-\log(\text{quadratic})$.
        Since $-\log(y)$ is SC and affine transformations preserve SC, the result holds.
        </p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.30: Analytic Center (Gradient vs Newton)</h3>
        <p><strong>Problem:</strong> Compute the analytic center of $a_i^T x \le 1, |x_i| \le 1$ using Gradient Descent and Newton's Method. Compare convergence.</p>
        <p><strong>Objective:</strong> $f(x) = -\sum_{i=1}^m \log(1-a_i^T x) - \sum_{j=1}^n \log(1-x_j^2)$.</p>

        <p><strong>Python Implementation:</strong></p>
<pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

def solve_analytic_center(m=200, n=50):
    # Generate data
    np.random.seed(1)
    A = np.random.randn(m, n) * 0.5  # Scale for feasibility at 0

    # Objective and Derivatives
    def f_val(x):
        s = 1.0 - A @ x
        t = 1.0 - x**2
        if np.any(s <= 0) or np.any(t <= 0): return np.inf
        return -np.sum(np.log(s)) - np.sum(np.log(t))

    def grad_hess(x):
        s = 1.0 - A @ x
        t = 1.0 - x**2
        # Gradient
        g = A.T @ (1/s) + 2*x/t
        # Hessian
        w = 1/(s**2)
        H = (A.T * w) @ A + np.diag(2*(1+x**2)/(t**2))
        return g, H

    # Backtracking Line Search
    def line_search(x, dx, g, alpha=0.1, beta=0.5):
        t = 1.0
        fx = f_val(x)
        g_dot_dx = g @ dx
        while f_val(x + t*dx) > fx + alpha * t * g_dot_dx:
            t *= beta
        return t

    # Newton's Method Loop
    x = np.zeros(n)
    history = []
    for iter in range(50):
        fx = f_val(x)
        g, H = grad_hess(x)
        lambda_sq = g @ np.linalg.solve(H, g) # Newton decrement
        history.append(lambda_sq)

        if lambda_sq < 1e-10: break

        dx = np.linalg.solve(H, -g)
        t = line_search(x, dx, g, alpha=0.1)
        x += t * dx

    return history

# Run and visualize
hist = solve_analytic_center()
# In a real plot: plt.semilogy(hist); plt.show()
</code></pre>
        <p><strong>Observation:</strong> Gradient descent will take thousands of iterations (linear convergence). Newton's method typically converges in 10-20 iterations (quadratic convergence), often accepting step size $t=1$ in the final phase.</p>
      </div>

      <div class="example-problem">
        <h3>Exercise 9.31: Approximate Newton Methods</h3>
        <p><strong>Problem:</strong> Compare full Newton with (a) Re-using Hessian every $N$ steps, and (b) Diagonal approximation.</p>
        <p><strong>Analysis:</strong></p>
        <ul>
          <li><strong>Reuse Hessian:</strong> Saves $O(n^3)$ factorization cost but uses "stale" curvature. Convergence becomes linear or superlinear but not quadratic. Good if Hessian evaluation is very expensive.</li>
          <li><strong>Diagonal Approximation:</strong> Only uses $H_{ii}$. Cost is $O(n)$. Becomes a scaled gradient descent. Effective if variables are loosely coupled (matrix $A$ is sparse or near-diagonal), but poor for highly correlated data.</li>
        </ul>
        <p><strong>Code Snippet (Diagonal Newton):</strong></p>
<pre><code class="language-python">
# Diagonal Hessian Approximation
H_diag = np.sum((A**2).T * (1/s**2), axis=1) + 2*(1+x**2)/(t**2)
dx = -g / H_diag  # Element-wise division
</code></pre>
      </div>

      <div class="example-problem">
        <h3>Example 13.5: BFGS Update Rank</h3>
        <p><strong>Problem:</strong> Show that the BFGS update is a rank-2 update.</p>
        <p><strong>Solution:</strong></p>
        <p>Update: $B_{k+1} = B_k + \frac{y y^\top}{y^\top s} - \frac{B s s^\top B}{s^\top B s}$.
        This adds two rank-1 matrices.
        It preserves positive definiteness if $y^\top s > 0$ (curvature condition).
        Avoids $O(n^3)$ cost of inverting Hessian.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.6: Convergence Rate Comparison</h3>
        <p><strong>Problem:</strong> Compare GD and Newton on $f(x) = x^4$.</p>
        <p><strong>Solution:</strong></p>
        <p>GD: $x_{k+1} = x_k - t 4 x_k^3$. For convergence, need small $t$. Rate is sublinear (slow).
        Newton: $\Delta x = -f'/f'' = -4x^3 / 12x^2 = -x/3$.
        $x_{k+1} = x_k - x_k/3 = (2/3)x_k$.
        Linear convergence with rate $2/3$.
        Note: $x^4$ is strictly convex but not strongly convex at 0 ($f''(0)=0$). Newton is not quadratic here!</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.7: Nesterov Acceleration</h3>
        <p><strong>Problem:</strong> Write the update equations for Nesterov's accelerated gradient.</p>
        <p><strong>Solution:</strong></p>
        <p>$x_k$: current position, $y_k$: momentum variable.
        1. $x_{k+1} = y_k - t \nabla f(y_k)$
        2. $y_{k+1} = x_{k+1} + \frac{k-1}{k+2} (x_{k+1} - x_k)$
        Step 1 is standard GD step from the lookahead position $y_k$.
        Step 2 adds momentum.
        Converges as $1/k^2$ for smooth convex functions.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.8: Newton Decrement Property</h3>
        <p><strong>Problem:</strong> Show $\lambda(x)^2 = \nabla f(x)^\top \Delta x_{nt}$.</p>
        <p><strong>Solution:</strong></p>
        <p>$\Delta x_{nt} = -H^{-1} g$.
        $\lambda(x)^2 = g^\top H^{-1} g$.
        Inner product: $g^\top \Delta x_{nt} = g^\top (-H^{-1} g) = -g^\top H^{-1} g = -\lambda(x)^2$.
        So $\lambda^2 = -g^\top \Delta x_{nt}$ (directional derivative in Newton direction is $-\lambda^2$).</p>
      </div>

    </section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 9 — Unconstrained Minimization</li>
        <li><strong>Detailed Lecture Notes:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">PDF (Chapter 9)</a></li>
      </ul>
    </section>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></span> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initGradientDescentVisualizer } from './widgets/js/gradient-descent-visualizer.js';
    initGradientDescentVisualizer('widget-1');
  </script>
  <script type="module">
    import { initGDvsNewton } from './widgets/js/gd-vs-newton.js';
    initGDvsNewton('widget-2');
  </script>
  <script type="module">
    import { initConvergenceRate } from './widgets/js/convergence-rate.js';
    initConvergenceRate('widget-3');
  </script>
  <script type="module">
    import { initNormSteepest } from './widgets/js/norm-steepest.js';
    initNormSteepest('widget-norm-steepest');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
  <script src="../../static/js/notes-widget.js"></script>
  <script src="../../static/js/pomodoro.js"></script>
  <script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
