<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>06. Convex Functions: Advanced Topics — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" />
  <script src="../../static/lib/feather/feather.min.js"></script>
</head>
<body>
  <header class="site-header sticky">
    <div class="container">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html"><i data-feather="grid"></i> All Lectures</a>
        <a href="../05-convex-functions-basics/index.html"><i data-feather="arrow-left"></i> Previous</a>
        <a href="../07-convex-problems-standard/index.html">Next <i data-feather="arrow-right"></i></a>
      </nav>
    </div>
  </header>

  <div class="lecture-container"><aside class="sidebar"><div id="toc-container"><h2><i data-feather="list"></i> Table of Contents</h2><nav id="toc"></nav></div></aside><main class="lecture-content">
    <header class="lecture-header section-card">
      <h1>06. Convex Functions: Advanced Topics</h1>
      <div class="lecture-meta">
        <span>Date: 2025-11-25</span>
        <span>Duration: 90 min</span>
        <span>Tags: conjugate, quasiconvex, log-concave, advanced</span>
      </div>
      <div class="lecture-summary">
        <p>This lecture explores advanced topics in convex analysis that bridge theory and modern applications. We begin with subgradients and subdifferentials, which extend the gradient concept to non-differentiable functions and are essential for optimality conditions and duality. We then examine smoothness and strong convexity, which determine convergence rates of optimization algorithms. We also study the convex conjugate (Fenchel conjugate) and its central role in duality, define quasi-convex functions and their properties, and examine log-concave functions, which are essential for probabilistic modeling and volume approximation.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05: Convex Functions Basics</a>.</p>
        <p><strong>Forward Connections:</strong> The convex conjugate is the key tool for deriving dual problems in <a href="../09-duality/index.html">Lecture 09</a>. Quasi-convexity appears in fractional programming problems in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>.</p>
      </div>
    </header>

    <section class="section-card">
      <h2><i data-feather="target"></i> Learning Objectives</h2>
      <p>After this lecture, you will be able to:</p>
      <ul>
        <li><b>Compute Conjugates:</b> Derive the convex conjugate for standard functions like norms, quadratics, and log-sum-exp.</li>
        <li><b>Apply Duality Mappings:</b> Connect primal domain properties to dual domain constraints via conjugation.</li>
        <li><b>Analyze Quasi-Convexity:</b> Identify functions defined by convex sublevel sets and apply operations that preserve this property.</li>
        <li><b>Work with Log-Concavity:</b> Recognize log-concave probability distributions and use their properties (integration, marginalization) in modeling.</li>
      </ul>
    </section>

    <article>
      <section class="section-card" id="section-1">
      <h2>1. Subgradients and Subdifferentials</h2>

      <h3>1.1 Definition of Subgradient</h3>

      <div class="warning-box">
        <h4>⚠️ Common Pitfall: |x| is NOT Differentiable</h4>
        <p>A classic mistake is to try to compute $\nabla |x|$ at $x=0$. It does not exist.
        <br>Instead, we use the <b>subdifferential</b>: $\partial |x|$ at $x=0$ is the interval $[-1, 1]$.
        <br><b>Rule:</b> If a function has a "kink", you must use subgradients, not gradients.</p>
      </div>

      <p>A vector $g \in \mathbb{R}^n$ is a <a href="#" class="definition-link" data-term="subgradient">subgradient</a> of a convex function $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ at a point $x \in \mathrm{dom}\, f$ if:</p>
      $$
      f(y) \ge f(x) + g^\top (y - x) \quad \text{for all } y \in \mathrm{dom}\, f
      $$
      <p>Geometrically, a subgradient $g$ defines a <b>supporting hyperplane</b> to the epigraph of $f$ at the point $(x, f(x))$. The existence of such a vector for convex functions is guaranteed by the <a href="../04-convex-sets-cones/index.html#section-2">Supporting Hyperplane Theorem</a>. The inequality states that the affine function $\ell(y) = f(x) + g^\top (y - x)$ is a <b>global underestimator</b> of $f$.</p>
        <div class="insight">
          <h4>1D Analogy: Slopes in the Corner</h4>
          <p>Consider $f(x) = |x|$ at $x=0$.
          <br>For $x>0$, the slope is $+1$. For $x<0$, the slope is $-1$.
          <br>At $x=0$, any line passing through the origin with slope between $-1$ and $+1$ stays below the graph. Thus, the subdifferential is the interval $[-1, 1]$.
          <br>This generalizes to higher dimensions as a "cone of normals".</p>
        </div>

      <div class="theorem-box">
        <h4>Connection to Gradients</h4>
        <p>If $f$ is differentiable at $\mathbf{x}$, then the gradient $\nabla f(\mathbf{x})$ is the <b>unique</b> subgradient. Conversely, if $f$ has a unique subgradient at $\mathbf{x}$, then $f$ is differentiable at $\mathbf{x}$ and the subgradient equals the gradient.</p>
      </div>

      <h3>1.2 The Subdifferential</h3>
      <p>The <a href="#" class="definition-link" data-term="subdifferential">subdifferential</a> $\partial f(\mathbf{x})$ is the set of all subgradients of $f$ at $\mathbf{x}$:</p>
      $$
      \partial f(\mathbf{x}) = \{\mathbf{g} \in \mathbb{R}^n \mid f(\mathbf{y}) \ge f(\mathbf{x}) + \mathbf{g}^\top (\mathbf{y} - \mathbf{x}) \ \forall \mathbf{y} \in \mathrm{dom}\, f\}
      $$
      <p><b>Key Properties:</b>
      <ul>
        <li><b>Convex Set:</b> The subdifferential is always a convex set (possibly empty if $\mathbf{x} \notin \mathrm{dom}\, f$).</li>
        <li><b>Closed:</b> The subdifferential is closed (contains all its limit points).</li>
        <li><b>Bounded:</b> If $f$ is Lipschitz continuous near $\mathbf{x}$, then $\partial f(\mathbf{x})$ is bounded.</li>
        <li><b>Non-empty on Interior:</b> If $\mathbf{x} \in \mathrm{int}(\mathrm{dom}\, f)$, then $\partial f(\mathbf{x})$ is non-empty.</li>
      </ul></p>

      <h3>1.3 Examples</h3>

      <div class="example">
        <h4>(a) Absolute Value: $f(x) = |x|$</h4>
        <p>At $x = 0$, the subdifferential is the interval $[-1, 1]$:
        $$ \partial |0| = \{g \in \mathbb{R} \mid |y| \ge 0 + g \cdot y \ \forall y\} = [-1, 1] $$
        At $x > 0$, we have $\partial |x| = \{1\}$ (the derivative).
        At $x < 0$, we have $\partial |x| = \{-1\}$.</p>
        <p><b>Intuition:</b> At the non-differentiable point $x=0$, any slope between $-1$ and $1$ provides a supporting line.</p>
      </div>

      <div class="example">
        <h4>(b) $\ell_1$ Norm: $f(\mathbf{x}) = \|\mathbf{x}\|_1 = \sum_{i=1}^n |x_i|$</h4>
        <p>The subdifferential is the Cartesian product of subdifferentials:
        $$ \partial \|\mathbf{x}\|_1 = \partial |x_1| \times \partial |x_2| \times \cdots \times \partial |x_n| $$
        At $\mathbf{x} = 0$, we get:
        $$ \partial \|0\|_1 = [-1, 1]^n $$
        This is the unit ball in the $\ell_\infty$ norm (the dual of $\ell_1$).</p>
      </div>

      <div class="example">
        <h4>(c) Indicator Function: $f(\mathbf{x}) = I_C(\mathbf{x})$</h4>
        <p>For a convex set $C$, the indicator function is:
        $$ I_C(\mathbf{x}) = \begin{cases} 0 & \mathbf{x} \in C \\ +\infty & \mathbf{x} \notin C \end{cases} $$
        The subdifferential at $\mathbf{x} \in C$ is the <b>normal cone</b> to $C$ at $\mathbf{x}$:
        $$ \partial I_C(\mathbf{x}) = N_C(\mathbf{x}) = \{\mathbf{g} \mid \mathbf{g}^\top (\mathbf{y} - \mathbf{x}) \le 0 \ \forall \mathbf{y} \in C\} $$
        This connects subgradients to the geometry of convex sets.</p>
      </div>

      <h3>1.4 Optimality Conditions</h3>
      <div class="theorem-box">
        <h4>Theorem: Subgradient Optimality Condition</h4>
        <p>For a convex function $f$, a point $\mathbf{x}^*$ is a global minimizer of $f$ if and only if $0 \in \partial f(\mathbf{x}^*)$.</p>
        <div class="proof-box">
          <h4>Proof</h4>
          <div class="proof-step">
            <strong>1. Direction: Minimizer $\implies$ $0 \in \partial f(\mathbf{x}^*)$</strong>
            <br>Assume $\mathbf{x}^*$ is a global minimizer. By definition, this means:
            $$ f(\mathbf{y}) \ge f(\mathbf{x}^*) \quad \text{for all } \mathbf{y} \in \mathrm{dom}\, f $$
            We can rewrite the right-hand side artificially as:
            $$ f(\mathbf{y}) \ge f(\mathbf{x}^*) + 0^\top (\mathbf{y} - \mathbf{x}^*) $$
            Comparing this to the definition of subgradient ($f(\mathbf{y}) \ge f(\mathbf{x}) + g^\top (\mathbf{y}-\mathbf{x})$), we see that the zero vector $\mathbf{g} = 0$ satisfies the condition. Therefore, $0 \in \partial f(\mathbf{x}^*)$.
          </div>
          <div class="proof-step">
            <strong>2. Direction: $0 \in \partial f(\mathbf{x}^*) \implies$ Minimizer</strong>
            <br>Assume the zero vector is in the subdifferential set. By definition, this means:
            $$ f(\mathbf{y}) \ge f(\mathbf{x}^*) + 0^\top (\mathbf{y} - \mathbf{x}^*) \quad \text{for all } \mathbf{y} $$
            The dot product term vanishes:
            $$ f(\mathbf{y}) \ge f(\mathbf{x}^*) $$
            This inequality holds for all $\mathbf{y}$ in the domain, which is exactly the definition of $\mathbf{x}^*$ being a global minimizer.
          </div>
        </div>
      </div>

      <div class="insight">
        <h4>Looking Ahead: Algorithms</h4>
        <p>The condition $0 \in \partial f(x^*)$ is not just theoretical—it's <b>algorithmic</b>.
        <br><b>Subgradient Descent</b> moves in the direction $-g$ where $g \in \partial f(x)$. It is the "Swiss Army Knife" for non-differentiable problems.
        <br><b>Proximal Algorithms</b> minimize $f(x) + (\rho/2)\|x-z\|^2$ using subdifferentials to handle non-smooth terms (like $\ell_1$ norm). You'll see these in advanced optimization courses (10-725, etc.).</p>
      </div>

      <div class="insight">
        <h4>Connection to Duality</h4>
        <p>Subgradients are essential for duality theory. In <a href="../09-duality/index.html">Lecture 09</a>, we will see that optimal Lagrange multipliers are subgradients of the dual function. The subdifferential provides a complete characterization of optimality without requiring differentiability.</p>
      </div>

      <h3>1.5 Calculus Rules for Subdifferentials</h3>

      <p>The subdifferential has algebraic properties that mirror those of ordinary derivatives, but with important caveats. These rules are fundamental for computing subdifferentials of composite functions and for deriving optimality conditions in complex problems.</p>

      <h4>1. Sum Rule</h4>
      <div class="theorem-box">
        <h4>Theorem: Sum Rule (Always Valid)</h4>
        <p>If $f_1, f_2$ are convex, then:
        $$ \partial (f_1 + f_2)(x) = \partial f_1(x) + \partial f_2(x) $$
        where the right-hand side is the <b>Minkowski sum</b> of sets:
        $$ \partial f_1(x) + \partial f_2(x) = \{g_1 + g_2 \mid g_1 \in \partial f_1(x), g_2 \in \partial f_2(x)\} $$
        </p>
      </div>
      <p><b>Key insight:</b> The sum rule for subdifferentials holds <i>without any regularity conditions</i>, unlike many other calculus rules. This is one of the major advantages of subdifferential calculus over smooth calculus.</p>

      <h4>2. Scalar Multiplication</h4>
      <p>For $\alpha > 0$:
      $$ \partial (\alpha f)(x) = \alpha \partial f(x) = \{\alpha g \mid g \in \partial f(x)\} $$
      For $\alpha = 0$: $\partial (0 \cdot f)(x) = \{0\}$ (constant function has zero subdifferential).
      <br>For $\alpha < 0$: This doesn't apply directly since $\alpha f$ is concave, not convex. Use $\partial(-f)(x) = -\partial f(x)$ for concave $f$.</p>

      <h4>3. Affine Composition</h4>
      <div class="theorem-box">
        <h4>Theorem: Affine Composition Rule</h4>
        <p>If $f: \mathbb{R}^m \to \mathbb{R}$ is convex and $g(x) = f(Ax + b)$ where $A \in \mathbb{R}^{m \times n}$, then:
        $$ \partial g(x) = A^\top \partial f(Ax + b) = \{A^\top v \mid v \in \partial f(Ax + b)\} $$
        </p>
      </div>

      <div class="example-box">
        <h4>Example: Ridge Regression Subdifferential</h4>
        <p>Consider $f(x) = \frac{1}{2}\|Ax - b\|_2^2 + \frac{\lambda}{2}\|x\|_2^2$.
        <br><b>Step 1:</b> Subdifferential of $h(r) = \frac{1}{2}\|r\|_2^2$ is $\partial h(r) = \{r\}$ (since $h$ is differentiable).
        <br><b>Step 2:</b> By affine composition with $r = Ax - b$:
        $$ \partial \left(\frac{1}{2}\|Ax - b\|_2^2\right) = A^\top (Ax - b) $$
        <b>Step 3:</b> By sum rule:
        $$ \partial f(x) = A^\top(Ax - b) + \lambda x $$
        Setting $0 \in \partial f(x)$ gives the normal equations: $(A^\top A + \lambda I)x = A^\top b$.</p>
      </div>

      <h4>4. Pointwise Maximum</h4>
      <div class="theorem-box">
        <h4>Theorem: Max Rule</h4>
        <p>Let $f(x) = \max\{f_1(x), \ldots, f_m(x)\}$ where each $f_i$ is convex. Then:
        $$ \partial f(x) = \mathrm{conv}\left(\bigcup_{i \in I(x)} \partial f_i(x)\right) $$
        where $I(x) = \{i \mid f_i(x) = f(x)\}$ is the set of <b>active</b> functions at $x$, and $\mathrm{conv}$ denotes the convex hull.</p>
      </div>
      <p><b>Interpretation:</b> The subdifferential is the convex hull of the subdifferentials of the active functions. If only one function is active (say $f_1(x) > f_i(x)$ for $i \neq 1$), then $\partial f(x) = \partial f_1(x)$.</p>

      <div class="example-box">
        <h4>Example: Hinge Loss</h4>
        <p>The hinge loss $f(z) = \max\{0, 1-z\}$ is used in SVMs. Let $f_1(z) = 0$ and $f_2(z) = 1-z$.
        <br><b>Case 1:</b> $z > 1$. Then $f(z) = 0$ (only $f_1$ active), so $\partial f(z) = \{0\}$.
        <br><b>Case 2:</b> $z < 1$. Then $f(z) = 1-z$ (only $f_2$ active), so $\partial f(z) = \{-1\}$.
        <br><b>Case 3:</b> $z = 1$. Both active! $\partial f_1(1) = \{0\}$, $\partial f_2(1) = \{-1\}$. Thus:
        $$ \partial f(1) = \mathrm{conv}\{0, -1\} = [-1, 0] $$
        The subdifferential is an entire interval at the kink.</p>
      </div>

      <h4>5. Composition with Monotone Functions (Chain Rule)</h4>
      <div class="theorem-box">
        <h4>Theorem: Chain Rule for Subdifferentials</h4>
        <p>Let $f: \mathbb{R}^n \to \mathbb{R}$ and $h: \mathbb{R} \to \mathbb{R}$. Define $g(x) = h(f(x))$.
        <ul>
          <li>If $h$ is <b>convex nondecreasing</b> and $f$ is <b>convex</b>, then:
          $$ \partial g(x) \supseteq \{v \cdot w \mid v \in \partial h(f(x)), w \in \partial f(x)\} $$
          Equality holds when $\mathrm{int}(\mathrm{dom}\, h) \cap \mathrm{range}(f) \neq \emptyset$.</li>
          <li>If $h$ is <b>convex nonincreasing</b> and $f$ is <b>concave</b>, the same holds.</li>
        </ul>
        </p>
      </div>
      <p><b>Warning:</b> The chain rule for subdifferentials requires monotonicity of the outer function $h$. Unlike smooth calculus, $\partial (h \circ f)$ does NOT always equal $h'(f(x)) \cdot \partial f(x)$ for general $h$.</p>

      <div class="example-box">
        <h4>Example: Subgradient of $e^{|x|}$</h4>
        <p>Let $g(x) = e^{|x|}$. This is the composition of $h(u) = e^u$ (convex, non-decreasing) and $f(x) = |x|$ (convex).
        <br>We compute $\partial g(0)$ using the chain rule.
        <ul>
            <li>At $x=0$, $f(0) = 0$. The outer subgradient is $\partial h(0) = \{e^0\} = \{1\}$.</li>
            <li>The inner subgradient is $\partial f(0) = [-1, 1]$.</li>
            <li>Chain rule set: $\{v \cdot w \mid v \in \{1\}, w \in [-1, 1]\} = [-1, 1]$.</li>
        </ul>
        Thus $\partial g(0) = [-1, 1]$. This matches the geometric intuition of a "V" shape curved exponentially.</p>
      </div>

      <h4>6. Conditions for Sum Rule Equality</h4>
      <p>While the sum rule $\partial (f_1 + f_2)(x) = \partial f_1(x) + \partial f_2(x)$ always holds, many other rules require <b>qualification conditions</b>:</p>
      <ul>
        <li><b>Relative Interior Condition:</b> $\mathrm{ri}(\mathrm{dom}(f_1)) \cap \mathrm{ri}(\mathrm{dom}(f_2)) \neq \emptyset$</li>
        <li>This ensures that the domains "overlap sufficiently" for the calculus to behave well</li>
        <li>When this fails, inclusions may be strict (e.g., $\partial (h \circ f)(x) \supsetneq$ the chain rule formula)</li>
      </ul>

      <h4>7. Indicator Function Calculus</h4>
      <p>The <b>indicator function</b> $I_C(x) = 0$ if $x \in C$, else $+\infty$, has subdifferential:
      $$ \partial I_C(x) = \begin{cases} N_C(x) & \text{if } x \in C \\ \emptyset & \text{if } x \notin C \end{cases} $$
      where $N_C(x) = \{g \mid g^\top (y - x) \le 0 \text{ for all } y \in C\}$ is the <b>normal cone</b>.
      <br><br>
      <b>Application:</b> For constrained optimization $\min f(x)$ subject to $x \in C$, the optimality condition is:
      $$ 0 \in \partial f(x^*) + N_C(x^*) $$
      This unifies KKT conditions for smooth problems and subdifferential optimality for nonsmooth ones.</p>

      <div class="insight">
        <h4>Summary: When Do Subdifferential Rules Hold?</h4>
        <table style="width: 100%; border-collapse: collapse; margin: 12px 0;">
          <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
            <th style="text-align: left; padding: 8px;"><b>Rule</b></th>
            <th style="text-align: left; padding: 8px;"><b>Conditions</b></th>
          </tr>
          <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
            <td style="padding: 8px;">Sum: $\partial (f_1 + f_2) = \partial f_1 + \partial f_2$</td>
            <td style="padding: 8px;">✅ Always (no conditions needed)</td>
          </tr>
          <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
            <td style="padding: 8px;">Max: $\partial \max\{f_i\} = \mathrm{conv}(\bigcup_{i \in I(x)} \partial f_i)$</td>
            <td style="padding: 8px;">✅ Always</td>
          </tr>
          <tr style="border-bottom: 1px solid rgba(255,255,255,0.1);">
            <td style="padding: 8px;">Affine composition: $\partial (f \circ A) = A^\top \partial f(Ax)$</td>
            <td style="padding: 8px;">✅ Always</td>
          </tr>
          <tr>
            <td style="padding: 8px;">Chain rule: $\partial (h \circ f)$</td>
            <td style="padding: 8px;">⚠️ Requires monotonicity of $h$ + qualification</td>
          </tr>
        </table>
      </div>

    </section>

      <section class="section-card" id="section-2">
      <h2>2. Smoothness and Strong Convexity</h2>

      <h3>2.1 Strictly Convex Functions</h3>
      <p>A function $f$ is <a href="#" class="definition-link">strictly convex</a> if the inequality in Jensen's inequality is strict whenever $\mathbf{x} \neq \mathbf{y}$ and $\theta \in (0, 1)$:</p>
      $$
      f(\theta \mathbf{x} + (1-\theta)\mathbf{y}) < \theta f(\mathbf{x}) + (1-\theta)f(\mathbf{y})
      $$
      <div id="widget-convex-function-inspector" style="width: 100%; height: 500px; position: relative; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 20px 0;"></div>
      <p><b>Implication:</b> Strictly convex functions have at most one minimizer (uniqueness of optimal solution).</p>
      <p><b>Hessian Criterion:</b> If $\nabla^2 f(\mathbf{x}) \succ 0$ (positive definite) for all $\mathbf{x} \in \mathrm{dom}\, f$, then $f$ is strictly convex. <b>Note:</b> The converse is false! $f(x) = x^4$ is strictly convex but $f''(0) = 0$.</p>

      <div id="widget-hessian-heatmap" style="width: 100%; height: 550px; position: relative; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 20px 0;"></div>

      <h3>2.2 Strongly Convex Functions</h3>
      <p>A function $f$ is <a href="#" class="definition-link">strongly convex</a> (with parameter $m > 0$) if for all $\mathbf{x}, \mathbf{y} \in \mathrm{dom}\, f$ and $\theta \in [0, 1]$:</p>
      $$
      f(\theta \mathbf{x} + (1-\theta)\mathbf{y}) \le \theta f(\mathbf{x}) + (1-\theta)f(\mathbf{y}) - \frac{m}{2} \theta(1-\theta) \|\mathbf{x} - \mathbf{y}\|_2^2
      $$
      <p>Equivalently, the function $g(\mathbf{x}) = f(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2$ is convex. For twice differentiable functions, this is equivalent to:</p>
      $$ \nabla^2 f(\mathbf{x}) \succeq mI $$
      <p>For differentiable functions, this implies a <b>quadratic lower bound</b>:</p>
      $$ f(\mathbf{y}) \ge f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) + \frac{m}{2}\|\mathbf{y} - \mathbf{x}\|_2^2 $$

      <div class="insight">
        <h4>Why This Matters: Fast Convergence in Optimization</h4>
        <p>Strong convexity is the <b>single most important property</b> for optimization algorithm performance. Here's why:</p>
        <ul>
          <li><b>Guarantees Fast Convergence:</b> Gradient descent on a $m$-strongly convex function converges at a <b>linear (exponential) rate</b>: $\|x_k - x^*\|^2 \le (1 - m/L)^k \|x_0 - x^*\|^2$, where $L$ is the Lipschitz constant. This is exponentially faster than the $O(1/k)$ sublinear rate for merely convex functions.</li>
          <li><b>Critical for Large-Scale Machine Learning:</b> In modern ML, we optimize over millions or billions of parameters. The difference between linear convergence (strong convexity) and sublinear convergence (plain convexity) is the difference between converging in minutes vs. days. This is why regularization (adding $\lambda\|w\|^2$ to loss functions) is ubiquitous—it adds strong convexity!</li>
          <li><b>Ensures Unique Minimizer:</b> Strong convexity implies strict convexity, which guarantees the optimal solution is unique. This is crucial for reproducibility and stability in ML pipelines.</li>
          <li><b>Enables Theoretical Guarantees:</b> Many advanced algorithms (Newton's method, accelerated gradient descent, proximal methods) require strong convexity for their convergence proofs. Without it, we lose guarantees.</li>
        </ul>
        <p><b>Real-World Example:</b> Ridge regression ($\min_w \|Xw - y\|^2 + \lambda\|w\|^2$) is $2\lambda$-strongly convex. Even if $X^\top X$ is singular (non-unique least squares solution), adding $\lambda > 0$ makes the problem strongly convex with unique solution and fast gradient descent convergence.</p>
        <p><b>Takeaway:</b> If your optimization problem isn't converging fast, check if it's strongly convex. If not, consider adding $\ell_2$ regularization.</p>
      </div>

      <h3>2.3 Smoothness (Lipschitz Gradients)</h3>
      <p>A function $f$ has <b>$L$-Lipschitz continuous gradients</b> if:</p>
      $$ \|\nabla f(\mathbf{x}) - \nabla f(\mathbf{y})\|_2 \le L \|\mathbf{x} - \mathbf{y}\|_2 $$
      <p>For twice differentiable functions, this is equivalent to:</p>
      $$ \nabla^2 f(\mathbf{x}) \preceq LI $$
      <p>This implies a <b>quadratic upper bound</b>:</p>
      $$ f(\mathbf{y}) \le f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) + \frac{L}{2}\|\mathbf{y} - \mathbf{x}\|_2^2 $$

      <h3>2.4 Optimization Significance</h3>
      <p><b>Condition Number:</b> If $f$ is $m$-strongly convex and has $L$-Lipschitz gradients ($mI \preceq \nabla^2 f \preceq LI$), the condition number is $\kappa = L/m$. Gradient descent converges at a linear rate determined by $(1 - 1/\kappa)$.</p>

      <div class="insight">
        <h4>Condition Number Intuition</h4>
        <p>The parameter $m$ controls the minimum curvature (flatness), while $L$ controls the maximum curvature (sharpness). The ratio $L/m$ is the condition number of the Hessian. If $L/m$ is large, the function is shaped like a long narrow valley (steep in some directions, flat in others). Optimization algorithms struggle with such "ill-conditioned" functions because steps must be small to handle the steep directions, but large to traverse the flat ones.</p>
      </div>

      <p><b>Stability:</b> The solution is stable to perturbations in the objective. A small change $\epsilon$ in the gradient moves the minimizer by at most $\epsilon/m$.</p>

      <div class="example">
        <h4>Example: Strictly Convex but Not Strongly Convex</h4>
        <p>Consider $f(x) = x^4$ on $\mathbb{R}$. This function is strictly convex (has a unique minimizer at $x=0$), but it is <b>not</b> strongly convex. To see this, note that $f''(x) = 12x^2$, which equals $0$ at $x=0$. There is no $m > 0$ such that $f''(x) \ge m$ for all $x$ near $0$.</p>
        <p>Strong convexity requires uniform lower curvature everywhere, while strict convexity only requires the function to "curve upward" (but the curvature can approach zero).</p>
      </div>
    </section>

      <section class="section-card" id="section-3">
      <h2>3. The Convex Conjugate (Fenchel Conjugate)</h2>

      <div class="insight">
        <h4>Roadmap for This Section</h4>
        <p>The conjugate is one of the most powerful tools in convex analysis. This section covers:</p>
        <ul>
          <li><b>§3.1:</b> What is the conjugate? (Definition, intuition, geometry)</li>
          <li><b>§3.2:</b> Core properties (always convex, Fenchel inequality, biconjugate)</li>
          <li><b>§3.3:</b> How to compute conjugates (worked examples)</li>
          <li><b>§3.4:</b> Algebraic rules (operations that preserve conjugacy)</li>
          <li><b>§3.5:</b> Advanced topics (Legendre transform, duality connections)</li>
        </ul>
      </div>

      <h3>3.1 Definition and Fundamental Intuition</h3>

      <div class="warning-box">
        <h4>⚠️ Notation Alert</h4>
        <p>The notation $f^*$ refers to the <b>Conjugate Function</b>. It is NOT $1/f$ (reciprocal) and NOT $\bar{f}$ (complex conjugate).
        <br>Also, $f^{**}$ is the biconjugate (conjugate of the conjugate), which is the convex envelope of $f$.</p>
      </div>

      <h4>Formal Definition</h4>
      <p>Start with a function $f : \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$. Allowing $+\infty$ is a standard way of encoding hard constraints ("$\mathbf{x}$ not allowed"). The <a href="#" class="definition-link" data-term="convex conjugate">Fenchel conjugate</a> is defined as:</p>
      $$
      \boxed{f^*(\mathbf{y}) := \sup_{\mathbf{x}\in\mathrm{dom} f} \big( \mathbf{y}^\top \mathbf{x} - f(\mathbf{x}) \big), \quad \mathbf{y}\in\mathbb{R}^n}
      $$

      <figure style="text-align: center; margin: 24px 0;">
        <img src="assets/epigraph_dual_norm.gif"
             alt="Animation showing the conjugate as the maximum gap between the function and a line"
             style="max-width: 700px; width: 100%; height: auto; border: 1px solid var(--border); border-radius: 8px; padding: 16px; background: white; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);" />
        <figcaption><i>Animation:</i> The conjugate $f^*(y)$ is the maximum vertical distance between the linear function $y^\top x$ and the function $f(x)$. Varying $y$ changes the slope of the line, and $f^*(y)$ records the intercept of the supporting hyperplane.</figcaption>
      </figure>

      <h4>Supporting Hyperplane Viewpoint</h4>
      <p>Fix $\mathbf{y} \in \mathbb{R}^n$. Consider affine functions of $\mathbf{x}$ with slope $\mathbf{y}$:
      $$ \ell_{\mathbf{y}}(\mathbf{x}) = \mathbf{y}^\top \mathbf{x} - c, \quad c \in \mathbb{R} $$
      We want $\ell_{\mathbf{y}}$ to lie <b>below</b> $f$ everywhere:
      $$ \mathbf{y}^\top \mathbf{x} - c \le f(\mathbf{x}) \quad \forall \mathbf{x} \iff c \ge \mathbf{y}^\top \mathbf{x} - f(\mathbf{x}) \quad \forall \mathbf{x} $$
      For this to hold for all $\mathbf{x}$, $c$ must be at least the supremum of the RHS. The <b>smallest</b> possible $c$ that maintains the lower bound property is:
      $$ c_{\min}(\mathbf{y}) = \sup_{\mathbf{x}} (\mathbf{y}^\top \mathbf{x} - f(\mathbf{x})) = f^*(\mathbf{y}) $$
      Thus, the tightest affine lower bound with slope $\mathbf{y}$ is:
      $$ \ell_{\mathbf{y}}(\mathbf{x}) = \mathbf{y}^\top \mathbf{x} - f^*(\mathbf{y}) $$</p>

      <h4>Three Complementary Views</h4>

      <div class="insight">
        <p><b>1. Supremum of Linear Family:</b> You can think of the conjugate as trying to <b>explain the function $f$ using lines</b>. For a fixed slope $y$, we want to find the best linear lower bound $y^\top x - c \le f(x)$. This requires $c \ge y^\top x - f(x)$ for all $x$. The tightest (smallest) such constant is $c = \sup_x (y^\top x - f(x)) = f^*(y)$. Thus, $f^*$ records the optimal intercepts for every possible slope $y$.</p>
      </div>

      <div class="insight">
        <p><b>2. Geometric View:</b></p>
        <ul>
            <li><b>Epigraph View:</b> The epigraph $\mathrm{epi} f$ is supported by the hyperplane $H = \{(x,t) \mid t = y^\top x - f^*(y)\}$. The conjugate $f^*$ encodes all non-vertical supporting hyperplanes of the epigraph.</li>
            <li><b>Gap View (1D):</b> For a fixed slope $y$, the quantity $yx - f(x)$ is the vertical gap between the line through the origin with slope $y$ and the graph of $f$. $f^*(y)$ is the <b>maximum vertical gap</b>.</li>
        </ul>
      </div>

      <div class="interpretation-box">
        <p><b>3. Economic Interpretation (Prices & Profit):</b> Imagine $\mathbf{x}$ is a production plan and $f(\mathbf{x})$ is the cost of production. If the market price vector is $\mathbf{y}$, then the revenue is $\mathbf{y}^\top \mathbf{x}$. The term $\mathbf{y}^\top \mathbf{x} - f(\mathbf{x})$ represents the <b>net profit</b>. The conjugate $f^*(\mathbf{y})$ is the <b>maximum possible profit</b> achievable given prices $\mathbf{y}$.</p>
      </div>


      <h3>3.2 Core Properties</h3>

      <h4>Property 1: $f^*$ is Always Convex</h4>
      <p>A crucial property is that $f^*$ is convex <b>regardless</b> of whether $f$ is convex.</p>
      <div class="proof-box">
        <h4>Proof</h4>
        <p>Take any fixed $\mathbf{x}$. As a function of $\mathbf{y}$, the expression $\phi_{\mathbf{x}}(\mathbf{y}) := \mathbf{y}^\top \mathbf{x} - f(\mathbf{x})$ is <b>affine</b> in $\mathbf{y}$ (linear term $\mathbf{y}^\top \mathbf{x}$ plus constant $-f(\mathbf{x})$).
        <br>Now $f^*$ is the pointwise supremum over all these affine functions:
        $$ f^*(y) = \sup_{x} \phi_x(y) $$
        <b>Fact:</b> The pointwise supremum of any family of convex (or affine) functions is convex.
        <br>Explicitly, if $g(y) = \sup_i g_i(y)$ where each $g_i$ is convex, then for any $\theta \in [0,1]$:
        $$
        \begin{aligned}
        g(\theta y_1+(1-\theta)y_2) &= \sup_i g_i(\theta y_1+(1-\theta)y_2) \\
        &\le \sup_i \big( \theta g_i(y_1) + (1-\theta)g_i(y_2) \big) \\
        &\le \theta \sup_i g_i(y_1) + (1-\theta)\sup_i g_i(y_2) \\
        &= \theta g(y_1) + (1-\theta)g(y_2)
        \end{aligned}
        $$
        Thus, $f^*$ is convex for <b>any</b> $f$. The conjugate lives in the "convex world" even if $f$ doesn't.</p>
      </div>

      <h4>Property 2: Fenchel's Inequality</h4>
      <p>From the definition $f^*(y) \ge y^\top x - f(x)$, we immediately get <b>Fenchel's Inequality</b>:</p>
      $$ \boxed{ f(x) + f^*(y) \ge x^\top y } $$
      <p>Equality holds exactly when $y \in \partial f(x)$ (for convex $f$). This is the foundation for optimality conditions in convex optimization.</p>

      <h4>Property 3: The Biconjugate Theorem</h4>
      <p>The conjugate of the conjugate is $f^{**}(x) = \sup_y (x^\top y - f^*(y))$.</p>

      <div class="theorem-box">
        <h4>Theorem: Fenchel-Moreau Biconjugate Theorem</h4>
        <p>For any function $f$:</p>
        <ol>
          <li>$f^{**} \le f$ always holds</li>
          <li>$f^{**}(x) = \text{cl}(\text{conv}(f))(x)$ — the <b>closed convex envelope</b> of $f$</li>
          <li>$f^{**} = f$ if and only if $f$ is closed and convex</li>
        </ol>
        <p><b>Geometric Intuition:</b> $\mathrm{epi}(f^{**})$ is the closed convex hull of $\mathrm{epi}(f)$. Conjugation encodes all supporting hyperplanes; taking it again reconstructs the convex envelope from these planes.</p>
      </div>


      <h3>3.3 Computing Conjugates: Examples</h3>

      <h4>3.3.1 One-Dimensional Functions</h4>

      <div class="example">
        <h4>(a) Affine function $f(x) = ax + b$</h4>
        <p>Compute $f^*(y) = \sup_{x} (yx - (ax+b)) = \sup_x ((y-a)x - b)$.</p>
        <ul>
              <li><b>Case 1: $y \ne a$.</b> The term $((y-a)x - b)$ is a line with nonzero slope. As $x \to \pm \infty$, this goes to $+\infty$ (since we can choose sign of $x$ to match sign of slope). So $f^*(y) = +\infty$.</li>
              <li><b>Case 2: $y = a$.</b> The term simplifies to $0 \cdot x - b = -b$. Supremum is $-b$.</li>
        </ul>
          <p><b>Result:</b> $f^*(y) = -b$ if $y=a$, else $+\infty$. Domain is the singleton $\{a\}$. This conjugate is the convex indicator of the point $a$, shifted by $-b$.</p>
      </div>

      <div class="example">
        <h4>(b) Negative Log: $f(x) = -\log x$ on $(0,\infty)$</h4>
        <p>We compute $f^*(y) = \sup_{x>0} (yx + \log x)$. Let $\phi(x) = yx + \log x$.</p>
        <ol>
          <li><b>Derivative:</b> $\phi'(x) = y + 1/x$. Setting to zero gives $x = -1/y$.</li>
          <li><b>Domain Check:</b> Since we require $x > 0$, we must have $y < 0$.
            <ul>
              <li>If $y \ge 0$, then $\phi'(x) > 0$ for all $x$. As $x \to \infty$, $\phi(x) \to \infty$. So $f^*(y) = \infty$.</li>
              <li>If $y < 0$, the critical point $x^* = -1/y$ is a global maximum (since $\phi''(x) = -1/x^2 < 0$).</li>
            </ul>
          </li>
          <li><b>Value:</b> Plug $x^*$ back in:
            $$ f^*(y) = y(-1/y) + \log(-1/y) = -1 + \log(1/(-y)) = -1 - \log(-y) $$
          </li>
        </ol>
        <p><b>Result:</b> $f^*(y) = -1 - \log(-y)$ for $y < 0$. Domain $(-\infty, 0)$.</p>
      </div>

      <h4>3.3.2 Two-Dimensional Examples (Visualizing Scaling)</h4>
      <p>Before jumping to matrices, let's see how scaling affects the conjugate in 2D.</p>

      <div class="example">
        <h4>(a) Isotropic Quadratic: $f(x) = \frac{1}{2}(x_1^2 + x_2^2)$</h4>
        <p>This separates into $f_1(x_1) + f_2(x_2)$.
        <br>$f^*(y) = \sup (y_1 x_1 - x_1^2/2) + \sup (y_2 x_2 - x_2^2/2) = \frac{1}{2}y_1^2 + \frac{1}{2}y_2^2 = \frac{1}{2}\|y\|_2^2$.
        <br><b>Lesson:</b> The conjugate of a standard quadratic is itself.</p>
      </div>

      <div class="example">
        <h4>(b) Scaled Quadratic: $f(x) = \frac{1}{2}(x_1^2 + 10x_2^2)$</h4>
        <p>Here $x_2$ is "steep" (high curvature).
        <br>Conjugate: $f^*(y) = \frac{1}{2}y_1^2 + \frac{1}{2(10)}y_2^2 = \frac{1}{2}y_1^2 + \frac{1}{20}y_2^2$.
        <br><b>Lesson:</b> High curvature in primal ($\lambda=10$) becomes low curvature in dual ($1/\lambda=0.1$). Steep valleys become shallow hills.</p>
      </div>

      <h4>3.3.3 Matrix and Vector Examples</h4>

      <div class="example">
        <h4>(c) Exponential: $f(x) = e^x$ on $\mathbb{R}$</h4>
        <p>We compute $f^*(y) = \sup_{x} (yx - e^x)$. Let $\phi(x) = yx - e^x$.</p>
        <ol>
          <li><b>Derivative:</b> $\phi'(x) = y - e^x$. Setting to zero gives $e^x = y$, or $x = \log y$. This requires $y > 0$.</li>
          <li><b>Case $y > 0$:</b> The critical point $x^* = \log y$ is a global max ($\phi'' = -e^x < 0$).
            $$ f^*(y) = y \log y - e^{\log y} = y \log y - y $$
          </li>
          <li><b>Case $y = 0$:</b> $\phi(x) = -e^x$. Supremum is $0$ (approached as $x \to -\infty$). Note that limit of $y \log y - y$ as $y \to 0$ is $0$.</li>
          <li><b>Case $y < 0$:</b> $\phi'(x) = y - e^x < 0$. Function is strictly decreasing. As $x \to -\infty$, $\phi(x) \approx yx \to \infty$ (since $y<0, x<0$). So $f^*(y) = \infty$.</li>
        </ol>
        <p><b>Result:</b> $f^*(y) = y \log y - y$ for $y \ge 0$, else $\infty$. Domain $[0, \infty)$.</p>
      </div>



      <div class="example">
        <h4>(d) Reciprocal: $f(x) = 1/x$ on $(0,\infty)$</h4>
        <p>We compute $f^*(y) = \sup_{x>0} (yx - 1/x)$. Let $\phi(x) = yx - 1/x$.</p>
        <ol>
          <li><b>Derivative:</b> $\phi'(x) = y + 1/x^2$. Setting to zero gives $x^2 = -1/y$, so $x = \sqrt{-1/y}$. This requires $y < 0$.</li>
          <li><b>Domain Analysis:</b>
            <ul>
              <li>If $y > 0$: $\phi(x) \approx yx \to \infty$ as $x \to \infty$. Supremum is $\infty$.</li>
              <li>If $y = 0$: $\phi(x) = -1/x$. Supremum is $0$ (approached as $x \to \infty$).</li>
              <li>If $y < 0$: The critical point $x^* = 1/\sqrt{-y}$ is a global max ($\phi'' = -2/x^3 < 0$).</li>
            </ul>
          </li>
          <li><b>Value (for $y<0$):</b>
            $$ f^*(y) = y \frac{1}{\sqrt{-y}} - \sqrt{-y} = \frac{-(-y)}{\sqrt{-y}} - \sqrt{-y} = -\sqrt{-y} - \sqrt{-y} = -2\sqrt{-y} $$
          </li>
        </ol>
        <p><b>Result:</b> $f^*(y) = -2\sqrt{-y}$ for $y \le 0$ (with value 0 at $y=0$), else $\infty$. Domain $(-\infty, 0]$.</p>
      </div>

      <h4>3.3.2 Quadratic and Matrix Functions</h4>

      <div class="example">
        <h4>(a) Quadratic: $f(x) = \frac{1}{2} x^\top Q x$ with $Q \succ 0$</h4>
        <p>Maximize $\phi(x) = y^\top x - \frac{1}{2} x^\top Q x$. This is a concave quadratic.
        <br>Gradient $\nabla \phi(x) = y - Qx = 0 \implies x^* = Q^{-1}y$.
        <br>Value: $y^\top (Q^{-1}y) - \frac{1}{2} (Q^{-1}y)^\top Q (Q^{-1}y) = y^\top Q^{-1} y - \frac{1}{2} y^\top Q^{-1} y = \frac{1}{2} y^\top Q^{-1} y$.
        <br><b>Result:</b> Quadratic with matrix $Q$ conjugates to quadratic with $Q^{-1}$.</p>
      </div>

      <div class="example">
        <h4>(b) Log-Determinant: $f(X) = -\log \det X$ on $\mathbb{S}^n_{++}$</h4>
        <p>Maximize $\mathrm{tr}(YX) + \log \det X$ over $X \succ 0$.
        <br>Gradient: $Y + X^{-1} = 0 \implies X = -Y^{-1}$.
        <br>Requires $Y$ invertible and $-Y^{-1} \succ 0 \implies Y \prec 0$ (negative definite).
        <br>Value: $\mathrm{tr}(Y(-Y^{-1})) + \log \det (-Y^{-1}) = -n - \log \det(-Y)$.
        <br><b>Result:</b> $f^*(Y) = -\log \det(-Y) - n$ on domain $\mathbb{S}^n_{--}$.</p>
      </div>

      <div class="example">
        <h4>(c) Indicator $\leftrightarrow$ Support Function</h4>
        <p>Let $I_S(x)$ be the indicator of a set $S$ (0 if $x \in S$, $+\infty$ else).
        <br>$I_S^*(y) = \sup_x (y^\top x - I_S(x)) = \sup_{x \in S} (y^\top x)$.
        <br><b>Result:</b> $I_S^* = \sigma_S$, the support function of $S$.</p>
      </div>

      <div class="example">
        <h4>(d) Norm $\leftrightarrow$ Indicator of Dual Ball</h4>
        <p>Let $f(x) = \|x\|$. Dual norm definition: $\|y\|_* = \sup_{\|x\|\le 1} y^\top x$.
        <br>$f^*(y) = \sup_x (y^\top x - \|x\|)$.
        <br>Case 1: $\|y\|_* \le 1$. Then $y^\top x \le \|y\|_* \|x\| \le \|x\|$. So $y^\top x - \|x\| \le 0$. Max is 0 (at $x=0$).
        <br>Case 2: $\|y\|_* > 1$. There exists $x_0$ with $\|x_0\|=1$ and $y^\top x_0 > 1$. Let $x = t x_0$. Value $t(y^\top x_0 - 1) \to \infty$.
        <br><b>Result:</b> $f^*$ is the indicator of the unit ball of the dual norm $\|\cdot\|_*$.
        $$ f^*(y) = \begin{cases} 0 & \|y\|_* \le 1 \\ +\infty & \|y\|_* > 1 \end{cases} $$</p>
      </div>

      <div class="example">
        <h4>(e) Negative Entropy & Log-Sum-Exp</h4>
          <p>Let $f(x) = \sum_{i=1}^n x_i \log x_i$ on the probability simplex $\Delta_n = \{x \mid x \ge 0, \sum x_i = 1\}$. This is the negative Shannon entropy.
          <br><b>Conjugate Derivation:</b>
          We wish to calculate $f^*(y) = \sup_{x \in \Delta_n} (y^\top x - \sum x_i \log x_i)$. This is a constrained optimization problem:
          $$ \begin{array}{ll} \text{maximize} & \sum y_i x_i - \sum x_i \log x_i \\ \text{subject to} & \sum x_i = 1, \ x \ge 0 \end{array} $$
          <br><b>Step 1: Form the Lagrangian.</b>
          We introduce a Lagrange multiplier $\nu$ for the equality constraint $\sum x_i = 1$. (We implicitly handle $x \ge 0$ by the domain of $\log$).
          $$ L(x, \nu) = \sum y_i x_i - \sum x_i \log x_i + \nu \left(\sum x_i - 1\right) $$
          <br><b>Step 2: Find Stationary Point.</b>
          Take the derivative with respect to $x_i$ and set to zero:
          $$ \frac{\partial L}{\partial x_i} = y_i - (\log x_i + 1) + \nu = 0 $$
          Solving for $x_i$:
          $$ \log x_i = y_i + \nu - 1 \implies x_i(\nu) = e^{y_i + \nu - 1} = e^{y_i} e^{\nu - 1} $$
          <br><b>Step 3: Determine Multiplier $\nu$.</b>
          Use the constraint $\sum x_i = 1$:
          $$ \sum_{i=1}^n x_i(\nu) = \sum_{i=1}^n (e^{y_i} e^{\nu - 1}) = e^{\nu - 1} \sum_{i=1}^n e^{y_i} = 1 $$
          Solving for the term $e^{\nu-1}$:
          $$ e^{\nu - 1} = \frac{1}{\sum_{k=1}^n e^{y_k}} $$
          <br><b>Step 4: Substitute Back.</b>
          The optimal $x_i^*$ is:
          $$ x_i^* = e^{y_i} \left( \frac{1}{\sum e^{y_k}} \right) = \frac{e^{y_i}}{\sum_{k=1}^n e^{y_k}} $$
          (This is the famous <b>Softmax</b> function!)
          <br><b>Step 5: Compute Optimal Value.</b>
          Plug $x^*$ into the objective $y^\top x - f(x)$:
          $$ \text{Value} = \sum_i y_i x_i^* - \sum_i x_i^* \log x_i^* $$
          Substitute $\log x_i^* = y_i - \log(\sum e^{y_k})$:
          $$ \begin{aligned}
             \text{Value} &= \sum y_i x_i^* - \sum x_i^* (y_i - \log(\sum e^{y_k})) \\
             &= \sum y_i x_i^* - \sum y_i x_i^* + \sum x_i^* \log(\sum e^{y_k}) \\
             &= \left(\sum x_i^*\right) \log\left(\sum e^{y_k}\right)
             \end{aligned} $$
          Since $\sum x_i^* = 1$, the result is:
          <br><b>Result:</b> $f^*(y) = \log\left(\sum_{i=1}^n e^{y_i}\right)$ (Log-Sum-Exp).
          <br>This duality is fundamental to Maximum Entropy modeling and Logistic Regression.</p>
      </div>

      <div class="example">
        <h4>(f) Geometric Mean via Conjugates</h4>
        <p>Consider the negative geometric mean $f(x) = -(\prod_{i=1}^n x_i)^{1/n}$ on $\mathbb{R}_{++}^n$. We wish to compute its conjugate to prove $f$ is convex.
        $$ f^*(y) = \sup_{x \succ 0} \left( y^\top x + \left(\prod_{i=1}^n x_i\right)^{1/n} \right) $$
        <br><b>Step 1: Homogeneity and Domain.</b>
        Note that the term inside the supremum is homogeneous of degree 1. Let $h(x) = y^\top x + (\prod x_i)^{1/n}$.
        Since $h(tx) = t h(x)$ for any $t > 0$:
        <ul>
          <li>If there exists <i>any</i> $x$ such that $h(x) > 0$, then letting $t \to \infty$ gives $\sup = \infty$.</li>
          <li>Therefore, for the conjugate to be finite, we must have $h(x) \le 0$ for all $x$.</li>
          <li>If $h(x) \le 0$ for all $x$, then the supremum is $0$ (achieved at $x \to 0$ or $x=0$).</li>
        </ul>
        Thus, $f^*(y)$ is the <b>indicator function</b> of the set $C = \{y \mid y^\top x + (\prod x_i)^{1/n} \le 0 \ \forall x \succ 0\}$.

        <br><b>Step 2: Explicit Condition for Set $C$.</b>
        First, if any $y_i > 0$, we can choose $x$ to align with $y$ and make $h(x)$ large positive. So we must have $y \le 0$.
        Let's analyze the condition $y^\top x + (\prod x_i)^{1/n} \le 0$.
        Rewrite as: $(\prod x_i)^{1/n} \le -y^\top x = \sum (-y_i) x_i$.
        This looks like the <b>AM-GM Inequality</b>: $\sqrt[n]{\prod a_i} \le \frac{1}{n} \sum a_i$.
        However, here we have weights $-y_i$. By the <b>Weighted AM-GM inequality</b>:
        $$ \prod_{i=1}^n x_i^{\lambda_i} \le \sum_{i=1}^n \lambda_i x_i \quad \text{where } \sum \lambda_i = 1, \lambda_i \ge 0 $$
        Comparing terms, let's try to match $\sum (-y_i) x_i$ to $\sum \lambda_i x_i$.
        This suggests we can force the inequality to hold if we relate $-y_i$ to $1/n$.
        <br>
        A precise derivation using Lagrange multipliers on the optimization problem $\sup ( \prod x_i^{1/n} )$ s.t. $y^\top x = -1$ yields the condition:
        $$ \left( \prod_{i=1}^n (-y_i) \right)^{1/n} \ge \frac{1}{n} \quad \text{and} \quad y \prec 0 $$
        (This set $C$ is the convex superlevel set of the geometric mean of $-y$).
        <br><b>Result:</b> $f^*(y) = 0$ if $y \in C$, else $\infty$. Since the domain $C$ is a convex set (superlevel set of concave function), $f^*$ is convex. This implies $f$ is convex.</p>
      </div>

      <h3>3.4 Algebraic Rules for Conjugates</h3>

      <p>The conjugate operation has nice algebraic properties that allow us to build complex conjugates from simpler ones.</p>

      <h4>Rule 1: Scaling and Vertical Shift</h4>
      <p>Let $g(x) = a f(x) + b$ with $a > 0, b \in \mathbb{R}$.
      $$
      \begin{aligned}
      g^*(y) &= \sup_x (y^\top x - a f(x) - b) \\
      &= \sup_x (y^\top x - a f(x)) - b \\
      &= a \sup_x \left( \frac{y}{a}^\top x - f(x) \right) - b \\
      &= a f^*(y/a) - b
      \end{aligned}
      $$
      So $\boxed{(af+b)^*(y) = a f^*(y/a) - b}$.</p>

      <h4>Rule 2: Affine Precomposition</h4>
      <p>Let $g(x) = f(Ax + b)$.
      $$ g^*(y) = \sup_x (y^\top x - f(Ax+b)) $$
      Let $z = Ax+b$. If $A$ is invertible, $x = A^{-1}(z-b)$.
      $$
      \begin{aligned}
      g^*(y) &= \sup_z (y^\top A^{-1}(z-b) - f(z)) \\
      &= \sup_z ((A^{-\top}y)^\top z - f(z)) - y^\top A^{-1}b \\
      &= f^*(A^{-\top}y) - y^\top A^{-1}b
      \end{aligned}
      $$
      If $A$ is not square/invertible, we use the general rule from convex analysis involving an infimum over preimages:
      $$ g^*(y) = \inf \{ f^*(z) - z^\top b \mid A^\top z = y \} $$
      If $A$ is invertible, $z$ is uniquely determined as $z = A^{-\top}y$, and the infimum collapses to the simple substitution:
      $$ \boxed{ g^*(y) = f^*(A^{-\top}y) - y^\top A^{-1}b } \quad (\text{invertible case}) $$</p>

      <h4>Rule 3: Sum of Independent Functions</h4>
      <p>Let $f(x, z) = f_1(x) + f_2(z)$.
      $$
      \begin{aligned}
      f^*(y, w) &= \sup_{x,z} (y^\top x + w^\top z - f_1(x) - f_2(z)) \\
      &= \sup_x (y^\top x - f_1(x)) + \sup_z (w^\top z - f_2(z)) \\
      &= f_1^*(y) + f_2^*(w)
      \end{aligned}
      $$
      So $\boxed{(f_1 \oplus f_2)^* = f_1^* \oplus f_2^*}$. This separation is key in duality.</p>

      <h4>Rule 4: Infimal Convolution</h4>
      <p>Let $h(x) = (f \square g)(x) = \inf_u (f(u) + g(x-u))$. Its conjugate is the sum of conjugates:
      $$ \boxed{ (f \square g)^* = f^* + g^* } $$
      <br><b>Proof:</b>
      $$
      \begin{aligned}
      (f \square g)^*(y) &= \sup_x \left( y^\top x - \inf_u (f(u) + g(x-u)) \right) \\
      &= \sup_x \sup_u \left( y^\top x - f(u) - g(x-u) \right)
      \end{aligned}
      $$
      Let $v = x-u$, so $x = u+v$. As $x, u$ vary over $\mathbb{R}^n$, $u, v$ vary independently over $\mathbb{R}^n$.
      $$
      \begin{aligned}
      &= \sup_{u, v} \left( y^\top(u+v) - f(u) - g(v) \right) \\
      &= \sup_u (y^\top u - f(u)) + \sup_v (y^\top v - g(v)) \\
      &= f^*(y) + g^*(y)
      \end{aligned}
      $$
      <b>Application:</b> The conjugate of a sum $(f+g)^*$ is related to the infimal convolution of conjugates $f^* \square g^*$. This duality (Sum $\leftrightarrow$ InfConv) is fundamental.</p>

      <h3>3.5 Advanced Topics</h3>

      <h4>3.5.1 The Legendre Transform (Smooth Case)</h4>
      <p>Assume $f$ is strictly convex and differentiable everywhere. The optimizer $x^*$ in the definition of $f^*(y)$ is characterized by:
      $$ \nabla_x (y^\top x - f(x)) = y - \nabla f(x) = 0 \implies y = \nabla f(x^*) $$
      Thus $x^* = (\nabla f)^{-1}(y)$. We can write:
      $$ f^*(y) = y^\top (\nabla f)^{-1}(y) - f((\nabla f)^{-1}(y)) $$
      Alternatively, parametrizing by $z$ where $y = \nabla f(z)$, we get a cleaner form:
      $$ f^*(\nabla f(z)) = z^\top \nabla f(z) - f(z) $$
      <b>Nice Identities:</b>
      <ul>
          <li>The gradient maps are inverses: $\nabla f^*(y) = x$ where $y = \nabla f(x)$. So $\nabla f^* = (\nabla f)^{-1}$.</li>
          <li>At the optimum, Fenchel's inequality holds with equality: $f(x) + f^*(y) = x^\top y$ where $y = \nabla f(x)$.</li>
      </ul></p>

      <div class="insight">
        <h4>Physics Interpretation: Lagrangian to Hamiltonian</h4>
        <p>In classical mechanics, the state is described by position $q$ and velocity $\dot{q}$. The <b>Lagrangian</b> $L(q, \dot{q})$ captures the energy balance ($T-V$).
        <br>To switch to the <b>Hamiltonian</b> formulation (position $q$ and momentum $p$), we take the Legendre transform with respect to velocity:
        $$ p = \nabla_{\dot{q}} L(q, \dot{q}) \quad (\text{Momentum is gradient of Lagrangian}) $$
        $$ H(q, p) = p^\top \dot{q} - L(q, \dot{q}) \quad (\text{Hamiltonian is conjugate}) $$
        This change of variables (from $\dot{q}$ to $p$) is valid because the kinetic energy term is typically strictly convex in $\dot{q}$.</p>
      </div>

      <h4>3.5.2 The Master Identity: Infimum via Conjugate</h4>

      <div class="theorem-box">
        <h4>Fundamental Identity for Duality</h4>
        <p>For any function $f$ and vector $a$, we have:</p>
        $$ \boxed{\inf_x \big( f(x) + a^\top x \big) = -f^*(-a)} $$
        <div class="proof-box">
          <h4>Proof</h4>
          <div class="proof-step">
            <strong>Algebra:</strong> $\inf_x (f(x) + a^\top x) = -\sup_x (-f(x) - a^\top x) = -\sup_x ((-a)^\top x - f(x))$.
          </div>
          <div class="proof-step">
            <strong>By definition:</strong> $\sup_x ((-a)^\top x - f(x)) = f^*(-a)$.
          </div>
          <div class="proof-step">
            <strong>Result:</strong> $\inf_x (f(x) + a^\top x) = -f^*(-a)$.
          </div>
        </div>
        <p><b>Why this matters:</b> This identity is how the conjugate enters Lagrangian duality. When computing $g(\lambda) = \inf_x (c^\top x + \lambda f(x))$, factoring out $\lambda$ and applying this identity gives $g(\lambda) = -\lambda f^*(-c/\lambda)$. This is the foundation for dual problem formulations in <a href="../09-duality/index.html">Lecture 09</a>.</p>
      </div>

      <h4>3.5.3 Curvature-Smoothness Duality</h4>
      <p>There is a beautiful duality between the curvature of $f$ (strong convexity) and the smoothness of $f^*$ (Lipschitz gradients).</p>
      <div class="theorem-box">
        <h4>Theorem: Strong Convexity $\iff$ Dual Smoothness</h4>
        <p>Let $f$ be a closed convex function. The following are equivalent:
        <ol>
            <li>$f$ is <b>$m$-strongly convex</b> ($ \nabla^2 f \succeq mI $).</li>
            <li>$f^*$ is <b>$1/m$-smooth</b> ($ \nabla^2 f^* \preceq \frac{1}{m}I $).</li>
        </ol>
        </p>
        <p><b>Intuition:</b>
        <ul>
            <li>If $f$ is very curved (large $m$), it looks like a sharp "V". Its conjugate $f^*$ (representing slopes) will have a domain that is large, and values that change slowly (flat, smooth).</li>
            <li>If $f$ is flat (small $m$), $f^*$ will be sharp.</li>
        </ul>
        <b>Example:</b> $f(x) = \frac{1}{2}x^\top Q x$. $f^*(y) = \frac{1}{2}y^\top Q^{-1} y$.
        <br>Strong convexity is $\lambda_{\min}(Q)$. Smoothness of dual is $\lambda_{\max}(Q^{-1}) = 1/\lambda_{\min}(Q)$.
        <br>This confirms the relationship exactly.</p>
      </div>

      <h4>3.5.4 Entropy and Log-Sum-Exp Connection</h4>

      <div class="theorem-box">
        <h4>Conjugate of Max is Entropy-Regularized</h4>
        <p>The conjugate of the max function connects to log-sum-exp and entropy. This is fundamental for smooth optimization:</p>
        <div class="proof-step">
          <strong>Variational Representation of Log-Sum-Exp:</strong>
          $$ \log\left(\sum_{i=1}^m e^{u_i}\right) = \max_{\lambda \in \Delta} \left( \lambda^\top u - \sum_{i=1}^m \lambda_i \log \lambda_i \right) $$
          where $\Delta = \{\lambda \ge 0 : \sum_i \lambda_i = 1\}$ is the probability simplex.
        </div>
        <div class="proof-step">
          <strong>Interpretation:</strong> The log-sum-exp function is the <b>conjugate</b> of the negative entropy function restricted to the simplex (see Example 3.3.2(e) above). The maximizer is the <b>softmax</b>: $\lambda_i^* = e^{u_i} / \sum_j e^{u_j}$.
        </div>
        <div class="proof-step">
          <strong>Connection to Smoothing:</strong> This explains why log-sum-exp is a "smooth max": it replaces the non-smooth max with an entropy-regularized objective. The regularization spreads probability across indices rather than concentrating on the maximum.
        </div>
        <p><b>Applications:</b> This connection underpins logistic regression, softmax classification, and interior-point methods for linear programming.</p>
      </div>
    </section>

    <section class="section-card" id="section-4">
        <h2>4. Quasiconvex Functions</h2>

        <h3>4.1 Definition: Sublevel Sets</h3>
        <p>A function $f: \mathbb{R}^n \to \mathbb{R}$ is <a href="#" class="definition-link">quasiconvex</a> if its domain is convex and all its <b>sublevel sets</b> are convex:</p>
        $$
        S_\alpha := \{x \in \mathrm{dom}\, f \mid f(x) \le \alpha\}
        $$
        <p>is a convex set for every $\alpha \in \mathbb{R}$.
        <div class="insight">
          <h4>Visual: Lake Water Levels</h4>
          <p>Imagine filling a landscape with water up to level $\alpha$. If the resulting lake is always a single connected convex shape (no islands, no pinched waists), the landscape function is quasiconvex.</p>
        </div>
        <br><i>Note:</i> Convex $\implies$ Quasiconvex, but not conversely. Quasiconvexity allows "bendy" graphs, as long as the "valley" shape is maintained.</p>

        <div class="theorem-box">
            <h4>Equivalent Condition: Max on a Segment</h4>
            <p>A function is quasiconvex if and only if for all $x, y \in \mathrm{dom} f$ and $\theta \in [0, 1]$:
            $$ f(\theta x + (1-\theta)y) \le \max\{f(x), f(y)\} $$
            This is the quasiconvex analog of Jensen's inequality: "Along any segment, the function value never exceeds the larger of the endpoints."</p>
        </div>

        <h3>4.2 The Optimization Viewpoint (Why this matters)</h3>
        <p>Quasiconvexity is the property that allows us to solve optimization problems via <b>bisection</b>.
        <br>If $f$ is quasiconvex, the problem "find $x$ such that $f(x) \le \alpha$" is a <b>convex feasibility problem</b> (since $S_\alpha$ is a convex set).
        <br>This allows us to reduce optimization to a sequence of convex feasibility checks:
        <ol>
            <li>Pick a target value $\alpha$.</li>
            <li>Check if the convex set $S_\alpha \cap \mathcal{F}$ is non-empty.</li>
            <li>If yes, the optimal value $p^\star \le \alpha$. If no, $p^\star > \alpha$.</li>
            <li>Bisect on $\alpha$.</li>
        </ol>
        This works because the feasibility predicate $\Phi(\alpha) = [\exists x: f(x) \le \alpha]$ is <b>monotone</b> in $\alpha$. We will detail the Bisection Algorithm in <a href="../08-convex-problems-conic/index.html">Lecture 08</a>.</p>

        <h3>4.3 Simple 1D Examples</h3>
        <div class="example">
            <h4>(a) Logarithm: $f(x) = \log x$</h4>
            <p>Sublevel set $\{x > 0 \mid \log x \le \alpha\} = \{x \mid x \le e^\alpha\} = (0, e^\alpha]$. This is a convex interval.
            <br>So $\log x$ is quasiconvex. (It is also concave, so $-\log x$ is convex).</p>
        </div>
        <div class="example">
            <h4>(b) Ceiling: $f(x) = \lceil x \rceil$</h4>
            <p>Sublevel set $\{x \mid \lceil x \rceil \le k\} = (-\infty, k]$. This is an interval.
            <br>So $\lceil x \rceil$ is quasiconvex. (It is step-like and definitely not convex).</p>
        </div>

        <h3>4.4 Vector Examples</h3>
        <div class="example">
            <h4>1. Length of a Vector</h4>
            <p>Define len$(x) = \max\{i \mid x_i \ne 0\}$ (index of last nonzero element).
            <br>Sublevel set $\{x \mid \text{len}(x) \le k\} = \{x \mid x_{k+1} = \dots = x_n = 0\}$.
            <br>This is a linear subspace, hence convex. So len$(x)$ is quasiconvex.</p>
        </div>
        <div class="example">
            <h4>2. Bilinear: $f(x_1, x_2) = x_1 x_2$ on $\mathbb{R}^2_{++}$</h4>
            <p>Hessian is indefinite (not convex).
            <br>Superlevel set $\{x \in \mathbb{R}^2_{++} \mid x_1 x_2 \ge \alpha\}$ is convex (bounded by hyperbola).
            <br>Thus $f$ is <b>quasiconcave</b> on the positive quadrant.</p>
        </div>
        <div class="example">
            <h4>3. Linear-Fractional: $f(x) = \frac{a^\top x + b}{c^\top x + d}$</h4>
            <p>Defined on the halfspace $D = \{x \mid c^\top x + d > 0\}$.
            <br>Consider the sublevel set $S_\alpha = \{x \in D \mid \frac{a^\top x + b}{c^\top x + d} \le \alpha\}$.
            $$ \frac{a^\top x + b}{c^\top x + d} \le \alpha \iff a^\top x + b \le \alpha (c^\top x + d) \iff (a - \alpha c)^\top x + (b - \alpha d) \le 0 $$
            This is a linear inequality, which defines a closed halfspace (if $a-\alpha c \ne 0$) or the whole space/empty set (degenerate cases).
            <br>Thus $S_\alpha$ is the intersection of the domain $D$ (halfspace) and another halfspace. The intersection of convex sets is convex.
            <br>Conclusion: Linear-fractional functions are quasiconvex. (Applying the same logic to $-f$ shows they are also quasiconcave, i.e., <b>quasilinear</b>).
            <br><b>Trap:</b> If the domain allows $c^\top x + d < 0$, the inequality flips, and the set becomes a union of disjoint regions (not convex).</p>
        </div>
        <div class="example">
            <h4>4. Distance Ratio (Rigorous Analysis)</h4>
            <p>Let $f(x) = \frac{\|x-a\|_2}{\|x-b\|_2}$. We analyze the sublevel set $S_\alpha = \{x \mid \|x-a\| \le \alpha \|x-b\|\}$.
            <br>Squaring leads to the quadratic inequality:
            $$ (1-\alpha^2)x^\top x + 2(\alpha^2 b - a)^\top x + (\|a\|^2 - \alpha^2 \|b\|^2) \le 0 $$
            <ul>
              <li><b>Case $\alpha < 1$:</b> The coefficient of $x^\top x$ is positive ($1-\alpha^2 > 0$).
                The inequality takes the form $|x-c|^2 \le R^2$. This describes a <b>Euclidean ball</b>, which is a convex set.</li>
              <li><b>Case $\alpha = 1$:</b> The quadratic term vanishes. We are left with a linear inequality $2(b-a)^\top x + C \le 0$, which describes a <b>halfspace</b>. This is a convex set.</li>
              <li><b>Case $\alpha > 1$:</b> The coefficient of $x^\top x$ is negative ($1-\alpha^2 < 0$).
            Dividing by the negative coefficient reverses the inequality: $\|x-c\|^2 \ge R^2$.
            This describes the <b>exterior of an open ball</b> (the hole is the ball). The complement of a ball is <b>not convex</b>.
            (For example, the midpoint of two points on opposite sides of the hole might fall inside the hole).</li>
        </ul>
        <p><b>Conclusion:</b> The distance ratio function is <b>quasiconvex</b> only on the set $\{x \mid f(x) \le 1\}$ (the Voronoi region of $a$). It is not globally quasiconvex on $\mathbb{R}^n$. This subtle distinction is important in facility location problems.</p>
        </div>
        <div class="example">
            <h4>5. Generalized Eigenvalue</h4>
            <p>Let $\lambda_{\max}(A, B) = \sup \{ \lambda \mid \det(\lambda B - A) = 0 \} = \sup_{x \ne 0} \frac{x^\top A x}{x^\top B x}$ for $B \succ 0$.
            <br>The sublevel set is $\{x \mid x^\top A x \le \alpha x^\top B x\} = \{x \mid x^\top (A - \alpha B) x \le 0\}$.
            This is not generally convex for the vector $x$.
            However, as a function of the <i>matrices</i>, $f(A, B) = \lambda_{\max}(A, B)$ is quasiconvex on $B \succ 0$.
            Sublevel set: $A \preceq \alpha B$, which is a convex cone in $(A, B)$.</p>
        </div>
        <div class="example">
            <h4>5. Norm-Ratio (Signal-to-Noise)</h4>
            <p>$f(x) = \frac{\|Ax-b\|_2}{c^\top x + d}$ on $c^\top x + d > 0$.
            <br>Sublevel set: $\|Ax-b\|_2 \le \alpha(c^\top x + d)$.
            For $\alpha \ge 0$, this is exactly a Second-Order Cone constraint.
            Thus sublevel sets are convex (specifically, SOC-representable).</p>
        </div>
        <div class="example">
            <h4>7. Internal Rate of Return (IRR)</h4>
            <p>Let $x = (x_0, \dots, x_n)$ be a cash flow stream.
            <br>Present value at rate $r$: $PV(x, r) = \sum_{i=0}^n x_i (1+r)^{-i}$.
            <br>IRR is defined as $\mathrm{IRR}(x) = \inf \{r \ge 0 \mid PV(x, r) = 0\}$.
            <br>Consider the superlevel set $S_R = \{x \mid \mathrm{IRR}(x) \ge R\}$.
            <br>Assuming standard cash flows (initial investment, then returns), $\mathrm{IRR}(x) \ge R$ roughly means the present value at rate $R$ is non-negative:
            $$ PV(x, R) = \sum_{i=0}^n (1+R)^{-i} x_i \ge 0 $$
            For a fixed $R$, this is a <b>linear inequality</b> in $x$.
            <br>A superlevel set formed by a linear inequality is a halfspace (convex).
            <br>So $\mathrm{IRR}(x)$ is a <b>quasiconcave</b> function of the cash flows.</p>
        </div>

        <h3>4.4 Second-Order Condition</h3>
        <p>Assume $f$ is twice differentiable.</p>

        <h4>Necessary Condition</h4>
        <p>If $f$ is quasiconvex, then for any $x \in \mathrm{dom}\, f$ and $y \in \mathbb{R}^n$:
        $$ y^\top \nabla f(x) = 0 \implies y^\top \nabla^2 f(x) y \ge 0 $$
        <i>Interpretation:</i> If we look in a direction $y$ tangent to the level set ($\nabla f \perp y$), the function must curve upwards (positive curvature). The level sets cannot curve "inward" to create a disconnected or non-convex shape.</p>
        <div class="insight">
            <h4>Quasiconvexity vs. Unimodality</h4>
            <p>In 1D, quasiconvexity is equivalent to being <b>unimodal</b> (decreasing then increasing).
            <br>In high dimensions, quasiconvexity requires that <b>every</b> 1D restriction is unimodal. This implies the sublevel sets are convex bodies (e.g., nested eggs), forbidding "two-humped" mountains or saddle shapes that would trap gradient descent.</p>
        </div>

        <h4>Sufficient Condition</h4>
        <p>If $f$ satisfies the stronger condition:
        $$ y^\top \nabla f(x) = 0, \ y \ne 0 \implies y^\top \nabla^2 f(x) y > 0 $$
        then $f$ is <b>strictly quasiconvex</b>. This ensures no "flat" regions along the contours that could hide non-convexity.</p>

        <h3>4.5 Operations Preserving Quasiconvexity</h3>
      <div id="widget-operations-preserving" style="width: 100%; height: 600px; position: relative; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 20px 0;"></div>
        <ul>
            <li><b>Max:</b> $f(x) = \max_i f_i(x)$ is quasiconvex (Intersection of convex sublevel sets).</li>
            <li><b>Composition:</b> $g(h(x))$ is quasiconvex if $h$ is quasiconvex and $g$ is <b>non-decreasing</b>.</li>
            <li><b>Warning:</b> Sums of quasiconvex functions are generally <b>not</b> quasiconvex.</li>
        </ul>

        <h3>4.6 Worked Example: Classification Challenge</h3>
        <p>Determine the convexity properties of the following functions.</p>

        <div class="example">
            <h4>(a) $f(x) = e^x - 1$ on $\mathbb{R}$</h4>
            <p><b>Analysis:</b> $f'(x) = e^x$, $f''(x) = e^x > 0$.
            <br>Strictly convex. Since it is monotone increasing, it is also both quasiconvex and quasiconcave.</p>
        </div>

        <div class="example">
            <h4>(b) $f(x_1, x_2) = x_1 x_2$ on $\mathbb{R}^2_{++}$</h4>
            <p><b>Analysis:</b> Hessian is indefinite ($\det = -1$). Not convex or concave.
            <br>Sublevel sets ($x_1 x_2 \le \alpha$) are non-convex (region under hyperbola). Not quasiconvex.
            <br>Superlevel sets ($x_1 x_2 \ge \alpha$) are convex (region above hyperbola). <b>Quasiconcave</b>.</p>
        </div>

        <div class="example">
            <h4>(c) $f(x_1, x_2) = 1/(x_1 x_2)$ on $\mathbb{R}^2_{++}$</h4>
            <p><b>Analysis:</b> $f(x) = (x_1 x_2)^{-1}$. The Hessian is $H = \begin{bmatrix} 2x_1^{-3}x_2^{-1} & x_1^{-2}x_2^{-2} \\ x_1^{-2}x_2^{-2} & 2x_1^{-1}x_2^{-3} \end{bmatrix}$. $\det(H) = 3(x_1 x_2)^{-4} > 0$. Trace $> 0$.
            <br><b>Convex</b> (and thus quasiconvex).
            <br>Superlevel sets are the same as sublevel sets of $x_1 x_2$. Non-convex. Not quasiconcave.</p>
        </div>

        <div class="example">
            <h4>(d) $f(x_1, x_2) = x_1 / x_2$ on $\mathbb{R}_{++}^2$</h4>
            <p><b>Analysis:</b> Linear-fractional function.
            <br>Sublevel sets $x_1 \le \alpha x_2$ are halfspaces (convex).
            <br>Superlevel sets $x_1 \ge \alpha x_2$ are halfspaces (convex).
            <br><b>Quasilinear</b> (both quasiconvex and quasiconcave). Not convex/concave.</p>
        </div>

        <div class="example">
            <h4>(e) $f(x_1, x_2) = x_1^2 / x_2$ on $\mathbb{R} \times \mathbb{R}_{++}$</h4>
            <p><b>Analysis:</b> This is the perspective of the square function $h(u)=u^2$.
            <br>Since $h$ is convex, its perspective is <b>convex</b>.</p>
        </div>

        <div class="example">
            <h4>(f) $f(x_1, x_2) = x_1^\alpha x_2^{1-\alpha}$ on $\mathbb{R}^2_{++}$ for $\alpha \in [0,1]$</h4>
            <p><b>Analysis:</b> Geometric mean (Cobb-Douglas function).
            <br>It is <b>concave</b> (Hessian is negative semidefinite).
            <br>Thus it is <b>quasiconcave</b>. It is generally not quasiconvex.</p>
        </div>
    </section>

    <section class="section-card" id="section-5">
        <h2>5. Log-Concave and Log-Convex Functions</h2>

        <h3>5.1 Definitions</h3>
        <p>Let $f: \mathbb{R}^n \to \mathbb{R}_{++}$.
        <ul>
            <li>$f$ is <b>log-concave</b> if $\log f(x)$ is concave.</li>
            <li>$f$ is <b>log-convex</b> if $\log f(x)$ is convex.</li>
        </ul>
        Equivalently, for log-concavity (multiplicative form):
        $$ f(\theta x + (1-\theta)y) \ge f(x)^\theta f(y)^{1-\theta} $$
        Log-concavity is crucial in probability (unimodality, tail bounds).</p>

        <div class="theorem-box">
          <h4>Gradient Condition (3.47)</h4>
          <p>If $f$ is differentiable and $f(x) > 0$, then $f$ is log-concave if and only if for all $x, y \in \mathrm{dom}\, f$:</p>
          $$ \boxed{ \frac{f(y)}{f(x)} \le \exp\left( \frac{\nabla f(x)^\top (y-x)}{f(x)} \right) } $$
          <p><b>Interpretation:</b> Log-concavity means $f$ grows at most exponentially, with a rate controlled by the tangent of the log. In log-space, this is simply the statement that the tangent hyperplane is a global upper bound.</p>

          <div class="proof-box">
            <h4>Detailed Proof</h4>
            <p>Let $h(x) = \log f(x)$. Then $f$ is log-concave $\iff h$ is concave.</p>
            <div class="proof-step">
                <strong>Step 1: The Concavity Inequality.</strong>
                A differentiable function $h$ is concave if and only if for all $x, y$:
                $$ h(y) \le h(x) + \nabla h(x)^\top (y-x) $$
            </div>
            <div class="proof-step">
                <strong>Step 2: Compute the Gradient of Log.</strong>
                Using the chain rule:
                $$ \nabla h(x) = \nabla (\log f(x)) = \frac{1}{f(x)} \nabla f(x) $$
            </div>
            <div class="proof-step">
                <strong>Step 3: Substitute and Rearrange.</strong>
                Substitute $\nabla h(x)$ into the inequality:
                $$ \log f(y) \le \log f(x) + \frac{\nabla f(x)^\top (y-x)}{f(x)} $$
                Move $\log f(x)$ to the left side:
                $$ \log f(y) - \log f(x) \le \frac{\nabla f(x)^\top (y-x)}{f(x)} \implies \log \left( \frac{f(y)}{f(x)} \right) \le \frac{\nabla f(x)^\top (y-x)}{f(x)} $$
            </div>
            <div class="proof-step">
                <strong>Step 4: Exponentiate.</strong>
                Since $e^z$ is strictly increasing, we can exponentiate both sides:
                $$ \frac{f(y)}{f(x)} \le \exp\left( \frac{\nabla f(x)^\top (y-x)}{f(x)} \right) $$
                This completes the proof.
            </div>
          </div>
        </div>

        <div class="theorem-box">
          <h4>Second-Order Characterization</h4>
          <p>If $f$ is twice differentiable, we can check log-concavity via the Hessian of $\log f(x)$.
          Using the chain rule and quotient rule, the Hessian of $h(x) = \log f(x)$ is:
          $$ \nabla^2 \log f(x) = \frac{1}{f(x)}\nabla^2 f(x) - \frac{1}{f(x)^2}\nabla f(x)\nabla f(x)^\top $$
          Since $f$ is log-concave if and only if $\nabla^2 \log f(x) \preceq 0$, this yields the condition:
          $$ f(x)\nabla^2 f(x) \preceq \nabla f(x)\nabla f(x)^\top $$
          This matrix inequality ($A \preceq bb^\top$) is a standard test for log-concavity.
          </p>
        </div>

        <div class="insight">
          <h4>Property: Shifting Down (3.48)</h4>
          <p>If $f$ is log-concave and $f(x) > a \ge 0$, then the shifted function $g(x) = f(x) - a$ is log-concave on its domain $\{x \mid f(x) > a\}$.
          <br><b>Proof:</b> We need to show $\log g$ is concave. Let $x_1, x_2$ be in the domain and $z = \theta x_1 + (1-\theta)x_2$.
          <br>We want to show $\log(f(z)-a) \ge \theta \log(f(x_1)-a) + (1-\theta)\log(f(x_2)-a)$.
          <br>Consider the function $h(u) = \log(u-a)$ for $u > a$.
          <br>Derivatives: $h'(u) = \frac{1}{u-a} > 0$ (increasing) and $h''(u) = -\frac{1}{(u-a)^2} < 0$ (concave).
          <br>1. From log-concavity of $f$: $f(z) \ge f(x_1)^\theta f(x_2)^{1-\theta}$.
          <br>2. Since $h$ is increasing and defined on the range of $f$ (where $f>a$), we can apply it:
          $$ h(f(z)) \ge h(f(x_1)^\theta f(x_2)^{1-\theta}) $$
          3. We rely on the property that for $u, v > a$, $\log(u^\theta v^{1-\theta} - a) \ge \theta \log(u-a) + (1-\theta)\log(v-a)$.
          This inequality holds because the function $\phi(t) = \log(e^t - a)$ is concave for $t > \log a$.
          Checking the second derivative of $\phi(t)$: $\phi'(t) = \frac{e^t}{e^t - a}$, $\phi''(t) = \frac{e^t(e^t - a) - e^t(e^t)}{(e^t - a)^2} = \frac{-a e^t}{(e^t - a)^2}$.
          Since $a \ge 0$, $\phi''(t) \le 0$, so $\phi$ is concave.
          <br>Conclusion: Subtracting a non-negative constant from a positive log-concave function preserves log-concavity.</p>
        </div>

        <h3>5.2 Examples</h3>
        <div class="example">
            <h4>1. Uniform Distribution on Convex Set</h4>
            <p>Let $C$ be convex. $f(x) = 1/\alpha$ if $x \in C$, else 0.
            <br>$\log f(x) = -\log \alpha$ on $C$, $-\infty$ outside.
            <br>This is a concave function (indicator of convex set). So uniform density is log-concave.</p>
        </div>
        <div class="example">
            <h4>2. Wishart Distribution</h4>
            <p>Density $f(X) \propto (\det X)^{k} e^{-\mathrm{tr}(\Sigma^{-1}X)}$.
            <br>$\log f(X) = c + k \log \det X - \mathrm{tr}(\Sigma^{-1}X)$.
            <br>$\log \det$ is concave; trace is linear. Sum is concave.
            <br>Thus Wishart is log-concave.</p>
        </div>

        <h3>5.3 Integration Rules</h3>
        <p>Log-convexity and log-concavity behave nicely under integration, though the conditions differ.</p>
        <div class="theorem-box">
            <h4>(a) Integrals of Log-Convex Functions</h4>
            <p>If $f(x, y) \ge 0$ is log-convex in $x$ for each fixed $y$, then $g(x) = \int_C f(x, y) dy$ is log-convex.</p>
            <div class="insight">
                <h4>Intuition Sketch</h4>
                <p>We need $g(\theta x + (1-\theta)z) \le g(x)^\theta g(z)^{1-\theta}$.
                <br>By log-convexity of $f$:
                $$ f(\theta x + (1-\theta)z, y) \le f(x, y)^\theta f(z, y)^{1-\theta} $$
                Integrate both sides over $y$. The RHS integral is bounded using <b>Hölder's Inequality</b> for integrals (with $p=1/\theta, q=1/(1-\theta)$):
                $$ \int f(x, y)^\theta f(z, y)^{1-\theta} dy \le \left(\int f(x, y) dy\right)^\theta \left(\int f(z, y) dy\right)^{1-\theta} = g(x)^\theta g(z)^{1-\theta} $$
                This proves log-convexity of $g$.
                <br><b>Examples:</b> Gamma function, Moment Generating Function ($M(z) = \mathbb{E} e^{z^\top X}$), Laplace Transform.</p>
            </div>
        </div>
        <div class="theorem-box">
            <h4>(b) Integrals of Log-Concave Functions (Prékopa-Leindler)</h4>
            <p>If $f(x, y)$ is <b>jointly</b> log-concave in $(x, y)$, then the marginal $g(x) = \int f(x, y) dy$ is log-concave.</p>
            <p>This is a consequence of the <b>Prékopa-Leindler Theorem</b>, which relates integrals of functions satisfying log-concave-like conditions:
            Let $f, g, h: \mathbb{R}^n \to \mathbb{R}_+$ be integrable functions. If for some $\lambda \in (0, 1)$ and all $x, y \in \mathbb{R}^n$:
            $$ h(\lambda x + (1-\lambda)y) \ge f(x)^\lambda g(y)^{1-\lambda} $$
            Then:
            $$ \int_{\mathbb{R}^n} h(z) dz \ge \left( \int_{\mathbb{R}^n} f(x) dx \right)^\lambda \left( \int_{\mathbb{R}^n} g(y) dy \right)^{1-\lambda} $$
            </p>
            <p><b>Consequence:</b> Convolution of log-concave functions is log-concave. If $f, g$ are log-concave, so is $(f*g)(x) = \int f(x-y)g(y) dy$. This theorem is a functional generalization of the Brunn-Minkowski inequality for volumes of convex bodies.</p>
        </div>

        <h3>5.4 Probability Examples</h3>
        <div class="example">
            <h4>(a) Hitting a Convex Set with Log-Concave Noise</h4>
            <p>Let $w$ be a random vector with log-concave density $p(w)$. Let $C$ be a convex set. Define:
            $$ f(x) = \mathbb{P}(x + w \in C) $$
            We can express this as a convolution:
            $$ f(x) = \int \mathbf{1}_C(x+w) p(w) dw $$
            Change of variables $u = x+w \implies w = u-x$:
            $$ f(x) = \int \mathbf{1}_C(u) p(u-x) du $$
            Consider the integrand $F(x, u) = \mathbf{1}_C(u) p(u-x)$.
            <ul>
                <li>$\mathbf{1}_C(u)$ is log-concave (indicator of convex set).</li>
                <li>$p(u-x)$ is log-concave in $(x, u)$ (composition of log-concave $p$ with affine map).</li>
                <li>The product is log-concave in $(x, u)$.</li>
            </ul>
            By the integration rule, $f(x)$ is log-concave.
            <br><i>Interpretation:</i> The probability of a random point landing in a moving target $C$ varies "smoothly" (unimodally) as we shift the center $x$.</p>
        </div>
        <div class="example">
            <h4>(b) Cumulative Distribution Function (CDF)</h4>
            <p>If a PDF $p(z)$ is log-concave on $\mathbb{R}^n$, then its CDF is log-concave:
            $$ F(x) = \mathbb{P}(w \preceq x) = \int \mathbf{1}_{(-\infty, x]}(z) p(z) dz $$
            We can view the indicator $\mathbf{1}_{(-\infty, x]}(z)$ as the function:
            $$ I(x, z) = \begin{cases} 1 & z_i \le x_i \ \forall i \\ 0 & \text{otherwise} \end{cases} $$
            The set $\{(x, z) \mid z_i \le x_i\}$ is a convex polyhedron (defined by linear inequalities). Thus its indicator $I(x, z)$ is log-concave in $(x, z)$.
            <br>Since $p(z)$ is log-concave, the product $I(x, z)p(z)$ is jointly log-concave.
            <br>Integrating out $z$ implies $F(x)$ is log-concave.
            <br><b>Example:</b> The Gaussian CDF $\Phi(x)$ is log-concave.</p>
        </div>
    </section>

    <section class="section-card" id="section-6">
      <h2>6. Summary & Cheat Sheet</h2>

      <div class="recap-box" style="margin-bottom: 24px;">
        <h3 style="margin-top: 0; font-size: 1.1em;">Key Takeaways</h3>
        <ul style="margin: 0 0 0 20px;">
          <li><b>Conjugates:</b> $f^*(y)=\sup_x (y^\top x - f(x))$ packages all supporting hyperplanes of $f$ into a single convex function.</li>
          <li><b>Fenchel Inequality:</b> $f(x) + f^*(y) \ge x^\top y$ is the fundamental source of dual bounds.</li>
          <li><b>Quasiconvexity:</b> Sublevel sets are convex even when the function is not; crucial for fractional and max-type objectives.</li>
          <li><b>Log-concavity:</b> Turns multiplicative structure (probabilities, volumes) into additive structure via $\log$, enabling convex optimization techniques.</li>
        </ul>
      </div>

      <h3>Conjugate Transformations Cheat Sheet</h3>
      <table class="data-table">
        <tr><th>Primal $f(x)$</th><th>Conjugate $f^*(y)$</th><th>Domain of $f^*$</th></tr>
        <tr><td>$\frac{1}{2}x^\top Q x$ ($Q \succ 0$)</td><td>$\frac{1}{2}y^\top Q^{-1} y$</td><td>$\mathbb{R}^n$</td></tr>
        <tr><td>$-\log x$</td><td>$-1 - \log(-y)$</td><td>$y < 0$</td></tr>
        <tr><td>$e^x$</td><td>$y \log y - y$</td><td>$y \ge 0$</td></tr>
        <tr><td>$1/x$ ($x>0$)</td><td>$-2\sqrt{-y}$</td><td>$y < 0$</td></tr>
        <tr><td>$I_C(x)$</td><td>$\sigma_C(y)$ (Support)</td><td>$\mathbb{R}^n$</td></tr>
        <tr><td>$\|x\|$</td><td>$I_{B_*}(y)$ (Dual Ball)</td><td>$\|y\|_* \le 1$</td></tr>
      </table>
    </section>

    <section class="section-card" id="section-7">
      <h2><i data-feather="edit-3"></i> 7. Exercises</h2>

<div class="problem">
  <h3>P6.1 — Conjugate of Norm Squared</h3>
  <p>Compute the conjugate of $f(x) = \frac{1}{2}\|x\|^2$ for a general norm $\|\cdot\|$. Show it is $\frac{1}{2}\|y\|_*^2$.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Self-Conjugacy:</b> For the Euclidean norm, $f(x)=\frac{1}{2}\|x\|_2^2$ is self-conjugate ($f^*=f$).</li>
        <li><b>Dual Norms:</b> For general norms, the conjugate of $\frac{1}{2}\|x\|^2$ is $\frac{1}{2}\|y\|_*^2$. This is the basis for the relationship between Primal and Dual problems in optimization.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Definition.</strong>
      $f^*(y) = \sup_x (y^\top x - \frac{1}{2}\|x\|^2)$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Dual Norm Bound.</strong>
      By definition of the dual norm, $y^\top x \le \|y\|_* \|x\|$.
      Let $u = \|x\|$. Then $y^\top x - \frac{1}{2}\|x\|^2 \le \|y\|_* u - \frac{1}{2}u^2$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Optimize Scalar $u$.</strong>
      The function $g(u) = \|y\|_* u - \frac{1}{2}u^2$ is a concave quadratic.
      Maximum occurs at $g'(u) = \|y\|_* - u = 0 \implies u^* = \|y\|_*$.
      Max value: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.
    </div>
    <div class="proof-step">
      <strong>Step 4: Tightness.</strong>
      Can we achieve this bound? By definition of the dual norm, there exists a vector $x_0$ with $\|x_0\|=1$ such that $y^\top x_0 = \|y\|_*$.
      Choose $x = \|y\|_* x_0$. Then $\|x\| = \|y\|_*$.
      $y^\top x = \|y\|_* (y^\top x_0) = \|y\|_*^2$.
      Objective: $\|y\|_*^2 - \frac{1}{2}\|y\|_*^2 = \frac{1}{2}\|y\|_*^2$.
      Thus the supremum is exactly $\frac{1}{2}\|y\|_*^2$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.2 — Quasi-Convexity of Ceiling</h3>
  <p>Show that $f(x) = \lceil x \rceil$ (on $\mathbb{R}$) is quasi-convex.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Monotonicity:</b> Any monotonic function on $\mathbb{R}$ is both quasi-convex and quasi-concave (quasilinear), regardless of discontinuities.</li>
        <li><b>Sublevel Sets:</b> The definition relies purely on the geometry of the sets $\{x \mid f(x) \le \alpha\}$, making it robust to re-parameterization.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Definition of Quasi-Convexity.</strong>
      A function is quasi-convex if its sublevel sets $S_\alpha = \{x \mid f(x) \le \alpha\}$ are convex for all $\alpha \in \mathbb{R}$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Analyze Sublevel Sets.</strong>
      Condition: $\lceil x \rceil \le \alpha$.
      Since $\lceil x \rceil$ is an integer, let $k = \lfloor \alpha \rfloor$. The condition is $\lceil x \rceil \le k$.
      This is equivalent to $x \le k$ (since if $x > k$, $\lceil x \rceil \ge k+1$).
      So $S_\alpha = (-\infty, \lfloor \alpha \rfloor]$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Conclusion.</strong>
      The set $(-\infty, k]$ is an interval, which is a convex set in $\mathbb{R}$.
      Therefore, $f$ is quasi-convex.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.3 — Softmax Analysis</h3>
  <p>For $f(x) = \log(\sum_{i=1}^n e^{x_i})$: <ol type="a"><li>Show $\max x_i \le f(x) \le \max x_i + \log n$.</li><li>Show $f$ is convex via Hessian analysis.</li></ol></p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Smooth Max:</b> Log-Sum-Exp is to $\max$ what the Huber loss is to absolute value—a smooth, convex differentiable approximation.</li>
        <li><b>Hessian-Covariance:</b> $\nabla^2 \text{lse}(x) = \text{Var}(p)$, where $p$ is the softmax distribution. Since variance is non-negative, convexity follows immediately.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Part A: Bounds.</strong>
      Let $x_{\max} = \max_i x_i$.
      Lower bound: $\sum e^{x_i} \ge e^{x_{\max}}$.
      $f(x) \ge \log(e^{x_{\max}}) = x_{\max}$.
      Upper bound: $\sum e^{x_i} = e^{x_{\max}} \sum e^{x_i - x_{\max}}$.
      Since $x_i - x_{\max} \le 0$, each term $e^{x_i - x_{\max}} \le 1$.
      Sum is bounded by $n$.
      $f(x) = x_{\max} + \log(\sum e^{x_i - x_{\max}}) \le x_{\max} + \log n$.
    </div>
    <div class="proof-step">
      <strong>Part B: Hessian.</strong>
      Gradient $\nabla f(x) = p$ where $p_i = e^{x_i}/\sum e^{x_k}$. Note $\mathbf{1}^\top p = 1, p > 0$.
      Hessian $\nabla^2 f(x) = \text{diag}(p) - pp^\top$.
      Let $v \in \mathbb{R}^n$.
      $v^\top \nabla^2 f v = \sum p_i v_i^2 - (p^\top v)^2 = \sum p_i v_i^2 - (\sum p_i v_i)^2$.
    </div>
    <div class="proof-step">
      <strong>Part C: Variance Interpretation.</strong>
      Let $Z$ be a random variable taking values $v_1, \dots, v_n$ with probabilities $p_1, \dots, p_n$ (where $p$ is the softmax vector).
      <br>The term $\sum p_i v_i$ corresponds to the expected value: $\mathbb{E}[Z]$.
      <br>The term $\sum p_i v_i^2$ corresponds to the second moment: $\mathbb{E}[Z^2]$.
      <br>The quadratic form of the Hessian is:
      $$ v^\top \nabla^2 f(x) v = \mathbb{E}[Z^2] - (\mathbb{E}[Z])^2 = \mathrm{Var}(Z) $$
      Since variance is always non-negative ($\mathrm{Var}(Z) \ge 0$), the Hessian is positive semidefinite.
      <br><b>Intuition:</b> The curvature of the Log-Sum-Exp function in direction $v$ is exactly the variance of the softmax distribution projected onto $v$. When the probabilities are uniform ($p_i \approx 1/n$), the variance is high (high curvature). When the probabilities concentrate on one index ($p_k \approx 1$), the variance approaches zero (flat), matching the behavior of the "hard" max function.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.4 — Concavity of Geometric Mean</h3>
  <p>Prove $G(x) = (\prod_{i=1}^n x_i)^{1/n}$ is concave on $\mathbb{R}^n_{++}$ using log-concavity properties.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>The H-L-C Theorem:</b> Homogeneity + Log-Concavity $\implies$ Concavity. This powerful theorem turns multiplicative properties (geometric mean) into additive inequalities.</li>
        <li><b>Superadditivity:</b> The core geometric property derived is $G(x+y) \ge G(x) + G(y)$.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Check Log-Concavity.</strong>
      Consider $\log G(x)$.
      $$ \log \left( \prod x_i^{1/n} \right) = \frac{1}{n} \sum_{i=1}^n \log x_i $$
      The function $\log x_i$ is concave. The sum of concave functions is concave.
      Thus $\log G(x)$ is a concave function.
      This means $G(x)$ is <b>log-concave</b>.
    </div>
    <div class="proof-step">
      <strong>Step 2: Homogeneity.</strong>
      $G(\alpha x) = (\prod (\alpha x_i))^{1/n} = (\alpha^n \prod x_i)^{1/n} = \alpha G(x)$.
      $G$ is homogeneous of degree 1.
    </div>
    <div class="proof-step">
      <strong>Step 3: Theorem.</strong>
      We show that a function $G$ that is log-concave and homogeneous of degree 1 is concave.
      We rely on the property of <strong>superadditivity</strong>: $G(x+y) \ge G(x) + G(y)$, which implies concavity for homogeneous functions.
      <br><b>Proof of Superadditivity:</b>
      Let $\alpha = G(x)$ and $\beta = G(y)$. If $\alpha=0$ or $\beta=0$, the result is trivial. Assume $\alpha, \beta > 0$.
      Define the mixing weight $\lambda = \frac{\alpha}{\alpha+\beta}$. Then $1-\lambda = \frac{\beta}{\alpha+\beta}$.
      Consider the point $z$ formed by combining normalized versions of $x$ and $y$:
      $$ z = \frac{x+y}{\alpha+\beta} = \frac{\alpha}{\alpha+\beta} \frac{x}{\alpha} + \frac{\beta}{\alpha+\beta} \frac{y}{\beta} = \lambda \frac{x}{\alpha} + (1-\lambda) \frac{y}{\beta} $$
      By log-concavity, $G(z) \ge G(x/\alpha)^\lambda G(y/\beta)^{1-\lambda}$.
      Using 1-homogeneity, $G(x/\alpha) = G(x)/\alpha = 1$ and $G(y/\beta) = 1$.
      Thus $G(z) \ge 1^\lambda 1^{1-\lambda} = 1$.
      Finally, using homogeneity on $G(x+y)$:
      $$ G(x+y) = G((\alpha+\beta)z) = (\alpha+\beta)G(z) \ge \alpha+\beta = G(x) + G(y) $$
      Superadditivity plus homogeneity implies concavity: $G(\theta x + (1-\theta)y) \ge G(\theta x) + G((1-\theta)y) = \theta G(x) + (1-\theta)G(y)$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.5 — Geometric Mean Cone</h3>
  <p>Show $K = \{x \in \mathbb{R}^n_+ \mid G(x) \ge \alpha A(x)\}$ is a convex cone for $\alpha \in [0,1]$, where $A(x) = \frac{1}{n}\sum x_i$ is the arithmetic mean.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Concave Superlevel Sets:</b> For a concave function $f$, the set $\{x \mid f(x) \ge 0\}$ is convex.</li>
        <li><b>Conic Extension:</b> Any inequality involving 1-homogeneous functions ($G(x) \ge A(x)$) naturally defines a cone, as scaling $x$ by $k$ scales both sides equally.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Rewrite condition.</strong>
      $G(x) - \alpha A(x) \ge 0$.
      Let $h(x) = G(x) - \alpha A(x)$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Check Concavity.</strong>
      $G(x)$ is concave (from P6.4).
      $A(x)$ is linear, so $-A(x)$ is concave.
      Since $\alpha \ge 0$, $-\alpha A(x)$ is concave.
      Thus $h(x)$ is a sum of concave functions, so $h$ is concave.
    </div>
    <div class="proof-step">
      <strong>Step 3: Superlevel Set.</strong>
      The set $K = \{x \mid h(x) \ge 0\}$ is the 0-superlevel set of a concave function.
      Superlevel sets of concave functions are convex sets.
    </div>
    <div class="proof-step">
      <strong>Step 4: Cone Property.</strong>
      $G(kx) = kG(x)$ and $A(kx) = kA(x)$ for $k \ge 0$.
      $G(kx) \ge \alpha A(kx) \iff kG(x) \ge k\alpha A(x) \iff G(x) \ge \alpha A(x)$ (for $k>0$).
      Thus $K$ is a cone.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.6 — Matrix Fractional Function</h3>
  <p>Prove convexity of $f(x, Y) = x^\top Y^{-1} x$ (for $Y \succ 0$) using the epigraph characterization.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Schur Complement:</b> The bridge between non-linear rational functions and linear matrix inequalities (LMIs). $x^\top Y^{-1} x \le t \iff \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0$.</li>
        <li><b>Joint Convexity:</b> This proves that optimizing jointly over the filter/controller $x$ and the preconditioner $Y$ is a convex problem.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Epigraph Definition.</strong>
      $(x, Y, t) \in \text{epi } f \iff t \ge x^\top Y^{-1} x$ and $Y \succ 0$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Schur Complement.</strong>
      The inequality $t - x^\top Y^{-1} x \ge 0$ is the Schur complement condition for the block matrix:
      $$ M = \begin{bmatrix} Y & x \\ x^\top & t \end{bmatrix} \succeq 0 $$
      (Assuming $Y \succ 0$. If $Y \succeq 0$ singular, we need $x \in \text{range}(Y)$ etc, but usually defined on open domain).
    </div>
    <div class="proof-step">
      <strong>Step 3: Convexity of LMI.</strong>
      The set of matrices $\mathcal{S}_+^k$ is a convex cone (PSD cone).
      The map $(x, Y, t) \to M(x, Y, t)$ is linear.
      The inverse image of a convex set under a linear map is convex.
      Thus $\text{epi } f = \{(x, Y, t) \mid M(x, Y, t) \succeq 0\}$ is convex.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.7 — Fenchel's Inequality & Biconjugate</h3>
  <p>For $f(x) = \frac{1}{2}x^\top Q x$ with $Q \succ 0$: 1. Verify Fenchel's inequality. 2. Verify $f^{**} = f$.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Fundamental Inequality:</b> $f(x) + f^*(y) \ge x^\top y$ is the "Cauchy-Schwarz" of Convex Analysis. It bounds the inner product by the sum of energies.</li>
        <li><b>Duality Gap:</b> In optimization, this inequality manifests as Weak Duality (Primal $\ge$ Dual). The gap closes (equality holds) exactly at optimality.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Part 1: Fenchel's Inequality.</strong>
      We need $f(x) + f^*(y) \ge x^\top y$.
      We found $f^*(y) = \frac{1}{2}y^\top Q^{-1} y$.
      LHS - RHS = $\frac{1}{2}x^\top Q x + \frac{1}{2}y^\top Q^{-1} y - x^\top y$.
      Let $u = Q^{1/2}x$ and $v = Q^{-1/2}y$. Then $x = Q^{-1/2}u, y = Q^{1/2}v$.
      $x^\top y = u^\top v$.
      Expression: $\frac{1}{2}u^\top u + \frac{1}{2}v^\top v - u^\top v = \frac{1}{2}\|u-v\|^2 \ge 0$.
      Verified.
    </div>
    <div class="proof-step">
      <strong>Part 2: Biconjugate.</strong>
      $f^{**}(x) = (f^*)^*(x)$. Let $g(y) = f^*(y) = \frac{1}{2}y^\top P y$ with $P = Q^{-1}$.
      The conjugate of a quadratic $\frac{1}{2}y^\top P y$ is $\frac{1}{2}x^\top P^{-1} x$.
      $P^{-1} = (Q^{-1})^{-1} = Q$.
      So $f^{**}(x) = \frac{1}{2}x^\top Q x = f(x)$. Verified.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.8 — Quasiconvexity of Distance Ratio</h3>
  <p>Show $S_\theta = \{x \mid \|x-a\|_2 \le \theta \|x-b\|_2\}$ is convex for $\theta \le 1$.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Voronoi Regions:</b> For $\theta=1$, this defines a halfspace (Voronoi boundary).</li>
        <li><b>Apollonius Circles:</b> For $\theta \ne 1$, the boundary is a sphere. For $\theta < 1$, the sublevel set is the ball; for $\theta > 1$, it is the exterior (not convex). Thus, distance ratios are only quasiconvex for "contractive" ratios.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Square the condition.</strong>
      $\|x-a\|^2 \le \theta^2 \|x-b\|^2$.
      $(x-a)^\top (x-a) \le \theta^2 (x-b)^\top (x-b)$.
      $x^\top x - 2a^\top x + a^\top a \le \theta^2 (x^\top x - 2b^\top x + b^\top b)$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Group terms.</strong>
      $(1 - \theta^2) x^\top x - 2(a - \theta^2 b)^\top x + (\|a\|^2 - \theta^2 \|b\|^2) \le 0$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Analyze coefficient.</strong>
      Let $q(x)$ be this quadratic expression. The quadratic term is $(1-\theta^2)\|x\|^2$.
      If $\theta \le 1$, then $1-\theta^2 \ge 0$.
      The sublevel set of a convex quadratic ($P \succeq 0$) is convex (an ellipsoid or halfspace).
      Thus $S_\theta$ is convex. (If $\theta > 1$, it's the complement of an ellipsoid, which is non-convex).
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.9 — Log-Concavity of Probability Measures</h3>
  <p>If $p(x)$ is a log-concave probability density, show that the function $f(x) = \mathbb{P}(x+W \in C)$ is log-concave, where $W \sim p$ and $C$ is a convex set.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Prékopa-Leindler Theorem:</b> The marginal of a log-concave function is log-concave. Since convolution is an integral operation, it preserves log-concavity.</li>
        <li><b>Robustness:</b> This implies that if your noise is "nice" (log-concave, e.g., Gaussian, Uniform) and your target set is convex, the success probability behaves nicely (unimodally).</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Write as Convolution.</strong>
      The condition $x+W \in C$ is equivalent to $W \in C-x$. The probability is:
      $$ f(x) = \int_{C-x} p(w) dw = \int_{\mathbb{R}^n} I_C(u) p(u-x) du $$
      where we used the substitution $u = w+x$. This integral is exactly the convolution of the indicator function $I_C(x)$ (which is 1 on $C$, 0 else) and the reflected density $\tilde{p}(x) = p(-x)$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Apply Prékopa-Leindler.</strong>
      The theorem states that the convolution of two log-concave functions is log-concave. We check the components:
      <ul>
          <li>$p(x)$ is log-concave (given), so $p(-x)$ is also log-concave.</li>
          <li>$I_C(x)$ is the indicator function of a convex set. $\log I_C(x)$ is 0 on $C$, $-\infty$ else. Since $C$ is convex, this is a concave function (extended value). So $I_C$ is log-concave.</li>
      </ul>
    </div>
    <div class="proof-step">
      <strong>Step 3: Conclusion.</strong>
      Since $f$ is the convolution of two log-concave functions, $f$ is log-concave.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.10 — Log-Concavity of Gaussian</h3>
  <p>Show that the multivariate Gaussian density function $f(x)$ is log-concave. $$ f(x) = \frac{1}{(2\pi)^{n/2} (\det \Sigma)^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu)\right) $$ where $\Sigma \succ 0$.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Log-Quadratic:</b> The Gaussian is the "most" log-concave function (log is strictly concave quadratic).</li>
        <li><b>Closure:</b> Because the Gaussian is log-concave, any marginal, conditional, or linear transformation of a Gaussian is also log-concave (and in fact, Gaussian).</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Take the logarithm:
      $$ \log f(x) = -\frac{n}{2}\log(2\pi) - \frac{1}{2}\log(\det \Sigma) - \frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) $$
    </div>
    <div class="proof-step">
      The first two terms are constants. The third term is a quadratic form $-\frac{1}{2}x^\top \Sigma^{-1} x + \text{linear term}$.
      Let $g(x) = \log f(x)$.
      <br><b>Gradient:</b>
      We use the identity $\nabla(x^\top A x) = (A+A^\top)x$. Since $\Sigma^{-1}$ is symmetric:
      $$ \nabla \left( -\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) \right) = -\Sigma^{-1}(x-\mu) $$
      So $\nabla g(x) = -\Sigma^{-1}(x-\mu)$.
      <br><b>Hessian:</b>
      $$ \nabla^2 g(x) = \nabla (-\Sigma^{-1}x + \Sigma^{-1}\mu) = -\Sigma^{-1} $$
    </div>
    <div class="proof-step">
      Since $\Sigma \succ 0$, its inverse $\Sigma^{-1} \succ 0$, so $-\Sigma^{-1} \prec 0$ (negative definite).
      Since the Hessian is negative definite everywhere, $g(x)$ is strictly concave.
      Therefore, $f(x)$ is log-concave.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.11 — Legendre Transform of Quadratic</h3>
  <p>Consider $f(x) = \frac{1}{2}x^\top Q x$ with $Q \succ 0$. Verify the Legendre transform formula $f^*(y) = x^\top y - f(x)$ where $y = \nabla f(x)$.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Variable Swap:</b> The Legendre transform changes coordinates from position $x$ to slope (force) $y = \nabla f(x)$.</li>
        <li><b>Hamiltonian Mechanics:</b> This is exactly the transformation from Lagrangian $L(q, \dot{q})$ to Hamiltonian $H(q, p)$ used in physics.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Gradient Map.</strong>
      $\nabla f(x) = Qx$. So we set $y = Qx$.
      This implies $x = Q^{-1}y$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Apply Formula.</strong>
      $f^*(y) = x^\top y - f(x)$ evaluated at $x = Q^{-1}y$.
      $$ f^*(y) = (Q^{-1}y)^\top y - \frac{1}{2}(Q^{-1}y)^\top Q (Q^{-1}y) $$
      $$ = y^\top Q^{-1} y - \frac{1}{2} y^\top Q^{-1} y = \frac{1}{2}y^\top Q^{-1} y $$
    </div>
    <div class="proof-step">
      <strong>Conclusion:</strong>
      This matches the result from Example 1.4(a). The Legendre transform provides a mechanical way to compute conjugates for smooth strictly convex functions.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.12 — Log-Concavity Examples (3.49)</h3>
  <p>Verify the log-concavity of the following functions.</p>
  <ol type="a">
    <li><b>Logistic:</b> $f(x) = e^x / (1+e^x)$ on $\mathbb{R}$.</li>
    <li><b>Harmonic Mean:</b> $f(x) = (\sum x_i^{-1})^{-1}$ on $\mathbb{R}_{++}^n$.</li>
    <li><b>Product over Sum:</b> $f(x) = (\prod x_i) / (\sum x_i)$ on $\mathbb{R}_{++}^n$.</li>
    <li><b>Determinant over Trace:</b> $f(X) = \det X / \mathrm{tr} X$ on $\mathbb{S}_{++}^n$.</li>
  </ol>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Ratio Rule:</b> $\frac{\text{Log-Concave}}{\text{Log-Convex}} = \text{Log-Concave}$.</li>
        <li><b>Examples:</b> Logistic function, Harmonic mean, Determinant over Trace. These structures appear frequently in information theory and spectral graph theory.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>(a) Logistic Function.</strong>
      <p>$f(x) = \frac{e^x}{1+e^x}$. We want $\log f$ concave.</p>
      <ul>
        <li><b>Step 1: Compute log.</b> $\log f(x) = \log e^x - \log(1+e^x) = x - \log(1+e^x)$.</li>
        <li><b>Step 2: Check curvature.</b>
        The term $x$ is affine (convex/concave).
        The term $g(x) = \log(1+e^x) = \log(e^0 + e^x)$ is Log-Sum-Exp, so $g$ is convex.
        Therefore $-g$ is concave.
        $\log f(x) = \text{Affine} + \text{Concave} = \text{Concave}$.
        So $f$ is log-concave.</li>
      </ul>
    </div>
    <div class="proof-step">
      <strong>(b) Harmonic-type Mean.</strong>
      <p>$f(x) = \frac{1}{\sum 1/x_i}$. We show $\log f$ is concave.</p>
      <ul>
        <li><b>Step 1: Analyze components.</b> Let $s(x) = \sum_{i=1}^n x_i^{-1}$. Since $u \mapsto 1/u$ is convex, $s(x)$ is a convex function.</li>
        <li><b>Step 2: Log-transformation.</b>
        $\log f(x) = -\log s(x)$.
        The function $h(u) = -\log u$ is convex and decreasing.
        The composition of a convex decreasing function ($h$) with a convex function ($s$) is <b>convex</b>? No, that rule yields convexity.
        Wait, we want $\log f$ to be <i>concave</i>.
        Let's check the Hessian directly for $g(x) = -\log(\sum x_i^{-1})$.
        Or simply recall that the harmonic mean is a concave function. Since $\log$ is concave and increasing, $\log(\text{concave})$ is concave.
        Therefore, $\log f$ is concave.</li>
      </ul>
    </div>

    <div class="proof-step">
          <h4>Deep Dive: Log-Concavity of Ratio Functions (Rigorous Hessian Proof)</h4>
          <p>We prove log-concavity for $f(x) = \frac{\prod x_i}{\sum x_i}$ and its matrix analog $F(X) = \frac{\det X}{\mathrm{tr} X}$.</p>

          <h5>(c) Product over Sum: $g(x) = \log f(x) = \sum \log x_i - \log(\sum x_i)$</h5>
          <p><b>Gradient:</b> $\frac{\partial g}{\partial x_j} = \frac{1}{x_j} - \frac{1}{S}$ where $S = \sum x_k$.
          <br><b>Hessian:</b> $H = \nabla^2 g$.
          $$ H_{jk} = -\frac{1}{x_j^2}\delta_{jk} + \frac{1}{S^2} $$
          <b>PSD Check:</b> For a vector $v$,
          $$ v^\top H v = -\sum \frac{v_j^2}{x_j^2} + \frac{1}{S^2} (\sum v_j)^2 $$
          Let $a_j = v_j/x_j$ and $b_j = x_j$. By Cauchy-Schwarz: $(\sum a_j b_j)^2 \le (\sum a_j^2)(\sum b_j^2)$.
          $$ (\sum v_j)^2 \le (\sum \frac{v_j^2}{x_j^2}) (\sum x_j^2) $$
          Thus $\frac{(\sum v_j)^2}{S^2} \le (\sum \frac{v_j^2}{x_j^2}) \frac{\sum x_j^2}{S^2}$.
          Since $\sum x_j^2 \le (\sum x_j)^2 = S^2$, the factor is $\le 1$.
          $$ v^\top H v \le -\sum \frac{v_j^2}{x_j^2} + \sum \frac{v_j^2}{x_j^2} = 0 $$
          Thus $\nabla^2 g \preceq 0$, so $f$ is log-concave.</p>

          <h5>(d) Matrix Analog: $G(X) = \log \det X - \log \mathrm{tr} X$</h5>
          <p>Let $X \in \mathbb{S}^n_{++}$ and direction $H$.
          <br><b>Second Directional Derivative:</b>
          $$ D^2 G(X)[H,H] = -\mathrm{tr}(X^{-1}HX^{-1}H) + \frac{(\mathrm{tr}H)^2}{(\mathrm{tr}X)^2} $$
          We need to show $(\mathrm{tr}H)^2 \le (\mathrm{tr}X)^2 \mathrm{tr}(X^{-1}HX^{-1}H)$.
          <br><b>Matrix Cauchy-Schwarz:</b> Define inner product $\langle A, B \rangle = \mathrm{tr}(A^\top B)$.
          Let $A = X^{-1/2} H X^{-1/2}$ and $B = X$.
          $$ \langle A, B \rangle = \mathrm{tr}(X^{-1/2} H X^{-1/2} X) = \mathrm{tr}(H) $$
          $$ \|A\|_F^2 = \mathrm{tr}(A^2) = \mathrm{tr}(X^{-1} H X^{-1} H) $$
          $$ \|B\|_F^2 = \mathrm{tr}(X^2) $$
          By Cauchy-Schwarz: $(\mathrm{tr}H)^2 \le \mathrm{tr}(X^{-1} H X^{-1} H) \mathrm{tr}(X^2)$.
          Since $\mathrm{tr}(X^2) = \sum \lambda_i^2 \le (\sum \lambda_i)^2 = (\mathrm{tr}X)^2$ (for $\lambda_i > 0$), we have:
          $$ \frac{(\mathrm{tr}H)^2}{(\mathrm{tr}X)^2} \le \mathrm{tr}(X^{-1} H X^{-1} H) $$
          Thus $D^2 G(X)[H,H] \le 0$. The function is log-concave.</p>
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.13 — Log-Convexity of the Gamma Function</h3>
  <p>The Gamma function is defined as $\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt$ for $x > 0$. Show that $\Gamma(x)$ is log-convex using the integration rule.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Artin's Theorem:</b> The log-convexity of the Gamma function (Bohr-Mollerup theorem) is a specific instance of the general rule: integrals of log-convex functions are log-convex.</li>
        <li><b>Moment Generating Functions:</b> The MGF $\mathbb{E}[e^{tX}]$ is always log-convex for the same reason.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Identify the Integrand.</strong>
      We can rewrite the integral as:
      $$ \Gamma(x) = \int_0^\infty f(x, t) dt $$
      where $f(x, t) = t^{x-1} e^{-t}$. Note that the domain of integration $(0, \infty)$ does not depend on $x$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Check Log-Convexity of Integrand.</strong>
      Consider $g(x) = \log f(x, t)$ for a fixed $t > 0$.
      $$ g(x) = \log(t^{x-1} e^{-t}) = (x-1)\log t - t $$
      This function is linear (affine) in $x$ (slope $\log t$, intercept $-\log t - t$).
      Since affine functions are convex, $g(x)$ is convex in $x$.
      Thus, $f(x, t)$ is log-convex in $x$ for every $t$.
    </div>
    <div class="proof-step">
      <strong>Step 3: Apply Integration Rule.</strong>
      The theorem states that if $f(x, t)$ is log-convex in $x$ for each $t$, then $\int f(x, t) dt$ is log-convex (assuming convergence).
      Therefore, $\Gamma(x)$ is log-convex.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.14 — Log-Sum-Exp Conjugate</h3>
  <p>Derive the conjugate of $f(x) = \log(\sum_{i=1}^n e^{x_i})$. Show that $f^*(y) = \sum y_i \log y_i$ if $y \in \Delta$ (simplex), else $\infty$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      See Example 3.3.3(e) for the derivation. The key steps are:
      1. Gradient condition $y = \nabla f(x)$ implies $y$ is a probability vector.
      2. If $y$ is not a probability vector, the supremum is infinite.
      3. Substituting $x$ back gives the negative entropy term.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.15 — Norm Conjugate</h3>
  <p>Let $\|\cdot\|$ be a norm. Show that its conjugate is the indicator function of the dual norm unit ball: $f^*(y) = 0$ if $\|y\|_* \le 1$, else $\infty$. Use Lemma A3 from the Drill.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      See Lemma A3 in Drill.md for the rigorous proof. Key idea: if $\|y\|_* > 1$, align $x$ with $y$ and scale to infinity. If $\|y\|_* \le 1$, Hölder implies $y^\top x - \|x\| \le 0$, max is 0.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.16 — Convex Hull of a Function</h3>
  <p>The convex envelope of $f$ is defined as the function $g$ whose epigraph is the convex hull of $\mathrm{epi}(f)$. Prove that $g(x) = \inf \{ \sum \theta_i f(x_i) \mid \sum \theta_i x_i = x, \theta \ge 0, \sum \theta_i = 1 \}$.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Epigraph Hull:</b> Convexifying the geometric object (epigraph) corresponds to convexifying the function values via mixtures.</li>
        <li><b>Lower Semicontinuity:</b> $g$ is also the biconjugate $f^{**}$ (if we take the closure).</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      A point $(x, t)$ is in $\mathrm{conv}(\mathrm{epi}(f))$ iff it is a convex combination of points $(x_i, t_i) \in \mathrm{epi}(f)$.
      $(x, t) = \sum \theta_i (x_i, t_i)$ where $t_i \ge f(x_i)$.
      To find the value of the function $g(x)$, we minimize $t$ for a fixed $x$.
      $g(x) = \inf \{ t \mid (x, t) \in \mathrm{conv}(\mathrm{epi}(f)) \} = \inf \{ \sum \theta_i t_i \mid \sum \theta_i x_i = x, t_i \ge f(x_i) \}$.
      Since we minimize $t$, we should choose $t_i$ as small as possible, i.e., $t_i = f(x_i)$.
      Thus $g(x) = \inf \{ \sum \theta_i f(x_i) \mid \sum \theta_i x_i = x \}$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.17 — Log-Concavity Characterization (Breakdown of 3.47)</h3>
  <p>Let $f > 0$ be differentiable on a convex domain. Prove that $f$ is log-concave if and only if $f(y) \le f(x) \exp\left( \frac{\nabla f(x)^\top (y-x)}{f(x)} \right)$ for all $x, y$.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>First-Order Condition:</b> A differentiable function $h$ is concave iff $h(y) \le h(x) + \nabla h(x)^\top (y-x)$.</li>
        <li><b>Chain Rule:</b> $\nabla (\log f(x)) = \frac{\nabla f(x)}{f(x)}$.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Bedrock Lemma (Concavity $\iff$ Tangent Upper Bound).</strong>
      We first prove that a differentiable function $h$ on a convex set is concave if and only if $h(y) \le h(x) + \nabla h(x)^\top (y-x)$ for all $x,y$.
      <br><b>Proof ($\Rightarrow$):</b>
      Let $h$ be concave. Restrict $h$ to the line segment between $x$ and $y$: $\phi(t) = h(x + t(y-x))$.
      $\phi$ is concave on $[0,1]$. Thus its secant slope is bounded by its derivative:
      $$ \phi(1) \le \phi(0) + \phi'(0) $$
      Substituting back: $h(y) \le h(x) + \nabla h(x)^\top (y-x)$.
      <br><b>Proof ($\Leftarrow$):</b>
      Assume the inequality holds. Let $z = \theta x + (1-\theta)y$.
      Apply the inequality at $z$:
      $h(x) \le h(z) + \nabla h(z)^\top (x-z)$ and $h(y) \le h(z) + \nabla h(z)^\top (y-z)$.
      Multiply by $\theta$ and $1-\theta$ and sum:
      $\theta h(x) + (1-\theta)h(y) \le h(z) + \nabla h(z)^\top (\theta(x-z) + (1-\theta)(y-z))$.
      The gradient term is zero because $\theta x + (1-\theta)y - z = 0$.
      Thus $\theta h(x) + (1-\theta)h(y) \le h(z)$, so $h$ is concave.
    </div>
    <div class="proof-step">
      <strong>Step 2: Application to Log-Concavity.</strong>
      Let $h(x) = \log f(x)$. Then $\nabla h(x) = \frac{\nabla f(x)}{f(x)}$.
      The condition for $f$ being log-concave is that $h$ is concave.
      By the lemma, this is equivalent to:
      $$ \log f(y) \le \log f(x) + \frac{\nabla f(x)^\top (y-x)}{f(x)} $$
    </div>
    <div class="proof-step">
      <strong>Step 3: Exponentiate.</strong>
      Since $\exp$ is strictly increasing, we can exponentiate both sides:
      $$ f(y) \le f(x) \exp\left( \frac{\nabla f(x)^\top (y-x)}{f(x)} \right) $$
      Dividing by $f(x)$ gives the stated inequality.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.18 — Convexity of Maximum Eigenvalue (3.10)</h3>
  <p>Define $f(X) = \lambda_{\max}(X)$ for symmetric matrices $X \in \mathbb{S}^n$. Show that $f$ is convex.</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Variational Form:</b> $\lambda_{\max}(X)$ is the supremum of the linear function $u^\top X u$ over the unit sphere.</li>
        <li><b>Pointwise Supremum:</b> The pointwise supremum of a family of convex (or linear) functions is convex. This is the "meta-theorem" for proving convexity of spectral functions.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Rayleigh Quotient Representation.</strong>
      For a symmetric matrix $X$, the largest eigenvalue is given by the maximum value of the quadratic form on the unit sphere:
      $$ \lambda_{\max}(X) = \sup_{\|u\|_2 = 1} u^\top X u $$
    </div>
    <div class="proof-step">
      <strong>Step 2: Linearity in $X$.</strong>
      For a fixed vector $u$, the function $g_u(X) = u^\top X u = \mathrm{tr}(u u^\top X)$ is a linear function of the matrix $X$.
      Linear functions are convex.
    </div>
    <div class="proof-step">
      <strong>Step 3: Supremum Property.</strong>
      $f(X)$ is the pointwise supremum of the family of linear functions $\{g_u(X) \mid \|u\|_2 = 1\}$.
      Since the pointwise supremum of any collection of convex functions is convex, $f(X)$ is convex.
    </div>
    <div class="proof-step">
        <strong>Alternative Proof: Density Matrix Representation.</strong>
        We can also write $\lambda_{\max}(X) = \sup \{ \langle X, W \rangle \mid W \succeq 0, \mathrm{tr}(W)=1 \}$.
        <br>Here $W$ represents a "density matrix" (convex combination of rank-1 projectors).
        <br>For any fixed density matrix $W$, the function $X \mapsto \langle X, W \rangle$ is linear.
        <br>Thus $\lambda_{\max}(X)$ is the support function of the set of density matrices, which implies convexity.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.19 — Convexity of Induced Matrix Norms (3.11)</h3>
  <p>Let $\|\cdot\|_a$ and $\|\cdot\|_b$ be norms on $\mathbb{R}^m$ and $\mathbb{R}^n$. The induced norm (operator norm) is defined as:
  $$ \|X\|_{a,b} = \sup_{v \ne 0} \frac{\|Xv\|_a}{\|v\|_b} = \sup_{\|v\|_b=1} \|Xv\|_a $$
  Show that $f(X) = \|X\|_{a,b}$ is convex.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Dual Norm Representation.</strong>
      Recall that any norm can be written as a supremum over the dual ball: $\|z\|_a = \sup_{\|u\|_{a^*} \le 1} u^\top z$.
      Substitute $z = Xv$:
      $$ \|Xv\|_a = \sup_{\|u\|_{a^*} \le 1} u^\top (Xv) $$
    </div>
    <div class="proof-step">
      <strong>Step 2: Bilinear Supremum (Dual Norm Derivation).</strong>
      We rigorously dualize the inner term $\|Xv\|_a$.
      Recall the dual norm identity: $\|z\|_a = \sup_{\|u\|_{a^*} \le 1} u^\top z$.
      Proof: By definition $\|u\|_{a^*} = \sup_{\|z\|_a \le 1} u^\top z$. This implies $u^\top z \le \|u\|_{a^*} \|z\|_a$.
      Thus $\sup_{\|u\|_{a^*} \le 1} u^\top z \le \|z\|_a$. Equality is achieved by a supporting hyperplane to the unit ball at $z/\|z\|_a$.
      <br>Substitute $z=Xv$: $\|Xv\|_a = \sup_{\|u\|_{a^*} \le 1} u^\top (Xv)$.
      <br>Now substitute back into the operator norm:
      $$ \|X\|_{a,b} = \sup_{\|v\|_b=1} \sup_{\|u\|_{a^*} \le 1} u^\top X v = \sup \{ u^\top X v \mid \|v\|_b=1, \|u\|_{a^*} \le 1 \} $$
    </div>
    <div class="proof-step">
      <strong>Step 3: Convexity via Linearity.</strong>
      For fixed $u$ and $v$, the mapping $X \mapsto u^\top X v$ is linear in $X$.
      The function $f(X)$ is the pointwise supremum of these linear functions.
      Therefore, the induced norm is convex.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.20 — Vector Composition Rules (3.14)</h3>
  <p>Use the composition rules to prove convexity/concavity for the following functions constructed from convex/concave components $g_i(x)$.
  <br><strong>(a)</strong> $f(x) = \sum_{i=1}^r g_{[i]}(x)$ (sum of $r$ largest), where $g_i$ are convex.
  <br><strong>(b)</strong> $f(x) = \log(\sum e^{g_i(x)})$, where $g_i$ are convex.
  <br><strong>(c)</strong> $f(x) = (\prod g_i(x))^{1/k}$, where $g_i$ are concave and non-negative.
  </p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>General Rule:</b> $h(g_1(x), \dots, g_k(x))$ is convex if $h$ is convex and non-decreasing in each argument (for convex $g_i$).</li>
        <li><b>Concave Rule:</b> If $h$ is concave and non-decreasing, and $g_i$ are concave, the composition is concave.</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>(a) Sum of $r$ Largest.</strong>
      Let $h(z) = \sum_{i=1}^r z_{[i]}$. This function is convex and non-decreasing in every argument $z_i$ (increasing one component cannot decrease the sum of the top $r$).
      Since $g_i(x)$ are convex, the composition $h(g(x))$ is convex.
    </div>
    <div class="proof-step">
      <strong>(b) Log-Sum-Exp Composition.</strong>
      Let $h(z) = \log(\sum e^{z_i})$. We know $h$ is convex.
      Check monotonicity: $\frac{\partial h}{\partial z_i} = \frac{e^{z_i}}{\sum e^{z_k}} > 0$.
      Since $h$ is convex and non-decreasing, and $g_i$ are convex, the composition is convex.
    </div>
    <div class="proof-step">
      <strong>(c) Geometric Mean Composition.</strong>
      Let $h(z) = (\prod z_i)^{1/k}$ on $\mathbb{R}^k_+$.
      We know $h$ is concave.
      Check monotonicity: $\frac{\partial h}{\partial z_i} = \frac{1}{k} \frac{h(z)}{z_i} \ge 0$ (for $z \ge 0$).
      Since $h$ is concave and non-decreasing, and $g_i$ are concave, the composition is concave.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.21 — Perspective of Affine Composition (3.20)</h3>
  <p>Let $f: \mathbb{R}^m \to \mathbb{R}$ be convex. Show that $g(x) = (c^\top x + d) f\left( \frac{Ax+b}{c^\top x + d} \right)$ is convex on $\{x \mid c^\top x + d > 0\}$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Perspective Transform.</strong>
      The perspective of $f$ is $\tilde{f}(y, t) = t f(y/t)$, which is convex for $t > 0$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Affine Mapping.</strong>
      Define the affine map $\mathcal{A}(x) = (Ax+b, c^\top x + d)$.
      The function $g(x)$ is the composition of the convex function $\tilde{f}$ with the affine map $\mathcal{A}$:
      $$ g(x) = \tilde{f}(\mathcal{A}(x)) $$
    </div>
    <div class="proof-step">
      <strong>Step 3: Conclusion.</strong>
      Since affine pre-composition preserves convexity, $g(x)$ is convex on the domain where $t(x) > 0$.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.22 — Convexity Drills (3.22)</h3>
  <p>Determine the convexity of the following functions using composition rules.</p>
  <p><strong>(a)</strong> $f(x) = -\log(-\log(\sum e^{a_i^\top x + b_i}))$ on domain where sum $< 1$.
  <br><strong>(b)</strong> $f(x, u, v) = -\sqrt{uv - x^\top x}$ on $uv > x^\top x, u,v > 0$.
  <br><strong>(c)</strong> $f(x, t) = -\log(t^p - \|x\|_p^p)$ for $p > 1, t > \|x\|_p$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>(a) Double Log.</strong>
      Let $s(x) = \log(\sum e^{a_i^\top x + b_i})$. This is convex (Log-Sum-Exp of affine).
      The domain restricts $s(x) < 0$.
      The outer function is $h(u) = -\log(-u)$ for $u < 0$.
      $h'(u) = -1/u > 0$, $h''(u) = 1/u^2 > 0$. So $h$ is convex and increasing.
      Composition: $h(s(x))$ is convex.
    </div>
    <div class="proof-step">
      <strong>(b) Rotated Lorentz.</strong>
      Rewrite as $-\sqrt{u(v - x^\top x/u)}$.
      The term $g(x, u) = x^\top x/u$ is convex (quadratic-over-linear).
      So $h(x, u, v) = v - g(x, u)$ is concave.
      The outer function is $\phi(a, b) = -\sqrt{ab}$, which is convex (negative geometric mean) and decreasing in $a, b$.
      Wait, monotonicity must match. $\phi(u, h)$ is decreasing in $h$. $h$ is concave. This fits the rule: Convex Decreasing $\circ$ Concave $\implies$ Convex.
      Actually, let's view it as perspective. $f(x, u, v) = -\sqrt{u} \sqrt{v - x^\top x/u}$.
      Alternative: The epigraph is $t \le -\sqrt{uv - x^\top x} \iff t^2 \ge uv - x^\top x, t \le 0 \iff x^\top x + t^2 \le uv$.
      This is a rotated second-order cone constraint, which defines a convex set. Thus $f$ is convex.
    </div>
    <div class="proof-step">
      <strong>(c) Log p-Norm Barrier.</strong>
      $t^p - \|x\|_p^p = t^p (1 - (\|x\|_p/t)^p)$.
      $-\log(t^p - \|x\|_p^p) = -p \log t - \log(1 - (\|x\|_p/t)^p)$.
      Let $u = \|x\|_p/t$. This is a convex function (perspective of norm) on $t>0$.
      Let $h(u) = -\log(1-u^p)$. We need to check if $h(u)$ is convex and increasing.
      For $p \ge 1$, $u^p$ is convex on $u \ge 0$. $-\log(1-y)$ is convex increasing.
      Composition holds.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.23 — Perspective Practice (3.23)</h3>
  <p>Show that $f(x, t) = \frac{\|x\|_p^p}{t^{p-1}}$ is convex for $t > 0, p \ge 1$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      Let $g(x) = \|x\|_p^p = \sum |x_i|^p$.
      For $p \ge 1$, $z \mapsto |z|^p$ is convex, so $g(x)$ is convex.
      The function $f(x, t)$ is exactly the perspective of $g$:
      $$ t g(x/t) = t \sum |x_i/t|^p = t \sum \frac{|x_i|^p}{t^p} = \frac{1}{t^{p-1}} \sum |x_i|^p = \frac{\|x\|_p^p}{t^{p-1}} $$
      Since the perspective of a convex function is convex, $f(x, t)$ is convex.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.24 — Shifting Log-Concavity (3.48)</h3>
  <p>If $f$ is log-concave and $a \ge 0$, show that $g(x) = f(x) - a$ is log-concave on its domain $\{x \mid f(x) > a\}$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>1. Setup Decomposition.</strong>
      Let $h(x) = \log f(x)$. Since $f$ is log-concave, $h$ is <b>concave</b>.
      We have $g(x) = e^{h(x)} - a$.
      The log-concavity of $g$ depends on $H(x) = \log g(x) = \log(e^{h(x)} - a)$.
      Define the scalar function $\phi(t) = \log(e^t - a)$ for $t > \log a$.
      Then $H(x) = \phi(h(x))$.
    </div>
    <div class="proof-step">
      <strong>2. Composition Rule Check.</strong>
      We have a composition $H(x) = \phi(h(x))$.
      The inner function $h(x)$ is concave.
      For $H(x)$ to be concave, it suffices that the outer function $\phi(t)$ is <b>concave</b> and <b>non-decreasing</b>.
    </div>
    <div class="proof-step">
      <strong>3. Verify Monotonicity of $\phi$.</strong>
      $$ \phi'(t) = \frac{d}{dt} \log(e^t - a) = \frac{e^t}{e^t - a} $$
      Since domain requires $e^t > a$, the denominator is positive. $e^t$ is positive.
      Thus $\phi'(t) > 0$, so $\phi$ is strictly increasing.
    </div>
    <div class="proof-step">
      <strong>4. Verify Concavity of $\phi$.</strong>
      $$ \phi''(t) = \frac{d}{dt} \left( \frac{e^t}{e^t - a} \right) = \frac{e^t(e^t - a) - e^t(e^t)}{(e^t - a)^2} = \frac{-a e^t}{(e^t - a)^2} $$
      Since $a \ge 0$ and $e^t > 0$, the numerator is $\le 0$. The denominator is positive.
      Thus $\phi''(t) \le 0$, so $\phi$ is concave.
      <br><i>Note:</i> If $a < 0$, $\phi$ would be convex, and the result would fail.
    </div>
    <div class="proof-step">
      <strong>5. Conclusion.</strong>
      Since $h$ is concave and $\phi$ is concave increasing, $H(x) = \log g(x)$ is concave. Thus $g$ is log-concave.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.25 — Cardinality on $\mathbb{R}_+^n$ (3.35)</h3>
  <p>Show that $\operatorname{card}(x)$ (number of nonzeros) is not quasiconvex on $\mathbb{R}_+^n$, but is quasiconcave.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Not Quasiconvex:</strong> Consider $x=(1,0)$ and $y=(0,1)$. Both have cardinality 1.
      Their midpoint $z=(0.5, 0.5)$ has cardinality 2.
      Quasiconvexity requires $\operatorname{card}(z) \le \max(\operatorname{card}(x), \operatorname{card}(y)) = 1$. Since $2 > 1$, it fails.
    </div>
    <div class="proof-step">
      <strong>Quasiconcave:</strong> We need to show superlevel sets $S_k = \{x \ge 0 \mid \operatorname{card}(x) \ge k\}$ are convex.
      Consider $z = \theta x + (1-\theta)y$ with $\theta \in (0,1)$.
      If $x_i > 0$ or $y_i > 0$, then $z_i > 0$ (since $x, y \ge 0$).
      Thus $\operatorname{supp}(z) = \operatorname{supp}(x) \cup \operatorname{supp}(y)$.
      Cardinality is $|\operatorname{supp}(x) \cup \operatorname{supp}(y)|$.
      Since $|A \cup B| \ge \max(|A|, |B|)$, we have $\operatorname{card}(z) \ge \max(\operatorname{card}(x), \operatorname{card}(y))$.
      Thus $\operatorname{card}(z) \ge \min(\operatorname{card}(x), \operatorname{card}(y))$ (actually stronger).
      So it is quasiconcave.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.26 — Rank on PSD Cone (3.36)</h3>
  <p>Show that $f(X) = \operatorname{rank}(X)$ is quasiconcave on $\mathbb{S}_+^n$.
  <br>Specifically, show $\operatorname{rank}(\theta X + (1-\theta)Y) \ge \min(\operatorname{rank}(X), \operatorname{rank}(Y))$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>Step 1: Nullspace Intersection.</strong>
      Let $Z = \theta X + (1-\theta)Y$ with $\theta \in (0,1)$.
      If $v \in \ker(Z)$, then $v^\top Z v = 0$. Since $X, Y \succeq 0$, this implies $v^\top X v = 0$ and $v^\top Y v = 0$.
      For PSD matrices, $v^\top A v = 0 \iff Av = 0$.
      Thus $v \in \ker(X) \cap \ker(Y)$.
      So $\ker(Z) = \ker(X) \cap \ker(Y)$.
    </div>
    <div class="proof-step">
      <strong>Step 2: Rank-Nullity.</strong>
      $\dim \ker(Z) = \dim(\ker(X) \cap \ker(Y)) \le \min(\dim \ker X, \dim \ker Y)$.
      $\operatorname{rank}(Z) = n - \dim \ker(Z) \ge n - \min(\dim \ker X, \dim \ker Y) = \max(n - \dim \ker X, n - \dim \ker Y)$.
      $\operatorname{rank}(Z) \ge \max(\operatorname{rank}(X), \operatorname{rank}(Y))$.
    </div>
    <div class="proof-step">
      <strong>Conclusion:</strong>
      Since the rank of a convex combination is at least the maximum of the ranks (which is $\ge$ the minimum), the function is quasiconcave.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.27 — Homogenization (3.31)</h3>
  <p>Let $f$ be convex with $f(0)=0$. Define $g(x) = \inf_{\alpha > 0} \frac{f(\alpha x)}{\alpha}$.
  Show that $g$ is convex, homogeneous, and the largest homogeneous underestimator of $f$.</p>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step">
      <strong>(a) Homogeneity:</strong>
      $g(tx) = \inf_\alpha \frac{f(\alpha t x)}{\alpha}$. Let $\beta = \alpha t$. Then $\alpha = \beta/t$.
      $g(tx) = \inf_\beta \frac{f(\beta x)}{\beta/t} = t \inf_\beta \frac{f(\beta x)}{\beta} = t g(x)$.
    </div>
    <div class="proof-step">
      <strong>(b) Underestimator:</strong>
      Set $\alpha=1$ in the infimum: $g(x) \le f(x)/1 = f(x)$.
      If $h$ is homogeneous and $h \le f$, then $h(\alpha x) \le f(\alpha x) \implies \alpha h(x) \le f(\alpha x) \implies h(x) \le f(\alpha x)/\alpha$.
      Taking infimum over $\alpha$, $h(x) \le g(x)$.
    </div>
    <div class="proof-step">
      <strong>(c) Convexity (Direct Jensen Proof).</strong>
      We want to show $g(\theta x + (1-\theta)y) \le \theta g(x) + (1-\theta)g(y)$.
      <br>Fix $\epsilon > 0$. Choose $\alpha_1, \alpha_2$ such that $f(\alpha_1 x)/\alpha_1 \le g(x) + \epsilon$ and $f(\alpha_2 y)/\alpha_2 \le g(y) + \epsilon$.
      <br>Let $z = \theta x + (1-\theta)y$. We need to choose a specific scaling factor $\alpha$ for $z$.
      <br>Choose the harmonic mean weighting: $\alpha = \frac{1}{\theta/\alpha_1 + (1-\theta)/\alpha_2}$.
      <br>Define $\lambda = \frac{\alpha \theta}{\alpha_1}$. Then $1-\lambda = \frac{\alpha (1-\theta)}{\alpha_2}$. Note $\lambda \in [0,1]$.
      <br>Key Identity: $\alpha z = \lambda (\alpha_1 x) + (1-\lambda)(\alpha_2 y)$.
      <br>By convexity of $f$:
      $$ f(\alpha z) \le \lambda f(\alpha_1 x) + (1-\lambda) f(\alpha_2 y) $$
      Dividing by $\alpha$:
      $$ \frac{f(\alpha z)}{\alpha} \le \frac{\lambda}{\alpha} f(\alpha_1 x) + \frac{1-\lambda}{\alpha} f(\alpha_2 y) = \theta \frac{f(\alpha_1 x)}{\alpha_1} + (1-\theta) \frac{f(\alpha_2 y)}{\alpha_2} $$
      Using the near-optimal choices:
      $$ \frac{f(\alpha z)}{\alpha} \le \theta (g(x) + \epsilon) + (1-\theta)(g(y) + \epsilon) = \theta g(x) + (1-\theta)g(y) + \epsilon $$
      Since $g(z) \le f(\alpha z)/\alpha$, we have $g(z) \le \theta g(x) + (1-\theta)g(y) + \epsilon$.
      Letting $\epsilon \to 0$ proves convexity.
    </div>
  </div>
</div>

<div class="problem">
  <h3>P6.28 — Strong Convexity of Quadratic</h3>
  <p>Let $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top P \mathbf{x} + \mathbf{q}^\top \mathbf{x} + r$. Under what condition on $P$ is $f$ strongly convex?</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Quadratic Growth:</b> Strong convexity means the function grows at least quadratically away from the minimum.</li>
        <li><b>Eigenvalue Bound:</b> For twice differentiable functions, strong convexity is equivalent to $\nabla^2 f(\mathbf{x}) \succeq mI$, or $\lambda_{\min}(\text{Hessian}) \ge m$.</li>
    </ul>
  </div>

  <div class="intuition-box">
    <p><b>Geometric picture:</b> A function is $m$-strongly convex if, after subtracting the tangent plane at any point, what remains is at least a quadratic bowl of curvature $m$. For quadratics, the Hessian <i>is</i> the curvature everywhere, so the smallest eigenvalue of $P$ sets the weakest direction of curvature.</p>
  </div>

  <div class="interpretation-box">
    <p><b>Why you care:</b> Strong convexity implies a <i>unique</i> minimizer and rules out "flat valleys." In algorithms, larger $m$ typically means better conditioning and faster convergence (connect this to conditioning geometry in Lecture 01).</p>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <p>The Hessian is $\nabla^2 f(\mathbf{x}) = P$ (assuming $P$ is symmetric).
    <br>Strong convexity requires $\nabla^2 f(\mathbf{x}) \succeq mI$ for some $m > 0$.
    <br>Therefore, we need $P \succeq mI$, which means the smallest eigenvalue $\lambda_{\min}(P) \ge m > 0$.
    <br>Thus, $f$ is strongly convex if and only if $P$ is <b>positive definite</b> ($P \succ 0$).</p>
  </div>
</div>

<div class="problem">
  <h3>P6.29 — Dual of a Strictly Convex Function</h3>
  <p>Let $f$ be a closed, strictly convex function. Prove that its conjugate $f^*(\mathbf{y}) = \sup_{\mathbf{x}} (\mathbf{y}^\top \mathbf{x} - f(\mathbf{x}))$ is differentiable. (Hint: The maximizing $\mathbf{x}$ is unique).</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Duality of Regularity:</b> There is a deep symmetry: Strict Convexity $\leftrightarrow$ Differentiability.</li><li><b>Envelope Theorem:</b> The gradient of the value function $\nabla f^*(\mathbf{y})$ is the optimal solution $\mathbf{x}^*(\mathbf{y})$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Uniqueness of Maximizer.</strong>
    Consider the maximization problem defining the conjugate: maximize $h_{\mathbf{y}}(\mathbf{x}) = \mathbf{y}^\top \mathbf{x} - f(\mathbf{x})$.
    Since $f$ is strictly convex, $h_{\mathbf{y}}(\mathbf{x})$ is strictly concave.
    A strictly concave function has at most one global maximizer. Let's assume it exists and denote it $\mathbf{x}^*(\mathbf{y})$.</div>
    <div class="proof-step"><strong>Step 2: Danskin's Theorem (simplified).</strong>
    For a function $g(\mathbf{y}) = \max_{\mathbf{x}} \phi(\mathbf{x}, \mathbf{y})$, if the maximizer $\mathbf{x}^*$ is unique, the gradient is $\nabla g(\mathbf{y}) = \nabla_{\mathbf{y}} \phi(\mathbf{x}^*, \mathbf{y})$.
    Here $\phi(\mathbf{x}, \mathbf{y}) = \mathbf{y}^\top \mathbf{x} - f(\mathbf{x})$.
    $\nabla_{\mathbf{y}} \phi = \mathbf{x}$.
    </div>
    <div class="proof-step"><strong>Conclusion:</strong>
    Since $\mathbf{x}^*(\mathbf{y})$ is unique (due to strict convexity of $f$), the gradient exists and is unique:
    $$ \nabla f^*(\mathbf{y}) = \mathbf{x}^*(\mathbf{y}) $$
    Thus $f^*$ is differentiable. This is a fundamental result: smoothness of the dual comes from curvature of the primal.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.30 — Dual via Conjugates</h3>
  <p>Using the definition of the convex conjugate, show that the dual of the problem $\min f(\mathbf{x})$ subject to $A\mathbf{x} = \mathbf{b}$ is $\max -\mathbf{b}^\top \boldsymbol{\nu} - f^*(-A^\top \boldsymbol{\nu})$.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Lagrangian Duality:</b> The dual function $g(\boldsymbol{\nu}) = \inf_{\mathbf{x}} L(\mathbf{x}, \boldsymbol{\nu})$.</li><li><b>Conjugate Link:</b> $\inf_{\mathbf{x}} (\dots)$ can often be rewritten as $-\sup_{\mathbf{x}} (\dots) = -f^*(\cdot)$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Lagrangian.</strong>
    The Lagrangian is $L(\mathbf{x}, \boldsymbol{\nu}) = f(\mathbf{x}) + \boldsymbol{\nu}^\top (A\mathbf{x} - \mathbf{b}) = f(\mathbf{x}) + \mathbf{x}^\top A^\top \boldsymbol{\nu} - \mathbf{b}^\top \boldsymbol{\nu}$.</div>
    <div class="proof-step"><strong>Step 2: Dual Function.</strong>
    $g(\boldsymbol{\nu}) = \inf_{\mathbf{x}} L(\mathbf{x}, \boldsymbol{\nu}) = -\mathbf{b}^\top \boldsymbol{\nu} + \inf_{\mathbf{x}} (f(\mathbf{x}) + (A^\top \boldsymbol{\nu})^\top \mathbf{x})$.
    We know $\inf_{\mathbf{x}} h(\mathbf{x}) = -\sup_{\mathbf{x}} (-h(\mathbf{x}))$.
    So $\inf_{\mathbf{x}} (f(\mathbf{x}) + \mathbf{y}^\top \mathbf{x}) = -\sup_{\mathbf{x}} (-\mathbf{y}^\top \mathbf{x} - f(\mathbf{x})) = -\sup_{\mathbf{x}} ((-\mathbf{y})^\top \mathbf{x} - f(\mathbf{x}))$.
    Let $\mathbf{y} = A^\top \boldsymbol{\nu}$.
    The term is $-\sup_{\mathbf{x}} ((-A^\top \boldsymbol{\nu})^\top \mathbf{x} - f(\mathbf{x}))$.
    By definition of conjugate, $\sup_{\mathbf{x}} (\mathbf{z}^\top \mathbf{x} - f(\mathbf{x})) = f^*(\mathbf{z})$.
    Here $\mathbf{z} = -A^\top \boldsymbol{\nu}$.
    So $\inf_{\mathbf{x}} (\dots) = -f^*(-A^\top \boldsymbol{\nu})$.</div>
    <div class="proof-step"><strong>Step 3: Conclusion.</strong>
    $g(\boldsymbol{\nu}) = -\mathbf{b}^\top \boldsymbol{\nu} - f^*(-A^\top \boldsymbol{\nu})$.
    The dual problem is to maximize this quantity.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.31 — Derivation: Dual of QP</h3>
  <p>Consider the equality constrained Quadratic Program: $\min \frac{1}{2}\mathbf{x}^\top P \mathbf{x} + \mathbf{q}^\top \mathbf{x}$ s.t. $A\mathbf{x} = \mathbf{b}$, with $P \succ 0$. Derive the dual problem using the conjugate method from P6.30.</p>

  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Conjugate of Quadratic:</b> If $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top P \mathbf{x} + \mathbf{q}^\top \mathbf{x}$, then $f^*(\mathbf{y}) = \frac{1}{2}(\mathbf{y}-\mathbf{q})^\top P^{-1} (\mathbf{y}-\mathbf{q})$.</li>
    </ul>
  </div>

  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Conjugate of the Objective.</strong>
    Let $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top P \mathbf{x} + \mathbf{q}^\top \mathbf{x}$.
    $f^*(\mathbf{y}) = \sup_{\mathbf{x}} (\mathbf{y}^\top \mathbf{x} - \frac{1}{2}\mathbf{x}^\top P \mathbf{x} - \mathbf{q}^\top \mathbf{x}) = \sup_{\mathbf{x}} ((\mathbf{y}-\mathbf{q})^\top \mathbf{x} - \frac{1}{2}\mathbf{x}^\top P \mathbf{x})$.
    Optimal $\mathbf{x}$: $\nabla = (\mathbf{y}-\mathbf{q}) - P\mathbf{x} = 0 \implies \mathbf{x} = P^{-1}(\mathbf{y}-\mathbf{q})$.
    Substitute back: $(\mathbf{y}-\mathbf{q})^\top P^{-1}(\mathbf{y}-\mathbf{q}) - \frac{1}{2}(\mathbf{y}-\mathbf{q})^\top P^{-1} P P^{-1} (\mathbf{y}-\mathbf{q}) = \frac{1}{2}(\mathbf{y}-\mathbf{q})^\top P^{-1}(\mathbf{y}-\mathbf{q})$.</div>
    <div class="proof-step"><strong>Step 2: Apply P6.30 Formula.</strong>
    Dual is $\max -\mathbf{b}^\top \boldsymbol{\nu} - f^*(-A^\top \boldsymbol{\nu})$.
    Substitute $\mathbf{y} = -A^\top \boldsymbol{\nu}$:
    $$ f^*(-A^\top \boldsymbol{\nu}) = \frac{1}{2}(-A^\top \boldsymbol{\nu} - \mathbf{q})^\top P^{-1} (-A^\top \boldsymbol{\nu} - \mathbf{q}) $$
    $$ = \frac{1}{2}(A^\top \boldsymbol{\nu} + \mathbf{q})^\top P^{-1} (A^\top \boldsymbol{\nu} + \mathbf{q}) $$
    (Minus signs cancel in quadratic form).</div>
    <div class="proof-step"><strong>Step 3: Final Dual Problem.</strong>
    $$ \max_{\boldsymbol{\nu}} \ -\mathbf{b}^\top \boldsymbol{\nu} - \frac{1}{2}(A^\top \boldsymbol{\nu} + \mathbf{q})^\top P^{-1} (A^\top \boldsymbol{\nu} + \mathbf{q}) $$
    This is an unconstrained concave quadratic maximization in $\boldsymbol{\nu}$.</div>
  </div>
</div>

<div class="problem">
  <h3>P6.32 — Dual of Single Inequality (Perspective of Conjugate)</h3>
  <p>Consider the problem $\min c^\top x$ subject to $f(x) \le 0$, where $f$ is convex. Show that the dual function is the perspective of the conjugate of $f$:
  $$ g(\lambda) = -\lambda f^*(-c/\lambda), \quad \lambda > 0 $$</p>
  <div class="recap-box">
    <h4 style="margin-top: 0; font-size: 0.9em; color: var(--primary-300); text-transform: uppercase; letter-spacing: 0.05em;">Recap & Key Concepts</h4>
    <ul style="margin: 8px 0 0 20px; font-size: 0.9em; color: var(--text-secondary);">
        <li><b>Homogeneity:</b> The dual function of a single constraint problem often involves a perspective transformation (homogenization).</li>
    </ul>
  </div>
  <div class="solution-box">
    <h4>Solution</h4>
    <div class="proof-step"><strong>Step 1: Lagrangian.</strong>
    $L(x, \lambda) = c^\top x + \lambda f(x)$.</div>
    <div class="proof-step"><strong>Step 2: Dual Function.</strong>
    $g(\lambda) = \inf_x (c^\top x + \lambda f(x))$.
    For $\lambda > 0$, we can factor it out:
    $$ g(\lambda) = \lambda \inf_x \left( \frac{c^\top}{\lambda} x + f(x) \right) $$
    $$ = \lambda \left( - \sup_x \left( \left(-\frac{c}{\lambda}\right)^\top x - f(x) \right) \right) $$</div>
    <div class="proof-step"><strong>Step 3: Conjugate Definition.</strong>
    The term in the supremum is exactly the definition of $f^*(-c/\lambda)$.
    Thus $g(\lambda) = -\lambda f^*(-c/\lambda)$.
    This is the perspective of $-f^*$, evaluated at $(-c, \lambda)$.</div>
  </div>
</div>

</section>

    <section class="section-card" id="section-8">
      <h2>8. Readings &amp; Resources</h2>
      <div class="citation-box">
        <h3>Primary Textbook</h3>
        <ul>
          <li><b>Boyd & Vandenberghe</b>, <i>Convex Optimization</i>: Chapter 3.
            <ul>
              <li>§3.3: The convex conjugate</li>
              <li>§3.4: Quasiconvex functions</li>
              <li>§3.5: Log-concave and log-convex functions</li>
            </ul>
          </li>
        </ul>
      </div>
      <div class="citation-box">
        <h3>Additional Resources</h3>
        <ul>
          <li><b>Rockafellar</b>, <i>Convex Analysis</i>: Section 12 (Conjugates), Section 26 (Closed convex functions).</li>
          <li><b>Bertsekas</b>, <i>Convex Optimization Theory</i>: Chapter 1 (Basic Convexity).</li>
        </ul>
      </div>
      <div class="interpretation-box">
        <p style="margin: 0;"><b>Forward look:</b> In <a href="../07-convex-problems-standard/index.html">Lecture 07</a> you will translate modeling statements into “standard form.” In <a href="../08-convex-problems-conic/index.html">Lecture 08</a> you will see how many nonlinear-looking constraints become SOC/SDP constraints. In <a href="../09-duality/index.html">Lecture 09</a>, conjugates and separation become dual problems and optimality certificates.</p>
      </div>
    </section>
    </article>

    <footer class="site-footer">
      <div class="container">
        <p>© <span id="year"></span> Convex Optimization Course</p>
      </div>
    </footer>
  </main></div>

  <script src="../../static/js/math-renderer.js"></script>
  <script src="../../static/js/ui.js"></script>
  <script src="../../static/js/toc.js"></script>
  <script>
    feather.replace();
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>

  <!-- Widget Loaders -->
  <script type="module">
    import { initConvexFunctionInspector } from './widgets/js/convex-function-inspector.js';
    initConvexFunctionInspector('widget-convex-function-inspector');
  </script>
  <script type="module">
    import { initHessianHeatmap } from './widgets/js/hessian-heatmap.js';
    initHessianHeatmap('widget-hessian-heatmap');
  </script>
  <script type="module">
    import { initOperationsPreserving } from './widgets/js/operations-preserving.js';
    initOperationsPreserving('widget-operations-preserving');
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
