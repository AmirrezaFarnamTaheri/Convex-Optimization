<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>13. Algorithms I: Unconstrained Minimization — Convex Optimization</title>
  <link rel="stylesheet" href="../../static/css/convex-unified.css" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
</head>
<body>
  <!-- Header with navigation -->
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <a href="../../index.html">Convex Optimization</a>
      </div>
      <nav class="nav">
        <a href="../../index.html">All Lectures</a>
        <a href="#widgets">Interactive</a>
        <a href="#readings">Readings</a>
      </nav>
    </div>
  </header>

  <!-- Main content -->
  <main class="container" style="padding: 32px 0 60px;">
    <!-- Lecture header -->
    <article class="section-card" style="margin-bottom: 32px;">
      <h1 style="margin-top: 0;">13. Algorithms I: Unconstrained Minimization</h1>
      <div class="meta">
        Date: 2025-12-16 · Duration: 90 min · Tags: algorithms, unconstrained
      </div>

      <!-- Brief introduction -->
      <section style="margin-top: 16px;">
        <p><strong>Overview:</strong> A rigorous "zero-to-hero" analysis of descent methods for unconstrained convex optimization. Covers gradient descent, steepest descent, and Newton's method with full convergence proofs.</p>
        <p><strong>Prerequisites:</strong> <a href="../05-convex-functions-basics/index.html">Lecture 05</a></p>
      </section>
    </article>

    <!-- Learning objectives -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Learning Objectives</h2>
      <p>After this lecture, you should understand:</p>
      <ul style="line-height: 1.8;">
        <li>The rigorous derivations of convergence rates for gradient descent under strong convexity and smoothness.</li>
        <li>How condition number $\kappa$ geometrically dictates convergence speed.</li>
        <li>The duality between choosing a norm for steepest descent and choosing a preconditioner.</li>
        <li>Newton's method as steepest descent in the Hessian geometry, and its two-phase convergence behavior.</li>
      </ul>
    </section>

    <!-- Main lecture content -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Key Concepts</h2>

      <h3>0. Notation and the "Model"</h3>
      <p>
        We study the problem of minimizing a function
        $$ f: \mathrm{dom}\,f \subseteq \mathbb{R}^n \to \mathbb{R} $$
        under the following assumptions:
      </p>
      <ul>
        <li>$f$ is <strong>convex</strong> on $\mathrm{dom}\,f$.</li>
        <li>$f$ is <strong>twice continuously differentiable</strong> ($f \in C^2(\mathrm{dom}\,f)$).</li>
        <li>$\mathrm{dom}\,f$ is <strong>open</strong>.</li>
        <li>The problem is <strong>solvable</strong>: the optimal value $p^\star = \min_x f(x)$ is finite and attained at some $x^\star$.</li>
      </ul>
      <p>
        <strong>Important Subtlety:</strong> If $f: \mathbb{R}^n \to \mathbb{R}$, then $\mathrm{dom}\,f = \mathbb{R}^n$ is automatically open. In the broader convex-analysis setting, we often allow <strong>extended-valued</strong> convex functions $f:\mathbb{R}^n\to\mathbb{R}\cup\{+\infty\}$, where $\mathrm{dom}\,f=\{x:f(x)<+\infty\}$ can be a strict open subset (e.g., the interior of a polytope for logarithmic barriers). Twice differentiability forces us to work on an open set to define derivatives.
      </p>

      <h3>1. The Core Equivalence (Section 9.1)</h3>
      <p>
        The unconstrained problem is:
        $$ \min_x f(x) \tag{9.1} $$
        "Unconstrained" means no explicit constraints are written, but constraints are implicit in the domain $\mathrm{dom}\,f$. Stepping outside the domain is not allowed.
      </p>
      <h4>1.1 Why Convexity + Differentiability Makes Optimality Global</h4>
      <p>
        The necessary and sufficient condition for optimality is:
        $$ \nabla f(x^\star)=0 \tag{9.2} $$
        In nonconvex calculus, $\nabla f=0$ could be a saddle or maximum. Convexity upgrades "stationary point" to "global minimizer."
      </p>
      <p>
        This relies on the <strong>first-order characterization of convexity</strong>:
        $$ f(y) \ge f(x) + \nabla f(x)^T (y-x) \qquad (\star) $$
        Geometrically, the tangent hyperplane lies below the graph everywhere.
      </p>
      <p><strong>Proof of $(\star)$:</strong></p>
      <ul>
          <li>By convexity: $f(x+t(y-x)) \le (1-t)f(x)+t f(y)$ for $t \in [0,1]$.</li>
          <li>Rearrange: $\frac{f(x+t(y-x))-f(x)}{t} \le f(y)-f(x)$.</li>
          <li>Take $t \downarrow 0$: The LHS becomes the directional derivative $\nabla f(x)^T(y-x)$, yielding the result.</li>
      </ul>
      <p><strong>Proof of Equivalence:</strong></p>
      <ul>
          <li><strong>Sufficiency ($\nabla f(x^\star)=0 \Rightarrow$ Min):</strong> Plug $x^\star$ into $(\star)$. $f(y) \ge f(x^\star) + 0^T(y-x^\star) = f(x^\star)$ for all $y$.</li>
          <li><strong>Necessity (Min $\Rightarrow \nabla f(x^\star)=0$):</strong> Since $\mathrm{dom}\,f$ is open, we can move in any direction $d$. Let $\phi(t)=f(x^\star+td)$. Since $x^\star$ is a minimizer, $t=0$ minimizes $\phi$, so $\phi'(0)=\nabla f(x^\star)^T d = 0$. Since this holds for all $d$, $\nabla f(x^\star)=0$.</li>
      </ul>

      <h3>2. Why Introduce the Initial Sublevel Set $S$?</h3>
      <p>
        We define the initial sublevel set:
        $$ S=\{x\in\mathrm{dom}\,f \mid f(x)\le f(x^{(0)})\} \tag{9.3} $$
        This "localizes" all global claims to the region the algorithm actually visits.
      </p>
      <ul>
          <li><strong>Algorithms stay inside $S$:</strong> Descent methods enforce $f(x^{(k+1)})\le f(x^{(k)})$. By induction, all iterates $x^{(k)}$ remain in $S$.</li>
          <li><strong>Closedness:</strong> We assume $S$ is closed. This allows us to bound continuous functions (like eigenvalues of the Hessian) over $S$. In convex analysis, if $f$ is a "closed function" (epigraph is closed), all sublevel sets are closed.</li>
      </ul>

      <h3>3. Examples: What Each Teaches Us</h3>
      <h4>3.1 Quadratic Minimization (The Archetype)</h4>
      <p>
        $$ \min_x \ \frac12 x^T P x + q^T x + r,\qquad P\succeq 0 \tag{9.4} $$
        Gradient: $\nabla f(x)=Px+q$. Optimality: $Px^\star = -q$.
      </p>
      <ul>
          <li>If $P \succ 0$: Unique solution $x^\star = -P^{-1}q$.</li>
          <li>If $P$ singular: Solvable if $-q \in \mathrm{range}(P)$. Unbounded below if $-q \notin \mathrm{range}(P)$.</li>
      </ul>
      <h4>3.2 Least Squares</h4>
      <p>
        $\min_x \|Ax-b\|_2^2$. This is a quadratic with $P=2A^TA \succeq 0$. Optimality yields normal equations $A^TAx^\star = A^Tb$.
      </p>
      <h4>3.3 Log-Sum-Exp (Softmax)</h4>
      <p>
        $$ f(x)=\log\Big(\sum_{i=1}^m e^{a_i^T x+b_i}\Big) $$
        Gradient is the expected value of $a_i$ under the softmax weights $w_i(x)$.
        Hessian is the weighted covariance matrix of $a_i$, ensuring convexity ($\nabla^2 f \succeq 0$).
      </p>
      <h4>3.4 Analytic Centers (Barriers)</h4>
      <p>
        <strong>Linear Inequalities (9.5):</strong> $f(x)=-\sum \log(b_i-a_i^T x)$. The barrier blows up at the boundary, forcing minimizers to the interior.
        <br>
        <strong>LMIs (9.6):</strong> $f(x) = -\log\det(F(x))$. The matrix analog.
      </p>

      <h3>4. Strong Convexity on $S$: The Engine of Convergence</h3>
      <p>
        We assume there exists $m>0$ such that for all $x \in S$:
        $$ \nabla^2 f(x) \succeq mI \tag{9.7} $$
        This means $f$ has at least "curvature $m$" in every direction.
      </p>
      <h4>4.1 Quadratic Lower Bound</h4>
      <p>
        Integrating $\nabla^2 f(x+td) \succeq mI$ twice along the line segment from $x$ to $y$ yields:
        $$ f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{m}{2}\|y-x\|_2^2 \tag{9.8} $$
        Interpretation: $f$ lies above a quadratic bowl tangent at $x$.
      </p>
      <h4>4.2 Suboptimality via Gradient Norm</h4>
      <p>
        Minimizing the RHS of (9.8) with respect to $y$ gives the minimum value $f(x) - \frac{1}{2m}\|\nabla f(x)\|^2$. Since $p^\star \le \min_y \text{RHS}$, we get:
        $$ p^\star \ge f(x) - \frac{1}{2m}\|\nabla f(x)\|^2 \implies f(x) - p^\star \le \frac{1}{2m}\|\nabla f(x)\|^2 \tag{9.9} $$
        This links the gradient magnitude (measurable) to the unknown suboptimality gap.
      </p>

      <h3>5. Smoothness: The Guarantee of Descent</h3>
      <p>
        Since $S$ is bounded (implied by strong convexity) and closed, and $f \in C^2$, the maximum eigenvalue of the Hessian is bounded by some $M$:
        $$ \nabla^2 f(x) \preceq MI \quad \forall x \in S \tag{9.12} $$
        Integrating this twice gives the <strong>Quadratic Upper Bound</strong>:
        $$ f(y) \le f(x) + \nabla f(x)^T(y-x) + \frac{M}{2}\|y-x\|_2^2 \tag{9.13} $$
        This ensures that stepping in the descent direction yields a predictable decrease.
      </p>

      <h3>6. Condition Number of Sublevel Sets</h3>
      <p>
        Combining bounds: $mI \preceq \nabla^2 f(x) \preceq MI$ on $S$.
        The ratio $\kappa = M/m$ is the <strong>condition number</strong>.
      </p>
      <h4>6.1 Geometric Interpretation</h4>
      <p>
        The condition number of a set $C$ is $\text{width}_{\max}^2 / \text{width}_{\min}^2$.
        For an ellipsoid $\mathcal{E}=\{x : x^T A^{-1} x \le 1\}$, the condition number is $\lambda_{\max}(A)/\lambda_{\min}(A)$.
        The sublevel set $C_\alpha = \{x : f(x) \le \alpha\}$ is sandwiched between two balls of radius ratio $\sqrt{M/m}$. Thus, $\kappa$ bounds the eccentricity (elongation) of the level sets.
      </p>

      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/kappa_effect_paths.gif" alt="Gradient descent zig-zag worsening with condition number" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;"><strong>The Condition Number Tax:</strong> As $\kappa = M/m$ grows (ellipses become thinner), gradient descent zig-zags significantly, slowing convergence linearly with $\kappa$.</figcaption>
      </figure>

      <h3>7. Gradient Descent Analysis</h3>
      <p>
        Iteration: $x^{(k+1)} = x^{(k)} - t^{(k)}\nabla f(x^{(k)})$.
      </p>
      <h4>7.1 Exact Line Search: Linear Convergence</h4>
      <p>
        We minimize the function along the ray $-g$.
        Using the upper bound (9.13) with $y = x - tg$:
        $$ f(x-tg) \le f(x) - t\|g\|^2 + \frac{M}{2}t^2\|g\|^2 $$
        Minimizing the RHS gives $t^* = 1/M$, yielding a decrease of $\frac{1}{2M}\|g\|^2$.
        $$ f(x^+) \le f(x) - \frac{1}{2M}\|g\|^2 $$
        Using the gradient-gap link $\|g\|^2 \ge 2m(f(x)-p^\star)$, we subtract $p^\star$ from both sides:
        $$ f(x^+) - p^\star \le f(x) - p^\star - \frac{m}{M}(f(x)-p^\star) = \left(1 - \frac{m}{M}\right)(f(x) - p^\star) $$
        This is <strong>linear convergence</strong> with rate $c = 1 - 1/\kappa$. The number of iterations scales with $\kappa$.
      </p>

      <h4>7.2 Backtracking (Armijo) Line Search</h4>
      <p>
        Instead of optimizing $t$, we accept $t$ if:
        $$ f(x-tg) \le f(x) - \alpha t \|g\|^2, \quad \alpha \in (0, 0.5) $$
        <strong>Termination:</strong> For sufficiently small $t$ ($t \le \frac{2(1-\alpha)}{M}$), the condition holds because the linear term dominates.
        <strong>Convergence:</strong> Since we backtrack by factor $\beta$, the step $t$ is bounded below. This preserves linear convergence, just with a slightly worse constant.
      </p>
      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/backtracking_armijo.gif" alt="Backtracking line search visualization" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Backtracking reduces step size $t$ until the function value drops below the linear extrapolation (Armijo line).</figcaption>
      </figure>

      <h3>8. Steepest Descent: Choosing a Geometry</h3>
      <p>
        "Steepest" depends on how we measure length.
      </p>
      <h4>8.1 Definition and Dual Norm</h4>
      <p>
        The <strong>normalized steepest descent direction</strong> minimizes the directional derivative subject to a norm constraint:
        $$ \Delta x_{\mathrm{nsd}} = \text{argmin}_{\|v\| \le 1} \nabla f(x)^T v $$
        Let $\|\cdot\|_*$ be the <strong>dual norm</strong>, defined as $\|g\|_* = \sup_{\|v\|\le 1} g^T v$.
        A key identity is $\inf_{\|v\|\le 1} g^T v = -\|g\|_*$.
        Thus, the steepest direction achieves a slope of $-\|\nabla f(x)\|_*$.
      </p>
      <figure style="margin: 24px auto; text-align: center; max-width: 600px;">
        <img src="assets/steepest_descent_norms.gif" alt="Steepest descent direction for L1 vs L2 norms" style="width: 100%; border-radius: 8px; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Steepest direction depends on the norm ball shape. For $\ell_2$, it is antiparallel to gradient. For $\ell_1$ (diamond ball), it points to a vertex (coordinate axis).</figcaption>
      </figure>

      <h4>8.2 Unnormalized Step and Cases</h4>
      <p>
        We define the unnormalized step $\Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_* \Delta x_{\mathrm{nsd}}$.
        This ensures $\nabla f(x)^T \Delta x_{\mathrm{sd}} = -\|\nabla f(x)\|_*^2$.
      </p>
      <ul>
          <li><strong>Euclidean Norm ($\ell_2$):</strong> Dual is $\ell_2$. $\Delta x_{\mathrm{sd}} = -\nabla f(x)$. This recovers standard Gradient Descent.</li>
          <li><strong>Quadratic Norm ($\|v\|_P = \sqrt{v^T P v}$):</strong> Dual is $\|g\|_{P^{-1}}$.
          $$ \Delta x_{\mathrm{sd}} = -P^{-1}\nabla f(x) $$
          This is <strong>Preconditioned Gradient Descent</strong>. It is equivalent to applying GD in a transformed coordinate system $\bar{x} = P^{1/2}x$. Ideally, $P \approx \nabla^2 f(x)$.
          <figure style="margin: 12px 0; text-align: center;">
            <img src="assets/preconditioning_comparison.gif" alt="Preconditioned GD vs GD vs Newton" style="width: 100%; max-width: 500px; border-radius: 8px; border: 1px solid #ddd;">
          </figure>
          </li>
          <li><strong>$\ell_1$ Norm:</strong> Dual is $\ell_\infty$. Steepest direction is along the coordinate axis with the largest partial derivative. This yields <strong>Coordinate Descent</strong>.
          <figure style="margin: 12px 0; text-align: center;">
            <img src="assets/l1_coordinate_descent_vs_gd.gif" alt="L1 Coordinate Descent vs GD" style="width: 100%; max-width: 500px; border-radius: 8px; border: 1px solid #ddd;">
          </figure>
          </li>
      </ul>

      <h3>9. Newton's Method (Section 9.5)</h3>
      <p>
        The Newton step is:
        $$ \Delta x_{\mathrm{nt}} = -\nabla^2 f(x)^{-1} \nabla f(x) $$
      </p>
      <h4>9.1 Three Interpretations</h4>
      <ol>
          <li><strong>Steepest Descent in Hessian Norm:</strong> It is steepest descent using the local norm $\|v\|_H = \sqrt{v^T \nabla^2 f(x) v}$. The geometry (preconditioner) is updated at every step to match local curvature.</li>
          <li><strong>Quadratic Model Minimization:</strong> It minimizes the second-order Taylor approximation:
          $$ \hat{f}(x+v) = f(x) + \nabla f(x)^T v + \frac{1}{2}v^T \nabla^2 f(x) v $$
          <figure style="margin: 12px 0; text-align: center;">
            <img src="assets/newton_quadratic_model.gif" alt="Newton step minimizes local quadratic model" style="width: 100%; max-width: 500px; border-radius: 8px; border: 1px solid #ddd;">
          </figure>
          </li>
          <li><strong>Affine Invariance:</strong> Newton's method is independent of linear coordinate changes, unlike GD.</li>
      </ol>

      <h4>9.2 Newton Decrement</h4>
      <p>
        We define the decrement $\lambda(x) = \sqrt{\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x)}$.
        It relates to the function decrease: $f(x) - \inf_v \hat{f}(x+v) = \frac{1}{2}\lambda(x)^2$.
        It serves as a rigorous, affine-invariant stopping criterion.
      </p>

      <h4>9.3 Convergence Analysis: The "Two-Phase" Story</h4>
      <ul>
          <li><strong>Phase 1: Damped Phase (Far from optimum):</strong>
          When $\lambda(x)$ is large, we use backtracking. The method guarantees a constant function decrease $\gamma$ at every step. This phase lasts finitely many steps ($\approx (f(x_0)-p^\star)/\gamma$).
          <figure style="margin: 12px 0; text-align: center;">
            <img src="assets/damped_newton_backtracking_2d.gif" alt="Damped Newton with backtracking" style="width: 100%; max-width: 500px; border-radius: 8px; border: 1px solid #ddd;">
          </figure>
          </li>
          <li><strong>Phase 2: Quadratic Phase (Close to optimum):</strong>
          Once $\lambda(x)$ is sufficiently small (and assuming Lipschitz Hessian), backtracking accepts $t=1$. The error squares at each step:
          $$ \|\nabla f(x^{(k+1)})\| \le C \|\nabla f(x^{(k)})\|^2 $$
          This is <strong>quadratic convergence</strong>. The number of correct digits doubles at every iteration. It takes extremely few steps ($\approx 6$) to reach machine precision once in this phase.
          </li>
      </ul>

      <h3>10. Self-Concordance</h3>
      <p>
        Standard convergence analysis depends on unknown constants $m, M, L$. <strong>Self-concordance</strong> is a condition ($|f'''(x)| \le 2 f''(x)^{3/2}$) that allows analysis independent of these constants.
        For self-concordant functions (like log barriers), Newton's method complexity depends only on parameters we know.
      </p>

      <h3>11. Quasi-Newton Methods</h3>
      <p>
        To avoid the $O(n^3)$ cost of computing the Newton step, Quasi-Newton methods (e.g., BFGS) build an approximation of the Hessian (or its inverse) using observed gradient changes. They achieve <strong>superlinear convergence</strong> (faster than linear, slower than quadratic) with $O(n^2)$ work per step.
      </p>
    </section>

    <!-- Interactive widgets -->
    <section class="section-card" id="widgets" style="margin-bottom: 32px;">
      <h2>Interactive Widgets</h2>
      <p>Below are tools to explore the concepts interactively.</p>

      <!-- Widget 1 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Gradient Descent Visualizer</h3>
        <p>Visualize the trajectory of gradient descent for a 2D quadratic function.</p>
        <div id="widget-1" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 4: Norm Steepest Descent -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Steepest Descent Geometry</h3>
        <p>Visualizing how the norm determines the "steepest" direction. L2 steepest opposes the gradient. L1 steepest snaps to an axis (coordinate descent).</p>
        <div id="widget-norm-steepest" style="width: 100%; height: auto; position: relative; text-align: center;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 2 -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">GD vs. Newton Race</h3>
        <p>Compare the convergence of gradient descent and Newton's method.</p>
        <div id="widget-2" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>

      <!-- Widget 3 (if exists) -->
      <div style="margin: 24px 0; padding: 16px; background: var(--panel); border: 1px solid var(--border); border-radius: 10px;">
        <h3 style="margin-top: 0;">Convergence Rate Comparison</h3>
        <p>Plots the convergence rates of different first-order methods.</p>
        <div id="widget-3" style="width: 100%; height: 400px; position: relative;">
          <!-- Widget will be rendered here -->
        </div>
      </div>
    </section>

    <!-- Readings -->
    <section class="section-card" id="readings" style="margin-bottom: 32px;">
      <h2>Readings & Resources</h2>
      <ul class="link-list">
        <li><strong>Boyd & Vandenberghe, Convex Optimization:</strong> Chapter 9 — Unconstrained Minimization</li>
        <li><strong>Course slides:</strong> <a href="https://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">PDF</a></li>
      </ul>
    </section>

    <!-- Example problems -->
    <section class="section-card" style="margin-bottom: 32px;">
      <h2>Example Problems</h2>

      <div class="example-problem">
        <h3>Example 13.1: Gradient Descent on Quadratic</h3>
        <p><strong>Problem:</strong> Apply GD to $f(x) = \frac{1}{2}(x_1^2 + \gamma x_2^2)$ with $\gamma = 10$. Start at $(10, 1)$. Estimate steps to reach $\|x\| \le 10^{-3}$.</p>
        <p><strong>Solution:</strong></p>
        <p>Condition number $\kappa = 10$. Optimal step rate $t = 2/(L+\mu) = 2/(10+1) \approx 0.18$.
        Convergence factor $\rho = (\kappa-1)/(\kappa+1) = 9/11 \approx 0.818$.
        Error decays as $(0.818)^k$.
        Need $(0.818)^k \cdot \sqrt{101} \le 10^{-3}$.
        $k \log(0.818) \le \log(10^{-4})$. $k \approx 45$.
        With exact line search, zigzagging occurs.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.2: Newton's Method for Logistic Regression</h3>
        <p><strong>Problem:</strong> Formulate Newton step for logistic regression.</p>
        <p><strong>Solution:</strong></p>
        <p>$f(w) = \sum \log(1 + e^{z_i}) - y_i z_i$ where $z_i = w^\top x_i$.
        Gradient $\nabla f = X^\top (p - y)$ where $p_i = \sigma(z_i)$.
        Hessian $H = X^\top D X$ where $D_{ii} = p_i(1-p_i)$.
        Newton step $\Delta w = -H^{-1} g = -(X^\top D X)^{-1} X^\top (p-y)$.
        This is iteratively reweighted least squares (IRLS).</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.3: Backtracking Line Search</h3>
        <p><strong>Problem:</strong> Describe the backtracking condition for $f(x) = x^2$ with $\Delta x = -1$ at $x=2$.</p>
        <p><strong>Solution:</strong></p>
        <p>Condition: $f(x+t\Delta x) \le f(x) + \alpha t f'(x)\Delta x$.
        $f(2)=4, f'(2)=4, \Delta x=-1$.
        $(2-t)^2 \le 4 + \alpha t (4)(-1) = 4 - 4\alpha t$.
        $4 - 4t + t^2 \le 4 - 4\alpha t$.
        $t^2 \le 4t(1-\alpha) \implies t \le 4(1-\alpha)$.
        For small $t$, this always holds. We start with $t=1$. If $1 > 4(1-\alpha)$, we reduce $t$.
        E.g., if $\alpha=0.25$, limit is $3$. $t=1$ is accepted immediately.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.4: Conjugate Gradient Direction</h3>
        <p><strong>Problem:</strong> For $f(x) = \frac{1}{2}x^\top A x - b^\top x$, how is the search direction updated?</p>
        <p><strong>Solution:</strong></p>
        <p>$d_{k+1} = -g_{k+1} + \beta_k d_k$.
        For quadratic CG, $\beta_k = \frac{g_{k+1}^\top g_{k+1}}{g_k^\top g_k}$ (Fletcher-Reeves).
        Ensures $d_{k+1}^\top A d_k = 0$ ($A$-conjugacy).
        This allows exact minimization in $n$ steps.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.5: BFGS Update Rank</h3>
        <p><strong>Problem:</strong> Show that the BFGS update is a rank-2 update.</p>
        <p><strong>Solution:</strong></p>
        <p>Update: $B_{k+1} = B_k + \frac{y y^\top}{y^\top s} - \frac{B s s^\top B}{s^\top B s}$.
        This adds two rank-1 matrices.
        It preserves positive definiteness if $y^\top s > 0$ (curvature condition).
        Avoids $O(n^3)$ cost of inverting Hessian.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.6: Convergence Rate Comparison</h3>
        <p><strong>Problem:</strong> Compare GD and Newton on $f(x) = x^4$.</p>
        <p><strong>Solution:</strong></p>
        <p>GD: $x_{k+1} = x_k - t 4 x_k^3$. For convergence, need small $t$. Rate is sublinear (slow).
        Newton: $\Delta x = -f'/f'' = -4x^3 / 12x^2 = -x/3$.
        $x_{k+1} = x_k - x_k/3 = (2/3)x_k$.
        Linear convergence with rate $2/3$.
        Note: $x^4$ is strictly convex but not strongly convex at 0 ($f''(0)=0$). Newton is not quadratic here!</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.7: Nesterov Acceleration</h3>
        <p><strong>Problem:</strong> Write the update equations for Nesterov's accelerated gradient.</p>
        <p><strong>Solution:</strong></p>
        <p>$x_k$: current position, $y_k$: momentum variable.
        1. $x_{k+1} = y_k - t \nabla f(y_k)$
        2. $y_{k+1} = x_{k+1} + \frac{k-1}{k+2} (x_{k+1} - x_k)$
        Step 1 is standard GD step from the lookahead position $y_k$.
        Step 2 adds momentum.
        Converges as $1/k^2$ for smooth convex functions.</p>
      </div>

      <div class="example-problem">
        <h3>Example 13.8: Newton Decrement Property</h3>
        <p><strong>Problem:</strong> Show $\lambda(x)^2 = \nabla f(x)^\top \Delta x_{nt}$.</p>
        <p><strong>Solution:</strong></p>
        <p>$\Delta x_{nt} = -H^{-1} g$.
        $\lambda(x)^2 = g^\top H^{-1} g$.
        Inner product: $g^\top \Delta x_{nt} = g^\top (-H^{-1} g) = -g^\top H^{-1} g = -\lambda(x)^2$.
        So $\lambda^2 = -g^\top \Delta x_{nt}$ (directional derivative in Newton direction is $-\lambda^2$).</p>
      </div>
    </section>
  </main>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p style="margin: 0;">
        © <span id="year"></a> Convex Optimization Course ·
        <a href="../../README.md" style="color: var(--brand);">About</a>
      </p>
    </div>
  </footer>

  <!-- Load Pyodide for Python widgets (optional) -->
  <script defer src="https://cdn.jsdelivr.net/pyodide/v0.26.4/full/pyodide.js"></script>

  <!-- Widget loaders -->
  <script type="module">
    import { initGradientDescentVisualizer } from './widgets/js/gradient-descent-visualizer.js';
    initGradientDescentVisualizer('widget-1');
  </script>
  <script type="module">
    import { initGDvsNewton } from './widgets/js/gd-vs-newton.js';
    initGDvsNewton('widget-2');
  </script>
  <script type="module">
    import { initConvergenceRate } from './widgets/js/convergence-rate.js';
    initConvergenceRate('widget-3');
  </script>
  <script type="module">
    import { initNormSteepest } from './widgets/js/norm-steepest.js';
    initNormSteepest('widget-norm-steepest');
  </script>

  <!-- Global utilities -->
  <script src="../../static/js/math-renderer.js"></script>
<script src="../../static/js/ui.js"></script>
<script src="../../static/js/toc.js"></script>
<script src="../../static/js/ui.js"></script>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <script src="../../static/js/glossary-loader.js"></script>
<script src="../../static/js/notes-widget.js"></script>
<script src="../../static/js/pomodoro.js"></script>
<script src="../../static/js/progress-tracker.js"></script>
</body>
</html>
